<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Cloud Computing Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Cloud Computing Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
							<email>yzhiheng@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Yin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Cloud Computing Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
							<email>jianqiang.hjq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Cloud Computing Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<email>chenzhibo@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>xiansheng.hxs@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Cloud Computing Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cloth-Changing person re-identification (CC-ReID) aims at matching the same person across different locations over a long-duration, e.g., over days, and therefore inevitably has cases of changing clothing. In this paper, we focus on handling well the CC-ReID problem under a more challenging setting, i.e., just from a single image, which enables an efficient and latency-free person identity matching for surveillance. Specifically, we introduce Gait recognition as an auxiliary task to drive the Image ReID model to learn cloth-agnostic representations by leveraging personal unique and cloth-independent gait information, we name this framework as GI-ReID. GI-ReID adopts a twostream architecture that consists of an image ReID-Stream and an auxiliary gait recognition stream (Gait-Stream). The Gait-Stream, that is discarded in the inference for high efficiency, acts as a regulator to encourage the ReID-Stream to capture cloth-invariant biometric motion features during the training. To get temporal continuous motion cues from a single image, we design a Gait Sequence Prediction (GSP) module for Gait-Stream to enrich gait information. Finally, a semantics consistency constraint over two streams is enforced for effective knowledge regularization. Extensive experiments on multiple image-based Cloth-Changing ReID benchmarks, e.g., LTCC, PRCC, Real28, and VC-Clothes, demonstrate that GI-ReID performs favorably against the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) aims at identifying a specific person across cameras, times, and locations. Abundant approaches have been proposed to address the challenging geometric misalignment among person images caused * This work was done when he was visiting Alibaba as a research intern. ? Corresponding author.  <ref type="figure">Figure 1</ref>. (a) shows a realistic wanted case that a suspect changed her coat from black to white for hiding. (b) reveals that the gait of person could help ReID, especially when the identity matching meets the cloth-changing challenge (All faces in the images are masked for anonymization).</p><p>by diversities of human poses <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b77">78]</ref>, camera viewpoints <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b73">74]</ref>, and style/scales <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. These methods usually inadvertently assume that both query and gallery images of the same person have the same clothing. In general, they perform well on the trained short-term datasets but suffer from significant performance degradations when testing on a long-term collected ReID dataset <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71]</ref>. Because large clothing variations occur over long-duration among these datasets, which seriously hinders the accuracy of ReID. For example, <ref type="figure">Figure 1</ref>(a) shows a realistic wanted case <ref type="bibr" target="#b0">1</ref> where a suspect that captured by surveillance devices at different times/locations changed her coat from black to white, which makes ReID difficult, especially when she wears a mask and the captured images are of low quality.</p><p>In recent years, to handle the cloth-changing ReID (CC-ReID) problem, some studies have contributed some new datasets where clothing changes are commonplace (e.g., Celebrities-reID <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, PRCC <ref type="bibr" target="#b68">[69]</ref>, LTCC <ref type="bibr" target="#b53">[54]</ref>, Real28 and VC-Clothes <ref type="bibr" target="#b62">[63]</ref>). They also propose some new algorithms that could learn cloth-agnostic representations for CC-ReID. For instance, Yang et al. <ref type="bibr" target="#b68">[69]</ref> propose a contoursketch-based network to overcome the moderate clothchanging problem. Similarly, Qian et al. <ref type="bibr" target="#b53">[54]</ref>, Li et al. <ref type="bibr" target="#b37">[38]</ref>, and Hong et al. <ref type="bibr" target="#b17">[18]</ref> all use body shape to tackle the CC-ReID problem. However, no matter of using a contour sketch or body shape, all these methods are prone to suffer from the estimation error problem. Because the single-view contour/shape inference (from 2D image) is extremely difficult due to the vast range of possible situations, especially when people wear thick clothes in winter. Besides, these contour-sketch-based or shape-based methods only focus on extracting static spatial cues from persons as extra clothagnostic representations, the rich dynamic motion information (e.g., gait, implied motion <ref type="bibr" target="#b32">[33]</ref>) are often ignored.</p><p>In this paper, we explore to leverage the unique gait features that imply dynamic motion cues of a pedestrian to drive a model to learn cloth-agnostic and discriminative ReID representations. As shown in <ref type="figure">Figure 1</ref>(b), although it is hard to identify the same person when he/she wears different clothes, or to distinguish the different persons when they wear similar/same clothes, we can still leverage their unique/discriminative gaits to achieve correct identity matching. It is because that gait, as a unique biometric feature, has the superior invariance compared with other easychanging appearance characteristics, e.g., face, body shape, contour <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b74">75]</ref>. Besides, gait can be authenticated at a long distance even with low-quality camera imaging.</p><p>Unfortunately, existing gait-related studies mainly rely on large video sequences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. Capturing videos requires time latency and saving videos needs a large hardware storage cost, which are both undesirable for the real-time ReID applications. Even the recent work <ref type="bibr" target="#b66">[67]</ref> first attempts to achieve gait recognition from a single image, how to leverage gait feature to handle CC-ReID problem from a single image is still under-studied and this task is more challenging due to the potential viewpoint-variations and occlusions.</p><p>In this paper, we propose a Gait-assisted Image-based ReID framework, termed as GI-ReID, which could learn cloth-agnostic ReID representations from a single image with the gait feature assistance. GI-ReID consists of a main image-based ReID-Stream and an auxiliary gait recognition stream (Gait-Stream). <ref type="figure">Figure 2</ref> shows the entire framework. The Gait-Stream aims to regularize the ReID-Stream to learn cloth-agnostic features from a single RGB image for effective CC-ReID. It is discarded in the inference for the high efficiency. Since the comprehensive gait features extraction typically needs a gait video sequence as input <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, we introduce a new Gait Sequence Prediction (GSP) module for Gait-Stream to approximately forecast continuous gait frames from a single input query image, which enriches the learned gait information. Finally, to encourage the main ReID-Stream's efficient learning from Gait-Stream, we further enforce a high-level Semantics Consistency (SC) constraint for the same person over two streams' features. We summarize our main contributions as follows: ? We specially aimed at handling the challenging clothchanging issue for image ReID to promote practical applications. A Gait-assisted Image-based cloth-changing ReID (GI-ReID) framework is proposed. As a regulator, the Gait-Stream in GI-ReID can be removed in the inference without sacrificing ReID performance. This reduces the dependency on the accuracy of gait recognition, making our method computationally efficient and robust. ? A well-designed Gait Sequence Prediction (GSP) module makes our method effective in the challenging imagebased ReID scenarios. And, a high-level semantics consistency (SC) constraint enables an effective regularization over two streams, enhancing the distinguishing power of ReID-Stream under the cloth-changing setting.</p><p>With the gait prediction and regularization, GI-ReID achieves a state-of-the-art performance on the image-based cloth-changing ReID. It is also general enough to be compatible with the existing ReID-specific networks, except ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, we also use OSNet <ref type="bibr" target="#b80">[81]</ref>, LTCC-shape <ref type="bibr" target="#b53">[54]</ref>, and PRCC-contour <ref type="bibr" target="#b68">[69]</ref> as our baselines for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Person Re-identification</head><p>General ReID. Without cloth-changing cases, the general ReID has achieved a great success with the deep learning. It includes exploring fine-grained pedestrian feature descriptions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b80">81]</ref>, and addressing spatial misalignment caused by (a) different camera viewpoints <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b58">59]</ref>, (b) different poses <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58]</ref>, (c) semantics inconsistency <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b76">77]</ref>, (d) occlusion/partial-observation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b81">82]</ref>, etc. These methods rely substantially on static spatial texture information. However, when person ReID meets changing clothes, the texture information is not so reliable since it changes significantly even for the same person. Compared to static texture, the gait information, as a discriminative biometric modality, is more consistent and reliable. Cloth-Changing ReID. Considering the wider application range and greater practical value of Cloth-Changing ReID (CC-ReID), more and more studies pay their attention to solve this challenging problem. Huang et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref> propose to use vector-neuron capsules <ref type="bibr" target="#b54">[55]</ref>   <ref type="bibr" target="#b62">[63]</ref> propose to leverage contour sketch, body shape, face/hairstyle to assist ReID under the cloth-changing setting, respectively. Nevertheless, these methods usually suffer from estimation error due to the difficulty of obtaining external cues (e.g., body shape, face, etc). Besides, they also ignore the explo- Recon. Layer <ref type="figure">Figure 2</ref>. Overview of the proposed GI-ReID, which consists of ReID-Stream and Gait-Stream, they are jointly trained with a highlevel Semantics Consistency (SC) constraint. The Gait-Stream plays the role of a regulator to drive ReID-Stream to learn cloth-agnostic representations from a single image, and it is discarded in the inference for computational efficiency. Gait Sequence Prediction (GSP) module aims at predicting gait frames from an image. GaitSet <ref type="bibr" target="#b2">[3]</ref> is responsible for extracting discriminative gait features. Clothes variation ration of discriminative dynamic motion cues, like gait. FITD <ref type="bibr" target="#b74">[75]</ref> solves the cloth-changing ReID problem based on true motion cues of videos. Our work differs from FITD for at least three perspectives: 1). FITD uses motion information derived from dense trajectories (optical flow), which requires continuous video sequences. Our GI-ReID handles cloth-changing ReID from a single image with gait prediction and regularization, which is more challenging and practical. 2). FITD directly uses human motion cues to complete ReID, which relies on the accurate motion prediction and may suffer from estimation errors. Our GI-ReID just takes the gait recognition task as a regulator to drive the main ReID model to learn cloth-independent features, which makes our method less sensitive to gait estimation errors. 3). FITD only characterizes temporal motion patterns for ReID, ignoring other distinguishable local spatial cues, like personal belongings (e.g., backpacks). Our GI-ReID not only explores dynamic gait cues, but also learns from raw RGB images, leading more comprehensive features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gait Recognition and Prediction</head><p>Gait recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b66">67]</ref> directly uses gait sequence for identity matching, which is also cloth-independent, but different from our work and cannot be directly applied into image-based cloth-changing ReID. We clarify the differences between two tasks in detail in Table 1: this paper focuses on image-based cloth-changing ReID where large viewpoint variations, occlusion, and complex environments make gait recognition failed. And, these gait sequence based methods are not optimal for the imagebased CC-ReID. Thus, we just take the gait recognition as an auxiliary regularization to drive ReID model to learn cloth-agnostic representations, which makes our method robust to the recognition errors. Moreover, the gait representations can be grouped into model-based <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> and appearance-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. The first one relies on human pose, while the latter relies on silhouettes. We use silhouette as gait representation for simplicity and robustness. Gait Prediction from a single frame, or said, the field of video frame prediction (i.e., motion prediction) has been widely studied and achieved a great success <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b66">67]</ref>, which verifies the feasibility of our work. This task is very challenging, that's why we carefully design the gait sequence prediction module while indirectly using the prediction results in a robust regularization manner to help clothchanging ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed GI-ReID Framework</head><p>GI-ReID framework aims to fully exploit the unique human gait to handle the cloth-changing challenge of ReID just depending on a single image. <ref type="figure">Figure 2</ref> shows the flowchart of the entire framework. Given a single person image, its silhouette (i.e., mask) will be first extracted as in-put to the Gait-Stream using semantic segmentation methods, such as PointRend <ref type="bibr" target="#b29">[30]</ref>. With the proposed gait sequence prediction (GSP) module, we could predict a gait sequence with more comprehensive gait information, which is then fed into the subsequent recognition network (Gait-Set <ref type="bibr" target="#b2">[3]</ref>) to extract discriminative gait features. Through a high-level semantics consistency (SC) constraint, the clothindependent Gait-Stream acts as a regulator to encourage the main ReID-Stream to capture cloth-agnostic features from a single RGB image. We discuss the details of each component in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Auxiliary Gait-Stream</head><p>Gait-Stream is composed of two parts: Gait Sequence Prediction (GSP) module and the pre-trained gait recognition network (GaitSet <ref type="bibr" target="#b2">[3]</ref>). GSP is designed for gait information augmentation. Then, GaitSet extracts clothindependent and discriminative motion feature cues from augmented gait to guide/regularize ReID-Stream's training.</p><p>Gait Sequence Prediction (GSP) Module: GSP module aims to predict a gait sequence that contains continuous gait frames. This module is related to the general video frame prediction task (i.e., frame interpolation and extrapolation studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref>), and gait sequence prediction can be deemed as a "gait frame synthesis" process.</p><p>As shown in <ref type="figure">Figure 2</ref>, GSP is based on an auto-encoder architecture <ref type="bibr" target="#b6">[7]</ref> with feature encoder E and decoder D. In order to reduce the prediction ambiguity and difficulty (e.g., given a dangling arm, it is hard to guess whether it will rise or fall in the next frame), we manually integrate an extra prior information of middle frame index into the inner learned feature through a position embedder P and a feature aggregator A. Intuitively, the middle frame index means that the input gait silhouette corresponds to the middle result of the predicted gait sequence. Such prior knowledge aims to drive GSP module to predict the adjacent walking statuses before and after the current input walking status so as to reduce prediction ambiguity.</p><p>(1). Encoder. Given a silhouette input S, the encoder E aims to extract a dimension-shrinked compact feature:</p><formula xml:id="formula_0">f S = E(S).<label>(1)</label></formula><p>Specific/detailed network structures (including other components in Gait-Stream) can be found in <ref type="figure">Supplementary.</ref> (2). Position Embedder and Feature Aggregator. Considering the prediction ambiguity <ref type="bibr" target="#b44">[45]</ref>, we introduce a middle frame input principle, which assumes that the input silhouette always corresponds to the middle one of the predicted gait sequence. During the GSP training, we take the gait frame in the middle position of the ground truth gait sequence as input to GSP, and use a one-dimensional vector p ? R 1 , to denote such position label. Given a ground truth gait sequence with N frames, the position label p mid ? R 1 of the input middle gait is defined as p mid = N//2 which indicates the relative position relationship of input frame to the entire sequence. For convenience, we convert position label to one-hot vector to calculate loss. In formula, the position embedder P works as:</p><formula xml:id="formula_1">p = P (S) ? R 1 , L position = || p ? p mid || 2 2 ,<label>(2)</label></formula><p>where we compare the embedded position output p with the ground truth p mid to construct a position loss L position . P is to build a mapping between input and middle position. Feature aggregator A, implemented by a fully connected layer, is inserted between the encoder and the decoder to convert the raw encoded features f S into middle-positionaware features f p S by taking the embedded middle position information p into account for the following decoder, which explicitly tells the decoder that we need to predict the gait statuses before and after the current input middle gait status, and thus reduces prediction ambiguity for the predicted results. This feature aggregation process is formulated as:</p><formula xml:id="formula_2">f p S = A([ f S , p ]),<label>(3)</label></formula><p>where [?] means a simple concatenation.</p><p>(3). Decoder. We feed the aggregated feature f p S into the decoder D, which has a symmetrical structure to that of the encoder E, to predict the gait sequence with a pre-defined fixed number of frames N . Such process is formulated as,</p><formula xml:id="formula_3">R = D(f p S ) ? R N * h * w , L pred. = || R ? GT || 2 2 ,<label>(4)</label></formula><p>where (h, w) denotes the (height, width) of predicted gait frames, same as the input silhouette image. A prediction loss L pred. is calculated to ensure the predicted gait sequence results is consistent with the ground truth (GT). Gait Feature Extraction: The predicted gait sequence R is fed into the pre-trained GaitSet <ref type="bibr" target="#b2">[3]</ref> to learn discriminative and cloth-independent gait feature g. GaitSet is a set-based gait recognition model that takes a set of silhouettes as an input and aggregates features over frames into a set-level feature, which is formulated as g = GaitSet( R). More details are presented in Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Main ReID-Stream</head><p>The backbone of the ReID-Stream could be any of the off-the-shelf networks, such as commonly-used ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, ReID-specific PCB <ref type="bibr" target="#b60">[61]</ref>, MGN <ref type="bibr" target="#b63">[64]</ref>, and OS-Net <ref type="bibr" target="#b80">[81]</ref>. And, we use the widely-adopted classification loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61]</ref>, and triplet loss with batch hard mining <ref type="bibr" target="#b15">[16]</ref>) on the ReID feature vector r as basic optimization objectives for training. The feature r is finally used for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Learning of Two Streams</head><p>Due to the potential rough silhouette extraction and the gait sequence prediction errors of GSP module, it is very difficult to directly exploit the gait information alone to complete effective ReID. Experimentally, we have attempted to conduct CC-ReID with only the predicted gait sequence R as input, and found this scheme failed to deliver good results (see ablation study for more details). Therefore, to exploit the cloth-independent merits of the gait information while avoiding the above-mentioned issues, we propose to jointly train Gait-Stream and ReID-Stream through a high-level semantics consistency (SC) constraint, where gait characteristics is taken as a regulator to drive the cloth-agnostic feature learning of ReID-Stream. Note that the SC constraint is also not needed in the inference. Semantics Consistency (SC) Constraint. SC constraint is essentially related to the common feature learning works, such as knowledge distillation <ref type="bibr" target="#b16">[17]</ref>, mutual learning <ref type="bibr" target="#b75">[76]</ref>, and knowledge amalgamation <ref type="bibr" target="#b69">[70]</ref>. Our SC constraint differs from them mainly in two perspectives: 1). SC is to encourage a high-level common feature learning from two modalities (dynamic gait and static RGB image). 2). SC ensures information integrity for each stream/modality.</p><p>The details of the SC constraint are shown in <ref type="figure">Figure 2</ref>. The learned gait feature g of Gait-Stream and ReID feature r of ReID-Stream are first transformed to a common and interactable space, via an embedding layer:r = Emb.(r) and? = Emb.(g), wherer and? have the same feature dimensions. Then, we enforce the transformed featuresr and? to be closed to each other by minimizing the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b11">[12]</ref>. MMD is a distance metric to measure the domain mismatch for probability distributions. We use it to measure the high-level semantics discrepancy between the transformed featuresr and?, and minimize it to drive ReID-Stream to pay more attention to cloth-independent gait biometric. An empirical approximation to the MMD distance ofr and? is simplified as follows:</p><formula xml:id="formula_4">L M M D = ??(?) ? ?(r)? 2 2 + ??(?) ? ?(r)? 2 2 , (5) where ?(?)</formula><p>, ?(?) denotes the mean, variance calculation functions for the transformed featuresr and?.</p><p>To avoid the information lost caused by feature regularization with SC constraint, we further enforce a reconstruction penalty to ensure that the transformed features? andr could be recovered to original versions. Specifically, we reconstruct the original output features through a Recon. layer (implemented by FC layer): r = Recon.(r) and g = Recon.(?), and calculate the corresponding reconstruction loss as follows:</p><formula xml:id="formula_5">L recon. = ? g ? g? 2 2 + ? r ? r? 2 2 .<label>(6)</label></formula><p>Training Pipeline. The whole training process of the proposed GI-ReID consists of three stages: 1). Pre-training GaitSet <ref type="bibr" target="#b2">[3]</ref> for gait feature extraction. 2). Joint Training for the proposed gait sequence prediction (GSP) module and  <ref type="bibr" target="#b68">[69]</ref>, and one general video ReID dataset MARS <ref type="bibr" target="#b78">[79]</ref> (to highlight the difficulty and necessity of image-based CC-ReID) to perform experiments. <ref type="table" target="#tab_9">Table 9</ref> gives a brief information and comparison of these ReID datasets. More detailed introductions can be found in Supplementary. Evaluation Metrics. We use the cumulative matching characteristics (CMC) at Rank-1/-10/-20, and mean average precision (mAP) to evaluate the performance. Experimental Setups. We build three kinds of different experiment settings to comprehensively validate the effectiveness of gait biometric for person ReID, and also validate the rationality/superiority of the proposed gait prediction and regularization in our GI-ReID framework: (1) Real Cloth-Changing Image ReID, (2) General Video ReID, and (3) Imitated Cloth-Changing Video ReID. In the main manuscripts, to save space and highlight core contributions of our paper, we only present the results related to the most challenging setting of (1) Real Cloth-Changing Image ReID. The rest results about (2)(3) are in Supplementary. For (1) Real Cloth-Changing Image ReID, we employ real image-based cloth-changing datasets Real28 <ref type="bibr" target="#b62">[63]</ref>, VC-Clothes <ref type="bibr" target="#b62">[63]</ref>, LTCC <ref type="bibr" target="#b53">[54]</ref>, and PRCC <ref type="bibr" target="#b68">[69]</ref> for experiments to validate the effectiveness of GSP module, SC constraint, and also compare our GI-ReID with SOTA cloth-changing ReID methods. In this setting, GSP module and GaitSet are both first pre-trained on gait-specific datasets CASIA-B <ref type="bibr" target="#b2">[3]</ref> and then fine-tuned on the CC-ReID datasets with the SC constraint L M M D &amp;L recon. and the ReID supervisions. ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, OSNet <ref type="bibr" target="#b80">[81]</ref>, LTCC-shape <ref type="bibr" target="#b53">[54]</ref>, and PRCCcontour <ref type="bibr" target="#b68">[69]</ref> are taken as ReID backbone for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Baseline means the model that only ingests RGB images. Results of Real Cloth-Changing Image ReID. We conduct ablation experiments on three cloth-changing datasets Real28, VC-Clothes, and LTCC. Real28 is too small for training, so we train model on VC-Clothes and only test on Real28 <ref type="bibr" target="#b62">[63]</ref>. In <ref type="table" target="#tab_4">Table 3</ref>, we see that 1) All Gait-Stream (GS) related schemes achieve obvious gains (over 2.7% in mAP) over Baseline, which demonstrates the effectiveness of using gait to handle cloth-changing issue. 2) With the well-designed GSP module, Baseline+ GS-GSP (concat) outperforms the ablated scheme Baseline+GS (concat) by 3.3%/6.7%/2.3% in mAP on Real28/VC-Clothes/LTCC, which demonstrates the effectiveness of gait sequence prediction (GSP) on gait information augmentation. Note that Baseline+GS (concat) just uses Gait-Stream (GS) but removes GSP, where we duplicate the only available single person silhouette as input to GaitSet. 3) Semantics consistency (SC) performs well in the cloth-changing settings, it helps our scheme GI-ReID achieve the best performance on the most evaluation cases while saving computational cost by discarding Gait-Stream in the inference. Improvement Comes From Gait Prediction, Not Silhouettes Usage. We believe that our GI-ReID could successfully address the cloth-changing ReID problem from a single image is indeed because it effectively leverages the gait prediction, instead of the introduction of human silhouettes (i.e., masks). To prove that, we additionally design a scheme of Silhouette-ReID that directly takes the person RGB-Silhouette pair as input to ReID model (following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57]</ref>), and compare it with our GI-ReID on the cloth-changing ReID dataset LTCC. ResNet-50 is taken as ReID backbone for all schemes for comparison fairness. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we found that Silhouette-ReID is even inferior to the baseline scheme Baseline (ResNet-50) by 1.06% in mAP under the cloth-changing setting. We analyze that directly using silhouette to remove the background clutters in pixel-level will make ReID model pay more attention on the foreground objects' appearance/clothes color information, which is unexpected and unreliable for clothchanging ReID, and thus leads to a performance drop. Study on Directly Using Gait Recognition Methods for Cloth-Changing ReID. As we have discussed in the related work, directly using the algorithms of gait recognition for solving cloth-changing ReID problem is not optimal, especially in the image-based CC-ReID scenarios. Experimentally, we compare the proposed GI-ReID with two popular pure gait recognition works, GaitSet <ref type="bibr" target="#b2">[3]</ref> and PA-GCR <ref type="bibr" target="#b66">[67]</ref>. GaitSet needs a set/sequence of person silhouettes as input, but recently-released cloth-changing ReID datasets are image datasets that lack of continuous frames for the same person. Thus, we duplicate the only available single one person silhouette to a set as input to approximately apply GaitSet into image-based CC-ReID task. As shown in <ref type="table" target="#tab_6">Table 5</ref>, these pure gait recognition works of GaitSet <ref type="bibr" target="#b2">[3]</ref> and PA-GCR <ref type="bibr" target="#b66">[67]</ref> are both inferior to the baseline scheme Baseline (ResNet-50) in mAP under the cloth-changing setting, which indicates that simply using gait biometric for person matching can not work well for cloth-changing ReID, our gait prediction and regularization idea performs better for handling CC-ReID, especially for the image-based CC-ReID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Design Choices in Our GI-ReID Framework</head><p>We study the different design choices in our GI-ReID framework. We train and test model on the real large-scale cloth-changing ReID dataset LTCC <ref type="bibr" target="#b53">[54]</ref>. Influence of the Length N of Predicted Gait Sequence. As shown in Eq-(4) of Sec. 3.1, the output of GSP R ? R N * h * w is a sequence with N predicted gait frames. We study the influence of length N w.r.t the ReID performance. <ref type="table">Table 6a</ref> shows that when N = 8, our GI-ReID gets the best performance, achieving a good trade-off between gait prediction error and gait information augmentation. Is 'Middle Frame Input Principle' Necessary? As described in Eq-(2) of GSP in Sec. 3.1, we employ a position embedder P and a feature aggregator A to set up a middle frame input principle to reduce the gait prediction am- <ref type="table">Table 6</ref>. Study on the different design choices in the (a)(b) GSP module, and (c) SC constraint of our GI-ReID framework. 'Cloth-Changing' setting means that the images with same identity, camera view and clothes are discarded during the testing.</p><p>(a) Study on the gait prediction length N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LTCC Standard</head><p>Cloth-Changing biguity and difficulty. Here we compare several schemes to show the necessity of such design. Arb.: we remove position embedder P , feature aggregator A, position loss L position for GSP, and take the gait silhouette at arbitrary position as input for training. BEGN and END: we respectively take the gait stance at the beginning and the end position as input to predict gait sequence during the GSP training. In <ref type="table">Table 6b</ref>, the scheme Mid. (ours) that uses the gait frame at middle position for gait sequence prediction achieves the best performance, outperforming Arb. by 2.3% in mAP in the standard setting, which reveals that predicting the gait statuses before and after the input middle gait status indeed could reduce prediction difficulty/ambiguity. Why Use MMD for Regularization? For the SC constraint, we shrink the gap between the embeded ReID vector r and gait vector? by minimizing MMD through L M M D .</p><p>We study this design in <ref type="table">Table 6c</ref> and find that when replacing L M M D with L M SE , the performance of w/ L M SE drops nearly 2.0% in mAP. That's because MMD loss is a distribution-level constraint and could better enforce the high-level semantics consistency between dynamic motion gait features and static spatial ReID features. MSE loss is an element-wise constraint, and not so suitable to coordinate two modalities of motion gait and RGB feature. Is Reconstruction Penalty Necessary? When removing L recon. in Eq-(6), as shown in <ref type="table">Table 6c</ref>, the shceme w/o L recon. is inferior to ours by 1.1%/0.8% in mAP in the two settings, which demonstrates that avoiding information lost caused by feature regularization could enhance the final ReID performance of our GI-ReID framework. Which One for ReID Inference? We compare several cases of using (1) predicted gait sequence R, (2) aligned features fusionr +?, (3) reconstructed features fusion r + g, and (4) reconstructed ReID vector r for ReID inference. <ref type="table" target="#tab_7">Table 7</ref> shows that 1) Directly using the predicted gait sequence R for CC-ReID failed to get satisfactory results, this also indicates that these gait recognition works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> are not optimal for CC-ReID. 2) Using the well-aligned features fusionr +? achieves the best performance, outperforming ours by 0.4%/0.5% in mAP in the two settings, but this scheme still needs Gait-Stream in the inference. 3) Using the reconstructed ReID vector r for inference suffers from information lost and is inferior to ours by 1.3% in mAP in the both two settings. 4) Our scheme that using the regularized ReID vector r achieves the second best performance while saving the computation costs brought by Gait-Stream. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">More Analysis, Visualization and Insights</head><p>To further prove that the proposed gait sequence prediction (GSP) module can actually predict unique human motion features, and the achieved improvements of GI-ReID indeed come from gait information, not from using additional gait-related datasets or person silhouette images, here we provide more analysis and visualization results. For example, when testing the gait-based recognition performance using GaitSet <ref type="bibr" target="#b2">[3]</ref> on the predicted human gait sequences R generated by GSP module (see Supplementary for details), it can achieve a competitive 62.4% in Rank-1 on CASIA-B. Gait Sequence Prediction Visualization. <ref type="figure" target="#fig_2">Figure 3</ref> further shows 6 groups of gait prediction results (left) and 2 groups of realistic gait samples from CASIA-B dataset <ref type="bibr" target="#b2">[3]</ref> (right). Compared to the realistic gait samples, the predicted gait results (i.e., the outputs of GSP) have the reasonable continuous movements, e.g., swing arms and opening/closing legs. The gait-stream could learn the discriminative dynamic clues from these predicted gait results, like the walking stride, the left-right swinging range of arms, the opening/closing angle of legs, etc, (see red circles in <ref type="figure" target="#fig_2">Figure 3</ref>). Feature Map Visualization. To better understand how our GI-ReID works, we visualize the intermediate activation feature maps of Baseline and our GI-ReID for comparison <ref type="table">Table 8</ref>. Performance (%) comparisons of our GI-ReID and other competitors on the cloth-changing datasets LTCC <ref type="bibr" target="#b53">[54]</ref> and PRCC <ref type="bibr" target="#b68">[69]</ref>. ' ?' means that only identities with clothes changing are used for training. More results are presented in <ref type="figure">Supplementary.</ref> (a) Comparison results on LTCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Standard Cloth-changing Standard ? Cloth-changing ? Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP LOMO <ref type="bibr" target="#b39">[40]</ref>   following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b80">81]</ref>. On the left of <ref type="figure" target="#fig_3">Figure 4</ref>, we show three examples of activation maps on Real28, and we observe that the feature maps of Baseline have high response mainly on person's clothes. In contrast, the activation features of our GI-ReID not only have high response on person's clothes, but also cover the holistic human body structure (gait) and local face information (robust to cloth changing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-Arts</head><p>The study on cloth-changing ReID is relatively rare <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71]</ref>, and most of them have not released source codes, even the dataset <ref type="bibr" target="#b70">[71]</ref>. We compare our GI-ReID with multiple general ReID algorithms, including PCB <ref type="bibr" target="#b60">[61]</ref>, HACNN <ref type="bibr" target="#b34">[35]</ref>, MuDeep <ref type="bibr" target="#b52">[53]</ref>, and specific cloth-changing ReID methods LTCC-shape <ref type="bibr" target="#b53">[54]</ref>, PRCCcontour <ref type="bibr" target="#b68">[69]</ref>, RCSANet <ref type="bibr" target="#b20">[21]</ref>. In <ref type="table">Table 8</ref>, we observe that 1) Thanks to the cloth-independent gait characteristics, our scheme GI-ReID (OSNet) achieves the best performance on PRCC, outperforming the second best PRCC-contour <ref type="bibr" target="#b68">[69]</ref> by 3.17% in Rank-1 in the cross-clothes setting. 2) The proposed Gait-Stream, as a kind of regularization, could benefit other methods, e.g., LTCC-shape <ref type="bibr" target="#b53">[54]</ref>. We find that the scheme of LTCC-shape + Gait-Stream could further obtain 1.79%/1.59% gain in mAP on LTCC. 3) For two cloth-changing settings of LTCC, our scheme GI-ReID (ResNet-50) both achieve obvious gains (2.28%/2.13% in mAP) over the Baseline (ResNet-50), which totally-fair results indicate that GI-ReID could handle clothes changes and learn identity-relevant features. 4) Our method is compatible with existing ReID networks, e.g., built upon the strong ReID-specific network OSNet <ref type="bibr" target="#b80">[81]</ref>, GI-ReID (OS-Net) further achieves gains than GI-ReID (ResNet-50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Failure Cases Analysis</head><p>Due to the large difference on the capture viewpoints and environments between gait and ReID training data, the predicted gait results of GSP are not always perfect (see Supplementary) when occlusion, partial, multi-person, etc, existed in the person images, which may affect the CC-ReID performance. That is why we indirectly use gait predictions in a knowledge regularization manner, which makes GI-ReID robust and not sensitive to these failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose to utilize human unique gait to address the cloth-changing ReID problem from a single image. A novel gait-involved two-stream framework GI-ReID is introduced, which takes gait as a regulator with a Gait-Stream (discarded in the inference), to encourage the cloth-agnostic representation learning of image-based ReID-Stream. To facilitate the gait utilization, a gait sequence prediction (GSP) module and a high-level semantics consistency (SC) constraint are further designed. Extensive experiments on multiple CC-ReID benchmarks demonstrate the effectiveness and superiority of GI-ReID. 64 <ref type="figure">Figure 5</ref>. We apply "resize+zero padding" in the person masks (right) when fine-tuning GSP module on the ReID-specific datasets, because the raw gait training data (left) typically have the height-width ratio of (1:1), which is important/necessary for training GSP to get satisfactory gait prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Detailed Network Structures of GI-ReID</head><p>GI-ReID, as a image-based cloth-changing ReID framework, with gait information as assistance, consists of an auxiliary Gait-Stream and a mainstream ReID-Stream. ReID-Stream can be arbitrary commonly-used network architectures, such as ResNet <ref type="bibr" target="#b13">[14]</ref>, and also can be some ReID-specific network architectures, such as PCB <ref type="bibr" target="#b60">[61]</ref>, OSNet <ref type="bibr" target="#b80">[81]</ref>. Thus, in this section, we mainly introduce/describe the detailed network architecture of Gait-Stream, which contains two key parts, GSP module for gait information prediction/augmentation and GaitSet <ref type="bibr" target="#b2">[3]</ref> for gait features extraction.</p><p>Architecture of Gait Sequence Prediction (GSP) Module: The proposed GSP module consists of a feature encoder E, a decoder D, a position embedder P , and a feature aggregator A. <ref type="bibr" target="#b0">(1)</ref>. Encoder E. The encoder E is a CNN with four Conv. layers (filter size = 4 ? 4 and stride = 2). The number of filters is increased from 64 ? 512. Each Conv. layer is followed by a batch-normalization (BN) layer <ref type="bibr" target="#b22">[23]</ref> and a rectified linear unit (ReLU) activation function <ref type="bibr" target="#b47">[48]</ref>. In the end, a 100-dimensional feature is obtained through a fully connected (FC) layer.</p><p>Note that, when pre-training GSP module on the gaitspecific datasets following <ref type="bibr" target="#b2">[3]</ref>, the input gait silhouette of encoder E has a size of 1 ? 64 ? 64 (height-width ratio is 1:1). We use CASIA-B <ref type="bibr" target="#b2">[3]</ref> as training dataset. On the ReIDspecific datasets, since the input person images usually have a height-width ratio of 2:1 (e.g., height-256, width-128), we need leverage an operation of "resize+zero padding" to handle such training data gap, which is pivotal for GSP's accurate gait sequence prediction. For better understanding, we vividly visualize such process in <ref type="figure">Figure 5</ref>.</p><p>(2). Position Embedder P and Feature Aggregator A. To reduce the gait prediction ambiguity and difficulty of GSP, a position embedder P and a feature aggregator A are introduced to integrate the prior information of input middle frame index into the prediction process of GSP. The position embedder P has a similar structure to that of the encoder E, but with one more FC layer to regress the 1D position label p. The feature aggregator A is inserted between the encoder and the decoder to convert the raw encoded features f S into middle-position-aware features f p S by taking the embedded middle position information p into account. With respect to the architecture of A, it is implemented only by a FC layer, which aims to regress to the aggregated 100-dimension feature f p S ? R 100 from the 101-dimension concatenated vector of the raw encoded feature f S ? R 100 and the embedded middle position prior vector p ? R 1 .</p><p>(3). Decoder D. The structure of the decoder D is symmetrical to that of the encoder E. A FC layer along with reshaping is first employed to convert the input 100D feature into the same size as the last feature output of the encoder E, and then four DeConv. layers are used for up-sampling. A sigmoid activation function is applied in the end, and outputs the gait predictions with a size of N ? 64 ? 64, where each channel indicates a predicted gait frame of final results.</p><p>Architecture of GaitSet: GaitSet [3] is a classic setbased gait recognition network, which takes a set of silhouettes/gait frames as input. After obtaining features from each input silhouette independently using a CNN, set pooling is applied to merge features over frames into a set-level feature. This set-level feature is then used for discrimination learning via horizontal pyramid mapping (HPM), which aims to extract features of different spatial locations on different scales. We recommend seeing more details from their original paper <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Training Details of our GI-ReID</head><p>Phase-1: Pre-training for GaitSet. The input is a set of aligned silhouettes in size of 64 ? 44. The silhouettes are directly provided by the datasets and are aligned based on methods in <ref type="bibr" target="#b61">[62]</ref>. The set cardinality in the training is set to be 30. Adam is chosen as an optimizer. The number of scales S in HPM is set as 5. The margin in separate triplet loss L sep tri <ref type="bibr" target="#b2">[3]</ref> is set as 0.2. The mini-batch is composed of P = 16 and N = 8 (P, N respectively mean the number of person identities and input gait frames). We set the number of channels in C1 and C2 as 32, in C3 and C4 as 64 and in C5 and C6 as 128 (following <ref type="bibr" target="#b2">[3]</ref>). The learning rate is set to be 1?10 ?4 , and the model is trained for 80 epochs. Phase-2: Joint Training for GSP module and GaitSet. After pre-training GaitSet, we jointly train the proposed gait sequence prediction (GSP) module and GaitSet for Gait-Stream. Specifically, during the joint-training, we also reuse CASIA-B dataset for effective gait prediction training. Following <ref type="bibr" target="#b15">[16]</ref>, a batch is formed by first randomly sampling P identities. For each identity, we sample N continuous gait frames as the ground-truth gait sequence. Then the batch size is B = P ? N . We set P = 4 and N = 8 (i.e., batch size B = P ? N = 32. As presented in the main manuscript, we use the middle one of the ground-truth gait sequence (i.e., the fourth one when N = 8) as input for GSP training. We first optimize GSP with the proposed position loss L position and prediction loss L pred (loss balance is set as 1:1) for 80 epochs, which enables GSP to output reasonable predicted gait sequence results. We train GSP with Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with a initial learning rate of 5?10 ?4 . We optimize the Adam optimizer with a weight decay of 1?10 ?4 . The learning rate is decayed by a factor of 0.1 at 40 epoch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24:</head><p>(?, ?, ?, ?) = (?, ?, ?, ?)??? (?,?,?,?) L total // Jointly update GSP module GSP ? , GaitSet (GS) GS ? , SC constraints related FC embedding layers SC ? , and ReID-Stream backbone ReID?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25: end for</head><p>After warming up the GSP module for 80 epochs, we jointly train GSP and GaitSet for extra 160 epochs with initial learning rate as 5?10 ?4 . We also use Adam optimizer <ref type="bibr" target="#b28">[29]</ref> for optimization with a weight decay of 1?10 ?4 , the learning rate is decayed by a factor of 0.5 at 40, 80, and 120 epochs. When jointly training GSP and GaitSet, excluding the GSP-related position loss L position and predic-tion loss L pred , we further use separate triplet loss L sep tri for training, which is introduced in details in GaitSet <ref type="bibr" target="#b2">[3]</ref>, and we also set the loss weight as 1.0 for this supervision. Phase-3: Joint Training for Gait-Stream and ReID-Stream. When we jointly training Gait-Stream and ReID-Stream on the ReID datasets, Gait-Stream is also finetuned/learnable. Since the full gait sequence ground truth are not available for ReID-specific datasets, we adjust the original prediction loss L pred in GSP by only calculating L1 distance between the single input person mask and the middle frame result of the entire predicted gait sequence.</p><p>On the large-scale cloth-changing datasets VC-Clothes <ref type="bibr" target="#b62">[63]</ref>, LTCC <ref type="bibr" target="#b53">[54]</ref>, and PRCC <ref type="bibr" target="#b68">[69]</ref>, we set training batch size as B = 80 = P ? N = 10 ? 8. Both of Gait-Stream (including GSP and GaitSet) and ReID-Stream use Adam optimizer <ref type="bibr" target="#b28">[29]</ref> for optimization, where the initial learning rate for Gait-Stream is 1?10 ?5 , for ReID-Stream is 5?10 ?4 . We optimize two Adam optimizers for Gait-Stream and ReID-Stream with a weight decay of 1?10 ?5 for total 240 epochs. The learning rate is decayed by a factor of 0.1 at 80 and 160 epochs for ReID-Stream, while no learning rate decay for Gait-Stream. For the losses usage, we adopt the widely-adopted classification loss L cla <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61]</ref>, and triplet loss with batch hard mining L HM tri <ref type="bibr" target="#b15">[16]</ref>) as basic optimization objectives for ReID-Stream training, and we set these two loss weights as 1.0. Besides, for the Gait-Stream related losses, including L position , L pred , L sep tri , we set all their loss weights as 0.1. For the semantics consistency (SC) constraints related FC embedding layers, we merge their learnable parameters into ReID-Stream's optimization, and set the balance weights for MMD loss L M M D and reconstruction penalty L recon. as 0.5. The pseudo code of the entire training process of our GI-ReID is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Details of Datasets</head><p>We use one widely-used video ReID dataset MARS <ref type="bibr" target="#b78">[79]</ref>, and four image-based cloth-changing ReID datasets Real28 <ref type="bibr" target="#b62">[63]</ref>, VC-Clothes <ref type="bibr" target="#b62">[63]</ref>, LTCC <ref type="bibr" target="#b53">[54]</ref>, PRCC <ref type="bibr" target="#b68">[69]</ref> to perform experiments. In <ref type="table" target="#tab_9">Table 9</ref>, we present the detailed information about these ReID datasets. MARS is a popular dataset for video-based person ReID. There are 20,715 track-lets come from 1,261 pedestrians who are captured by at least 2 cameras. We use the train/test split protocol defined in <ref type="bibr" target="#b78">[79]</ref>.</p><p>Real28, VC-Clothes, LTCC and PRCC are all newly released image datasets for cloth-changing ReID <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69]</ref>.</p><p>Real28 is a small real-scenario dataset, which is collected in 3 different days (with different clothing) by 4 cameras. It consists of totally 4,324 images from 28 different identities with 2 indoor scenes and 2 outdoors. Similar to <ref type="bibr" target="#b62">[63]</ref>, since the size of Real28 is not big enough for training deep learning models, we just use it for evaluation. There are totally 336 images in the query and 3,988 images in the gallery.</p><p>VC-Clothes is a synthetic dataset where images are rendered by the Grand Theft Auto V (GTA5). It has 512 identities, 4 scenes (cameras) and on average 9 images/scenes for each identity and a total number of 19,060 images. Following <ref type="bibr" target="#b62">[63]</ref>, we equally split the dataset by identities: 256 identities for training and the other 256 for testing. We randomly chose 4 images per person from each camera as query, and have the other images serve as gallery images. Eventually, we get totally 9,449 images in the training, 1,020 images as queries and 8,591 others in the gallery.</p><p>LTCC is a large-scale real-scenario cloth-changing dataset, which contains 17,138 person images of 152 identities. On average, there are 5 different clothes for each cloth-changing person, with the numbers of outfit changes ranging from 2 to 14. Following <ref type="bibr" target="#b53">[54]</ref>, we split the LTCC dataset into training and testing sets. The training set consists of 77 identities, where 46 people have cloth changes and the rest of 31 people wear the same outfits during the recording. Similarly, the testing set contains 45 people with changing clothes and 30 people wearing the same outfits.</p><p>PRCC is also a large-scale real-scenario clothchanging dataset, recently published by Yang et al. <ref type="bibr" target="#b68">[69]</ref>. It consists of 221 identities with three camera views Camera A, Camera B, and Camera C. Each person in Cameras A and B is wearing the same clothes, but the images are captured in different rooms. For Camera C, the person wears different clothes, and the images are captured in a different day. The images in the PRCC dataset include not only clothing changes for the same person across different camera views but also other variations, e.g. changes in illumination, occlusion, pose and viewpoint. In summary, nearly 50 images exists for each person in each camera view. Therefore, approximately 152 images of each person are included in the dataset, for 33,698 images in total.</p><p>Following <ref type="bibr" target="#b68">[69]</ref>, we split the PRCC dataset into a training set and a testing set. The training set consists of 150 people, and the testing set consists of 71 people, with no overlap between them in terms of identities. The testing set was further divided into a gallery set and a probe set. For each identity in the testing set, we chose one image in Camera view A to form the gallery set for a single-shot matching. All im-ages in Camera views B and Camera C were used for the probe set. Specifically, the person matching between Camera views A and B was performed without clothing changes, whereas the matching between Camera views A and C was cross-clothes matching. The results were assessed in terms of the cumulated matching characteristics, specifically, the Rank-1, Rank-10, and Rank-20 matching accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results of Different Settings</head><p>Experimental Setups. As we described in the main manuscript, we build three kinds of different experiment settings to comprehensively validate the effectiveness of gait biometric for person ReID, and also validate the rationality/superiority of the proposed gait prediction and regularization in our GI-ReID framework: (1) Real Cloth-Changing Image ReID, (2) General Video ReID, (3) Imitated Cloth-Changing Video ReID. In the main manuscripts, we have presented all the results related to the most challenging setting of (1) real cloth-changing image ReID. The rest results about (2)(3) are shown here. Baseline means the model that only ingests RGB images.</p><p>2) General Video ReID. In this setting, we use a general video ReID dataset MARS for experiments. This dataset has no cloth-changing cases. This group of experiments aims to verify two things: 1) gait could benefit ReID even without clothes variations. 2) extracting gait feature from video is easier than that from image, or said, exploiting gait feature in the image-based CC-ReID is more challenging. Since MARS itself contains continuous video frames/clips and human gait masks 2 , we don't need GSP to additionally predict gait sequence, so we discard it for simplicity.</p><p>3) Imitated Cloth-Changing Video ReID. We still use MARS as dataset to perform experiments in this setting. But the difference is that we imitate cloth-changing cases for the images with the same identity through a data augmentation strategy-body-wise color jitter (i.e., randomly change the brightness, contrast and saturation of the human body region in an person image) for training. This group of experiments aims to show that gait information could alleviate the ReID interference caused by clothes changing. GSP module is also removed in this setting. Results of General Video ReID. <ref type="table" target="#tab_1">Table 10</ref> shows the results. We observe that: 1) Thanks to the leverage of gait characteristics through the proposed Gait-Stream (GS), Baseline + GS (concat) and Baseline + GS + SC outperform Baseline by 1.07%/1.29% in mAP respectively, which demonstrates that gait information indeed benefits ReID.</p><p>2) We find that Baseline + GS + SC further outperforms Baseline + GS (concat) by 0.22% in mAP. This result validates the superiority of our gait utilization manner (i.e., regularization), which makes ReID-Stream not only robust to <ref type="table" target="#tab_1">Table 10</ref>. Performance (%) comparison on the general video ReID dataset MARS <ref type="bibr" target="#b78">[79]</ref>. GS refers to Gait-Stream and SC refers to semantics consistency constraints. Note that 'concat' means concatenating ReID vector r and gait vector g together for ReID. The backbone is ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARS mAP</head><p>Rank-1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with State-of-the-Arts (Complete version)</head><p>To save space, we only present the latest approaches in the main manuscripts, and here we show comparisons with more approaches and more evaluation settings on LTCC <ref type="table" target="#tab_1">(Table 12</ref>) and PRCC datasets <ref type="table" target="#tab_1">(Table 13)</ref>.</p><p>From the comparison results on PRCC that are shown in <ref type="table" target="#tab_1">Table 13</ref>, we observe that 1) Although person ReID with no clothing change (i.e."Same Clothes" in the <ref type="table" target="#tab_1">Table 13</ref>) is not the purpose in our work, our method GI-ReID can still achieve an accuracy of 85.97% in Rank-1, which is better than that of all hand-crafted features with metric learning methods and most deep learning methods. 2) When the input images are RGB images without clothing changes, Alexnet <ref type="bibr" target="#b33">[34]</ref>, VGG16 <ref type="bibr" target="#b55">[56]</ref>, HA-CNN <ref type="bibr" target="#b34">[35]</ref>, and PCB <ref type="bibr" target="#b60">[61]</ref> all achieve good performance, but they have a sharp performance drop when a clothing change occurs, illustrating the challenge of person ReID when a person dresses differently. Therefore, the application of existing person ReID methods is not straightforward in this scenario. In contrast, our GI-ReID that leverages gait information is beneficial to learn the clothing invariant feature, which makes our method achieve satisfactory performance 37.55% in Rank-1 even under the cloth-changing scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Study on Failure Cases (Limitations)</head><p>As we described in the main manuscript, since the existed large difference on the capture viewpoint and environment between gait and ReID training data, the predicted results of gait sequence prediction (GSP) module are not so accurate when occlusion, partial, multi-person, etc, existed in the person images. As shown in <ref type="figure">Figure 6</ref>, GSP gives unsatisfactory gait prediction results, where large estimation errors exist in the predicted gait frames, which will hurt the ReID performance. That is why we indirectly use gait prediction results in a two-stream knowledge regularization manner, which makes our GI-ReID robust/less sensitive to these failure cases. <ref type="figure">Figure 6</ref>. Failure cases of gait sequence prediction (GSP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Social Impact</head><p>Positive. In this paper, we propose to utilize human unique gait to address the cloth-changing ReID (CC-ReID) problem from a single image. A novel gait-involved two-stream framework GI-ReID is introduced for image-based CC-ReID. To our best knowledge, this paper is the first attempt to take gait as a regulator with a Gait-Stream (discarded in the inference), to encourage the cloth-agnostic representation learning of image-based ReID-Stream. This is very important for both of academic community and industry, and <ref type="table" target="#tab_1">Table 12</ref>. Performance (%) comparisons of our GI-ReID and other competitors on the cloth-changing dataset LTCC <ref type="bibr" target="#b53">[54]</ref>. 'Standard' and 'Cloth-changing' respectively mean the standard setting and cloth-changing setting as mentioned in our main manuscript. '(Image)' or '(Parsing)' represents that the input data is the person image or the body parsing image. ' ?' means the setting that only identities with clothes changing are used for training. it is also valuable and meaningful to bridge the gap between the fast-developing ReID algorithms and practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Negative. Due to the urgent demand of public safety and increasing number of surveillance cameras, person ReID is imperative in intelligent surveillance systems with significant research impact and practical importance, but this task also might raise questions about the risk of leaking private information. On the other hand, the data collected from the surveillance equipments or downloaded from the internet may violate the privacy of human beings. Therefore, we appeal and encourage research that understands and mitigates the risks arising from surveillance applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Same person with different clothesGait tells us that they are the same personDifferent persons with similar clothesGait tells us that they are different persons (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Six predicted gait sequences vs. realistic gait samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Left: three examples of activation maps comparison between baseline and our GI-ReID, which shows GI-ReID not only focuses on people's clothes, but also pay attention to the holistic human gait and local face; Right: Top-3 ranking list of GI-ReID for two query images on Real28. GI-ReID could identify the same person with different clothes based on the assistance of gait.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This work was (partially) supported by the National Key R&amp;D Program of China under Grant 2020AAA0103902, Alibaba Innovative Research (AIR) program, and NSFC under Grant U1908209, 61632001, 62021001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 2 :</head><label>12</label><figDesc>Training Process of GI-ReID 1: Input: gait dataset G (e.g., CASIA-B<ref type="bibr" target="#b2">[3]</ref>), ReID dataset R (e.g., LTCC<ref type="bibr" target="#b53">[54]</ref>). Learning rate is simply denoted as ?. The entire GI-ReID framework consists of GSP module GSP ? , GaitSet (GS) GS ? , SC constraints related FC layers SC ? , and ReID-Stream backbone ReID?. Output: inference ReID vector r.3: ### Phase-1: Pre-training for GaitSet 4: for epoch = 1 to 80 do 5: Sample P ? N = 16 ? 8 samples from gait training set G. 6: L total = L sep tri // Use the separate triplet loss as objective function [3]. 7: ? = ? ? ?? ? L sep tri // Update GaitSet (GS) GS ? . 8: end for 9: ### Phase-2: Joint Training for GSP module and GaitSet 10: for epoch = 1 to 80 do 11: Sample P ? N = 4 ? 8 samples from gait training set G. 12: L total = L position + L pred // Use the proposed position loss and prediction loss as objective functions. 13: ? = ? ? ?? ? L total // Warm up GSP module GSP ? . 14: end for 15: for epoch = 1 to 160 do 16: Sample P ? N = 4 ? 8 samples from gait training set G. 17: L total = L position + L pred + L sep tri // Use the position loss, prediction loss , and separate triplet loss as objective functions. 18: (?, ?) = (?, ?) ? ?? (?,?) L total // Jointly update GSP module GSP ? and GaitSet (GS) GS ? . 19: end for 20: ### Phase-3: Joint Training for Gait-Stream and ReID-Stream 21: for epoch = 1 to 240 do 22: Sample P ? N = 10 ? 8 samples from ReID training set R. 23: L total = 0.1 * L position + 0.1 * L pred + 0.1 * L sep tri + L cla + L HM tri + 0.5 * L M M D + 0.5 * Lrecon. // Total objective functions consists of the position loss, prediction loss, separate triplet loss (for Gait-Stream), and the classification loss, triplet loss (with hard-mining, HM) (for ReID-Stream), and the MMD loss, reconstruction loss (SC constraints).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to perceive cloth changes of the same person. Yang et al. [69], Qian et al. [54]/ Li et al. [38], Yu et al. [71]/Wan et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Differences between Gait Recognition and CC-ReID.</figDesc><table><row><cell>Task</cell><cell>Gait Recognition</cell><cell>Cloth-Changing Person ReID</cell></row><row><cell>Data format</cell><cell>Gait Energy Image (GEI) / Sequence set of silhouette / Video sequences</cell><cell>Discontinuous RGB images across cameras</cell></row><row><cell>Datasets</cell><cell>USF, CASIA-B, OU-ISIR, OU-MVLP, etc.</cell><cell>COCAS, PRCC, LTCC, Real28, VC-Clothes, etc.</cell></row><row><cell>Unsolved problems</cell><cell>1) Viewing angles (e.g., frontal view); 2) Occlusion, body incompleteness; 3) Cluttered/complex background;</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>GaitSet in Gait-Stream on gait-related datasets. 3). JointTraining for Gait-Stream and ReID-Stream on CC-ReIDrelated datasets. More details are provided in Supplementary, including pseudo code and loss balance strategy.</figDesc><table><row><cell>4. Experiment</cell></row></table><note>4.1. Datasets, Metric and Experimental Setups Datasets Details. We use four recent cloth-changing ReID datasets Real28 [63], VC-Clothes [63], LTCC [54], PRCC</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Brief introduction and comparison of datasets.</figDesc><table><row><cell></cell><cell cols="4">MARS Real28 VC-Clothes LTCC</cell><cell>PRCC</cell></row><row><cell>Category</cell><cell>Video</cell><cell>Image</cell><cell>Image</cell><cell>Image</cell><cell>Image</cell></row><row><cell>Photo Style</cell><cell>Real</cell><cell cols="2">Real Synthetic</cell><cell>Real</cell><cell>Real</cell></row><row><cell>Scale</cell><cell>Large</cell><cell>Small</cell><cell>Large</cell><cell>Large</cell><cell>Large</cell></row><row><cell>Cloth Change</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Identities</cell><cell>1,261</cell><cell>28</cell><cell>512</cell><cell>152</cell><cell>221</cell></row><row><cell>Samples</cell><cell>20,715</cell><cell>4,324</cell><cell>19,060</cell><cell>17,138</cell><cell>33,698</cell></row><row><cell>Cameras</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>N/A</cell><cell>3</cell></row><row><cell>Usage</cell><cell cols="5">Train&amp;Test Test Train&amp;Test Train&amp;Test Train&amp;Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance (%) comparison on the real image-based cloth-changing datasets Real28, VC-Clothes, LTCC. GS-GSP means Gait-Stream (GS) with gait sequence prediction (GSP) module. The ReID backbone is ResNet-50. 'Standard' is the setting where the images in the test set with the same identity and camera view are discarded when computing mAP/Rank-1<ref type="bibr" target="#b53">[54]</ref>.GSP  (concat) 10.1 10.8 59.0 63.7 28.8 64.5 + GS-GSP + SC (ours) 10.4 11.1 57.8 64.5 29.4 63.2</figDesc><table><row><cell></cell><cell cols="2">Real28</cell><cell cols="2">VC-Clothes LTCC (Standard)</cell></row><row><cell>Methods</cell><cell cols="4">mAP Rank-1 mAP Rank-1 mAP Rank-1</cell></row><row><cell>Baseline</cell><cell>4.1</cell><cell cols="2">6.7 49.1 53.7 23.2</cell><cell>55.1</cell></row><row><cell>+ GS (concat)</cell><cell>6.8</cell><cell cols="2">7.9 52.3 58.9 26.5</cell><cell>60.0</cell></row><row><cell>+ GS-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance (%) comparison on the cloth-changing dataset LTCC. Such experiment aims to show that our GI-ReID can bring gains because of the exploration of gait information, rather than simply introducing silhouettes (i.e., human masks). The ReID backbone is ResNet-50.</figDesc><table><row><cell>Methods</cell><cell cols="2">LTCC (Cloth-Changing) mAP Rank-1</cell></row><row><cell>Baseline</cell><cell>8.10</cell><cell>19.58</cell></row><row><cell>Silhouette-ReID</cell><cell>7.04</cell><cell>17.92</cell></row><row><cell>GI-ReID (ours)</cell><cell>10.38</cell><cell>23.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="2">LTCC (Cloth-Changing) mAP Rank-1</cell></row><row><cell>Baseline</cell><cell>8.10</cell><cell>19.58</cell></row><row><cell>GaitSet [3]</cell><cell>2.14</cell><cell>7.22</cell></row><row><cell>PA-GCR [67]</cell><cell>3.36</cell><cell>9.01</cell></row><row><cell>GI-ReID (ours)</cell><cell>10.38</cell><cell>23.72</cell></row></table><note>Performance (%) comparison on the cloth-changing dataset LTCC. Such experiment aims to show that these pure gait recognition works can not work well for cloth-changing ReID. The ReID backbone is ResNet-50.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Study on the input gait position p in GSP. Study on the used losses in SC constraint. Study on the different ReID inference strategies.</figDesc><table><row><cell></cell><cell cols="3">mAP Rank-1 mAP Rank-1</cell><cell cols="2">(b) Methods</cell><cell>LTCC Cloth-Changing mAP Rank-1 mAP Rank-1 Standard</cell><cell>(c) Methods</cell><cell>LTCC Cloth-Changing mAP Rank-1 mAP Rank-1 Standard</cell></row><row><cell cols="3">Baseline 23.2 55.1 N=4 26.9 59.2 N=6 28.2 61.9 N=8 (ours) 29.4 63.2 10.4 8.1 8.9 9.8 N=10 28.4 63.1 10.4 N=12 27.7 60.8 10.0</cell><cell>19.6 21.7 22.6 23.7 22.8 22.5</cell><cell></cell><cell cols="2">Baseline 23.2 55.1 Arb. 27.1 59.5 BEGN 28.4 61.2 END 28.1 61.5 Mid. (ours) 29.4 63.2 10.4 8.1 9.2 9.8 9.5</cell><cell>19.6 20.5 22.0 22.4 23.7</cell><cell>Baseline w/ L M SE 27.5 61.0 23.2 55.1 w/o Lrecon. 28.3 62.7 ours 29.4 63.2 10.4 8.1 9.0 9.6</cell><cell>19.6 21.4 22.9 23.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LTCC</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell>Standard</cell><cell></cell><cell cols="2">Cloth-Changing</cell></row><row><cell></cell><cell>mAP</cell><cell cols="2">Rank-1</cell><cell>mAP</cell><cell cols="2">Rank-1</cell></row><row><cell>Baseline</cell><cell>23.2</cell><cell>55.1</cell><cell></cell><cell>8.1</cell><cell>19.6</cell></row><row><cell>R</cell><cell>8.6</cell><cell>21.1</cell><cell></cell><cell>4.3</cell><cell cols="2">9.9</cell></row><row><cell>r +?</cell><cell>29.8</cell><cell>64.0</cell><cell></cell><cell>10.9</cell><cell>24.4</cell></row><row><cell>r + g</cell><cell>28.9</cell><cell>63.2</cell><cell></cell><cell>9.7</cell><cell>23.1</cell></row><row><cell>r</cell><cell>28.1</cell><cell>60.8</cell><cell></cell><cell>9.1</cell><cell>21.3</cell></row><row><cell>r (ours)</cell><cell>29.4</cell><cell>63.2</cell><cell></cell><cell>10.4</cell><cell>23.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Brief introduction/comparison of datasets.</figDesc><table><row><cell></cell><cell cols="4">MARS Real28 VC-Clothes LTCC</cell><cell>PRCC</cell></row><row><cell>Category</cell><cell>Video</cell><cell>Image</cell><cell>Image</cell><cell>Image</cell><cell>Image</cell></row><row><cell>Photo Style</cell><cell>Real</cell><cell cols="2">Real Synthetic</cell><cell>Real</cell><cell>Real</cell></row><row><cell>Scale</cell><cell>Large</cell><cell>Small</cell><cell>Large</cell><cell>Large</cell><cell>Large</cell></row><row><cell>Cloth Change</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Identities</cell><cell>1,261</cell><cell>28</cell><cell>512</cell><cell>152</cell><cell>221</cell></row><row><cell>Samples</cell><cell>20,715</cell><cell>4,324</cell><cell>19,060</cell><cell>17,138</cell><cell>33,698</cell></row><row><cell>Cameras</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>N/A</cell><cell>3</cell></row><row><cell>Usage</cell><cell cols="5">Train&amp;Test Test Train&amp;Test Train&amp;Test Train&amp;Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Baseline suffers from large degradation, 68.52% on CC-MARS vs. 79.12% on raw MARS in mAP. 2) With the assistance of gait, Baseline+GS (concat) and Baseline+GS+SC improve Baseline near 5.0% in mAP. 3) On CC-MARS, the gait 'concat' scheme shows a little superiority than ours. We analyse that's because the 'concat' could help ReID more explicitly, especially when meeting changing clothes. But, the 'concat' scheme needs maintain Gait-Stream in the inference, leading extra computational cost. 4) As video ReID datasets, it is relatively easy to extract gait features on MARS/CC-MARS.</figDesc><table><row><cell>Baseline</cell><cell>79.12</cell><cell>87.34</cell></row><row><cell>Baseline + GS (concat)</cell><cell>80.19</cell><cell>88.16</cell></row><row><cell>Baseline + GS + SC (ours)</cell><cell>80.41</cell><cell>88.32</cell></row><row><cell cols="3">Results of Imitated Cloth-Changing Video ReID. To</cell></row><row><cell cols="3">prove that gait indeed could alleviate clothes variation is-</cell></row><row><cell cols="3">sue, we imitate cloth-changing cases for MARS (denoted as</cell></row><row><cell cols="3">CC-MARS). In Table 11, we observe that 1) Disturbed by</cell></row><row><cell>the synthetic clothing change,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Performance (%) comparison on the imitated (using color jitter) cloth-changing video ReID dataset, termed as CC-MARS. The ReID backbone is ResNet-50.</figDesc><table><row><cell></cell><cell></cell><cell>CC-MARS</cell></row><row><cell>Methods</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>Baseline</cell><cell>68.52</cell><cell>72.31</cell></row><row><cell>Baseline + GS (concat)</cell><cell>73.46</cell><cell>79.34</cell></row><row><cell>Baseline + GS + SC (ours)</cell><cell>73.13</cell><cell>79.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Performance (%) comparisons of our GI-ReID and other competitors on the cloth-changing dataset PRCC<ref type="bibr" target="#b68">[69]</ref>. "RGB" means the inputs of the model are RGB images; "Sketch" means the inputs of the model are contour sketch images</figDesc><table><row><cell></cell><cell cols="2">Standard</cell><cell cols="2">Cloth-changing</cell><cell cols="2">Standard  ?</cell><cell cols="2">Cloth-changing  ?</cell></row><row><cell></cell><cell cols="4">Rank-1 mAP Rank-1 mAP</cell><cell cols="4">Rank-1 mAP Rank-1 mAP</cell></row><row><cell>LOMO [40] + KISSME [31]</cell><cell>26.57</cell><cell>9.11</cell><cell>10.75</cell><cell>5.25</cell><cell>19.47</cell><cell>7.37</cell><cell>8.32</cell><cell>4.37</cell></row><row><cell>LOMO [40] + XQDA [40]</cell><cell>25.35</cell><cell>9.54</cell><cell>10.95</cell><cell>5.56</cell><cell>22.52</cell><cell>8.21</cell><cell>10.55</cell><cell>4.95</cell></row><row><cell>LOMO [40] + NullSpace [73]</cell><cell>34.83</cell><cell>11.92</cell><cell>16.45</cell><cell>6.29</cell><cell>27.59</cell><cell>9.43</cell><cell>13.37</cell><cell>5.34</cell></row><row><cell>ResNet-50 (Image) [14]</cell><cell>58.82</cell><cell>25.98</cell><cell>20.08</cell><cell>9.02</cell><cell>57.20</cell><cell>22.82</cell><cell>20.68</cell><cell>8.38</cell></row><row><cell>ResNet-50 (Parsing) [14]</cell><cell>19.87</cell><cell>6.64</cell><cell>7.51</cell><cell>3.75</cell><cell>18.86</cell><cell>6.16</cell><cell>6.28</cell><cell>3.46</cell></row><row><cell>PCB (Parsing) [61]</cell><cell>27.38</cell><cell>9.16</cell><cell>9.33</cell><cell>4.50</cell><cell>25.96</cell><cell>7.77</cell><cell>10.54</cell><cell>4.04</cell></row><row><cell>ResNet-50 + Face [68]</cell><cell>60.44</cell><cell>25.42</cell><cell>22.10</cell><cell>9.44</cell><cell>55.37</cell><cell>22.23</cell><cell>20.68</cell><cell>8.99</cell></row><row><cell>PCB [61]</cell><cell>65.11</cell><cell>30.60</cell><cell>23.52</cell><cell>10.03</cell><cell>59.22</cell><cell>26.61</cell><cell>21.93</cell><cell>8.81</cell></row><row><cell>HACNN [35]</cell><cell>60.24</cell><cell>26.71</cell><cell>21.59</cell><cell>9.25</cell><cell>57.12</cell><cell>23.48</cell><cell>20.81</cell><cell>8.27</cell></row><row><cell>MuDeep [53]</cell><cell>61.86</cell><cell>27.52</cell><cell>23.53</cell><cell>10.23</cell><cell>56.99</cell><cell>24.10</cell><cell>18.66</cell><cell>8.76</cell></row><row><cell>Face [68]</cell><cell>60.44</cell><cell>25.42</cell><cell>22.10</cell><cell>9.44</cell><cell>55.37</cell><cell>22.23</cell><cell>20.68</cell><cell>8.99</cell></row><row><cell>Baseline (ResNet-50)</cell><cell>55.14</cell><cell>23.21</cell><cell>19.58</cell><cell>8.10</cell><cell>54.27</cell><cell>21.98</cell><cell>19.14</cell><cell>7.74</cell></row><row><cell>GI-ReID (ResNet-50, ours)</cell><cell>63.21</cell><cell>29.44</cell><cell>23.72</cell><cell>10.38</cell><cell>61.39</cell><cell>27.88</cell><cell>22.59</cell><cell>9.87</cell></row><row><cell>Baseline (OSNet)</cell><cell>66.07</cell><cell>31.18</cell><cell>23.43</cell><cell>10.56</cell><cell>61.22</cell><cell>27.41</cell><cell>22.97</cell><cell>9.74</cell></row><row><cell>GI-ReID (OSNet, ours)</cell><cell>73.59</cell><cell>36.07</cell><cell>28.11</cell><cell>13.17</cell><cell>66.94</cell><cell>33.04</cell><cell>26.71</cell><cell>12.69</cell></row><row><cell>Baseline (LTCC-shape [54])</cell><cell>-</cell><cell>-</cell><cell>26.15</cell><cell>12.40</cell><cell>-</cell><cell>-</cell><cell>25.15</cell><cell>11.67</cell></row><row><cell>LTCC-shape + Gait-Stream (ours)</cell><cell>-</cell><cell>-</cell><cell>28.86</cell><cell>14.19</cell><cell>-</cell><cell>-</cell><cell>26.41</cell><cell>13.26</cell></row><row><cell>Methods</cell><cell cols="4">Cameras A and C (Cross-Clothes) Rank-1 Rank-10 Rank-20</cell><cell cols="4">Cameras A and B (Same Clothes) Rank-1 Rank-10 Rank-20</cell></row><row><cell>LBP [51] + KISSME [32]</cell><cell>18.71</cell><cell cols="2">58.09</cell><cell>71.40</cell><cell>39.03</cell><cell></cell><cell>76.18</cell><cell>86.91</cell></row><row><cell>HOG [6] + KISSME [32]</cell><cell>17.52</cell><cell cols="2">49.52</cell><cell>63.55</cell><cell>36.02</cell><cell></cell><cell>68.83</cell><cell>80.49</cell></row><row><cell>LBP [51] + HOG [6] + KISSME [32]</cell><cell>17.66</cell><cell cols="2">54.07</cell><cell>67.85</cell><cell>47.73</cell><cell></cell><cell>81.88</cell><cell>90.54</cell></row><row><cell>LOMO [41] + KISSME [32]</cell><cell>18.55</cell><cell cols="2">49.81</cell><cell>67.27</cell><cell>47.40</cell><cell></cell><cell>81.42</cell><cell>90.38</cell></row><row><cell>LBP [51] + XQDA [41]</cell><cell>18.25</cell><cell cols="2">52.75</cell><cell>61.98</cell><cell>40.66</cell><cell></cell><cell>77.74</cell><cell>87.44</cell></row><row><cell>HOG [6] + XQDA [41]</cell><cell>22.11</cell><cell cols="2">57.33</cell><cell>69.93</cell><cell>42.32</cell><cell></cell><cell>75.63</cell><cell>85.38</cell></row><row><cell>LBP [51] + HOG [6] + XQDA [41]</cell><cell>23.71</cell><cell cols="2">62.04</cell><cell>74.49</cell><cell>54.16</cell><cell></cell><cell>84.11</cell><cell>91.21</cell></row><row><cell>LOMO [41] + XQDA [41]</cell><cell>14.53</cell><cell cols="2">43.63</cell><cell>60.34</cell><cell>29.41</cell><cell></cell><cell>67.24</cell><cell>80.52</cell></row><row><cell>Shape [1]</cell><cell>11.48</cell><cell cols="2">38.66</cell><cell>53.21</cell><cell>23.87</cell><cell></cell><cell>68.41</cell><cell>76.32</cell></row><row><cell>LNSCT [66]</cell><cell>15.33</cell><cell cols="2">53.87</cell><cell>67.12</cell><cell>35.54</cell><cell></cell><cell>69.56</cell><cell>82.37</cell></row><row><cell>Alexnet [34] (RGB)</cell><cell>16.33</cell><cell cols="2">48.01</cell><cell>65.87</cell><cell>63.28</cell><cell></cell><cell>91.70</cell><cell>94.73</cell></row><row><cell>VGG16 [56] (RGB)</cell><cell>18.21</cell><cell cols="2">46.13</cell><cell>60.76</cell><cell>71.39</cell><cell></cell><cell>95.89</cell><cell>98.68</cell></row><row><cell>HA-CNN [35] (RGB)</cell><cell>21.81</cell><cell cols="2">59.47</cell><cell>67.45</cell><cell>82.45</cell><cell></cell><cell>98.12</cell><cell>99.04</cell></row><row><cell>PCB [61] (RGB)</cell><cell>22.86</cell><cell cols="2">61.24</cell><cell>78.27</cell><cell>86.88</cell><cell></cell><cell>98.79</cell><cell>99.62</cell></row><row><cell>Alexnet [34] (Sketch)</cell><cell>14.94</cell><cell cols="2">57.68</cell><cell>75.40</cell><cell>38.00</cell><cell></cell><cell>82.15</cell><cell>91.91</cell></row><row><cell>VGG16 [56] (Sketch)</cell><cell>18.79</cell><cell cols="2">66.01</cell><cell>81.27</cell><cell>54.00</cell><cell></cell><cell>91.33</cell><cell>96.73</cell></row><row><cell>HA-CNN [35] (Sketch)</cell><cell>20.45</cell><cell cols="2">63.87</cell><cell>79.58</cell><cell>58.63</cell><cell></cell><cell>90.45</cell><cell>95.78</cell></row><row><cell>PCB [61] (Sketch)</cell><cell>22.48</cell><cell cols="2">61.07</cell><cell>77.05</cell><cell>57.36</cell><cell></cell><cell>92.12</cell><cell>96.72</cell></row><row><cell>SketchNet [72] (Sketch+RGB)</cell><cell>17.89</cell><cell cols="2">43.70</cell><cell>58.62</cell><cell>64.56</cell><cell></cell><cell>95.09</cell><cell>97.84</cell></row><row><cell>Face [65]</cell><cell>2.97</cell><cell>9.85</cell><cell></cell><cell>13.52</cell><cell>4.75</cell><cell></cell><cell>13.40</cell><cell>45.54</cell></row><row><cell>Deformable Conv. [5]</cell><cell>25.98</cell><cell cols="2">71.67</cell><cell>85.31</cell><cell>61.87</cell><cell></cell><cell>92.13</cell><cell>97.65</cell></row><row><cell>STN [24]</cell><cell>27.47</cell><cell cols="2">69.53</cell><cell>83.22</cell><cell>59.21</cell><cell></cell><cell>91.43</cell><cell>96.11</cell></row><row><cell>RCSANet [21]</cell><cell>31.60</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>PRCC-contour [69]</cell><cell>34.38</cell><cell cols="2">77.30</cell><cell>88.05</cell><cell>64.20</cell><cell></cell><cell>92.62</cell><cell>96.65</cell></row><row><cell>+ Gait-Stream (ours)</cell><cell>36.19</cell><cell cols="2">79.93</cell><cell>91,67</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline (ResNet-50)</cell><cell>22.23</cell><cell cols="2">61.08</cell><cell>76.44</cell><cell>75.81</cell><cell></cell><cell>97.34</cell><cell>98.95</cell></row><row><cell>GI-ReID (ResNet-50)</cell><cell>33.26</cell><cell cols="2">75.09</cell><cell>87.44</cell><cell>78.95</cell><cell></cell><cell>97.89</cell><cell>99.11</cell></row><row><cell>Baseline (OSNet)</cell><cell>28.70</cell><cell cols="2">72.34</cell><cell>85.89</cell><cell>83.68</cell><cell></cell><cell>98.24</cell><cell>99.26</cell></row><row><cell>GI-ReID (OSNet)</cell><cell>37.55</cell><cell cols="2">82.25</cell><cell>93.76</cell><cell>85.97</cell><cell></cell><cell>98.82</cell><cell>99.72</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Information comes from https://www.wjr.com/2016/01/06/womanwanted-in-southwest-detroit-bank-robbery/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pan.baidu.com/s/16ZrlM1f 1 T-eZHmQTTkYg.the gait estimation error, but also computationally efficient (Gait-Stream is not needed in the inference).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification from gait using an autocorrelation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandra</forename><surname>Carley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaitset: Regarding gait as a set for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8126" to="8133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person search via a mask-guided two-stream cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gait recognition for person reidentification</title>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<editor>Omar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, and Ahmed Bouridane</editor>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gaitpart: Temporal part-based model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjie</forename><surname>Chao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunshui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vincent Le Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person reidentification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained shape-appearance mutual learning for cloth-changing person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixian</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10513" to="10522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Celebrities-reid: A benchmark for clothes variation in longterm person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clothing status awareness for long-term person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11895" to="11904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Beyond scalar neuron: Adopting vector-neuron capsules for long-term person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Global distance-distributions separation for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantics-aligned representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9799" to="9808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Crowdsourcing user studies with mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongwon</forename><surname>Suh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Activation in human mt/mst by static images with implied motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoe</forename><surname>Kourtzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="55" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gait recognition via semi-supervised disentangled representation learning to identity and covariate features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13309" to="13319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end model-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning shape representations for clothing variations in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07340</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A model-based gait recognition method with body pose and human prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">107069</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep video frame interpolation using cyclic frame generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Tung</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8794" to="8802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification by integrating gait biometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="1144" to="1156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint intensity and spatial metric learning for robust gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuyuki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5705" to="5715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gait-based person recognition using arbitrary view transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Zasim Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="154" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Model-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Nixon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Posenormalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Leader-based multi-scale attention deep architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Long-term cloth-changing person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangrui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dissecting person reidentification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02162</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dissecting person reidentification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="608" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-view large population gait dataset and its performance evaluation for crossview gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomio</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ Transactions on Computer Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">When person re-identification meets changing clothes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="830" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Extraction of illumination invariant facial features from a single image using nonsubsampled contourlet transform. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4177" to="4189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Gait recognition from a single image using a phaseaware gait cycle reconstruction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Clothing change aware person identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Katipally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Zon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2112" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Person reidentification by contour sketch under moderate clothing change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Student becoming the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairi</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2829" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cocas: A large-scale clothes changing person dataset for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3400" to="3409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sketchnet: Sketch classification with web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identificatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Longterm person re-identification using true motion from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="494" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Partial person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
