<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting 3D Object Detection via Object-Focused Image Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<email>haoy@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
							<email>shichen@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
							<email>chenyihong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting 3D Object Detection via Object-Focused Image Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D deep learning</term>
					<term>3D object detection</term>
					<term>Multi-modal fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection has achieved remarkable progress by taking point clouds as the only input. However, point clouds often suffer from incomplete geometric structures and the lack of semantic information, which makes detectors hard to accurately classify detected objects. In this work, we focus on how to effectively utilize object-level information from images to boost the performance of point-based 3D detector. We present DeMF, a simple yet effective method to fuse image information into point features. Given a set of point features and image feature maps, DeMF adaptively aggregates image features by taking the projected 2D location of the 3D point as reference. We evaluate our method on the challenging SUN RGB-D dataset, improving state-of-the-art results by a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at https://github.com/haoy945/DeMF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D object detection aims at localizing and recognizing objects in 3D scenes, which plays a vital role in various real-world applications, such as autonomous driving <ref type="bibr" target="#b36">[37]</ref>, robotics manipulation <ref type="bibr" target="#b35">[36]</ref>, and augmented reality <ref type="bibr" target="#b0">[1]</ref>. Among different approaches towards solving 3D object detection, methods based on point cloud <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b3">4]</ref> have gained much attention and shown state-of-the-art performance, thanks to the depth and geometric structure information provided by LiDAR points.</p><p>While simple and efficient, incomplete geometric structure and the lack of semantic information in LiDAR points make it difficult for these point-based methods to classify objects accurately. This is demonstrated in <ref type="figure">Figure 1</ref>, where the geometric structure of the truncated sofa resembles the structure of a chair. This observation motivates us to introduce information from other modalities (e.g., RGB image) to compensate this shortage of point-based methods.</p><p>Previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b17">18]</ref> have proposed several ways to fuse information from images and point clouds. Some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26]</ref>   <ref type="figure">Fig. 1</ref>: The truncated sofa looks like a chair in point cloud. VoteNet <ref type="bibr" target="#b24">[25]</ref> only employs information from point cloud and misclassifies the sofa as a chair. Our model can accurately recognize it by utilizing information from both point cloud and camera image.</p><p>2D detector to generate initial proposals in the form of frustums. This reduces the size of the search space of possible 3D bounding boxes, but objects missed by the 2D detector could not be recovered in the subsequent steps. Other works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22</ref>] take a more 3D-focused way to fuse image information into point clouds. EPNet <ref type="bibr" target="#b21">[22]</ref> uses bilinear interpolation to extract image feature from the 2D projected location of the 3D point. Then the image feature is concatenated with the original point feature. ImVoteNet <ref type="bibr" target="#b23">[24]</ref> compensates the original point feature by a feature vector containing geometric, semantic and texture cues extracted from the image. Despite getting impressive improvements over their 3Donly baselines, these methods have their limitations. In EPNet <ref type="bibr" target="#b21">[22]</ref>, object-level information is missing as only the feature of the projected 2D location of the 3D point is considered. ImVoteNet <ref type="bibr" target="#b23">[24]</ref> needs to use a trained 2D object detector to extract the category information, which makes the overall framework quite heavy. Moreover, ImVoteNet is hard to train as it needs to balance the training loss of different modalities carefully. The defects of these methods motivate us to rethink what information we should extract from images and how they should be fused into the point features while keeping the framework succinct and efficient. To endow the point feature with the ability to classify its proposed bounding box accurately, we argue that it should be supplemented with the object-level information extracted from image. Moreover, the model should also have the ability to adaptively adjust its focus area when given objects of different shapes.</p><p>In this work, we solve these problems in a flexible way. Inspired by deformable convolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45]</ref> and deformable attention <ref type="bibr" target="#b45">[46]</ref> in the 2D object detection field, we design a novel fusion module named Deformable Attention based Multi-modal Fusion (DeMF) module to learn the sampling locations by taking the projected 2D location of the 3D point as reference. Extensive experiments prove that the adaptively learned sampling strategy can effectively focus on informative locations of objects to extract object-level image features.</p><p>We validate our method on the challenging SUN RGB-D dataset <ref type="bibr" target="#b32">[33]</ref>. Results show that our fusion module offers significant gains over the 3D geometry only VoteNet (+5.6 mAP@0.25, +4.8 mAP@0.5), proving the usefulness of objectlevel information from 2D images. Moreover, based on a state-of-the-art pointbased 3D detector FCAF3D that achieves 64.2 mAP@0.25 and 48.9 mAP@0.5, the proposed approach can still improve the performance by +3.2 mAP@0.25 and +2.3 mAP@0.5 respectively, reaching 67.4 mAP@0.25 and 51.2 mAP@0.5 which is the new state-of-the-art on this benchmark.</p><p>In summary, the contributions of our work are as follows:</p><p>1. We conduct an in-depth analysis on what aspects can image features help 3D object detection. 2. Based on the analysis, we propose a novel fusion module that could adaptively extract object-level image information and fuse it into LiDAR-based 3D detection pipeline. 3. We conduct extensive ablation studies to show the effectiveness of the proposed DeMF module. It also demonstrates state-of-the-art performance in 3D object detection on the SUN RGB-D dataset. We hope our proposed method could serve as an effective baseline for the field of multi-modality 3D object detection and intrigue people to think about what is the right way to extract and fuse information from different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>3D object detection based on point clouds To effectively use sparse and irregular point cloud data, two different approaches are proposed: point-based and voxel-based. Powered by PointNet <ref type="bibr" target="#b26">[27]</ref> and PointNet++ <ref type="bibr" target="#b27">[28]</ref>, Point R-CNN <ref type="bibr" target="#b31">[32]</ref> directly generates 3D box proposals from points, and VoteNet <ref type="bibr" target="#b24">[25]</ref> designs a vote-based method that shifts points closer to object centers and then group points to get candidates. Some follow-up works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38]</ref> further improve the VoteNet in speed, voting strategy and object box localization. Group-Free <ref type="bibr" target="#b20">[21]</ref> removes the voting stage and uses a transformer module instead, obtaining the feature of an object from the backbone outputs with the help of attention mechanism. Voxel-based methods first convert points into regular voxels and employ 3D ConvNets to process them. For example, VoxelNet <ref type="bibr" target="#b43">[44]</ref> uses stacked encoding layers to extract voxel features. However, voxel-based methods suffer from the large memory and computational cost when inputting large scenes. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11]</ref> introduces a sparse convolution operation to improve the computational efficiency, which only executes convolution operations on non-empty voxels. Based on sparse 3D convolutions, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31]</ref> can perform on par with or outperform previous methods which directly use raw point cloud data.</p><p>Notably, current 3D detection methods have achieved great success with only geometric input. To further boost detection performance, leveraging image information is a potential direction. We propose a fusion module to utilize image input, which can be integrated into these geometric-only detectors with a few modifications.</p><p>3D object detection based on multiple sensors Obviously, semantic information contained in images is useful to 3D object detection. Early methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref> use mature 2D detectors to detect objects in 2D image, which are used to constrain the size of the 3D search space. Recent methods take a more 3D-focused way. Some works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> first generate region proposals which are then enhanced by the 2D image feature. ImVoteNet <ref type="bibr" target="#b23">[24]</ref> encodes 2D detection results to guide the voting operations in VoteNet. EPNet <ref type="bibr" target="#b15">[16]</ref> and EPNet++ <ref type="bibr" target="#b21">[22]</ref> fuse image features to point backbone's intermediate layers. Our fusion module shares some similarities with EPNet but differs in two important aspects. First, the fusion operation is applied only at the end of the point backbone, so it is compatible with almost all existing detectors. Second, we use image features from multiple locations to enlarge the receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism in Vision</head><p>The attention-based Transformer <ref type="bibr" target="#b34">[35]</ref> has achieved great success in the field of NLP <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref>. Some works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref> also apply it into 2D object detection. The most related work to our method is Deformable DETR <ref type="bibr" target="#b45">[46]</ref>. It proposes deformable attention to capture long-range relationships and avoid unacceptable complexity. We find deformable attention is suitable for extracting image information and fusing it to points. However, direct application is not feasible and we need to make adjustments by taking the properties of 3D inputs into consideration. Specifically, we use points instead of fixed embeddings as queries, and directly use geometric mapping to get reference points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting deformable attention</head><p>Deformable attention is proposed in <ref type="bibr" target="#b45">[46]</ref> as a variant of multi-head attention <ref type="bibr" target="#b34">[35]</ref> to address the problem of slow convergence exists in the original DETR <ref type="bibr" target="#b1">[2]</ref>. Different from the original multi-head attention, deformable attention only attends to a small set of nearby points around the reference point to reduce the computation cost and improve the convergence speed.</p><p>Given a query element q, a 2D reference point p and an input feature map x ? R C?H?W , the output of the deformable attention module can be formulated as:</p><formula xml:id="formula_0">DeformAttn (q, p, x) = M m=1 W m K k=1 A mk ? W ? m x (p + ?p mk )<label>(1)</label></formula><p>where m and k index the attention head and the sampled key respectively. ?p mk ? R 2 indicates sampling offset and since p + ?p mk is fractional, bilinear interpolation is applied when computing x (p + ?p mk ). A mk ? [0, 1] denotes the attention weight of k-th sampling point in the m-th attention head, and is normalized by</p><formula xml:id="formula_1">K k=1 A mk = 1.</formula><p>Here, both ?p mk and A mk are calculated by linear projection over the query element q.</p><p>To deal with multi-scale feature maps, <ref type="bibr" target="#b45">[46]</ref> also proposes a multi-scale version of the deformable attention module, which shares a similar form to Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">How Can Image Features Help?</head><p>GT Preds (a) VoteNet  LiDAR point clouds often suffer from incomplete geometric structures and the lack of semantic information, usually caused by occlusions, non-reflective surfaces, and long distances from the sensor. As analyzed in <ref type="figure">Figure 1</ref>, VoteNet can not classify objects into the correct category in the above case. This is further verified by <ref type="figure" target="#fig_1">Figure 2a</ref>, where VoteNet struggles when classifying chair, desk and table. However, it is easier to differentiate these classes when given an image. This motivates us to find a method to fuse image features into point features to enhance their recognition ability.</p><p>We argue that object-level information extracted from images could be the key to lifting the recognition performance of current point-based 3D detection methods. By fusing the object-level image feature, the corresponding point feature could sense what object it is supposed to detect. It has been proven to be effective in the field of 2D object detection that introducing object-level features into the model could help improve the detection performance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>We propose an efficient fusion module named Deformable Attention based Multi-modal Fusion (DeMF) module to extract object-level information without the need for any handcrafted features. The 3D point is first projected to the 2D image plane, and then sampling locations are adaptively learned by taking the projected 2D point as reference, as shown in the right part of <ref type="figure" target="#fig_2">Figure 3</ref>. These learned sampling locations could focus on the semantically salient parts of the object as shown in <ref type="figure" target="#fig_5">Figure 5</ref>. Compared to ImVoteNet <ref type="bibr" target="#b23">[24]</ref> which also tries to utilize object-level information, our method does not need the introduction of a trained 2D detector, which makes the whole pipeline succinct. Moreover, compared to the one-hot category information utilized in ImVoteNet, the extracted feature from the sampling points are more fine-grained and informative. As shown in <ref type="figure" target="#fig_1">Figure 2b</ref>, the proposed method could reduce the misclassification results by a lot. For example, the number of cases of classifying table or desk as chair is reduced by nearly 100. Besides, our method can find more objects (e.g., nearly 10% more bookshelf are detected). In the following section, we will elaborate the structure and training details of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Structure</head><p>An overview of the pipeline is given in <ref type="figure" target="#fig_2">Figure 3</ref>. It contains three main components, (1) a two-stream backbone to extract the image and point feature representation, <ref type="bibr" target="#b1">(2)</ref>  Two-stream backbone The two-stream backbone takes point clouds and RGB images as input. Each input will be sent to the corresponding stream to get its feature representation, which the DeMF module will then process.</p><p>Image stream Given an input image x ? R H0?W0?3 , a CNN backbone (e.g., ResNet), following by a deformable transformer encoder is applied to extract multi-scale feature maps x l L l=1 (L = 4). Each encoder layer comprises a multiscale deformable attention module and a feed forward network (FFN). Pixels from the multi-scale feature maps are taken as key, query and value elements. For every query pixel, the reference point is its 2D coordinates normalized by the image width and height. Every feature map is of C = 256 channels.</p><p>Point stream Given a set of N 0 points S ? R N0?3 , a point backbone (e.g., Point-Net++) is applied to extract point features. Due to unacceptable complexities for processing all points, a typical point backbone usually applies a sampling module to sample high-quality object candidates (e.g., points near the object's center). Formally, a set of size N ? N 0 is generated by the point stream. It can be represented as   <ref type="figure">Figure 4</ref>.</p><formula xml:id="formula_2">{(z i , s i )} N i=1 , where z i is a C-channel</formula><p>At first, a self-attention module is applied to transfer information among the set of point features</p><formula xml:id="formula_3">{(z i , s i )} N i=1</formula><p>. After that, the enhanced point features, together with their corresponding 3D coordinates, interact with the multi-scale image feature maps to extract object-level information through a multi-scale deformable attention module. Through this cross-attention module, we can adaptively aggregate the image information and fuse them into point features. Finally, a feed-forward network (FFN) is applied to transform point features. This process could be stacked several times to further enhance the feature representation. Formally it could be written a?</p><formula xml:id="formula_4">z i = z i + SelfAttn(z i , {z j } N j=1 ),<label>(2)</label></formula><formula xml:id="formula_5">z i =? i + MSDeformAttn(? i , RefPoint(s i ), x l L l=1 ),<label>(3)</label></formula><formula xml:id="formula_6">z i =z i + FFN(z i ),<label>(4)</label></formula><p>where i indexes the point feature. The two parameters of SelfAttn represent the query and key elements, respectively. And RefPoint in Equation 3 is a mapping function that projects a 3D coordinate to a 2D coordinate, as described below.</p><p>Reference points The original deformable attention adopts a set of fixed reference points for all images, which is unsuitable for our setting, as fixed reference points may prohibit point features from obtaining corresponding image features. We expect the attention module could automatically attend to the most salient parts by taking the 3D point coordinate as prior. Inspired by this, we propose to use the mapped coordinate of 3D points in 2D images as the reference points. In this way, image features could be extracted from more relevant and informative areas. Formally, the mapping function ? : R 3 ? R 2 can be denoted as:</p><formula xml:id="formula_7">? (x, y, z) = ( ? 1 x + ? 2 y + ? 3 z ? 7 x + ? 8 y + ? 9 z , ? 4 x + ? 5 y + ? 6 z ? 7 x + ? 8 y + ? 9 z ),<label>(5)</label></formula><p>where ? 1?9 are the parameters of the mapping function ? which can be calculated by the parameters of different sensors. For every point feature in the set</p><formula xml:id="formula_8">{(z i , s i )} N i=1</formula><p>, the coordinate of reference pointp i is calculated by</p><formula xml:id="formula_9">RefPoint (s i ) = ? (? (s ix , s iy , s iz )) ,<label>(6)</label></formula><p>where ? is a normalization function that normalize the 2D coordinate into the range [0, 1] 2 based on the size of the image.</p><formula xml:id="formula_10">?(x, y) = ( x W 0 , y H 0 ),<label>(7)</label></formula><p>where W 0 , H 0 are the width and height of the image, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Details</head><p>After point features are enhanced by image features through the DeMF module, the original prediction process and loss function L of the underlying point-based detector can be directly applied.</p><p>During training, we found it is helpful to use auxiliary losses to supervise intermediate features. The same prediction process without parameter sharing and loss function are applied to each layer in the DeMF module. And the final loss is the average of the losses of all layers:</p><formula xml:id="formula_11">L total = 1 L + 1 L l=0 L (l) ,<label>(8)</label></formula><p>where L is the number of DeMF layer. Note that we also calculate the loss on the input of the DeMF module. Moreover, we use iterative object box prediction following <ref type="bibr" target="#b20">[21]</ref>. Box predictions from the previous layer are used to produce the refined spatial encoding via a linear layer. Then the refined spatial encoding is added to the input query of the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments Setup</head><p>Dataset and Evaluation Protocol We use SUN RGB-D benchmark dataset <ref type="bibr" target="#b32">[33]</ref> for evaluation, which is an indoor, single-view RGB-D dataset for 3D scene understanding. The dataset is composed of ?10K RGB-D images with per-point semantic labels and oriented bounding object bounding boxes for 37 object categories. Following <ref type="bibr" target="#b33">[34]</ref>, we train and evaluate the model on the 10 most common categories. In order to get relatively stable results, we run training for 5 times and test each trained model for 5 times independently for each setting. We report both the best and average performance across all 5 ? 5 trials. To get the point cloud data, we use the provided camera parameters to generate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We implement our module using the MMdetection3D <ref type="bibr" target="#b4">[5]</ref> framework. We mainly conduct our studies based on VoteNet <ref type="bibr" target="#b24">[25]</ref>, which is a point-based method. The sampling module in point stream select 256 points, and we directly use voting candidates after voting stage as sampling results in VoteNet. We also conduct experiments on FCAF3D <ref type="bibr" target="#b6">[7]</ref>. For FCAF3D, we reserve the native prediction head and choose points according to their classification scores. ResNet50 <ref type="bibr" target="#b12">[13]</ref> is utilized as the image backbone. More details are given in Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">State-of-the-art Comparison</head><p>In this section, we compare with previous state-of-the-arts on SUN RGB-D dataset. We first compare with previous methods that all use VoteNet as the point backbone in  <ref type="table" target="#tab_4">Table 2</ref>. We validate the effectiveness of our fusion module on FCAF3D. When applied to this stronger backbone, our method could still achieve considerable gains, outperforming the original FCAF3D by a large margin of 3.2 and 2.3 on mAP@0.25 and mAP@0.5, respectively. We also achieve the state-of-the-art results, 67.4 mAP@0.25 and 51.2 mAP@0.5, on this benchmark. <ref type="table" target="#tab_5">Table 3</ref> shows the per-class 3D object detection results on SUN RGB-D. Equipped with the DeMF module, the model gets better results on nearly all categories and has the biggest improvements on object categories that are difficult to distinguish by geometrical appearance (+19.8 AP for dressers, +8.1 AP for bookshelves and +7.9 AP for desks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this subsection, we discuss the key design choices of the proposed DeMF module. By default, we report average mAP@0.25 and mAP@0.5 of 25 trails on SUN RGB-D for all experiments. If not specified, VoteNet is chosen as the base 3D detector in the point-stream. <ref type="table" target="#tab_6">Table 4</ref>, the DeMF module could improve the result of VoteNet by a large margin, demonstrating the importance of image information. When only self attention module is applied, the performance only improves by a little, showing that most of the performance gain comes from the image information extracted by the deformable attention module. To show that our method could use the image information more effectively, we compare our method with ImVoteNet <ref type="bibr" target="#b23">[24]</ref>, which is also VoteNet based 3D detector. It uses 2D detection boxes to guide the point voting process. Additionally, it uses 2D object classification results and RGB color as semantic and texture information to supplement point features. As shown in <ref type="table" target="#tab_6">Table 4</ref>, these handdesigned operations can improve the performance of VoteNet by a large margin but still fall behind our proposed method. We attribute this superiority to the adaptive learning nature of the DeMF module. It can automatically focus on salient parts of the object to extract useful information. Moreover, we do not observe additional performance gain when applying our method to ImVoteNet, as shown in the last two rows of <ref type="table" target="#tab_6">Table 4</ref>, indicating the feature extracted by ImVoteNet is covered by the features extracted by the DeMF module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image information As shown in</head><p>Receptive fields Compared to previous multi-modal detectors (e.g., EPNet <ref type="bibr" target="#b15">[16]</ref>), DeMF has a larger receptive field on the image, enabling the capture of objectlevel information. To show that a larger receptive field in images is helpful for  improving the recognition ability, we conduct extensive experiments on components that may affect it. Results are given in <ref type="table">Table 5</ref>, 6. <ref type="table">Table 5</ref> ablates how the hyperparameters in DeMF would affect the results. When only one location is sampled on a single level of the feature maps, the performance degrades a lot as shown in the 1st row, since the receptive field is small. When increasing the number of heads and the number of sampling locations per head, the receptive field is enlarged and the results are getting better as shown in the 2nd row. By utilizing multi-scale feature maps, the results can get further improvement since sampling locations in different feature levels can focus on different grained details. We find the results get saturated when further increasing the number of sampling locations. So we choose the settings in the 4th row of <ref type="table">Table 5</ref> as the default setting, which has a remarkable performance improvement when compared to sampling only one location (+1.4 mAP@0. <ref type="bibr" target="#b24">25</ref> and +1.7 mAP@0.5).</p><p>By adaptively learning the sample location, DeMF could automatically focus on the informative parts of the object. <ref type="table" target="#tab_8">Table 6</ref> validates its effectiveness. By replacing it with fixed grid sampling, the performance drops by 0.7 on mAP@0.25 and 0.6 on mAP@0.5 as it could not find the most suitable sampling location for objects of different shapes.</p><p>Reference Points The generation of the reference points would also influence the final performance as it tells the model where to focus. We ablate two different ways to get reference points. One is the mapping function we adopt, and the other one is using point feature to predict it. The latter way could only achieve 62.7 <ref type="table">Table 5</ref>: Ablation analysis on sampling location distribution. "Scales", "Heads" and "Samples" indicate image feature scales, attention heads and sampling locations for each scales and heads of deformable attention in DeMF, respectively. All models are VoteNet <ref type="bibr" target="#b24">[25]</ref>   mAP@0.25 and 42.2 mAP@0.5, which is much lower than the mapping function's 65.3 mAP@0.25 and 45.4 mAP@0.5. We suspect the reason is that it is hard for the point feature to directly predict the precise location of the object in the 2D plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results and Discussion</head><p>In <ref type="figure" target="#fig_5">Figure 5</ref>, we visualize the learned sample locations on 2D image. We can observe that some samples are clustered near the reference point and some samples locate near the boundary of the object. They together could provide meaningful object-level information for the original point feature. <ref type="figure">Figure 6</ref> illustrates the qualitative results on SUN RGB-D. Detection results from the original VoteNet and VoteNet equipped with DeMF module are given. In the first example, the confidence of the detected dresser in VoteNet is only 0.24, while it is 0.99 when the DeMF module is applied. In the second example, the bookshelf is small and far away from the camera. Its geometric structure is indistinct in point cloud. Though VoteNet can find it, the classification score is only 0.02, which will be filtered in real application. However, it is easy to identify the bookshelf in the 2D image, so DeMF can help the detector to be more confident about the detection result. In the third example, there are few points located on the black sofa and this scarcity of geometric information makes VoteNet misclassify it as a chair. Thanks to the object-level information provided by the image, our model can accurately recognize it.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented DeMF, a new design for multi-modal 3D detection. Our method could adaptively extract object-level image information and fuse it into point features extracted by LiDAR-based 3D detection framework. The experimental results conducted on SUN RGB-D dataset show that our method can improve the performance of 3D detection by a large margin and achieves a new state-ofthe-art. In addition, DeMF is a flexible and general method that can be applied to most current point-based detectors. We hope our proposed method could serve as an effective baseline for the field of multi-modality 3D object detection and intrigue people to think about what is the right way to extract and fuse information from different modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Image Stream Architecture Details</head><p>Image stream comprises an image backbone and an image encoder. In our implementation, we use ResNet50 <ref type="bibr" target="#b12">[13]</ref> as the image backbone and deformable transformer encoder <ref type="bibr" target="#b45">[46]</ref> as the image encoder. The input multi-scale feature maps of the encoder x l L?1 l=1 (L = 4) are extracted from the output feature maps of stages C 3 through C 5 in ResNet50 <ref type="bibr" target="#b12">[13]</ref> (transformed by a 1?1 convolution). The lowest resolution feature map x L is obtained via a 3 ? 3 stride 2 convolution on the final C 5 stage. And every output feature map of the encoder is of C = 256 channels.</p><p>We first train a deformable DETR <ref type="bibr" target="#b45">[46]</ref> on the training set of SUN-RGBD and then we use the pretrained parameters to initialize the image stream. Note that the parameters of the image stream are frozen when training the 3D detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Point Stream Architecture Details</head><p>We implement our method on VoteNet <ref type="bibr" target="#b24">[25]</ref> and FCAF3D <ref type="bibr" target="#b6">[7]</ref> respectively. Most of the settings of point stream are the same as the original version, but with some modifications as described below.</p><p>VoteNet We follow the setting of MMdetection3D <ref type="bibr" target="#b4">[5]</ref> implementation, but differs in three details: (1) we use class-agnostic head for size prediction, <ref type="bibr" target="#b1">(2)</ref> we include the IoU loss function to strengthen the supervision of the bounding box regression, (3) seed points instead of voting points are used to sample highquality object candidates. The above three changes can improve the performance of the VoteNet <ref type="bibr" target="#b24">[25]</ref>, especially in the case of mAP@0.5 (see <ref type="table" target="#tab_10">Table 7</ref>).</p><p>FCAF3D The original FCAF3D <ref type="bibr" target="#b6">[7]</ref> does not have a sampling module and will output tens of thousands of detection results, which makes it inappropriate to use the DeMF directly due to the unacceptable complexity in self-attention module. We add a sampling module to FCAF, which simply selects the top-K (K=256) detection results with the highest detection scores as object candidates for DeMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DeMF Architecture Details</head><p>DeMF receives 256 point features, and each feature is of 256 channels. As for query positional embeddings, we apply linear layers on the parameterization vector of a 3D box to get the embeddings of 256 channels.</p><p>In training, the DeMF is trained from scratch, and the learning rate of DeMF module is set as 1/20 of the point stream network. For the VoteNet-based version, we find it helpful to ensemble the predictions of different layers in DeMF to produce final detection results, and we set the dropout rate in attention module of DeMF to 0.4. <ref type="table">Table 8</ref>: Per-class 3D object detection results on SUN RGB-D. Reported metric is average precision with IoU threshold 0.25. Our results are reported by using FCAF3D as base detector. "bkshf" and "nstand" indicate bookshelf and nightstand, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Confusion matrix</head><p>We use confusion matrix to measure the classification ability of detector. The i-th row in confusion matrix indicates the number of predictions assigned to gts of i-th category. The label assignment strategy in confusion matrix is illustrated in algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Label assignment in confusion matrix</head><p>Input: S = {s1, s2, ..., sN } is the list of predicted scores. K and N represent the numbers of ground truth boxes and predicted boxes. IOU is the iou matrix between ground truth boxes and predicted boxes, whose shape is K ? N . Output: A = {a1, a2, ..., aN }, ai indicates that the predicted box pi is assigned to the ai-th ground truth box. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head><p>We present per-category results on SUN RGB-D. <ref type="table">Table 8</ref> and <ref type="table">Table 9</ref> show the results of mAP@0.25 and mAP@0.5, respectively. With FCAF3D, our approach gets better results on nearly all categories compared to previous methods.</p><p>We provide some qualitative comparisons between original FCAF3D and FCAF3D equipped with our DeMF module on SUN RGB-D, as shown in <ref type="figure" target="#fig_7">Fig 7</ref>. <ref type="table">Table 9</ref>: Per-class 3D object detection results on SUN RGB-D. Reported metric is average precision with IoU threshold 0.50. Our results are reported by using FCAF3D as base detector. "bkshf" and "nstand" indicate bookshelf and nightstand respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Input bath bed bkshf chair desk dresser nstand sofa  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Confusion matrix of VoteNet and our proposed method. Details provided in the Appendix. Given a set of N 0 points S ? R N0?3 and the corresponding RGB image x ? R H0?W0?3 , our goal is to boost the 3D object detection performance of point-based detection frameworks with the help of image information. To answer the question in what aspect can image features help 3D object detection, we should first investigate the defects of existing point-based detection frameworks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the proposed pipeline. The network takes a set of points and the corresponding RGB image as input. They are processed by a point backbone and an image backbone to extract point features and image features, respectively. Then object-level image information is extracted by the proposed Deformable Attention based Multi-modal Fusion (DeMF) module and fused into point feature to enhance its recognition ability. Finally, the enhanced feature is forwarded to a feed forward network (FFN) to give the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>vector representation and s i is its corresponding 3D coordinates. The set of point features and coordinates are then used for the final bounding box prediction. Thanks to the flexibility of our proposed fusion module, there are no restrictions on the choice of the point backbone and the sampling module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Deformable</head><label></label><figDesc>Attention based Multi-modal Fusion After point and image features are extracted, DeMF module is applied afterward to supplement point features with object-level information extracted from images. DeMF module is built upon the multi-scale deformable attention module, while the specific modification is applied by taking the unique properties of 3D point cloud input into consideration. The overall process of DeMF module is depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of sample locations on the 2D image. Red dots in 2D images are reference points of red points in point clouds. Green dots in 2D images are sampling locations where image features are extracted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>begin Sort S in descending order and get sorted index I U = {F alse, F alse, ..., F alse}K for i in I iou ? IOU * i ? IOU * i is the i-th column of IOU c ? argmax iou if iou[c] ? 0.25 and U [i] == F alse ai = c U [i] = T rue break else ai = 0 ? ai = 0 indicates the prediction is assigned to background end if end for return A end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization results on SUN RGB-D Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>use a pre-trained ? Equal contribution. arXiv:2207.10589v1 [cs.CV] 21 Jul 2022</figDesc><table><row><cell>Camera image</cell><cell>VoteNet</cell><cell>Ours</cell></row><row><cell></cell><cell>chair</cell><cell>sofa</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the proposed Deformable Attention based Multi-modal Fusion (DeMF) module that aggregates image information and fuses them into point features, (3) a feed forward network (FFN) that give the final detection prediction result. The architecture of the DeMF module. DeMF first uses self-attention to capture the inner relation among the input point features. Simultaneously, a mapping function ? is applied to project the 3D coordinates to the 2D image plane as the reference points. Given the enhanced point features, the image features and the reference points as input, the deform attention module aggregates the image features extracted at the sampling locations and finally fuses them into the point features.</figDesc><table><row><cell>Point Feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell></cell><cell></cell><cell cols="2">Linear</cell></row><row><cell>Linear</cell><cell></cell><cell cols="2">Softmax</cell></row><row><cell></cell><cell cols="4">Attention Weight</cell><cell>Linear</cell></row><row><cell>Sampling Offset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell></row><row><cell>Self Attention</cell><cell cols="3">Aggregate</cell></row><row><cell>Image Feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deform</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attention</cell></row><row><cell>Fig. 4:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison on SUN RGB-D with different fusion methods. All methods use VoteNet as the base point detector. For our results, the reported metric is the best result and the number within the bracket is the average result. ImVoteNet ? indicates using Deformable DETR as the 2D image detector instead of Faster RCNN. PC stands for point cloud.</figDesc><table><row><cell>Methods</cell><cell>Point Backbone</cell><cell>Input</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>VoteNet [25] 1</cell><cell>PointNet++</cell><cell>PC</cell><cell>60.0</cell><cell>41.3</cell></row><row><cell>EPNet [16]</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>60.9</cell><cell>-</cell></row><row><cell>EPNet++ [22]</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>61.5</cell><cell>-</cell></row><row><cell>ImVoteNet [24] 2</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>64.4</cell><cell>43.3</cell></row><row><cell>ImVoteNet  ? [24]</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>64.6</cell><cell>43.4</cell></row><row><cell>Ours (VoteNet based)</cell><cell>PointNet++</cell><cell cols="3">PC+RGB 65.6 (65.3) 46.1 (45.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>As shown in the table, our DeMF module achieves a boost of +5.6 mAP@0.25 and +4.8 mAP@0.5 over the original VoteNet, which outperforms all previous fusion methods by a large margin. Particularly, it surpasses ImVoteNet by 1.0 on mAP@0.25 and 2.7 on mAP@0.5, demonstrating the superiority of our fusion operations.Next, we compare with previous state-of-the-art methods. The results are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison on SUN RGB-D with state-of-the-art methods. For Group-Free, FCAF3D and our results, the reported metric is the best result and the number within the bracket is the average result. PC stands for point cloud.</figDesc><table><row><cell>Methods</cell><cell>Point Backbone</cell><cell>Input</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>3DETR [23]</cell><cell>Transformers</cell><cell>PC</cell><cell>59.1</cell><cell>32.7</cell></row><row><cell>MLCVNet [39]</cell><cell>PointNet++</cell><cell>PC</cell><cell>59.8</cell><cell>-</cell></row><row><cell>H3DNet [43]</cell><cell>4?PointNet++</cell><cell>PC</cell><cell>60.1</cell><cell>39.0</cell></row><row><cell>BRNet [4]</cell><cell>PointNet++</cell><cell>PC</cell><cell>61.1</cell><cell>43.7</cell></row><row><cell>Group-Free [21]</cell><cell>PointNet++</cell><cell>PC</cell><cell cols="2">63.0 (62.6) 45.2 (44.4)</cell></row><row><cell>FCAF3D [7]</cell><cell>HDResNet34</cell><cell>PC</cell><cell cols="2">64.2 (63.8) 48.9 (48.2)</cell></row><row><cell>PointFusion[40]</cell><cell>PointNet</cell><cell>PC+RGB</cell><cell>45.4</cell><cell>-</cell></row><row><cell>COG[30]</cell><cell>-</cell><cell>PC+RGB</cell><cell>47.6</cell><cell>-</cell></row><row><cell>F-PointNet [26]</cell><cell>PointNet</cell><cell>PC+RGB</cell><cell>54.0</cell><cell>-</cell></row><row><cell>ImVoteNet [24]</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>64.4</cell><cell>43.3</cell></row><row><cell>EPNet [16]</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>64.6</cell><cell>-</cell></row><row><cell>EPNet++ [22]</cell><cell>PointNet++</cell><cell>PC+RGB</cell><cell>65.3</cell><cell>-</cell></row><row><cell>Ours (FCAF3D based)</cell><cell>HDResNet34</cell><cell cols="3">PC+RGB 67.4 (67.1) 51.2 (50.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Per-class 3D object detection results on SUN RGB-D. Reported metric is average precision with IoU threshold 0.25. "bkshelf" and "nstand" indicate bookshelf and nightstand respectively.</figDesc><table><row><cell>Methods</cell><cell cols="3">Input bathtub bed bkshelf chair desk dresser nstand sofa table toilet mAP</cell></row><row><cell>VoteNet 1</cell><cell>PC</cell><cell>75.7 83.4 36.0 78.2 25.9 26.6</cell><cell>64.7 66.1 53.6 90.2 60.0</cell></row><row><cell cols="4">w/ DeMF PC+RGB 79.5 87.0 44.1 80.7 33.8 46.4 66.3 72.5 52.8 92.7 65.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation analysis on importance of image information.</figDesc><table><row><cell>Detector</cell><cell cols="3">DeMF mAP@0.25 mAP@0.5</cell></row><row><cell>VoteNet</cell><cell></cell><cell>60.0</cell><cell>41.3</cell></row><row><cell>+ self-attn</cell><cell></cell><cell>60.8</cell><cell>41.8</cell></row><row><cell>ImVoteNet</cell><cell></cell><cell>64.6</cell><cell>43.4</cell></row><row><cell>VoteNet</cell><cell>?</cell><cell>65.3</cell><cell>45.4</cell></row><row><cell>ImVoteNet</cell><cell>?</cell><cell>65.4</cell><cell>45.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>based.</figDesc><table><row><cell cols="5">#Scales #Samples #Heads mAP@0.25 mAP@0.5</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>63.9</cell><cell>43.7</cell></row><row><cell>1</cell><cell>4</cell><cell>8</cell><cell>64.5</cell><cell>44.2</cell></row><row><cell>4</cell><cell>1</cell><cell>8</cell><cell>65.2</cell><cell>45.2</cell></row><row><cell>4</cell><cell>2</cell><cell>8</cell><cell>65.3</cell><cell>45.4</cell></row><row><cell>4</cell><cell>4</cell><cell>8</cell><cell>65.2</cell><cell>45.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation analysis on sampling strategy on 2D image. "Grid" indicates sampling in a fixed grid. "Offset" indicates sampling in the grid with learnable offsets.</figDesc><table><row><cell cols="3">Grid Offset mAP@0.25 mAP@0.5</cell></row><row><cell>?</cell><cell>64.6</cell><cell>44.8</cell></row><row><cell>?</cell><cell>65.3</cell><cell>45.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Results of VoteNet<ref type="bibr" target="#b24">[25]</ref> for different implementation methods. "Origin" indicates the results are from official paper. "MMdet3D" indicates results are from MMdetection3D<ref type="bibr" target="#b4">[5]</ref> repository.</figDesc><table><row><cell>Methods</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>Origin</cell><cell>57.7</cell><cell>-</cell></row><row><cell>MMdet3D [5]</cell><cell>59.1</cell><cell>35.8</cell></row><row><cell>Ours</cell><cell>60.0</cell><cell>41.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>69.8 11.6 68.8 14.8 30.1 59.8 58.2 35.5 74.5 48.9 ImVoteNet[24] 2 PC+RGB 49.6 60.5 12.9 63.4 10.5 20.4 55.4 59.2 28.9 72.4 43.3 Ours PC+RGB 65.3 72.1 13.2 67.8 16.2 38.7 61.7 62.5 35.4 78.5 51.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>table toilet mAP</cell></row><row><cell>VoteNet [25] 1</cell><cell>PC</cell><cell cols="6">54.9 64.1 11.0 61.3 8.2</cell><cell>15.9</cell><cell>46.7 55.5 26.9 68.9 41.3</cell></row><row><cell>GroupFree [21]</cell><cell>PC</cell><cell cols="7">64.0 67.1 12.4 62.6 14.5 21.9</cell><cell>49.8 58.2 29.2 72.2 45.2</cell></row><row><cell>FCAF3D [7] Camera image</cell><cell>PC</cell><cell cols="8">66.2 GT FCAF3D Ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell>18%</cell><cell></cell></row><row><cell></cell><cell></cell><cell>toilet</cell><cell>bed</cell><cell>sofa</cell><cell>bathtub</cell><cell>dresser</cell><cell>nightstand</cell><cell>bookshelf</cell><cell>table</cell><cell>chair</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We report the results of our improved implementation instead of the official paper, which reported 57.7 mAP@0.25.<ref type="bibr" target="#b1">2</ref> We report the results of our improved implementation instead of the official paper, which reported 63.4 mAP@0.25.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We report the results of our improved implementation instead of the official paper, which reported 57.7 mAP@0.25.<ref type="bibr" target="#b1">2</ref> We report the results of our improved implementation instead of the official paper, which reported 63.4 mAP@0.25.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We report the results of our improved implementation instead of the official paper, which reported 57.7 mAP@0.25.<ref type="bibr" target="#b1">2</ref> We report the results of our improved implementation instead of the official paper, which reported 63.4 mAP@0.25.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Azuma</surname></persName>
		</author>
		<title level="m">A survey of augmented reality. Presence: teleoperators &amp; virtual environments</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A hierarchical graph network for 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Back-tracing representative points for voting-based 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MMDetection3D: OpenMMLab next-generation platform for general 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vorontsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00322</idno>
		<title level="m">Fcaf3d: Fully convolutional anchor-free 3d object detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Voxel r-cnn: Towards high performance voxel-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jan Latecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative sparse detection networks for 3d singleshot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<title level="m">Group-free 3d object detection via transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Epnet++: Cascade bi-directional fusion for multi-modal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11088</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An end-to-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monocular plan view networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Venet: Voting enhancement network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mlcvnet: Multilevel context votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">3dssd: Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deformable convnets V2: more deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The network is optimized by using the AdamW optimizer (? 1 =0.9, ? 2 =0.999) with 36 epochs. The learning rate is set to 0.001 and decayed by 10? after 24th and 33th epochs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (2020) as input</title>
		<imprint/>
	</monogr>
	<note>Deformable detr: Deformable transformers for end-to-end object detection. All the models are trained on 2 NVidia RTX3090 with a batch size of 8</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
