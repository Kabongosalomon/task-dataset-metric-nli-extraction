<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution" key="instit1">NIST</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing (Suzhou)</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
							<email>yangjirui16@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>zhangyue@aircas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution" key="instit1">NIST</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing (Suzhou)</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
							<email>zhangtengfei16@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution" key="instit1">NIST</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing (Suzhou)</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
							<email>guozhi@mail.ie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution" key="instit1">NIST</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing (Suzhou)</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
							<email>sunxian@mail.ie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution" key="instit1">NIST</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing (Suzhou)</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
							<email>fukun@mail.ie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution" key="instit1">NIST</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing (Suzhou)</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection has been a building block in computer vision. Though considerable progress has been made, there still exist challenges for objects with small size, arbitrary direction, and dense distribution. Apart from natural images, such issues are especially pronounced for aerial images of great importance. This paper presents a novel multicategory rotation detector for small, cluttered and rotated objects, namely SCRDet. Specifically, a sampling fusion network is devised which fuses multi-layer feature with effective anchor sampling, to improve the sensitivity to small objects. Meanwhile, the supervised pixel attention network and the channel attention network are jointly explored for small and cluttered object detection by suppressing the noise and highlighting the objects feature. For more accurate rotation estimation, the IoU constant factor is added to the smooth L1 loss to address the boundary problem for the rotating bounding box. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as natural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the state-of-the-art performance of our detector. The code and models will be available at https://github.com/DetectionTeamUCAS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the fundamental tasks in computer vision and various general-purpose detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref> have been devised. Promising results have been achieved on a few benchmarks including COCO <ref type="bibr" target="#b23">[24]</ref> and VOC2007 <ref type="bibr" target="#b8">[9]</ref> etc. However, most existing detectors do not pay particular attention to some useful aspects for robust object detection in open environment: small objects, cluttered arrangement and arbitrary orientations.</p><p>In real-world problems, due to limitation of camera resolution and other reasons, the objects of interest can be of very small size e.g. for detection of traffic signs, tiny faces under public cameras on the streets. Also, the objects can range in a very dense fashion e.g. goods in shopping malls. Moreover, the objects can no longer be positioned horizontally as in COCO, VOC2007, e.g. for scene text detection whereby the texts can be in any direction and position.</p><p>In particular, the above three challenges are pronounced for images in remote sensing, as analyzed as follows:</p><p>1) Small objects. Aerial images often contain small objects overwhelmed by complex surrounding scenes;</p><p>2) Cluttered arrangement. Objects for detection are often densely arranged, such as vehicles and ships;</p><p>3) Arbitrary orientations. Objects in aerial images can appear in various orientations. It is further challenged by the large aspect ratio issue which is common in remote sensing.</p><p>In this paper, we mainly discuss our approach in the context of remote sensing, while the approach and the problems are general and we have tested with various datasets beyond aerial images as will be shown in the experiments.</p><p>Many existing general-purpose detectors such Faster-RCNN <ref type="bibr" target="#b30">[31]</ref> have been widely employed for aerial object detection. However, the design of such detectors are often based on the implicit assumption that the bounding boxes are basically in horizontal position, which is not the case for aerial images (and other detection tasks e.g. scene text detection). This limitation is further pronounced by the popular non-maximum suppression (NMS) technique as post- <ref type="figure">Figure 1</ref>: SCRDet includes SF-Net, MDA-Net against small and cluttered objects and rotation branch for rotated objects.</p><p>processing as it will suppress the detection of densely arranged objects in arbitrary orientation over the horizontal line. Moreover, horizontal region based methods have a coarse resolution on orientation estimation, which is key information to extract for remote sensing.</p><p>We propose a novel multi-category rotation detector for small, cluttered and rotated objects, called SCRDet which is designated to address the following issues: 1) small object: a sampling fusion network (SF-Net) is devised that incorporates feature fusion and finer anchor sampling; 2) noisy background: a supervised multi-dimensional attention network (MDA-Net) is developed which consists of pixel attention network and channel attention network to suppress the noise and highlight foreground. 3) cluttered and dense objects in arbitrary orientation: an angle sensitive network is devised by introducing an angle related parameter for estimation. Combing these three techniques as a whole, our approach achieves state-of-the-art performance on public datasets including two remote sensing benchmarks DOTA and NWPU VHR-10. The contributions of this paper are: 1) For small objects, a tailored feature fusion structure is devised by feature fusion and anchor sampling.</p><p>2) For cluttered, small object detection, a supervised multi-dimensional attention network is developed to reduce the adverse impact of background noise.</p><p>3) Towards more robust handling of arbitrarily-rotated objects, an improved smooth L1 loss is devised by adding the IoU constant factor, which is tailored to solve the boundary problem of the rotating bounding box regression. 4) Perhaps more importantly, in Section 4.2 we show that the proposed techniques are general, and can also be applied on natural images and combined with general detection algorithms, which surpass the state-of-the-art method or further improves the existing methods by combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing detection methods mainly assume the objects for detection are located along the horizontal line in images.</p><p>In the seminal work <ref type="bibr" target="#b11">[12]</ref>, a multi-stage R-CNN network for region based detection is presented with a subsequent line of improvements on both accuracy and efficiency, including Fast R-CNN <ref type="bibr" target="#b10">[11]</ref>, Faster R-CNN <ref type="bibr" target="#b30">[31]</ref>, and regionbased fully convolutional networks (R-FCN) <ref type="bibr" target="#b4">[5]</ref>. On the other hand, there is also a line of recent works that directly regress the bounding box, e.g. Single-Shot Object Detector (SSD) <ref type="bibr" target="#b25">[26]</ref> and You only look once (YOLO) <ref type="bibr" target="#b29">[30]</ref> leading to improved speed.</p><p>As discussed above, there are challenging scenarios regarding with small objects, dense arrangement and arbitrary rotation. However they have not been particularly addressed by the above detectors despite their importance in practice. In particular for aerial images, due to its strategic value to the nation and society, efforts have also been made to develop tailored methods to remote sensing. The R-P-Faster R-CNN framework is developed in <ref type="bibr" target="#b13">[14]</ref> for small objects. While both deformable convolution layers <ref type="bibr" target="#b5">[6]</ref> and R-FCN are combined by <ref type="bibr" target="#b39">[40]</ref> to improve detection accuracy. More recently, the authors in <ref type="bibr" target="#b39">[40]</ref> adopt top-down and skipped connections to produce a single high-level feature map of a fine resolution, improving the performance of the deformable Faster R-CNN. However such horizontal region based detectors still are confronted with the challenges for the aforementioned bottlenecks in terms of scale, orientation and density, which call for more principled methods beyond the setting for horizontal region detection. On the other hand, there is a thread of works on remote sensing, for detecting objects in arbitrary direction. However, these methods are often tailored to specific object categories, e.g. vehicle <ref type="bibr" target="#b35">[36]</ref>, ship <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27]</ref>, aircraft <ref type="bibr" target="#b24">[25]</ref> etc.. Though there are recently a few methods for multi-category rotational region detection models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>, while they lack a principled way of handling small size and high density.</p><p>Compared with the detection methods for natural images, literature on scene text detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> often pay more attention to object orientation. While such methods still have difficulty in dealing with aerial image based object detection: one reason is that most text detection methods are restricted to single-category object detection <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7]</ref>, while there are often many different categories to discern for remote sensing. Another reason is that the objects in aerial images are often more closer to each other than in scene texts, which limits the applicability of segmentation based detection algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b43">44]</ref> that otherwise work well on scene texts. Moreover, there are often a large number of densely distributed objects that call for efficient detection. This paper considers all the above aspects comprehen-  The orange-yellow bounding box represents the anchor, the green represents ground-truth, and the red box represents the anchor with the largest IoU of ground-truth. sively, and proposes a principled method for multi-category arbitrary-oriented object detection in aerial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>We first give an overview of our two-stage method as sketched in <ref type="figure">Fig. 1</ref>. In the first stage, the feature map is expected to contain more feature information and less noise by adding SF-Net and MDA-Net. For positional sensitivity of the angle parameters, this stage still regresses the horizontal box. By the improved five-parameter regression and the rotation nonmaximum-suppression (R-NMS) operation for each proposal in the second stage, we can obtain the final detection results under arbitrary rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Finer Sampling and Feature Fusion Network</head><p>In our analysis, there are two main obstacles in detecting small objects: insufficient object feature information and inadequate anchor samples. The reason is that due to the use of the pooling layer, the small object loses most of its feature information in the deep layers. Meanwhile, larger sampling stride of high-level feature maps tend to skip smaller objects directly, resulting in insufficient sampling.</p><p>Feature fusion. It is generally regarded that low-level feature map can preserve location information of small object, while high-level feature map can contain higher-level semantic cues. Feature pyramid networks (FPN) <ref type="bibr" target="#b22">[23]</ref>, Top-Down Modulation (TDM) <ref type="bibr" target="#b34">[35]</ref>, and Reverse connection with objectness prior networks (RON) <ref type="bibr" target="#b20">[21]</ref> are common feature fusion methods that involve the combination of both high and low level feature maps in different forms.</p><p>Finer sampling. Insufficient training samples and imbalance can affect the detection performance. By introducing the expected max overlapping (EMO) score, the authors in <ref type="bibr" target="#b44">[45]</ref> calculate the expected max intersection over union (IoU) between anchor and object. They find the smaller stride of the anchor (S A ) is, the higher EMO score achieves, <ref type="figure">Figure 3</ref>: SF-Net. F3 has a small S A , while fully considering the feature fusion and adaptability to different scales. statistically leading to improved average max IoU of all objects. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the results of small object sampling given stride step 16 and 8, respectively. It can be seen that a smaller S A can sample more high-quality samples well capturing the small objects which is of help for both detector training and inference.</p><p>Based on the above analysis, we design the finer sampling and feature fusion network (SF-Net) as shown in <ref type="figure">Fig. 3</ref>. In the anchor based detection framework, the value of S A is equal to the reduction factor of the feature map relative to the original image. In other words, the value of S A can only be an exponential multiple of 2. SF-Net solves this problem by changing the size of the feature map, making the setting of S A more flexible to allow for more adaptive sampling. For the purpose of reducing network parameters, SF-Net only uses C3 and C4 in Resnet <ref type="bibr" target="#b15">[16]</ref> for fusion to balance the semantic information and location information while ignoring other less relevant features. In simple terms, the first channel of SF-Net upsamples the C4 so that its S A = S, where S is the expected anchor stride. The second channel also upsamples the C3 to the same size. Then, we pass C3 through an inception structure to expand its receptive field and increase semantic information. The inception structure contains a variety of ratio convolution kernels to capture the diversity of object shapes. Finally, a new feature map F3 is obtained by element-wise addition of the two channels. <ref type="table" target="#tab_1">Table 1</ref> shows the detection accuracy and training overhead on DOTA under different S A . We find that the optimal S A depends on specific dataset, espe-   cially on the size distribution of small objects. In this paper, the value of S is universally set to 6 for tradeoff between accuracy and speed.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Dimensional Attention Network</head><p>Due to the complexity of real-world data such as aerial images, the proposals provided by RPN may introduce a large amount of noise information, as shown in <ref type="figure" target="#fig_2">Fig. 4b</ref>. Excessive noise can overwhelm the object information, and the boundaries between the objects will be blurred (see <ref type="figure" target="#fig_2">Fig. 4a</ref>), resulting in missed detection and increasing false alarms. Therefore, it is necessary to enhance the object cues and weaken the non-object information. Many attention structures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> have been proposed to solve problems of occlusion, noise, and blurring. However, most of the methods are unsupervised, which have difficulty to guide the network to learn specific purposes.</p><p>To more effectively capture the objectness of small objects against complex background, we design a supervised multi-dimensional attention leaner (MDA-Net), as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Specifically, in the pixel attention network, the feature map F3 passes through an inception structure with different ratio convolution kernels, and then a two-channel saliency map is learned (see <ref type="figure" target="#fig_2">Fig. 4d</ref>) through a convolution operation. The saliency map represents the scores of the foreground and background, respectively. Then, Softmax operation is performed on the saliency map and one of the channels is selected to multiply with F3. Finally, a new information feature map A3 is obtained, as shown in <ref type="figure" target="#fig_2">Fig. 4c</ref>. It should be noted that the value of the saliency map after the Softmax function is between [0, 1]. In other words, it can reduce the noise and relatively enhance the object information. Since the saliency map is continuous, non-object information will not be eliminated entirely, which is beneficial to retain certain context information and improve robustness. To guide the network to learning this process, we adopt a supervised learning method. Firstly, we can easily get a binary map as a label (as shown in <ref type="figure" target="#fig_2">Fig. 4e</ref>) according to ground truth, and then use the cross-entropy loss of the binary map and the saliency map as the attention loss. Besides, we also use SENet <ref type="bibr" target="#b17">[18]</ref> as the channel attention network for auxiliary, and the value of reduction ratio is 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rotation Branch</head><p>The RPN network provides coarse proposals for the second stage. In order to improve the calculation speed of RPN, we take the highest score of 12,000 regression boxes for NMS operation in the training stage and get 2,000 as proposals. In the test stage, 300 proposals are taken from 10,000 regression boxes by NMS.</p><p>In the second stage, we use five parameters (x, y, w, h, ?) to represent arbitrary-oriented rectangle.</p><p>Ranging in [??/2, 0), ? is defined as the acute angle to the x-axis, and for the other side we denote it as w. This definition is consistent with OpenCV. Therefore, IoU computation on axis-aligned bounding box may lead to an inaccurate IoU of the skew interactive bounding box and further ruin the bounding box prediction. An implementation for skew IoU computation <ref type="bibr" target="#b28">[29]</ref> with thought to triangulation is proposed to deal with this problem. We use rotation nonmaximumsuppression (R-NMS) as a post-processing operation based on skew IoU computation. For the diversity of shapes in the dataset, we set different R-NMS thresholds for differ- <ref type="figure">Figure 6</ref>: Boundary discontinuity of the rotation angle. ent categories. In addition, to make full use of the pretraining weight ResNet, we replace the two fully connected layers fc6 and fc7 with C5 block and global average pooling (GAP). The regression of the rotation bounding box is:</p><formula xml:id="formula_1">t x = (x ? x a )/w a , t y = (y ? y a )/h a t w = log(w/w a ), t h = log(h/h a ), t ? = ? ? ? a (1) t x = (x ? x a )/w a , t y = (y ? y a )/h a t w = log(w /w a ), t h = log(h /h a ), t ? = ? ? ? a<label>(2)</label></formula><p>where x, y, w, h, ? denote the box's center coordinates, width, height and angle, respectively. Variables x, x a , x are for the ground-truth box, anchor box, and predicted box, respectively (likewise for y, w, h, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>The multi-task loss is used which is defined as follows:</p><formula xml:id="formula_2">L = ? 1 N N n=1 t n j?{x,y,w,h,?} L reg (v nj , v nj ) |L reg (v nj , v nj )| | ? log(IoU )| + ? 2 h ? w h i w j L att (u ij , u ij ) + ? 3 N N n=1 L cls (p n , t n )<label>(3)</label></formula><p>where N indicates the number of proposals, t n represents the label of object, p n is the probability distribution of various classes calculated by Softmax function, t n is a binary value (t n = 1 for foreground and t n = 0 for background, no regression for background). v * j represents the predicted offset vectors, v * j represents the targets vector of groundtruth. u ij , u ij represent the label and predict of mask's pixel respectively. IoU denotes the overlap of the prediction box and ground-truth. The hyper-parameter ? 1 , ? 2 , ? 3 control the tradeoff. In addition, the classification loss L cls is Softmax cross-entropy. The regression loss L reg is smooth L1 loss as defined in <ref type="bibr" target="#b10">[11]</ref>, and the attention loss L att is pixelwise Softmax cross-entropy.</p><p>In particular, there exists the boundary problem for the rotation angle, as shown in <ref type="figure">Fig. 6</ref>. It shows that an ideal form of regression (the blue box rotates counterclockwise to the red box), but the loss of this situation is very large due to the periodicity of the angle. Therefore, the model has to be regressed in other complex forms (such as the blue box rotating clockwise while scaling w and h), increasing the difficulty of regression, as shown in <ref type="figure" target="#fig_4">Fig. 7a</ref>. To better solve this problem, we introduce the IoU constant factor |?log(IoU )| |Lreg(v j ,vj )| in the traditional smooth L1 loss, as shown in Eq. 3. It can be seen that in the boundary case, the loss function is approximately equal to | ? log(IoU )| ? 0, eliminating the sudden increase in loss, as shown in <ref type="figure" target="#fig_4">Fig. 7b</ref>.</p><p>The new regression loss can be divided into two parts,</p><formula xml:id="formula_3">Lreg(v j ,vj )</formula><p>|Lreg(v j ,vj )| determines the direction of gradient propagation, and | ? log(IoU )| for the magnitude of gradient. In addition, using IoU to optimize location accuracy is consistent with IoU-dominated metric, which is more straightforward and effective than coordinate regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Tests are implemented by TensorFlow [1] on a server with Nvidia Geforce GTX 1080 GPU and 8G memory. We perform experiments on both aerial benchmarks and natural images to verify the generality of our techniques. Note our techniques are orthogonal to specific network backbone. In experiments, we use Resnet-101 as backbone for remote sensing benchmarks, and FPN and R 2 CNN for COCO, VOC2007 and ICDAR2015 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Aerial Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Protocls</head><p>The benchmark DOTA <ref type="bibr" target="#b38">[39]</ref> is for object detection in aerial images. It contains 2,806 aerial images from different sensors and platforms. The image size ranges from around 800 ? 800 to 4, 000 ? 4, 000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These images are then annotated by experts using 15 common object categories. The fully annotated DOTA benchmark contains 188,282 instances, each of which is labeled by an arbitrary quadrilateral. There are two detection tasks for DOTA: horizontal bounding boxes (HBB) and oriented bounding boxes (OBB). Half of the original images are randomly selected as the training set, 1/6 as the validation set, and 1/3 as the testing set. We divide the images into 800 ? 800 subimages with an overlap of 200 pixels.</p><p>The public benchmark NWPU VHR-10 [4] contains 10class geospatial object for detection. This dataset con-   <ref type="table">Table 3</ref>: Performance evaluation of OBB and HBB task on DOTA datasets. tains 800 very-high-resolution (VHR) remote sensing images that are cropped from Google Earth and Vaihingen dataset and then manually annotated by experts.</p><p>We use the pretrained ResNet-101 model for initialization. For DOTA, the model is trained by 300k iterations in total, and the learning rate changes during the 100k and 200k iterations from 3e-4 to 3e-6. For NWPU VHR-10, the split ratios of the training dataset, validation dataset, and test dataset are 60%, 20%, and 20%, respectively. The model is trained by totally 20k iterations with the same learning rate as for DOTA. Besides, weight decay and momentum are 0.0001 and 0.9, respectively. We employ MomentumOptimizer as optimizer and no data augmentation is performed except random image flip during training.</p><p>For parameter setting, the expected anchor stride S as discussed in Sec. 3.1 is set to 6, and we set the base anchor size to 256, and the anchor scales setting from 2 ?4 to 2 1 . Since the multi-categories objects in DOTA and NWPU VHR-10 have different shapes, we set anchor ratios to [1/1,1/2,1/3,1/4,1/5,1/6,1/7,1/9]. These settings ensure that each ground-truth can be assigned with positive samples. When IoU &gt; 0.7, the anchor is assigned as a positive sample, and as a negative sample if IoU &lt; 0.3. Besides, due to the sensitivity between angle and IoU in the large aspect ratio rectangle, the two thresholds in the second stage are all set to 0.4, respectively. For training, the mini-batch size in two stages is 512. The hyperparameters in Eq. 3 are set to ? 1 = 4, ? 2 = 1, ? 3 = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation Study</head><p>Baseline setup. We choose Faster-RCNN-based R 2 CNN <ref type="bibr" target="#b18">[19]</ref> as the baseline for ablation study, but not limited to this method. For fairness, all experimental data and parameter settings are strictly consistent. We use mean average precision (mAP) as a measure of performance. The results of DOTA reported here are obtained by submitting our predictions to the official DOTA evaluation server 1 .</p><p>Effect of MDA-Net. As discussed in Sec. 3.2, the attention structure is beneficial to suppress the influence of noise and highlight the object information. It also can be evidenced in <ref type="table" target="#tab_3">Table 2</ref> that the detection results of most objects have been improved to varying degrees after adding the pixel attention network, and the total mAP increase by 3.67%. MDA-Net further improves the detection accuracy of large aspect ratio targets such as bridge, large vehicle, ship, harbor and so on. Compared to pixel attention, MDA-Net increases mAP by about 1% to 65.33%. <ref type="table" target="#tab_5">Table 5</ref> shows that supervised learning is the main contribution of MDA-Net rather than computation. Effect of SF-Net. Reducing the stride size of the anchor and the feature fusion are effective means to improve the detection for small objects. In <ref type="table" target="#tab_3">Table 2</ref> we also study on the techniques presented in <ref type="bibr" target="#b44">[45]</ref>. Both shifted anchors (SA) and shift jittering (SJ) follow the idea of using a single feature point to regress the bounding boxes of multiple sub-areas. Experiments show that these two strategies can hardly contribute to the accuracy in accordance with the observation in the original paper. Enlarged feature maps is a good strategy to reduce S A , including bilinear upsampling (BU), bilinear upsampling with skip connection (BUS), dilated convolution (DC). Although these methods take into account the importance of sampling for small object detection and their detection performance have been improved to varying degrees, the S A settings are still inflexible and cannot achieve the best sampling results. SF-Net effectively models the feature fusion and the flexibility of the S A setting, and it achieves the best performance of 68.89%, especially benefited from the improvement of small object such as vehicle, ship and storage tank.</p><p>Effect of IoU-Smooth L1 Loss. IoU-Smooth L1 Loss eliminates the boundary effects of the angle, making it easier for the model to regress to the objects coordinates. This new loss improves the detection accuracy to 69.83%.</p><p>Effect of image pyramid. Image pyramid based training and test is an effective means to improve performance. The method ICN <ref type="bibr" target="#b1">[2]</ref> uses the image cascade network struc-Method mAP R-P-Faster R-CNN <ref type="bibr" target="#b13">[14]</ref> 76.50 SSD512 <ref type="bibr" target="#b25">[26]</ref> 78.40 DSSD321 <ref type="bibr" target="#b9">[10]</ref> 78.80 DSOD300 <ref type="bibr" target="#b32">[33]</ref> 79.80 Deformable R-FCN <ref type="bibr" target="#b39">[40]</ref> 79.10 Deformable Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> 84.40 RICADet <ref type="bibr" target="#b21">[22]</ref> 87.12 RDAS512 <ref type="bibr" target="#b2">[3]</ref> 89.50 Multi-Scale CNN <ref type="bibr" target="#b12">[13]</ref> 89.60 SCRDet (proposed) 91.75   <ref type="bibr" target="#b1">[2]</ref> and RoI-Transformer <ref type="bibr" target="#b7">[8]</ref>, which are all applicable to multi-category rotation object detection. <ref type="table">Table  3</ref> shows the performance of these methods. The excellent performance of RoI-Transformer, ICN and SCRDet in small object detection is attributed to feature fusion. SCRDet con- siders the expansion of the receptive field and the attenuation of noise in the fusion, so it is better than ICN and RoI-Transformer for large objects. Our method ranks first among existing published results, reaching 72.61% mAP. HBB Task. We use DOTA and NWPU VHR-10 to validate our proposed approach and shield the angle parameter in the code. <ref type="table">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref> show the performance on the two datasets, respectively. We also get the first place among existing methods in literature on DOTA, at 75.35% or so. For the NWPU VHR-10 dataset, we compare it with nine methods and achieve the best detection performance, at 91.75%. Our approach achieves the best detection accuracy on more than half of the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Natural Images</head><p>To verify the universality of our model, we further validate the proposed techniques on generic datasets and general-purpose detection networks FPN <ref type="bibr" target="#b22">[23]</ref>    <ref type="bibr" target="#b18">[19]</ref>. We choose COCO <ref type="bibr" target="#b23">[24]</ref> and VOC2007 <ref type="bibr" target="#b8">[9]</ref> datasets as they contain many small objects. We also use ICDAR2015 <ref type="bibr" target="#b19">[20]</ref> because there are rotated texts for scene text detection.</p><p>By <ref type="table" target="#tab_7">Table 6</ref>, FPN * with MDA-Net can increase by 0.7% and 2.22% on COCO <ref type="bibr" target="#b23">[24]</ref> and VOC2007 <ref type="bibr" target="#b8">[9]</ref> datasets, respectively. As shown in <ref type="figure">Fig. 9</ref>, the MDA-Net has good performance in both dense and small objects detection. IoU-Smooth loss does not bring high improvement to horizontal region detection, hence this also reflects its pertinence to rotation detection boundary problem.</p><p>For ICDAR2015, R 2 CNN-4 achieves 74.36% in single scale according to <ref type="bibr" target="#b18">[19]</ref>. As it is not open sourced, we reimplement it and term our version as R 2 CNN-4 * according to the definition of the rotation box in the paper without multiple pooled sizes structure, and our version can achieve the mAP of 77.23%. Then, we equip R 2 CNN-4 * with our proposed techniques and term it SCRDet-R 2 CNN. It achieves the highest performance 80.08% in single scale. Once again, the validity of the structure proposed in this paper is proved. According to <ref type="figure">Fig. 10</ref>, SCRDet-R 2 CNN, achieves a notably better recall for dense objects detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented an end-to-end multi-category detector designated for objects in arbitrary rotations, which are common in aerial image. Considering the factors of feature fusion and anchor sampling, a sampling fusion network with smaller S A is added. Meanwhile, the algorithm weakens the influence of noise and highlights the object information through a supervised multi-dimensional attention network. Moreover, we implement rotation detection to preserve orientation information and solve intensive problems. Our approach achieves state-of-the-art performance on two public remote sensing datasets: DOTA and NWPU VHR-10. Finally, we have further validated our structure on nature datasets such as COCO, VOC2007 and ICDAR2015.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(a) SA = 16 (</head><label>16</label><figDesc>b) SA = 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Anchor sampling with different anchor stride S A .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the multi-dimensional attention network. (a) Blurred boundaries. (b) Input feature map of attention network. (c) Output feature map of attention network. (d) Saliency map. (e) Binary map. (f) Ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The devised MDA-Net consisting of channel attention network and pixel attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of detection results by two losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Examples on DOTA. Our method performs better on those with small size, in arbitrary direction, and high density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 : 2 CNN R 2 CNN- 4 *Figure 10 :</head><label>922410</label><figDesc>Detection results of COCO. The first column is the result of FPN * +MDA-Net and the second column is FPN * . The red boxes represent missed objects and the orange boxes represent false alarm. Detection results of COCO and ICDAR2015. The first column is the result of R 2 CNN-4 * equipped with our techniques (SCRDet-R 2 CNN) and the second column is vanilla R 2 CNN-4 * . Red arrows denote missed objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy and average training overhead per image with 18K iterations on DOTA under varying stride S A .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>CNN (baseline) [19] 80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67 +Pixel Attention 81.17 75.23 36.71 68.14 62.33 48.22 55.75 89.57 78.40 76.61 54.08 58.32 63.76 61.94 54.89 64.34 +MDA 84.89 77.07 38.55 67.88 61.78 51.87 56.23 89.82 75.77 76.30 53.68 63.25 63.85 65.05 53.99 65.33 +SA [45]+MDA 81.27 76.49 38.16 69.13 54.03 46.51 55.03 89.80 69.92 75.11 57.06 58.51 62.70 59.72 48.20 62.78 +SJ [45]+MDA 81.13 76.02 32.79 66.94 60.73 48.12 54.86 90.29 74.54 76.25 54.00 57.27 63.87 60.24 43.48 62.70 +BU [45] +MDA 84.63 75.34 42.84 68.47 63.11 53.69 57.13 90.70 76.93 75.28 55.63 58.28 64.57 67.10 49.19 65.53 +BUS [45]+MDA 87.50 75.60 42.41 69.48 62.45 50.89 56.10 90.87 78.41 75.68 58.94 58.68 63.87 67.38 52.78 66.07 +DC [45]+MDA 87.01 76.66 42.25 68.95 62.55 53.62 56.22 90.83 78.54 75.49 58.54 57.17 63.99 66.77 57.43 66.40 +SF+MDA 89.65 79.51 43.86 67.69 67.41 55.93 64.86 90.71 77.77 84.42 57.67 61.38 64.29 66.12 62.04 68.89 +SF+MDA+IoU 89.41 78.83 50.02 65.59 69.96 57.63 72.26 90.73 81.41 84.39 52.76 63.62 62.01 67.62 61.16 69.83 +SF +MDA+IoU+P 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61</figDesc><table><row><cell>Method</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>R 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablative study of each components in our proposed method on the DOTA dataset. The short names for categories 69.<ref type="bibr" target="#b11">12</ref> 17.17 63.49 34.20 37.16 36.20 89.19 69.60 58.96 49.4 52.52 46.69 44.80 46.30 52.93 R-DFPN [41] 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94 R 2 CNN [19] 80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67 RRPN [29] 88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01 ICN [2] 81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20 68.20 RoI-Transformer [8] 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 SCRDet (proposed) 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 33.87 22.73 34.88 38.73 32.02 52.37 61.65 48.54 33.91 29.27 36.83 36.44 38.26 11.61 39.20 R-FCN [5] 79.33 44.26 36.58 53.53 39.38 34.15 47.29 45.66 47.74 65.84 37.92 44.23 47.23 50.64 34.90 47.24 FR-H [31] 80.32 77.55 32.86 68.13 53.66 52.49 50.04 90.41 75.05 59.59 57.00 49.81 61.69 56.46 41.85 60.46 FPN [23] 88.70 75.10 52.60 59.20 69.40 78.80 84.50 90.60 81.30 82.60 52.50 62.10 76.60 66.30 60.10 72.00 ICN [2] 90.00 77.70 53.40 73.30 73.50 65.00 78.20 90.80 79.10 84.80 57.20 62.10 73.50 70.20 58.10 72.50 SCRDet (proposed) 90.18 81.88 55.30 73.29 72.09 77.65 78.06 90.91 82.44 86.39 64.53 63.45 75.77 78.21 60.11 75.35</figDesc><table><row><cell cols="17">are defined as: PL-Plane, BD-Baseball diamond, BR-Bridge, GTF-Ground field track, SV-Small vehicle, LV-Large vehicle,</cell></row><row><cell cols="17">SH-Ship, TC-Tennis court, BC-Basketball court, ST-Storage tank, SBF-Soccer-ball field, RA-Roundabout, HA-Harbor, SP-</cell></row><row><cell cols="4">Swimming pool, and HC-Helicopter.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>OBB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FR-O [39] 79.09 HBB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD [10]</cell><cell cols="3">44.74 11.21 6.22</cell><cell>6.91</cell><cell cols="8">2.00 10.24 11.34 15.59 12.56 17.94 14.73 4.55</cell><cell>4.55</cell><cell>0.53</cell><cell cols="2">1.01 10.94</cell></row><row><cell>YOLOv2 [30]</cell><cell>76.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance for HBB task on NWPU VHR-10.</figDesc><table><row><cell>dataset train/test</cell><cell>baseline</cell><cell cols="3">MDA-Net MDA-Net  ? baseline  ?</cell></row><row><cell cols="2">DOTA trainval/test 60.67% (R 2 CNN)</cell><cell>65.33%</cell><cell>61.23%</cell><cell>65.08%</cell></row><row><cell>VOC 07+12/07</cell><cell>80.39% (FPN  *  )</cell><cell>82.27%</cell><cell>80.53%</cell><cell>82.11%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>MDA-Net ? means MDA-Net without supervised learning. baseline ? means baseline with supervision. ture, which is similar to the idea of image pyramid. Here we randomly scale the original image to [600 ? 600, 800 ? 800, 1, 000 ? 1, 000, 1, 200 ? 1, 200] and send it to the network for training. For testing, each image is tested at four scales and combined by R-NMS. As shown inTable 2, image pyramid can notably improve the detection efficiency and achieves 72.61% mAP. The detection results for each class on DOTA are shown inFig. 8.</figDesc><table><row><cell>4.1.3 Peer Methods Comparison</cell></row><row><cell>OBB Task. Besides the official baseline given by DOTA,</cell></row><row><cell>we also compare with RRPN [29], R 2 CNN [19], R-DFPN</cell></row><row><cell>[41], ICN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and R 2 CNN</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="2">Backbone mAP/F1</cell></row><row><cell></cell><cell>FPN  *</cell><cell>Res50</cell><cell>36.1</cell></row><row><cell>COCO</cell><cell>FPN  *  +IoU-Smooth</cell><cell>Res50</cell><cell>36.2</cell></row><row><cell></cell><cell>FPN  *  +MDA-Net</cell><cell>Res50</cell><cell>36.8</cell></row><row><cell>VOC2007</cell><cell>FPN  *  FPN  *  +MDA-Net</cell><cell>Res101 Res101</cell><cell>76.14 78.36</cell></row><row><cell>ICDAR2015</cell><cell>R 2 CNN-4  *  SCRDet-R 2 CNN</cell><cell>Res101 Res101</cell><cell>77.23 80.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effectiveness of the proposed structure on generic datasets. Notation * indicates our own implementation. For VOC 2007, all methods are trained on VOC2007 trainval sets and tested on VOC 2007 test set. For COCO, all the results are obtained on the minival set. For ICDAR2015, results are obtained by submitting it to the official website.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://captain-whu.github.io/DOTA/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02700</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geospatial object detection in remote sensing imagery based on multiscale single-shot detector with activated semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghui</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">820</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peicheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks. CoRR, abs/1703</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">06211</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01315</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning roi transformer for detecting oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00155</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">131</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">666</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5936" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rotationinsensitive and context-augmented object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented ship detection framework in optical remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="941" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotated region based cnn for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deformable faster r-cnn with aggregating multi-layer features for partially occluded object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changren</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1470</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanxin</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Face attention network: An effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07246</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deformable convnet with aspect ratio constrained nms for object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozhuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangling</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1312</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchors perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5127" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
