<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Filtered Channel Features for Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Bernt Schiele</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Filtered Channel Features for Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low-level features in combination with a boosted decision forest. Based on this observation we propose a unifying framework and experimentally explore different filter families. We report extensive results enabling a systematic analysis.</p><p>Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets, while using only HOG+LUV as low-level features. When adding optical flow features we further improve detection quality and report the best known results on the Caltech dataset, reaching 93% recall at 1 FPPI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection is an active research area, with 1000+ papers published in the last decade 1 , and well established benchmark datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. It is considered a canonical case of object detection, and has served as playground to explore ideas that might be effective for generic object detection.</p><p>Although many different ideas have been explored, and detection quality has been steadily improving <ref type="bibr" target="#b1">[2]</ref>, arguably it is still unclear what are the key ingredients for good pedestrian detection; e.g. it remains unclear how effective parts, components, and features learning are for this task.</p><p>Current top performing pedestrian detection methods all point to an intermediate layer (such as max-pooling or filtering) between the low-level feature maps and the classification layer <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24]</ref>. In this paper we explore the simplest of such intermediary: a linear transformation implemented as convolution with a filter bank. We propose a framework for filtered channel features (see <ref type="figure">figure 1</ref>) that unifies multiple top performing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24]</ref>, <ref type="figure">Figure 1</ref>: Filtered feature channels illustration, for a single weak classifier reading over a single feature channel. Integral channel features detectors pool features via sums over rectangular regions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1]</ref>. We can equivalently rewrite this operation as convolution with a filter bank followed by single pixel reads (see ?2). We aim to answer: What is the effect of selecting different filter banks?</p><p>and that enables a systematic exploration of different filter banks. With our experiments we show that, with the proper filter bank, filtered channel features reach top detection quality.</p><p>It has been shown that using extra information at test time (such as context, stereo images, optical flow, etc.) can boost detection quality. In this paper we focus on the "core" sliding window algorithm using solely HOG+LUV features (i.e. oriented gradient magnitude and colour features). We consider context information and optical flow as add-ons, included in the experiments section for the sake of completeness and comparison with existing methods. Using only HOG+LUV features we already reach top performance on the challenging Caltech and KITTI datasets, matching results using optical flow and significantly more features (such as LBP and covariance <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Recent survey papers discuss the diverse set of ideas explored for pedestrian detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>. The most recent survey <ref type="bibr" target="#b1">[2]</ref> indicates that the classifier choice (e.g. linear/non-linear SVM versus decision forest) is not a clear differentiator regarding quality; rather the features used seem more important.</p><p>Creativity regarding different types of features has not been lacking. HOG) The classic HOG descriptor is based on local image differences (plus pooling and normalization steps), and has been used directly <ref type="bibr" target="#b4">[5]</ref>, as input for a deformable parts model <ref type="bibr" target="#b10">[11]</ref>, or as features to be boosted <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. The integral channel features detector <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1]</ref> uses a simpler HOG variant with sum pooling and no normalizations. Many extensions of HOG have been proposed (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref>). LBP) Instead of using the magnitude of local pixel differences, LBP uses the difference sign only as signal <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref>. Colour) Although the appearance of pedestrians is diverse, the background and skin areas do exhibit a colour bias. Colour has shown to be an effective feature for pedestrian detection and hence multiple colour spaces have been explored (both hand-crafted and learned) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. Local structure) Instead of simple pixel values, some approaches try to encode a larger local structure based on colour similarities (soft-cue) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15]</ref>, segmentation methods (hard-decision) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>, or by estimating local boundaries <ref type="bibr" target="#b19">[20]</ref>. Covariance) Another popular way to encode richer information is to compute the covariance amongst features (commonly colour, gradient, and oriented gradient) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28]</ref>. Etc.) Other features include bag-of-words over colour, HOG, or LBP features <ref type="bibr" target="#b3">[4]</ref>; learning sparse dictionary encoders <ref type="bibr" target="#b31">[32]</ref>; and training features via a convolutional neural network <ref type="bibr" target="#b33">[34]</ref>. Additional features specific for stereo depth or optical flow have been proposed, however we consider these beyond the focus of this paper. For our flow experiments we will use difference of frames from weakly stabilized videos (SDt) <ref type="bibr" target="#b28">[29]</ref>.</p><p>All the feature types listed above can be used in the integral channel features detector framework <ref type="bibr" target="#b7">[8]</ref>. This family of detectors is an extension of the old ideas from Viola&amp;Jones <ref type="bibr" target="#b36">[37]</ref>. Sums of rectangular regions are used as input to decision trees trained via Adaboost. Both the regions to pool from and the thresholds in the decision trees are selected during training. The crucial difference from the pioneer work <ref type="bibr" target="#b36">[37]</ref> is that the sums are done over feature channels other than simple image luminance.</p><p>Current top performing pedestrian detection methods (dominating INRIA <ref type="bibr" target="#b4">[5]</ref>, Caltech <ref type="bibr" target="#b8">[9]</ref> and KITTI datasets <ref type="bibr" target="#b12">[13]</ref>) are all extensions of the basic integral channel features detector (named ChnFtrs in <ref type="bibr" target="#b7">[8]</ref>, which uses only HOG+LUV features). SquaresChnFtrs <ref type="bibr" target="#b1">[2]</ref>, InformedHaar <ref type="bibr" target="#b42">[43]</ref>, and LDCF <ref type="bibr" target="#b23">[24]</ref>, are discussed in detail in section 2.2. Katamari exploits context and optical flow for improved performance. SpatialPooling(+) <ref type="bibr" target="#b27">[28]</ref> adds max-pooling on top of sum-pooling, and uses additional features such as covariance, LBP, and optical flow. Similarly, Regionlets <ref type="bibr" target="#b39">[40]</ref> also uses extended features and max-pooling, together with stronger weak classifiers and training a cascade of classifiers. Out of these, Regionlets is the only method that has also shown good performance on general classes datasets such as Pascal VOC and ImageNet.</p><p>In this paper we will show that vanilla HOG+LUV features have not yet saturated, and that, when properly used, they can reach top performance for pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>? We point out the link between ACF <ref type="bibr" target="#b6">[7]</ref>, (Squares)ChnFtrs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, InformedHaar <ref type="bibr" target="#b42">[43]</ref>, and LDCF <ref type="bibr" target="#b23">[24]</ref>. See section 2.</p><p>? We provide extensive experiments to enable a systematic analysis of the filtered integral channels, covering aspects not explored by related work. We report the summary of 65+ trained models (corresponding ?10 days of single machine computation). See sections 4, 5 and 7.</p><p>? We show that top detection performance can be reached on Caltech and KITTI using HOG+LUV features only. We additionally report the best known results on Caltech. See section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Filtered channel features</head><p>Before entering the experimental section, let us describe our general architecture. Methods such as ChnFtrs <ref type="bibr" target="#b7">[8]</ref>, SquaresChnFtrs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and ACF <ref type="bibr" target="#b6">[7]</ref> all use the basic architecture depicted in figure 1 top part (best viewed in colours). The input image is transformed into a set of feature channels (also called feature maps), the feature vector is constructed by sum-pooling over a (large) set of rectangular regions. This feature vector is fed into a decision forest learned via Adaboost. The split nodes in the trees are a simple comparison between a feature value and a learned threshold. Commonly only a subset of the feature vector is used by the learned decision forest. Adaboost serves both for feature selection and for learning the thresholds in the split nodes.</p><p>A key observation, illustrated in figure 1 (bottom), is that such sum-pooling can be re-written as convolution with a filter bank (one filter per rectangular shape) followed by reading a single value of the convolution's response map. This "filter + pick" view generalizes the integral channel features <ref type="bibr" target="#b7">[8]</ref> detectors by allowing to use any filter bank (instead of only rectangular shapes). We name this generalization "filtered channel features detectors".</p><p>In our framework, ACF <ref type="bibr" target="#b6">[7]</ref> has a single filter in its bank, corresponding to a uniform 4?4 pixels pooling region. ChnFtrs <ref type="bibr" target="#b7">[8]</ref> was a very large (tens of thousands) filter bank comprised of random rectangular shapes. SquaresChnFtrs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, on the other hand, was only 16 filters, each with a square-shaped uniform pooling region of different sizes. See <ref type="figure" target="#fig_0">figure 2a</ref> for an illustration of the SquaresChnFtrs filters, the upper-left filter corresponds to ACF's one.</p><p>The InformedHaar <ref type="bibr" target="#b42">[43]</ref> method can also be seen as a filtered channel features detector, where the filter bank (and read locations) are based on a human shape template (thus the "informed" naming). LDCF <ref type="bibr" target="#b23">[24]</ref> is also a particular instance of this framework, where the filter bank consists of PCA bases of patches from the training dataset. In sections 4 and 5 we provide experiments revisiting some of the design decisions of these methods.</p><p>Note that all the methods mentioned above (and in the majority of experiments below) use only HOG+LUV feature channels 2 (10 channels total). Using linear filters and decision trees on top of these does not allow to reconstruct the decision functions obtained when using LBP or covariance features (used by SpatialPooling and Regionlets). We thus consider the approach considered here orthogonal to adding such types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Evaluation protocol</head><p>For our experiments we use the Caltech <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref> and KITTI datasets <ref type="bibr" target="#b12">[13]</ref>. The popular INRIA dataset is considered too small and too close to saturation to provide interesting results. All Caltech results are evaluated using the provided toolbox, and summarised by log-average miss-rate (MR, lower is better) in the 10 ?2 , 10 0 FPPI range for the "reasonable" setup. KITTI results are evaluated via the online evaluation portal, and summarised as average precision (AP, higher is better) for the "moderate" setup. Caltech10x The raw Caltech dataset consists of videos (acquired at 30 Hz) with every frame annotated. The standard training and evaluation considers one out of each 30 frames (1 631 pedestrians over 4 250 frames in training, 1 014 pedestrians over 4 024 frames in testing).</p><p>In our experiments of section 5 we will also consider a 10? increased training set where every 3rd frame is used (linear growth in pedestrians and images). We name this extended training set "Caltech10x". LDCF [24] uses a similar extended set for training its model (every 4th frame). Flow Methods using optical flow do not only use additional neighbour frames during training (1 ? 4 depending on the method), but they also do so at test time. Because they have access to additional information at test time, we consider them as a separate group in our results section.</p><p>Validation set In order to explore the design space of our pedestrian detector we setup a Caltech validation set by splitting the six training videos into five for training and one for testing (one of the splits suggested in <ref type="bibr" target="#b8">[9]</ref>). Most of our experiments use this validation setup. We also report (a posteriori) our key results on the standard test set for comparison to the state of the art. For the KITTI experiments we also validate some design choices (such as search range and number of scales) before submission on the evaluation server. There we use a 2 /3+ 1 /3 validation setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Baselines</head><p>ACF Our experiments are based on the open source release of ACF <ref type="bibr" target="#b6">[7]</ref>. Our first baseline is vanilla ACF re-trained on the standard Caltech set (not Caltech10x). On the Caltech test set it obtains 32.6% MR (50.2% MR on validation set). Note that this baseline already improves over more than 50 previously published methods [2] on this dataset. There is also a large gap between ACF-Ours (32.6% MR) and the original number from ACF-Caltech (44.2% MR <ref type="bibr" target="#b6">[7]</ref>). The improvement is mainly due to the change towards a larger model size (from 30?60 pixels to 60?120). All parameter details are described in section 2.3, and kept identical across experiments unless explicitly stated.</p><p>InformedHaar Our second baseline is a reimplementation of InformedHaar <ref type="bibr" target="#b42">[43]</ref>. Here again we observe an important gain from using a larger model size (same change as for ACF). While the original InformedHaar paper reports 34.6% MR, Informed-Haar-Ours reaches 27.0% MR on the Caltech test set (39.3% MR on validation set).</p><p>For both our baselines we use exactly the same training set as the original papers. Note that the Informed-Haar-Ours baseline (27.0% MR) is right away the best known result for a method trained on the standard Caltech training set. In section 3 we will discuss our reimplementation of LDCF [24].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model parameters</head><p>Unless otherwise specified we train all our models using the following parameters. Feature channels are HOG+LUV only. The final classifier includes 4096 level-2 decision trees (L2, 3 stumps per tree), trained via vanilla discrete Adaboost. Each tree is built by doing exhaustive greedy search for each node (no randomization). The model has size 60?120 pixels, and is built via four rounds of hard negative mining (starting from a model with 32 trees, and then 512, 1024, 2048, 4096 trees). Each round adds 10 000 additional negatives to the training set. The sliding window stride is 6 pixels (both during hard negative mining and at test time). Compared to the default ACF parameters, we use a bigger model, more trees, more negative samples, and more boosting rounds. But we do use the same code-base and the same training set.</p><p>Starting from section 5 we will also consider results with the Caltech10x data, there we use level-4 decision trees (L4), and Realboost <ref type="bibr" target="#b11">[12]</ref> instead of discrete Adaboost. All other parameters are left unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Filter bank families</head><p>Given the general architecture and the baselines described in section 2, we now proceed to explore different types of filter banks. Some of them are designed using prior knowledge and they do not change when applied across datasets, others exploit data-driven techniques for learning their filters. Sections 4 and 5 will compare their detection quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InformedFilters</head><p>Starting from the Informed-Haar <ref type="bibr" target="#b42">[43]</ref> baseline we use the same "informed" filters but let free the positions where they are applied (instead of fixed in InformedHaar); these are selected during the boosting learning. Our initial experiments show that removing the position constraint has a small (positive) effect. Additionally we observe that the original InformedHaar filters do not include simple square pooling regions (? la SquaresChnFtrs), we thus add these too. We end up with 212 filters in total, to be applied over each of the 10 feature channels. This is equivalent to training decision trees over 2120 (non filtered) channel features. As illustrated in <ref type="figure" target="#fig_0">figure 2d</ref> the InformedFilters have different sizes, from 1?1 to 4?3 cells (1 cell = 6? 6 pixels), and each cell takes a value in {?1, 0, +1}. These filters are applied with a step size of 6 pixels. For a model of 60?120 pixels this results in 200 features per channel, 2 120 ? 200 = 424 000 features in total <ref type="bibr" target="#b2">3</ref> . In practice considering border effects (large filters are not applied on the border of the model to avoid reading outside it) we end up with ?300 000 features. When training 4 096 level-2 decision trees, at most 4 096 ? 3 = 12 288 features will be used, that is ?3% of the total. In this scenario (and all others considered in this paper) Adaboost has a strong role of feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checkerboards</head><p>As seen in section 2.2 InformedHaar is a strong baseline. It is however unclear how much the "informed" design of the filters is effective compared to other possible choices. Checkerboards is a na?ve set of filters that covers the same sizes (in number of cells) as InformedHaar/InformedFilters and for each size defines (see <ref type="figure" target="#fig_0">figure 2b</ref>): a uniform square, all horizontal and vertical gradient detectors (?1 values), and all possible checkerboard patterns. These configurations are comparable to InformedFilters but do not use the human shape as prior. The total number of filters is a direct function of the maximum size selected. For up to 4?4 cells we end up with 61 filters, up to 4?3 cells 39 filters, up to 3?3 cells 25 filters, and up to 2?2 cells 7 filters.</p><p>RandomFilters Our next step towards removing a hand-crafted design is simply using random filters (see figure 2c). Given a desired number of filters and a maximum filter size (in cells), we sample the filter size with uniform distribution, and set its cell values to ?1 with uniform probability. We also experimented with values {?1, 0, +1} and observed a (small) quality decrease compared to the binary option).  The design of the filters considered above completely ignores the available training data. In the following, we consider additional filters learned from data.</p><p>LDCF <ref type="bibr" target="#b23">[24]</ref> The work on PCANet <ref type="bibr" target="#b2">[3]</ref> showed that applying arbitrary non-linearities on top of PCA projections of image patches can be surprisingly effective for image classification. Following this intuition LDCF <ref type="bibr" target="#b23">[24]</ref> uses learned PCA eigenvectors as filters (see <ref type="figure" target="#fig_0">figure 2e</ref>). We present a re-implementation of <ref type="bibr" target="#b23">[24]</ref> based on ACF's <ref type="bibr" target="#b6">[7]</ref> source code. We try to follow the original description as closely as possible. We use the same top 4 filters of 10?10 pixels, selected per feature channel based on their eigenvalues (40 filters total). We do change some parameters to be consistent amongst all experiments, see sections 2.3 and 5. The main changes are the training set (we use Caltech10x, sampled every 3 frames, instead of every 4 frames in <ref type="bibr" target="#b23">[24]</ref>), and the model size (60?120 pixels instead of 32?64). As will be shown in section 7, our implementation (LDCF-Ours) clearly improves over the previously published numbers <ref type="bibr" target="#b23">[24]</ref>, showing the potential of the method. For comparison with PcaForeground we also consider training LDCF8 where the top 8 filters are selected per channel (80 filters total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PcaForeground</head><p>In LDCF the filters are learned using all of the training data available. In practice this means that the learned filters will be dominated by background information, and will have minimal information about the pedestrians. Put differently, learning filters from all the data assumes that the decision boundary is defined by a single distribution (like in Linear Discriminant Analysis <ref type="bibr" target="#b22">[23]</ref>), while we might want to define it based on the relation between the background distribution and the foreground distribution (like Fisher's Discriminant Analysis <ref type="bibr" target="#b22">[23]</ref>). In PcaForeground we train 8 filters per feature channel, 4 learned from background image patches, and 4 learned from patches extracted over pedestrians (see <ref type="figure" target="#fig_0">figure 2f</ref>). Compared to LDCF8 the obtained filters are similar but not identical, all other parameters are kept identical. Other than via PcaForeground/LDCF8, it is not clear how to further increase the number of filters used in LDCF. Past 8 filters per channel, the eigenvalues decrease to negligible values and the eigenvectors become essentially random (similar to RandomFilters).</p><p>To keep the filtered channel features setup close to InformedHaar, the filters are applied with a step of 6 pixels. However, to stay close to the original LDCF, the LDCF/PcaForeground filters are evaluated every 2 pixels. Although (for example) LDCF8 uses only ?10% of the number of filters per channel compared to Che-ckerboards4x4, due to the step size increase, the obtained feature vector size is ?40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">How many filters?</head><p>Given a fixed set of channel features, a larger filter bank provides a richer view over the data compared to a smaller one. With enough training data one would expect larger filter banks to perform best. We want thus to analyze the trade-off between number of filters and detection quality, as well as which filter bank family performs best. <ref type="figure" target="#fig_2">Figure 3</ref> presents the results of our initial experiments on the Caltech validation set. It shows detection quality versus number of filters per channel. This figure densely summarizes ?30 trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InformedFilters</head><p>The first aspect to notice is that there is a meaningful gap between Informed-Haar-Ours and InformedFilters despite having a similar number of filters (209 versus 212). This validates the importance of letting Adaboost choose the pooling locations instead of hand-crafting them. Keep in mind that InformedHaar-Ours is a top performing baseline (see ?2.2). Secondly, we observe that (for the fixed training data available) ?50 filters is better than ?200. Below 50 filters the performance degrades for all methods (as expected). To change the number of filters in InformedFilters we train a full model (212 filters), pick the N most frequently used filters (selected from node splitting in the decision forest), and use these to train the desired reduced model.</p><p>We can select the most frequent filters across channels or per channel (marked as Inf.FiltersPerChannel). We observe that per channel selection is slightly worse than across channels, thus we stick to the latter. Using the most frequently used filters for selection is clearly a crude strategy since frequent usage does not guarantee discriminative power, and it ignores relation amongst fil- ters. We find this strategy good enough to convey the main points of this work.</p><p>Checkerboards also reaches best results in the ?50 filters region. Here the number of filters is varied by changing the maximum filter size (in number of cells). Regarding the lowest miss-rate there is no large gap between the "informed" filters and this na?ve baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandomFilters</head><p>The hexagonal dots and their deviation bars indicate the mean, maximum and minimum missrate obtained out of five random runs. When using a larger number of filters (50) we observe a lower (better) mean but a larger variance compared to when using fewer filters <ref type="bibr" target="#b14">(15)</ref>.</p><p>Here again the gap between the best random run and the best result of other methods is not large. Given a set of five models, we select the N most frequently used filters and train new reduced models; these are shown in the RandomFilters line. Overall the random filters are surprisingly close to the other filter families. This indicates that expanding the feature channels via filtering is the key step for improving detection quality, while selecting the "perfect" filters is a secondary concern.</p><p>LDCF/PcaForeground In contrast to the other filter bank families, LDCF under-performs when increasing the number of filters (from 4 to 8) while using the standard Caltech training set (consistent with the observations in <ref type="bibr" target="#b23">[24]</ref>).</p><p>PcaForeground improves marginally over LDCF8. Takeaways From <ref type="figure" target="#fig_2">figure 3</ref> we observe two overall trends. First, the more filters the merrier, with ?50 filters as sweet spot for Caltech training data. Second, there is no flagrant difference between the different filter types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional training data</head><p>One caveat of the previous experiments is that as we increase the number of filters used, so does the number of features Adaboost must pick from. Since we increased the model capacity (compared to ACF which uses a single filter), we consider using the Caltech10x dataset ( ?2.1) to verify that our models are not starving for data. Similar to the experiments in <ref type="bibr" target="#b23">[24]</ref>, we also reconsider the decision tree depth, since additional training data enables bigger models.  Results for two representative methods are collected in table 1. First we observe that already with the original training data, deeper trees do provide significant improvement over level-2 (which was selected when tuning over INRIA data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1]</ref>). Second, we notice that increasing the training data volume does provide the expected improvement only when the decision trees are deep enough. For our following experiments we choose to use level-4 decision trees (L4) as a good balance between increased detection quality and reasonable training times.</p><p>Realboost Although previous papers on ChnFtrs detectors reported that different boosting variants all obtain equal results on this task <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1]</ref>, the recent <ref type="bibr" target="#b23">[24]</ref> indicated that Realboost has an edge over discrete Adaboost when additional training data is used. We observe the same behaviour in our Caltech10x setup.</p><p>As summarized in table 2 using filtered channels, deeper trees, additional training data, and Realboost does provide a significant detection quality boost. For the rest of the paper our models trained on Caltech10x all use level-4 trees and RealBoost, instead of level-2 and discrete Adaboost for the Caltech1x models.</p><p>Timing When using Caltech data ACF takes about one hour for training and one for testing. Checkerboards-4x4 takes about 4 and 2 hours respectively. When using Caltech10x the training times for these methods augment to 2 and 29 hours, respectively. The training time does not increase proportionally with the training data volume because the hard negative mining reads a variable amount of images to attain the desired quota of negative samples. This amount increases when a detector has less false positive mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Validation set experiments</head><p>Based on the results in table 2 we proceed to evaluate on Caltech10x the most promising configurations (filter type and number) from section 4. The results over the Caltech validation set are collected in table 3. We observe a clear overall gain from increasing the training data.</p><p>Interestingly with enough RandomFilters we can outperform the strong performance of LDCF-Ours. We   also notice that the na?ve Checkerboards outperforms the manual design of InformedFilters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Add-ons</head><p>Before presenting the final test set results of our "core" method (section 7), we also consider some possible "addons" based on the suggestions from <ref type="bibr" target="#b1">[2]</ref>. For the sake of evaluating complementarity, comparison with existing method, and reporting the best possible detection quality, we consider extending our detector with context and optical flow information.</p><p>Context Context is modelled via the 2Ped re-scoring method of <ref type="bibr" target="#b26">[27]</ref>. It is a post-processing step that merges our detection scores with the results of a two person DPM <ref type="bibr" target="#b10">[11]</ref> trained on the INRIA dataset (with extended annotations). In <ref type="bibr" target="#b26">[27]</ref> the authors reported an improvement of ?5 pp (percent points) on the Caltech set, across different methods. In <ref type="bibr" target="#b1">[2]</ref> an improvement of 2.8 pp is reported over their strong detector (SquaresChnFtrs+DCT+SDt 25.2% MR). In our experiments however we obtain a gain inferior to 0.5 pp. We have also investigated fusing the 2Ped detection results via a different, more principled, fusion method <ref type="bibr" target="#b40">[41]</ref>. We observe consistent results: as the strength of the starting point increases, the gain from 2Ped decreases. When reaching our Checkerboards results, all gains have evaporated. We believe that the 2Ped approach is a promising one, but our experiments indicate that the used DPM template is simply too weak in comparison to our filtered channels. Optical flow Optical flow is fed to our detector as an additional set of 2 channels (not filtered). We use the implementation from SDt <ref type="bibr" target="#b28">[29]</ref> which uses differences of weakly stabilized video frames. On Caltech, the authors of <ref type="bibr" target="#b28">[29]</ref> reported a ?7 pp gain over ACF (44.2% MR), while <ref type="bibr" target="#b1">[2]</ref> reported a ?5 pp percent points improvement over their strong baseline (SquaresChnFtrs+DCT+2Ped 27.4% MR). When using +SDt our results are directly comparable to Katamari <ref type="bibr" target="#b1">[2]</ref> and SpatialPooling+ <ref type="bibr" target="#b27">[28]</ref> which both use optical flow too.   Using our stronger Checkerboards results SDt provides a 1.4 pp gain. Here again we observe an erosion as the starting point improves (for confirmation, reproduced the ACF+SDt results <ref type="bibr" target="#b28">[29]</ref>, 43.9% ? 33.9% MR). We name our Checkerboards+SDt detector All-in-one.</p><p>Our filtered channel features results are strong enough to erode existing context and flow features. Although these remain complementary cues, more sophisticated ways of extracting this information will be required to further progress in detection quality.</p><p>It should be noted that despite our best efforts we could not reproduce the results from neither 2Ped nor SDt on the KITTI dataset (in spite of its apparent similarity to Caltech). Effective methods for context and optical flow across datasets have yet to be shown. Our main contribution remains on the core detector (only HOG+LUV features over local sliding window pixels in a single frame).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Test set results</head><p>Having done our exploration of the parameters space on the validation set, we now evaluate the most promising methods on the Caltech and KITTI test sets. Caltech test set <ref type="figure" target="#fig_5">Figures 5 and 4</ref> present our key results on the Caltech test set. For proper comparison, only methods using the same training set should be compared (see <ref type="bibr">[2,</ref>  <ref type="figure" target="#fig_2">figure 3</ref>] for a similar table comparing 50+ previous methods). We include for comparison the baselines mentioned in section 2.2, Roerei <ref type="bibr" target="#b0">[1]</ref> the best known method trained without any Caltech images, MT-DPM <ref type="bibr" target="#b41">[42]</ref> the best known method based on DPM, and SDN <ref type="bibr" target="#b20">[21]</ref> the best known method using convolutional neural networks. We also include the top performers Katamari <ref type="bibr" target="#b1">[2]</ref> and SpatialPooling+ <ref type="bibr" target="#b27">[28]</ref>. We mark as "CaltechN ?" both the Caltech10x training set and the one used in LDCF <ref type="bibr" target="#b23">[24]</ref> (see section 5).   <ref type="figure">Figure 6</ref>: Pedestrian detection on the KITTI dataset (using images only).</p><p>ments Checkerboards4x3 gets best performance given the available training data. RandomFilters reaches the same result, but requires training and merging multiple models.</p><p>Our results cut by half miss-rate of the best known convnet for pedestrian detection (SDN <ref type="bibr" target="#b20">[21]</ref>), which in principle could learn similar low-level features and their filtering.</p><p>When adding optical flow we further push the state of the art and reach 17.1% MR, a comfortable ?5 pp improvement over the previous best optical flow method (Spa-tialPooling+). This is the best reported result on this challenging dataset.</p><p>The results on the KITTI dataset confirm the strength of our approach, reaching 54.0% AP, just 1 pp below the best known result on this dataset. Competing methods (Regionlets <ref type="bibr" target="#b39">[40]</ref> and SpatialPooling <ref type="bibr" target="#b27">[28]</ref>) both use HOG together with additional LBP and covariance features. Adding these remains a possibility for our system. Note that our results also improve over methods using LIDAR + Image, such as Fusion-DPM <ref type="bibr" target="#b29">[30]</ref> (46.7% AP, not included in <ref type="figure">figure 6</ref> for clarity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Through this paper we have shown that the seemingly disconnected methods ACF, (Squares)ChnFtrs, InformedHaar, and LDCF can be all put under the filtered channel features detectors umbrella. We have systematically explored different filter banks for such architecture and shown that they provide means for important improvements for pedestrian detection. Our results indicate that HOG+LUV features have not yet saturated, and that competitive results (over Caltech and KITTI datasets) can be obtained using only them. When optical flow information is added we set the new state of art for the Caltech dataset, reaching 17.1% MR (93% recall at 1 false positive per image).</p><p>In future work we plan to explore how the insights of this work can be exploited into a more general detection architecture such as convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learned model</head><p>In figures 7 and 8 we present some qualitative aspects of the final learned models Checkerboards4x3 and RandomFilters (see results section of main paper), not included in the main submission due to space limitations.</p><p>In <ref type="figure" target="#fig_7">figure 7</ref> we compare the spatial distribution of our models versus a significantly weaker model (Roerei, trained on INRIA, see <ref type="figure">figure 5</ref> of main paper). We observe that our strong models focalize in similar areas than the weak Roerei model. This indicates that using filtered channels does not change which areas of the pedestrian are informative, but rather that at the same locations filtered channels are able to extract more discriminative information.</p><p>In all three models we observe that diagonal oriented channels focus on left and right shoulders. The U colour channel is mainly used around the face, while L (luminance) and gradient magnitude ( ? ) channels are used all over the body. Overall head, feet, and upper torso areas provide most clues for detection.</p><p>In <ref type="figure" target="#fig_8">figure 8</ref> we observe that the filters usage distribution is similar across different filter bank families.  Uniform filters are clearly the most frequently used ones (also used in methods such as (Roerei, ACF and (Squares)ChnFtrs), there is no obvious ordering pattern in the remaining ones. Please note that each decision tree will probably use multiple filters across multiple channels to reach its weak decision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) SquaresChntrs filters (b) Checkerboards filters (c) RandomFilters (d) InformedFilters (e) LDCF8 filters (f) PcaForeground filters Illustration of the different filter banks considered. Except for SquaresChntrs filters, only a random subset of the full filter bank is shown. { Red, White, Green} indicate {?1, 0, +1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Detection quality (log-average miss-rate MR, lower is better) versus number of filters used. All models trained and tested on the Caltech validation set (see ?4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Filters</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Some of the top quality detection methods for Caltech-USA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>miss-rate (lower is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Filters from Roerei (scale 1) model. Copied from [1, supplementary material] Spatial distribution of learned models. Per channel on the left, and across channels on the right. Red areas indicate pixels that influence most the decision (used by more decision trees). Figures 7b and 7c show our learned models (reach ?18% MR on Caltech test set), figure 7a show a similar visualization for a weaker model (?46% MR). See text for discussion. Filters used in our final RandomFilters model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Frequency of usage of each filter as feature for decision tree split node (independent of the feature channel). Left and right we show the top-10 and bottom-10 most frequent filters respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Caltech Checker-32.9 30.4 28 .0 31.5 Caltech10x boards 37.0 31.6 24 .7 24 .7 Effect of the training volume and decision tree depth (Ln) over the detection quality (average miss-rate on validation set, lower is better), for ACF-Ours and Checkerboards variant with (61) filters of 4?4 cells. We observe a similar trend for other filter banks.</figDesc><table><row><cell>Training</cell><cell>Method</cell><cell>L2</cell><cell>L3</cell><cell>L4</cell><cell>L5</cell></row><row><cell>Caltech Caltech10x</cell><cell>ACF</cell><cell cols="4">50.2 42 .1 48.8 52.6 49.9 44.9 41 .3 48.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ingredients to build our strong detectors (using Checkerboards4x4 in this example, 61 filters). Validation set log-average miss-rate (MR).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Effect of increasing the training set for different</cell></row><row><cell>methods, quality measured on Caltech validation set (MR:</cell></row><row><cell>log-average miss-rate).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>All-in-one which uses Cal-tech10x. CaltechN ? indicates Caltech10x for all methods but the original LDCF (see section 2.1).KITTI test setFigure 6presents the results on the KITTI test set ("moderate" setup), together with all other reported methods using only monocular image content (no stereo or LIDAR data). The KITTI evaluation server only recently has started receiving submissions (14 for this task, 11 in the last year), and thus is less prone to dataset over-fitting. We train our model on the KITTI training set using almost identical parameters as for Caltech. The only change is a subtle pre-processing step in the HOG+LUV computation. On KITTI the input image is smoothed (radius 1 pixel) before the feature channels are computed, while on Caltech we do not. This subtle change provided a ?4 pp (percent points) improvement on the KITTI validation set.?10 pp (percent points) gap between ACF/In-formedHaar and ACF/InformedHaar-Ours (see figure 5), the results of our baselines show the importance of proper validation of training parameters (large enough model size and negative samples). InformedHaar--Ours is the best reported result when training with Cal-tech1x.When considering methods trained on Caltech10x, we obtain a clear gap with the previous best results (LDCF 24.8% MR ? Checkerboards 18.5% MR). Using our architecture and the adequate number of filters one can obtain strong results using only HOG+LUV features. The exact type of filters seems not critical, in our experi-</figDesc><table><row><cell>Roerei 48.4% ACF-Caltech 44.2% MT-DPM 40.5% SDN 37.9% ACF+SDt 37.3% SquaresChnFtrs 34.8% InformedHaar 34.6% ACF-Ours 32.6% SpatialPooling 29.2% Inf.Haar-Ours 27.0% LDCF 24.8% Katamari 22.5% SpatialPooling+ 21.9% LDCF-Ours 21.4% InformedFilters 18.7% RandomFilters 18.5% Checkerboards 18.5% All-in-one 17.1% Detection quality on Caltech test set INRIA training Caltech training CaltechN ? training Optical flow Figure 5: Some of the top quality detection methods for Caltech test set (see text), and our results (highlighted with white hatch). Methods using optical flow are trained on original Caltech except our 7.1. Analysis With a KITTI Pedestrians, moderate difficulty 0 0.2 0.4 0.6 0 0.25 0.75 1 Recall 0.5 Precision</cell><cell>0.8 Regionlets 55.0% SpatialPooling 54.5% Ours-Checkboards4x3 54.0% DA-DPM 45.5% SquaresChnFtrs 44.4% DPM 38.4% SubCat 36.0%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Papers from 2004 to 2014 with "pedestrian detection" in the title, according to Google Scholar.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use "raw" HOG, without any clamping, cell normalization, block normalization, or dimensionality reduction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">"Feature channel" refers to the output of the first transformation in figure 1 bottom. "Filters" are the convolutional operators applied to the feature channels. And "features" are entries in the response maps of all filters applied over all channels. A subset of these features are the input to the learned decision forest.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Jan Hosang for the help provided setting up some of the experiments. We also thank Seong Joon Oh and Sabrina Hoppe for their useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, CVRSUAD workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pcanet: A simple deep learning baseline for image classification? In arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word channel based multiscale pedestrian detection without image resizing and using only one classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved hog descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<editor>KSE</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Additive logistic regression: a statistical view of boosting. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Survey of pedestrian detection for advanced driver assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cs-hog: Color similarity-based hog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Korea-Japan Joint Workshop on Frontiers of Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiview pedestrian detection based on vector boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<idno>ACCV. 2007. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Color attributes for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative color descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ducottet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving object detection with boosted histograms. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving object localization using macrofeature layout selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, Visual Surveillance Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit color segmentation features for pedestrian and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pedestrian detection combining RGB and dense LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using segmentation to verify object hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Histograms of sparse codes for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human detection by quadratic classification on subspace of extended histogram of gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving hog with image segmentation: Application to human detection. In Advanced Concepts for Intelligent Vision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Socarras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">New features and insights for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An hog-lbp human detector with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evidential combination of pedestrian detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust multi-resolution pedestrian detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
