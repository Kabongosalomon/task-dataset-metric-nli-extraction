<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ResNet strikes back: An improved training procedure in timm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ResNet strikes back: An improved training procedure in timm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The influential Residual Networks designed by He et al. remain the gold-standard architecture in numerous scientific publications. They typically serve as the default architecture in studies, or as baselines when new architectures are proposed. Yet there has been significant progress on best practices for training neural networks since the inception of the ResNet architecture in 2015. Novel optimization &amp; dataaugmentation have increased the effectiveness of the training recipes.</p><p>In this paper, we re-evaluate the performance of the vanilla ResNet-50 when trained with a procedure that integrates such advances. We share competitive training settings and pre-trained models in the timm open-source library, with the hope that they will serve as better baselines for future work. For instance, with our more demanding training setting, a vanilla ResNet-50 reaches 80.4% top-1 accuracy at resolution 224?224 on ImageNet-val without extra data or distillation. We also report the performance achieved with popular models with our training procedure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last decade we have witnessed significant advances in image classification, as reflected by improvement on benchmarks such as the ILSVRC'2012 challenge <ref type="bibr" target="#b34">[36]</ref> or other image classification benchmarks, which are visible on popular websites 1 . Schematically, the increase of performance reflects the maximization by the community of a problem of the form</p><formula xml:id="formula_0">accuracy (model) = f (A, T , N ),</formula><p>where A is the architecture design, T is the training setting along with its hyperparameters, and N is the measurement noise, in which we also include overfitting that typically occurs when selecting the maximum over a large set of hyper-parameters or choices of methods. Several good practices exist to mitigate N , like measuring the standard deviation with different seeds, using a separate evaluation dataset <ref type="bibr" target="#b32">[34]</ref> or evaluating models on transfer tasks. Putting aside N , measuring progress on A or T poses a challenge as both A and T progress over time. When optimizing jointly over (A, T ), there is no guarantee that the optimal choice T 1 for a given architecture A 1 remains the best for another model design A 2 . Therefore even when one compare models under the same training procedure, one may implicitly favor one model over another. One good practice to disentangle the improvement resulting from the training procedure from that of the architecture is to ensure that the baseline incorporates new "ingredients" from the literature, and to put a reasonable amount of effort in adjusting the hyperparameters. Ideally, i.e., without resource and time constraints, one would optimally adopt the best possible training procedure for each architecture</p><formula xml:id="formula_1">T (A) = max T f (A, T , N ),<label>(1)</label></formula><p>but realistically this is not possible. When comparing architectures, most papers compare their results to other reported in older publications, but for which architectures were trained with potentially weaker recipes. In the best case, the same or a similar procedure is used to compare two architectures. We are not aware of an effort specifically targeted at improving the ResNet-50 training procedure with an extensive ingredient selection and hyper-parameter search. In the literature, the performance reported on ImageNet-1k-val for this architecture ranges from 75.2% to 79.5%, depending on the paper. It is unclear whether a sufficient effort has been invested in pushing the baseline further. We want to fill this gap: in this paper, we focus on the vanilla ResNet-50 architecture 2 as described by He et al. <ref type="bibr" target="#b11">[13]</ref>, and we optimize the training so as to maximize the performance of this model for the original test resolution of 224 ? 224. We solely consider the training recipe. Therefore we exclude all variations of the ResNet-50 such as SE-ResNet-50 <ref type="bibr" target="#b18">[20]</ref> or ResNet-50-D <ref type="bibr" target="#b12">[14]</ref>, which usually improve the accuracy under the same training procedure. In summary, in this paper,</p><p>? We propose three training procedures intended to be strong baselines for a vanilla ResNet-50 used at inference resolution 224 ? 224. The three variants correspond to different numbers of epochs (100, 300 and 600) with adjustment of hyperparameters and ingredients.</p><p>? Our procedure include recent advances from the literature as well as new proposals. Noticeably, we depart from the usual cross-entropy loss. Instead, our training solves a multi-classification problem when using Mixup and CutMix: we minimize the binary cross entropy for each concept selected by these augmentations, assuming that all the mixed concepts are present in the synthetized image.</p><p>? We measure the stability of the accuracy over a large number of runs with different seeds, and discuss the overfitting issue by jointly comparing the performance on ImageNet-val versus the one obtained in ImageNet-V2 <ref type="bibr" target="#b32">[34]</ref>.</p><p>? We train popular architectures and re-evaluate their performance. We also discuss the necessity to optimize jointly the architecture and the training procedure: we showcase that having the same training procedure is not sufficient for comparing the merits of different architectures.</p><p>We provide ablations in Section 5. Our supplemental material may interest the community: Appendix A details augmentations variants that have been introduced by the timm library <ref type="bibr" target="#b1">3</ref> . Appendix B covers alternative procedures for training a ResNet-50 that significantly differ in their ingredients from our three focal training procedures. They achieve noteworthy performance and possibly better results with different architectures and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Image Classification is a core problem in computer vision. It is often employed as a benchmark task to measure progress in computer vision. Pre-trained models for image classification, particularly trained on ImageNet <ref type="bibr" target="#b7">[9]</ref>, are used in a large variety of downstream tasks like detection or segmentation. Progress in image classification generally translates to progress on these tasks.</p><p>The timm library <ref type="bibr" target="#b48">[50]</ref> has recently gained significant momentum in the scientific community as it provides implementations for numerous popular models for image classification, as well as training methods. Pre-trained weights -either adapted from originals or trained in timm with newer procedures -are included for many models. While model architectures are timm's focus, it also includes implementations of many data augmentations, regularization techniques, optimizers, and learning rate schedulers that are leveraged in the training procedures described in this paper. In many cases these implementations include functionality beyond the original implementations or papers that they were based upon. We describe these additions in Appendix A.</p><p>ResNet <ref type="bibr" target="#b11">[13]</ref> is one of the most popular image classification architectures. It was a noteworthy improvement at the time it was introduced and continues to serve as the referent architecture for some analysis <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56]</ref>, or as a baseline in papers introducing new architectures <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b55">57]</ref>.</p><p>Some works have modernized the ResNet training procedure and obtained some improvement over the original model (e.g. Dollar et al. <ref type="bibr" target="#b8">[10]</ref>). This allows a more direct comparison when considering new models or methods involving more elaborate training procedures than the one initially used. Nevertheless, improving the ResNet-50 baseline <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b52">54]</ref> was not the main objective of these works. As a consequence and as we will see, the best performance reported so far with a ResNet-50 is still far from the maximum performance (peak or average) that one can achieve with this architecture. In this paper, our goal is to offer the best possible training procedure that we could find for the ResNet-50 based on existing ingredients and practices. We hope that it will serve as a strong baseline for subsequent works. Note, some papers have also focused on ResNet-50 training <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b47">49]</ref>, but they have either modified the architecture or changed the resolution, which does not allow for a direct comparison to the original ResNet-50 at resolution 224?224. For instance, Lee et al. <ref type="bibr" target="#b25">[27]</ref> use ResNet-D <ref type="bibr" target="#b12">[14]</ref> with SE attention <ref type="bibr" target="#b18">[20]</ref>. Bello et al. <ref type="bibr" target="#b0">[2]</ref> also optimize ResNet without architectural changes, but they don't report competitive results for ResNet-50 at 224?224.</p><p>Training ingredients &amp; recipes for image classification have significantly evolved since the inception of AlexNet <ref type="bibr" target="#b24">[26]</ref>. Several trends have changed over time. Common modifications include replacing the waterfall schedule (classical division by 10 of learning rate every 30 epochs) by a longer and more progressive schedule <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b50">52]</ref>. Increasing jointly the number of epochs <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b43">45]</ref> and the batch size while using mixed precision better leverages powerful GPUs. Modern procedures make use of stronger data-augmentation <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b56">58]</ref>, stronger regularization <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b35">37]</ref>, weight averaging <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31]</ref> and correct the train-test resolution discrepancy <ref type="bibr" target="#b46">[48]</ref> by differentiating the train from the test resolution <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b9">11]</ref>. Different losses have also been experimented with <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b21">23]</ref> even if cross-entropy remains the standard. For the optimization, SGD with Nesterov momentum <ref type="bibr" target="#b36">[38]</ref> is a common default for CNNs. RMSProp is also used for specific CNN architecture families like in Inception <ref type="bibr" target="#b37">[39]</ref>, NASNet <ref type="bibr" target="#b57">[59]</ref>, AmoebaNet <ref type="bibr" target="#b31">[33]</ref>, MobileNet <ref type="bibr" target="#b17">[19]</ref>, EfficientNet <ref type="bibr" target="#b39">[41]</ref> .For training image classifiers based on transformers <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b43">45]</ref> and MLP <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>, AdamW <ref type="bibr" target="#b26">[28]</ref> and Lamb <ref type="bibr" target="#b51">[53]</ref> optimizers are popular choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Procedures</head><p>We offer three different training procedures with different costs and performance so as to cover different use-cases, see <ref type="table">Table 1</ref> for resource usage and corresponding accuracies. Our procedures target the best performance of ResNet-50 when tested at resolution 224 ? 224. We have explored numerous variations with different optimizers, choice of regularization, and a reasonable amount of grid search for the hyperparameters. We refer the reader to Section 4.2 for control experiments on quantifying the amount of overfitting. See Section 4.1 in the Appendix for the exact ingredient list and parametrization. We focus on three different operating points:</p><p>Procedure A1 aims at providing the best performance for ResNet-50. It is therefore the longest in terms of epochs (600) and training time (4.6 days on one node with 4 V100 32GB GPUs).</p><p>Procedure A2 is a 300 epochs schedule that is comparable to several modern procedures like DeiT, except with a larger batch size of 2048 and other choices introduced for all our recipes.</p><p>Procedure A3 aims at outperforming the original ResNet-50 procedure with a short schedule of 100 epochs and a batch size 2048. It can be trained in 15h on 4 V100 16GB GPUs and could be a good setting for exploratory research or studies.</p><p>Note, Section B gives alternative training procedures that may serve as interesting choices when considering other models. In the rest of this section, we focus on the ingredients included in A1-A3.</p><p>Loss: multi-label classification objective. Mixup and CutMix augmentation synthesize an image from several images having in most cases different labels. By using crossentropy, the output is implicitly treated as a probability of presence of each of the mixed concepts. In our training, we assume instead that these concepts are all present, and treat the classification as a multi-label classification problem (1-vs-all). For this purpose, we adopt the binary cross-entropy (BCE) loss instead of the typical cross-entropy (CE). This loss is consistent with the Mixup and CutMix data augmentation: The targets are defined for each class to 1 (or 1 ? ? with smoothing) if the class is selected by Mixup or Cutmix, independent of other classes. Over the best settings that we have explored, BCE slightly outperforms cross-entropy in their best respective configurations. We point out that Beyer et al. <ref type="bibr" target="#b2">[4]</ref> previously adopted BCE with the motivation to produce multiple non-exclusive labels, and obtained excellent results with it. But to the best of our knowledge they did not use it with CutMix or Mixup as we propose to do.</p><p>In our experiments, even when using BCE, setting all mixed concepts with a target to 1 (or 1 ? ?) is more effective than considering a distribution of concepts that sum to 1. Conceptually we believe it is more aligned with what Mixup and CutMix are actually doing: it is likely that a human could recognize each of two mixed concepts.</p><p>Data-Augmentation. We adopt the following combination of data augmentations: on top of standard Random Resized Crop (RRC) and horizontal flip (commonly used since GoogleNet <ref type="bibr" target="#b38">[40]</ref>), we apply timm <ref type="bibr" target="#b48">[50]</ref> variants of RandAugment <ref type="bibr" target="#b6">[8]</ref>, Mixup <ref type="bibr" target="#b54">[56]</ref>, and CutMix <ref type="bibr" target="#b53">[55]</ref>. This combination was used for instance in DeiT <ref type="bibr" target="#b43">[45]</ref>. Many of the model weights in timm have also been trained with RandAugment and Mixup, but with Random Erasing <ref type="bibr" target="#b56">[58]</ref> and increased regularization instead of CutMix. We refer the reader to Appendix A for more details about the variants offered in timm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization.</head><p>Across our three training procedures, regularization differs the most. In addition to adapting the weight decay, we use label smoothing, Repeated-Augmentation <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b15">17]</ref> (RA) and stochastic-Depth <ref type="bibr" target="#b19">[21]</ref>. We use more regularization for longer training schedules. For instance we adopt label smoothing only for A1. Both RA and stochastic depth tend to improve the results at convergence, but they slow down the training in the early stages as reported by Berman et al. <ref type="bibr" target="#b1">[3]</ref> for RA. For short schedules they are therefore less effective or even detrimental, which is why we adopt them only with A1 and A2. Note that for other architectures, or larger ResNets, it is beneficial to add additional regularization, therefore one would have to adapt the corresponding hyper-parameters for such architectures. For instance, for a ResNet-152 the performance increases from 81.8% to 82.4% on Imagenet-val by putting more of Ran-dAugment, mixup and stochastic depth regularization on top of A2 recipe. At resolution 256?256 this model obtains 82.7%, which is above the accuracy (82.2%) reported by Bello et al. <ref type="bibr" target="#b0">[2]</ref> for a ResNet-200 before architectural changes (Table 1 in their paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization.</head><p>Since AlexNet, the most used optimizer to train convnets is SGD. In contrast transformers <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b44">46]</ref> and MLP <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref> use AdamW <ref type="bibr" target="#b26">[28]</ref> or LAMB optimizer. Dosovitskiy et al. <ref type="bibr" target="#b9">[11]</ref> report similar performance between AdamW <ref type="bibr" target="#b26">[28]</ref> and SGD for ResNet-50. This concurs with our observations for intermediate batch sizes (e.g., 512). We use larger batches, e.g., 2048. When combined with repeated augmentation and the binary cross entropy loss, we found that LAMB <ref type="bibr" target="#b51">[53]</ref> makes it easier to consistently achieve good results. We found it difficult to achieve convergence when using both SGD and BCE. We therefore focus on LAMB with cosine schedule as the default optimizer for training our ResNet-50. Alternative training procedures using different optimizer, loss, augmentation, and regularization combinations can be found in Appendix B. <ref type="table" target="#tab_1">Table 2</ref> we compare different recipes used to train vanilla ResNet-50 to ours. We consider only the results with the unmodified ResNet-50 architecture. We have chosen a wide range of training procedures to try to be as representative as possible but obviously it cannot be exhaustive. We do not consider approaches using advanced training settings like distillation, or models pre-trained self-supervised or with pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of our ingredients and comparison to existing training procedures. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we first compare our training procedure to existing ones and evaluate them with different architectures. Importantly, we discuss the significance of our re-  sults with experiments that aim at (1) quantifying the sensitivity of the performance to random factors; (2) evaluating the overfitting by measuring on a different test set. <ref type="table">Table 1</ref> summarizes the main characteristics of our training procedure. To the best of our knowledge, our procedure A1 surpasses the current state of the art on ImageNet with a vanilla ResNet-50 architecture at resolution 224?224. Our other procedures A2 and A3 achieve lower but still high performance with less resources.</p><formula xml:id="formula_2">SGD-M SGD-M SGD-M AdamW SGD-M LAMB LAMB LAMB LR 0.1 0.1 0.2 1 ? 10 ?3 2.0 5 ? 10 ?3 5 ? 10 ?3 8 ? 10 ?3 LR</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of training procedures for ResNet-50</head><p>Performance comparison with other architectures. In <ref type="table">Table 3</ref> we report the performance obtained when training different architectures with our training procedures. This allows us to see how well they generalize to other models. Or procedures improves the performance of several models over results reported in the literature, in particular older ones and/or those most comparable to ResNet-50 in terms of architecture and size. In some cases like ViT-B, we observe that A2 is better than A1, which suggests that the hyper-parameters are not adapted to longer schedules (typically requiring more regularization). For instance, the A2 training recipe achieves 81.8% top-1 accuracy when training a ResNet-152, but by increasing a bit the regularization we improved it to 82.4% at resolution 224?224, which translates to 82.7% when evaluated at resolution 256?256.</p><p>In <ref type="table">Table 3</ref>, we compare the performance and resources associated with our 3 training recipes when using them to train other architectures. We complement these results with <ref type="table">Table 4</ref>, where we additionally include the performance and efficiency on ImageNet-1k, ImageNet-V2 and ImageNet-Real for different architectures trained with our best performing A1 training recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Significance of measurements: seed experiments</head><p>For a fixed set of choices and hyper-parameters, there is some inherent variability on the performance due to the presence of random factors in several stages. It is the case for the weight initialization, but also for the optimization procedure itself. For instance the order in which the images are fed to the network through batches depends on a random generator. This variability raises the question of the significance of accuracy measurements. For this purpose, we measure the distribution of performance when changing the random generator choices. This is conveniently done by changing the seed, as previously done by Picard <ref type="bibr" target="#b28">[30]</ref>, who concludes to the exist of outliers significantly outperforming or underperforming the average outcome of a traing procedure. In <ref type="figure">Figure 1</ref>, we report several statistics on the performance with the A2 training procedure when considering 100 distinct seeds (from 1 to 100, note that we have used seed=0 in all other experiments). In these experiments, we focus on the performance reached at the end of the training: we do not select the maximum obtained by intermediate checkpoints in the last epochs. This would have a similar effect as a seed selection, but the measures would not be IID and less disentangled from the training duration itself.</p><p>The standard deviation is typically around 0.1 on ImageNet-val, see <ref type="figure">Figure 1</ref>. This concurs with statistics reported in the literature for ResNet and other convnets <ref type="bibr" target="#b30">[32]</ref>. The variance is higher on ImageNet-V2 (std=0.23), which consists of a smaller set (10000 vs 50000 for -val) of images not present in the validation set. The mean 79.72% shows that our main weights (seed 0) overestimates the average performance by about +0.13%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peak performance and control of overfitting</head><p>To prevent to over-estimate too much the accuracy on validation, during our exploration process we have selected only the <ref type="table">Table 3</ref>: Comparison on ImageNet classification between other architectures trained with our ResNet-50 optimized training procedure without any hyper-parameters adaptation. In particular, our procedure must be adapted for deeper/larger models, which benefit from more regularization. For the training cost we report the training time (time) in hours, the number of GPU used (#GPU) and the peak memory by GPU (Pmem) in GB. For A1 and A2, we adopt the same training and test resolution as in the original publication introducing the architecture. For A3 we use a smaller training resolution to reduce the compute-time. ? : torchvision [1] results. * : DeiT <ref type="bibr" target="#b43">[45]</ref> results. final checkpoint and we use relatively coarse grid for hyper-parameters search to prevent introducing an additional seed effect. However optimizing over a large number of choices typically leads to overfitting. In <ref type="figure">Figure 1</ref>, we observe that the maximum (or peak performance) is close to 80.0% with the A2 training procedure. Note, <ref type="figure">Figure 2</ref> provides the distribution of accuracy as an histogram; One question is whether this model is intrinsically better than the average ones, or if it was just lucky on this particular measurement set. To attempt to answer this question, we measure how the performance transfers to another measurement dataset: we compute for all the seeds the couples (ImageNet-val top-1 acc., ImageNet-V2 top-1 acc.), and plot them as a point cloud in <ref type="figure">Figure 1</ref>. We observe that the correlation between the performance on ImageNet-val and -V2 is limited. Noticeably the best performance is not achieved by the same seed on the two datasets. This observation suggests some significant measurement noise, which advocates to report systematically the performance on different datasets, and more particularly one making a clear distinction between validation and test.</p><formula xml:id="formula_3">A1-A2-org. A3 Cost ImageNet-1k-val train test train test A1 A2 A1-A2 A3 A1 A2 A3 org.</formula><p>More on sensitivity analysis: variance along epochs. <ref type="figure" target="#fig_0">Figure 3</ref> shows how the performance variability evolves along epochs. <ref type="table">Table 4</ref>: Performance of models trained with A1 training procedure. We measure peak memory and throughput on one GPU V100 32GB with batch size 128, FP16 precision and test resolution from <ref type="table">Table 3</ref>. Note that the throughput is indicative, since it depends on the GPU hardware, the software that runs the models, and other factors like the adjustment of batch size (we keep it fix in this table </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transfer Learning</head><p>In <ref type="table" target="#tab_7">Table 5</ref> we provide transfer learning performance on seven fine grained dataset with our different pre-training procedures, and provide a comparison with the default Py-Torch pre-training. For each pre-training we use exactly the same fine-tuning procedure inspired by the fine-tuning procedure used in DeiT <ref type="bibr" target="#b43">[45]</ref>. For each dataset we adapt the fine-tuning hyper-parameters. We observe that the fine-tuning tend to smooth the difference of performance on certain datasets, such as CIFAR or Stanford Cars. Overall our A1 procedure leads to the best performance on downstream tasks, but the performance of the Pytorch default and A2 tend to be similar, while on Imagenet-val and -v2 A2 was significantly better. A3 is significantly inferior on downstream tasks, which may be related to the lower training resolution at 160?160.  <ref type="figure">Figure 2</ref>: Distribution of the performance on ImageNet-val with the A2 procedure. It is measured with 100 different seeds. We also depict the Gaussian-fit of this distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparing architectures and training procedures: a show-case of contradictory conclusions</head><p>In this paragraph we case how difficult it is to compare two architectures, even under the same training procedure, or conversely how it is difficult to compare different procedures with a single architecture. We choose ResNet-50 and DeiT-S. The latter <ref type="bibr" target="#b43">[45]</ref> is essentially a ViT parameterized so that it has approximately the same number of parameters as a ResNet-50. For each architecture, we have put a significant effort in optimizing the procedure to maximize the performance on Imagenet-val with the same 300 epochs training schedule and same batch size. Under this constraint, the best training procedure that we have designed for ResNet-50 is A2. We denote by T2 the corresponding training procedure for DeiT-S. Note that this training procedure achieves a significantly better performance on Imagenet-val than the one initially proposed for DeiT-S (80.4% versus 79.8% in the original paper).   As one can see, by choosing the procedure optimized for any of the two architectures, one may conclude that this architecture is better based on ImageNet-val accuracy: with A2 training, ResNet50 is better than DeiT-S, with T2 training, DeiT-S is better than ResNet50. The measurements on ImageNet-v2 would lead to a different conclusion, as DeiT-S is better for both procedure. But even in that case, by focusing on A2 one may conclude that the difference between ResNet-50 and DeiT-S with A2 training is not statistically significant: 67.9% vs 68.1%. Conversely, if the goal is to compare A2 to T2, we could draw different conclusions on ImageNet-val if considering a single architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablations</head><p>In this section we provide a few ablations of hyper-parameters or selection of ingredients. Some modifications are difficult to ablate individually since they require to re-adjust several other parameters to work properly. This is the case in particular of the optimizer, which strongly interacts with other choices and hyper-parameters. In Appendix B we provide alternative training procedures that we have developed for other optimizers: RMSProp, SGD and AdamP. <ref type="table" target="#tab_9">Table 6</ref> we provide an ablation of major ingredients. We focus on the intermediate A2 training procedure as it is a good compromise between compute-cost and accuracy. We make the following observations:  formance. However increasing it further increases the risk of divergence. We have typically set the weight decay in the range [0.02, 0.03] that we have identified in our preliminary exploration. This parameter is a bit sensitive and can interact with other forms of regularization. In some cases we observe significant differences between 0.02 and 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main ingredients and hyper-parameters. In</head><formula xml:id="formula_4">?</formula><p>? Loss: Binary Cross Entropy versus Cross Entropy. In this ablation, moving back from how we use BCE to the vanilla CE loss significantly reduces the performance. As discussed in our main paper, we use the flexibility of BCE to regard Mixup/Cutmix as activating a multi-class 1-vs-all classification problem as discussed in our paper, as opposed to the choice of enforcing probabilities that sum to 1. If we enforce probabilities to sum to 1 as implicitly done with cross-entropy, we obtain a slightly lower accuracy as reported in <ref type="table">Table 8</ref>. By itself, i.e., with the same target, we do not conclude that BCE is necessarily better than CE. But it is with that loss that we reach the configuration with the highest accuracy overall.</p><p>? Repeated augmentation is providing a small boost in this ablation. This augmentation has some complex interaction with other hyper-parameters, and is not well understood in our opinion. In some cases we observed that it was neutral or detrimental, for instance with shortest schedules (A3 procedure), or in <ref type="table">Table 8</ref> with higher values of the Mixup parameter. Overall, it was best to include this ingredient in our most accurate procedures A1 and A2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Depth &amp; Smoothing.</head><p>We have included stochastic depth in the A1 and A2 training procedures. In <ref type="table" target="#tab_11">Table 7</ref> we observe that it provides an improvement for A2 compared to setting the drop-rate to 0 (i.e., no stochastic depth), not for A3. Label smoothing is not effective at 300 epochs with other hyper-parameters and ingredients fixed. This is why we only use it for the longer 600-epoch schedule in A1, where our exploration concluded that it has a positive impact.</p><p>Augmentation. <ref type="table">Table 8</ref> evidences the role of augmentations when we modify a few parameters (of Mixup and RandAugment): each modification that we have done has some impact on the measured score. While it would be unrealistic (and not ecological) to ensure that all our choices are statistically significant, one can observe that all modifications in this table decrease the top-1 accuracy below the average performance (79.72% -std 0.1) that we report over 100 seeds in <ref type="figure">Figure 1</ref>.  <ref type="table">Table 8</ref>: Ablation of some data-augmentation choices for our training procedure A2 on Imagenetval, all computed with "Seed 0". The first row contains our default choices, see <ref type="table" target="#tab_1">Table 2</ref> for the full set of hyper-parameters. Each other row corresponds to an ablation for which we have changed only one or two hyper-parameters or ingredient. Activating "BCE target" is our default. It refers to our choice to regard Mixup/Cutmix as activating a multi-class classification 1-vs-all problem as discussed in our paper. Not using it means that we also use BCE, but we enforce the probabilities of the concepts sum to 1 as with the regular cross-entropy loss.</p><p>Crop-ratio. We evaluate the influence of the crop-ratio used at inference time. The one most commonly adopted in the literature is 0.875. Recently researchers have considered larger values for this parameter, noticeably for vision transformers after significant gains were reported by the author of the timm library with these models. <ref type="table">Table 9</ref> provides an analysis as a function of this parameter.</p><p>Evaluation at other resolutions. While we primarily focus on the performance when inferring at resolution 224 ? 224, we also evaluate our models when feeding images at larger resolutions. We report these results in <ref type="figure">Figure 4</ref>, where we see that the models trained with A1 and A2 have a better performance when used at higher resolutions. A1 A2 A3</p><p>crop-ratio mean (std) max -min seed 0 mean (std) max -min seed 0 mean <ref type="bibr">(std)</ref>   <ref type="table">Table 9</ref>: Ablation of the crop-ratio when training with A1. We compute the Imagenet-val top-1 accuracy as a function of this parameter for 10 different seeds, for ResNet50 trained with our procedures. Our selection of 0.95 was based on Seed 0 in early experiments. It is comparable but not statistically better than the standard 0.875. Note that we have one A1 seed that leads to a top 80.54% top-1 accuracy at crop-ratio 0.9. We regard it as being overfit and therefore we do not recommend to report this number. ImageNet Top-1(%) A1 A2 A3 <ref type="figure">Figure 4</ref>: We compare ImageNet Top-1 accuracy according to the test resolution for our three training procedures A1, A2 and A3 with ResNet-50 architecture. Our training procedure and models also benefit from the FixRes effect <ref type="bibr" target="#b46">[48]</ref>: the performance increases when using a larger image at test time for the procedures A1 and A2. This observation is not true for A3, which is expected since this procedure was already relying on feeding smaller images at train time, so as to maximize the accuracy at test resolution 224 ? 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have proposed new training procedures for a vanilla ResNet-50. We have integrated new ingredients and put a significant effort in exploring diverse procedures under different resource constraints. As a result, we have established the new state of the art for training this gold-standard model. We have two other procedures to train strong ResNet-50 with less compute power. Nevertheless, we do not claim that our procedures are universal, quite the opposite: the architecture and training should be optimized jointly. Our procedure is not ideal for training other models: while, on some models, our training recipes lead to excellent results outperforming those reported in the literature, they exhibit suboptimal performance on others, typically for deeper architectures that require more regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>This supplemental material provides complementary results referred in the main document, noticeably a presentation of the augmentation and regularization specificity in timm and some alternative training procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Augmentations and Regularization in timm [50]</head><p>The timm library includes a variety of image augmentations, regularization techniques, optimizers, and learning rate schedulers that can be used to produce leading results on ImageNet classification and other 2D image tasks. Many timm training components have modifications and improvements from original implementations or papers describing them. One should be aware of these changes if using them.</p><p>Data Augmentation in timm includes implementations of RandAugment <ref type="bibr" target="#b6">[8]</ref>, Au-toAugment <ref type="bibr" target="#b5">[7]</ref>, AugMix <ref type="bibr" target="#b13">[15]</ref>, Random Erasing <ref type="bibr" target="#b56">[58]</ref>, and an integrated implementation of Mixup <ref type="bibr" target="#b54">[56]</ref> and CutMix <ref type="bibr" target="#b53">[55]</ref>. The base for all augmentations is typically Random Resized Crop with horizontal flipping.</p><p>RandAugment is the most used of the AA (AutoAugment) variants in timm -it also contains the most significant additions from the original paper and Tensorflow based implementations -so we will focus on that implementation. The original RandAugment specification has two hyper-parameters, M and N; where M is the distortion magnitude and N is the number of distortions uniformly sampled and applied per-image. The goal of RandAugment was that both M and N be human interpretable. However, that ended up not being the case for M. The scales of several augmentations were backwards or not monotonically increasing over the range such that increasing M does not increase the strength of all augmentations. This is most visible for image enhancement blending operations (color, contrast, brightness, sharpness) where the argument value defines the behavior as follows:</p><p>0. selects the degenerate image The implementation in timm attempts to improve this situation by adding an 'increasing' mode (always enabled for recipes in this paper) where all augmentation strengths increase with magnitude; solarize and posterize increase with M (instead of decrease), and interpolation vs extrapolation for the blending operations is randomly chosen with a strength that increases with M. This makes increasing M more intuitive and allows an additional hyper-parameter to work well: timm adds a MSTD parameter which adds gaussian noise with the specified standard deviation to the M value per distortion application. Additionally, if MSTD is set to '-inf', M is uniformly sampled from 0-M for each distortion. Without correcting the scales, one would often end up with completely empty or heavily inverted images in ranges of M that are supposed to be low in strength.</p><p>Care was taken in timm's RandAugment to reduce impact on image mean, the normalization parameters can be passed as a parameter such that all augmentations that may introduce border pixels can use the specified mean instead of defaulting to 0 or a hard-coded tuple as in other implementations. And lastly, Cutout is excluded by default to favour separate use of timm's Random Erasing implementation which has less impact on mean and standard deviation of the augmented images.</p><p>Random Erasing is another commonly used timm augmentation with modifications from the original paper. The implementation in timm follows the original but allows 'erasing' image regions with per-pixel gaussian noise (mean 0, std 1.0) instead of a uniform random or constant color (black or image mean) per-region. When applied to images at the recommended location in the augmentation pipeline -after images have been normalized (standardized) -this maintains image statistics and allows better results with stronger application of the augmentation. A count parameter was also added to timm's Random Erasing such that multiple regions can be erased per-image. Regularization in timm is standard. It allows use of similar regularization for many of the included models. Weight decay is available via either native PyTorch or timm optimizers. The ability to enable pre-classifier dropout is included in all model architectures. Stochastic-Depth has been added as an option to many of the most popular model architectures (via a layer named DropPath). Label-smoothing is included via a cross-entropy loss function and possible to use in combination with the label manipulation of CutMix and Mixup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixup and CutMix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Alternative Training Procedures</head><p>The main training recipes in this paper uses the LAMB optimizer. Several sets of hyperparameter variations with differing training costs were presented with leading results for the vanilla ResNet-50 architecture. Here, we introduce alternative training recipes that also produce results matching or exceeding the best existing ResNet-50. The reader may find these are better suited for use or adaptation for their specific model architecture, dataset, or task. The alternative recipes are:  Procedure C -SGD with Nesterov's momentum, Adaptive Gradient Clipping, and a cosine learning rate decay. We have two variants of it (C1 and C2) depending on whether we use repeated augmentation or not;</p><formula xml:id="formula_5">Procedure</formula><p>Procedure D -AdamP with a cosine learning rate decay and binary cross-entropy.</p><p>The above procedures have been used to product excellent results for many pretrained models in the timm library, including many non-ResNet architectures. <ref type="table" target="#tab_16">Table 10</ref> summarizes their best ResNet-50 oriented settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training procedure B details (RMSProp)</head><p>This procedure is inspired by the Ran-dAugment <ref type="bibr" target="#b6">[8]</ref> recipes used to train EfficientNet architectures but leverages features in timm's implementation of RandAugment and Random Erasing. The step decay has been adjusted to decay every epoch (instead of every 2.4 as with EfficientNet, weight decay has been slightly decreased from EfficientNet defaults, and the learning rate is a bit higher. Additional augmentation was added in the form of per-pixel noise Random Erasing and Mixup. It should be noted that the RMSProp optimizer used is the rmsprop tf implementation in timm which carefully matches behaviours of the Tensorflow (before version 2.0) implementation. The native PyTorch RMSProp implementation will not produce the same results, even if adjusting for the epsilon location.</p><p>With long decay constants for the EMA weight averaging, it can be beneficial to perturb the learning rate (currently once per epoch) with noise in later stages of training (typically 40-50% of the way through until the end). In exploration so far, learning rate noise appears to increase sensitivity of training results to random seed but has often produced the best result in (so far, limited) sweeps with the same hyper-parameters. Further analyzing the interplay between learning rate value, schedule, and noise, EMA decay constant, and random seed is a future objective for refining this training recipe.</p><p>This training strategy varies somewhat in effectiveness with batch size. Running experiments for this paper with larger batch sizes in the 1024-2048 range has often come slightly below (0.1 to 0.3 top-1) prior training runs with smaller sizes in the 256-768 range used for numerous timm pre-trained weights. It is unclear if this can be addressed with further hyper-parameter adjustments and different learning rate scaling (linear used by default) across batch sizes.</p><p>See <ref type="table" target="#tab_17">Table 11</ref> for a summary of the procedure, including ranges of recommended of values to search over for applying to different classification task and architecture combinations. For larger model architectures it is advisable to focus on stronger augmentation and regularization values within the suggested ranges. Looking at <ref type="table">Table 3</ref>, the original results for the timm specific EfficientNetV2-S <ref type="bibr" target="#b40">[42]</ref> variant and ECA-ResNet-269-D were trained using this procedure, but with higher levels of augmentation and regularization than for ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training procedure C details (SGD with Nesterov's momentum and AGC)</head><p>This recipe is based on the published procedure for training NFNets <ref type="bibr" target="#b3">[5]</ref>: using SGD with Nesterov's momentum, Adaptive Gradient Clipping <ref type="bibr" target="#b3">[5]</ref> (AGC), and heavy augmentation and regularization. AGC allows for stable large batch training at higher learning rates. Stronger default augmentation and regularization make up for the loss of batch normalization's regularizing effect when paired with NFNets, but strengths can be relaxed when used with other architectures that use batch normalization. With some adjustments, this procedure has been useful training architectures in timm such as ECA-NFNet, ResNet, and EfficientNet variants to impressive performance levels. The original result for the timm EfficientNetV2-M <ref type="bibr" target="#b40">[42]</ref> variant in <ref type="table">Table 3</ref> was trained with the C.1 recipe, but with significantly higher augmentation and regularization than for ResNet-50.</p><p>The C.1 vs C.2 versions of this procedure seen in <ref type="table" target="#tab_16">Table 10</ref> differ most significantly in the application of Repeated Augmentation. It should be noted that a shorter training length of 600 epochs also works quite well in both cases, with an expected drop of roughly 0.15-0.2 top-1 for the same seed. <ref type="table" target="#tab_1">Table 12</ref> includes ranges of the ingredients for exploring with different tasks and architectures.</p><p>Training Procedure D details (AdamP) Late in the process for this report a training trial using AdamP <ref type="bibr" target="#b14">[16]</ref> showed promise. With limited runs so far a recipe based on AdamP has achieved a 79.8 top-1 on ImageNet-1k. Further experimentation is necessary, the trials so far were run at a comparatively small batch size, but the promising results warrant exploration. Note that unlike RMSProp or SGD (but similar to Adam and LAMB), it is recommended to use square root scaling when adjusting the learning rate for this recipe across different batch sizes. <ref type="table" target="#tab_19">Table 13</ref> contains the recommended ranges for the ingredients of this recipe. These ranges have not been explored extensively across different model architectures as with procedures B and C.</p><p>Other Recipes Undoubtedly, other training recipes with different combinations of optimizer, learning rate schedule, augmentation, and regularization exist that can match or surpass the performance of the procedures detailed in this report. Ingredients aside, putting in the time and effort to tune the recipe with the target architecture is key. The authors already have an AdamW recipe in the works that is looking promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommended Range</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-50</head><p>Epochs 400-700 600</p><p>Initial LR (per batch size 256) .01-.025 0.0225 LR Schedule</p><p>Step</p><p>Step     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>We show how the mean, standard deviation, minimum and maximum of the top-1 accuracy on ImageNet-val evolves during training with the A2 procedure (ResNet-50 architecture). (Left) For all 300 training epochs. (Right) Same but for the last epochs. We note that the variance in accuracy is high at the beginning, see for instance at epoch 100, where the difference in performance can be as large as 10% in accuracy. Towards the end of the training, most of the networks converge to similar values and the range significantly decreases in the last 50 epochs. Credit: this figure and experiment was inspired by Picard<ref type="bibr" target="#b28">[30]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0.-1.0 interpolates between the degenerate and original image 1.0 returns original image &gt; 1.0 extrapolates the original image way from the degenerate Taking sharpness as an example, magnitudes of M0, M5, and M10 are mapped in the original implementation to produce strong blurring (0.1), no-change (1.0), or strong sharpening (1.9) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>are cleanly integrated in timm in a manner not common in other implementations. Both can be enabled at the same time with a variety of different mixing strategies: batchwise CutMix vs Mixup selection, lambda and CutMix region sampling performed per-batch pairwise mixing, lambda, and region sampling performed per mixing sample pair within batch elementwise mixing, lambda, and region sampling performed per sample within batch half the same as elementwise but one of each mixing pair is discarded so that each sample is seen once per epoch The default is to use either CutMix or Mixup with probability of 0.5 per-batch if both are enabled -this is the case for all mentioned training procedures in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ingredients and hyper-parameters used for ResNet-50 training in different papers. We compare existing training procedures with ours.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Previous approaches</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell cols="9">Procedure ? A3 Epochs ResNet PyTorch FixRes DeiT FAMS (?4) A1 A2 90 90 120 300 400 600 300 100</cell></row><row><cell># of forward pass</cell><cell>450k</cell><cell>450k</cell><cell>300k</cell><cell>375k</cell><cell>500k</cell><cell>375k</cell><cell>188k</cell><cell>63k</cell></row><row><cell>Batch size</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>1024</cell><cell>2048</cell><cell>2048</cell><cell>2048</cell></row><row><cell>Optimizer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on transfer-learning tasks for different pre-training recipes.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Train size Test size #classes Pytorch [1]</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell></row><row><cell>ImageNet-val [36]</cell><cell>1,281,167</cell><cell>50,000</cell><cell>1000</cell><cell>76.1</cell><cell cols="2">80.4 79.8 78.1</cell></row><row><cell>iNaturalist 2019 [18]</cell><cell>265,240</cell><cell>3,003</cell><cell>1,010</cell><cell>73.2</cell><cell cols="2">73.9 75.0 73.8</cell></row><row><cell>Flowers-102 [29]</cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell><cell>97.9</cell><cell cols="2">97.9 97.9 97.5</cell></row><row><cell>Stanford Cars [24]</cell><cell>8,144</cell><cell>8,041</cell><cell>196</cell><cell>92.5</cell><cell cols="2">92.7 92.6 92.5</cell></row><row><cell>CIFAR-100 [25]</cell><cell>50,000</cell><cell>10,000</cell><cell>100</cell><cell>86.6</cell><cell cols="2">86.9 86.2 85.3</cell></row><row><cell>CIFAR-10 [25]</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell><cell>98.2</cell><cell cols="2">98.3 98.0 97.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Learning rate and Weight Decay. The learning rate has an important effect on performance. The higher value 5.10 ?3 presented in this table leads to the best per-</figDesc><table><row><cell>loss</cell><cell>LR</cell><cell>WD RA</cell><cell>A2</cell></row><row><cell cols="3">BCE 2 ? 10 ?3 0.02</cell><cell>78.24</cell></row><row><cell cols="3">BCE 2 ? 10 ?3 0.03</cell><cell>78.47</cell></row><row><cell cols="3">BCE 3 ? 10 ?3 0.02</cell><cell>79.16</cell></row><row><cell cols="3">BCE 3 ? 10 ?3 0.03</cell><cell>79.28</cell></row><row><cell cols="3">BCE 5 ? 10 ?3 0.01</cell><cell>79.66</cell></row><row><cell cols="3">BCE 5 ? 10 ?3 0.02</cell><cell>79.85</cell></row><row><cell cols="3">BCE 5 ? 10 ?3 0.03</cell><cell>79.73</cell></row><row><cell cols="3">BCE 8 ? 10 ?3 0.02</cell><cell>79.63</cell></row><row><cell cols="3">BCE 3 ? 10 ?3 0.02</cell><cell>78.74</cell></row><row><cell cols="3">BCE 5 ? 10 ?3 0.02</cell><cell>79.57</cell></row><row><cell cols="3">BCE 5 ? 10 ?3 0.03</cell><cell>79.58</cell></row><row><cell>CE</cell><cell cols="2">2 ? 10 ?3 0.02</cell><cell>77.37</cell></row><row><cell>CE</cell><cell cols="2">3 ? 10 ?3 0.02</cell><cell>78.22</cell></row><row><cell>CE</cell><cell cols="2">5 ? 10 ?3 0.02</cell><cell>79.18</cell></row><row><cell>CE</cell><cell cols="2">5 ? 10 ?3 0.03</cell><cell>79.23</cell></row><row><cell>CE</cell><cell cols="2">5 ? 10 ?3 0.05</cell><cell>79.31</cell></row><row><cell>CE</cell><cell cols="2">8 ? 10 ?3 0.03</cell><cell>79.12</cell></row><row><cell>CE</cell><cell cols="2">3 ? 10 ?3 0.02</cell><cell>77.71</cell></row><row><cell>CE</cell><cell cols="2">5 ? 10 ?3 0.01</cell><cell>78.93</cell></row><row><cell>CE</cell><cell cols="2">5 ? 10 ?3 0.02</cell><cell>79.00</cell></row><row><cell>CE</cell><cell cols="2">5 ? 10 ?3 0.03</cell><cell>78.62</cell></row><row><cell>CE</cell><cell cols="2">8 ? 10 ?3 0.02</cell><cell>78.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Main ablation table with A2 procedure. We compare BCE vs CE, including repeated augmentation or not, and vary the learning rate LR and weight decay WD in ranges that our exploration phase has identified as being the most adapted. All results are reported with Seed 0 and therefore all the ResNet-50 are initialized with the same weights when the training starts.</figDesc><table /><note>The highlighted row corresponds to our A2 procedure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation of stochastic Depth &amp; smoothing for our training procedures. In blue , we highlight the results corresponding to the default selection for each procedure, seeTable 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>max -min seed 0 0.875 80.18 (0.14) 80.45 -79.90 80.14 79.67 (0.08) 79.91 -79.59 79.91 77.69 (0.10) 77.85 -77.48 77.69</figDesc><table><row><cell>0.9</cell><cell>80.22 (0.15) 80.54 -79.98 80.25 79.73 (0.09) 79.89 -79.56 79.75 77.86 (0.09) 78.01 -77.62 77.83</cell></row><row><cell>0.95</cell><cell>80.24 (0.14) 80.49 -79.91 80.38 79.68 (0.09) 79.85 -79.57 79.85 78.00 (0.09) 78.09 -77.83 78.06</cell></row><row><cell>1.0</cell><cell>80.15 (0.11) 80.15 -79.66 80.19 79.58 (0.13) 79.88 -79.32 79.88 78.02 (0.10) 78.16 -77.83 77.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>RMSProp with EMA weight averaging and step LR decay;</figDesc><table><row><cell>Epochs</cell><cell>600</cell><cell>800</cell><cell>800</cell><cell>600</cell></row><row><cell># of forward pass</cell><cell>375k</cell><cell>500k</cell><cell>500k</cell><cell>2,000k</cell></row><row><cell>Batch Size</cell><cell>2048</cell><cell>2048</cell><cell>2048</cell><cell>384</cell></row><row><cell>Optimizer</cell><cell>RMSProp</cell><cell>SGD</cell><cell>SGD</cell><cell>AdamP</cell></row><row><cell>Initial LR</cell><cell>0.18</cell><cell>0.88</cell><cell>0.88</cell><cell>0.0033</cell></row><row><cell>LR Scheduler</cell><cell>step</cell><cell>cosine</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>Decay Rate</cell><cell>0.988 per 1-epoch</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR Noise (% of training)</cell><cell>0.45 to 1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weight Decay</cell><cell>7.0 ? 10 ?6</cell><cell cols="2">1.0 ? 10 ?5 1.0 ? 10 ?5</cell><cell>0.01</cell></row><row><cell>Warmup Epochs</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Label Smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Dropout</cell><cell>0.2</cell><cell>0.25</cell><cell>0.25</cell><cell>0.1</cell></row><row><cell>Stochastic Depth</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.05</cell></row><row><cell>Repeated Augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grad Clipping</cell><cell></cell><cell>AGC .025</cell><cell>AGC .05</cell><cell></cell></row><row><cell>RandAugment (M/N/MSTD)</cell><cell>8/2/1.0</cell><cell>7/3/1.0</cell><cell>7/3/1.0</cell><cell>7/3/1.0</cell></row><row><cell>Mixup</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>CutMix</cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Random Erasing (Prob/Count)</cell><cell>0.35/3</cell><cell>0.4/1</cell><cell>0.4/1</cell><cell>.35/1</cell></row><row><cell>EMA weight averaging</cell><cell>0.9999</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE loss</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BCE loss</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 acc.</cell><cell>79.4%</cell><cell>79.8%</cell><cell>80.0%</cell><cell>79.8%</cell></row></table><note>B -</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Alternative training procedures giving good performance with ResNet-50 architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table><row><cell cols="2">Procedure B summary</cell><cell></cell></row><row><cell></cell><cell cols="2">Recommended Range ResNet-50</cell></row><row><cell>Epochs</cell><cell>300-800</cell><cell>800</cell></row><row><cell>Initial LR (per batch size 256)</cell><cell>.08-.12</cell><cell>0.11</cell></row><row><cell>LR Schedule</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>Grad Clipping</cell><cell>AGC .01 -.05</cell><cell>AGC .05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Procedure C summary</figDesc><table><row><cell></cell><cell cols="2">Recommended Range ResNet-50</cell></row><row><cell>Epochs</cell><cell>300-600</cell><cell>600</cell></row><row><cell>Initial LR (per batch size 256)</cell><cell>.002 -.003</cell><cell>0.0027</cell></row><row><cell>LR Schedule</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>Grad Clipping</cell><cell>None</cell><cell>None</cell></row><row><cell>Dropout</cell><cell>0-0.3</cell><cell>0.1</cell></row><row><cell>Stoch. Depth</cell><cell>0-0.1</cell><cell>0.05</cell></row><row><cell>Repeated Augmentation</cell><cell>Off, On</cell><cell>Off</cell></row><row><cell cols="2">RandAugment (M / N / MSTD) 6-9 / 2-4 / 0.5-1.0</cell><cell>7 / 3 / 1.0</cell></row><row><cell cols="2">Random Erasing (Prob / Count) 0.1-0.5 / 1-3</cell><cell>0.35 / 1</cell></row><row><cell>Mixup</cell><cell>0.2, 0.5, 0.8</cell><cell>0.2</cell></row><row><cell>CutMix</cell><cell>Off, 0.8, 1.0</cell><cell>1.0</cell></row><row><cell>Loss</cell><cell>CE, BCE</cell><cell>BCE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc></figDesc><table /><note>Procedure D summary</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ResNet-50 V1.5 (the PyTorch [1] ResNet50) a slight adjustment of He et al. [13] that was made in torch7, stride was moved from 1x1 to 3x3 in bottleneck. 3 available at http://github.com/rwightman/pytorch-image-models/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/rwightman/pytorch-image-models/discussions</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments &amp; feedback</head><p>Ross Wightman thanks NVIDIA for the donation of a V100 DGX Station and Google's TPU Research Cloud (TRC) for Cloud TPUs used in this research. All authors thank Mike Rabbat and Jakob Verbeek for their feedback. We welcome feedback regarding these or other noteworthy procedures via the timm GitHub Discussions 4 .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Jonathon Shlens, and Barret Zoph. Revisiting ResNets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with ImageNet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical automated data augmentation with a reduced search space</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01412</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08217,2020.21</idno>
		<title level="m">Adamp: Slowing down the weight norm increase in momentumbased optimizers</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The iNaturalist species classification and detection dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dmitrii Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>CIFAR</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeryun</forename><surname>Jung Kyu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiho</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">torch.manual seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08203</idno>
		<imprint>
			<date type="published" when="2021-09" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13630,2020.3</idno>
		<title level="m">Tresnet: High performance gpu-dedicated architecture</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mlp-Mixer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<title level="m">An all-MLP architecture for vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">ResMLP: feedforward networks for image classification with data-efficient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention. International Conference on Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<title level="m">Grafit: Learning fine-grained image representations with coarse labels. International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy. Neurips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Ismet Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04899</idno>
		<title level="m">Regularization strategy to train strong classifiers with localizable features</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
