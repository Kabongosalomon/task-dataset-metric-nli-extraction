<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
							<email>dimitrios.kollias15@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FaceSoft.io</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FaceSoft.io</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Zafeiriou@imperial Ac</forename><surname>Uk</surname></persName>
						</author>
						<title level="a" type="main">Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>KOLLIAS, ZAFEIRIOU: AFF-WILD2, MULTI-TASK LEARNING &amp; ARCFACE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Affective computing has been largely limited in terms of available data resources. The need to collect and annotate diverse in-the-wild datasets has become apparent with the rise of deep learning models, as the default approach to address any computer vision task. Some in-the-wild databases have been recently proposed. However: i) their size is small, ii) they are not audiovisual, iii) only a small part is manually annotated, iv) they contain a small number of subjects, or v) they are not annotated for all main behavior tasks (valencearousal estimation, action unit detection and basic expression classification). To address these, we substantially extend the largest available in-the-wild database (Aff-Wild) to study continuous emotions such as valence and arousal. Furthermore, we annotate parts of the database with basic expressions and action units. As a consequence, for the first time, this allows the joint study of all three types of behavior states. We call this database Aff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures that use visual and audio modalities; these networks are trained on Aff-Wild2 and their performance is then evaluated on 10 publicly available emotion databases. We show that the networks achieve state-of-the-art performance for the emotion recognition tasks. Additionally, we adapt the ArcFace loss function in the emotion recognition context and use it for training two new networks on Aff-Wild2 and then re-train them in a variety of diverse expression recognition databases. The networks are shown to improve the existing state-of-the-art. The database, emotion recognition models and source code are available at http://ibug.doc.ic.ac.uk/ resources/aff-wild2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Until recently affective computing has been mostly studied in controlled settings of the environments <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>, with limited amount of participants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, using pre-defined scenarios that users have to follow, depicting posed expressions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. However, with the development of large and diverse datasets in the field of computer vision (and the accompanying performance gains), it has c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1910.04855v1 [cs.CV] <ref type="bibr" target="#b24">25</ref> Sep 2019 become apparent that the diversity of human participants and spontaneous expressions have to become the prerogatives in deployment of the affective computing models in practice. Large datasets with in-the-wild settings have been recently collected to study facial expression (Expr) analysis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, facial action units (AUs) <ref type="bibr" target="#b10">[11]</ref> and continuous emotions of valence and arousal (VA) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref> in-the-wild.</p><p>In <ref type="bibr" target="#b28">[29]</ref> a static in-the-wild database (AffectNet) has been created that contains VA annotations for 1M images. However, from those images, only around 450K are manually annotated and from those only 350K are valid faces. This number is moderate for training deep neural networks <ref type="bibr">(DNNs)</ref>. Also this database is static, meaning that it contains only images and no video/audio. It is worth to mention that this database also contains annotations for the seven basic expressions plus the contempt class for 290K images. In <ref type="bibr" target="#b1">[2]</ref> a static in-the-wild database (Emotionet) has been created that contains AU annotations for 1M images. However, from those images, only 50K are manually annotated. Half of those consist the validation set and the other half the test set. Again, these sets have a small size to adequately train DNNs and generalize on other databases. Emotionet database also contains annotations for 6 basic and 10 compound emotion categories. Nevertheless, their total size is less than 3K and the classes are heavily imbalanced, making it impossible to train DNNs. In <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref>, the authors have developed the largest existing audiovisual (A/V) in-the-wild database annotated in terms of VA for around 1.25M. All these annotations are manual. However this database contains annotations only for VA and the number of subjects in the videos is moderate (298 subjects in total).</p><p>Up to the present, there is no database that contains annotations for all main behavior tasks (VA estimation, AU detection, Expr classification). Also, most of the existing databases do not contain sufficiently large numbers of annotated samples for effectively training DNNs. The first contribution in this paper is the creation of a new dataset that contains 260 videos with around 1.4M frames, annotated for VA. We merge this dataset with Aff-Wild (since this database also contains annotated videos), generating the so-called Aff-Wild2 database. Next, we annotate parts of Aff-Wild2 with AUs and seven basic expression labels, creating about 398K and 403K AU and Expr annotations, respectively. To the best of our knowledge, Aff-Wild2 is the first large scale in-the-wild database containing annotations for all 3 main behavior tasks. It is also the first audiovisual database with annotations for AUs. All AU annotated databases do not contain audio, but only images or videos.</p><p>Next, we conduct multi-task experiments on this database, for emotion recognition. Many questions arise hereafter: how can we combine these three tasks? what loss function should we use? The first apparent answer is to use a loss function equal to the sum of the loss functions of each task. The binary cross entropy loss is used for AU detection. The MSE and Concordance Correlation Coefficient (CCC) losses are used for VA estimation. The standard loss for expression classification is the categorical cross entropy. The second contribution of the paper is the development of multi-task CNNs, multi-task CNN-RNNs and multi-modal, multi-task CNN-RNNs, which are trained on Aff-Wild2 and then applied to 10 publicly available databases (including the Aff-Wild one). The results are very promising, beating the state-of-the-art on emotion recognition in these databases; exceptions are two databases annotated for Expr Recognition. In one of them, a best performing network used a locality preserving loss function <ref type="bibr" target="#b21">[22]</ref>. This, as well as the recent tendency to develop elaborate loss functions for specific tasks <ref type="bibr" target="#b17">[18]</ref>, has led us to search In fact, in the related face recognition field, it has been shown <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref> that categorical cross entropy loss is insufficient to acquire discriminating power for face classification. Several loss functions have been proposed for maximizing inter-class and minimizing intra-class variance. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> propose multi-loss learning to increase feature discriminating power. These, require thorough mining of pair/triplet samples, which is a time-consuming procedure. <ref type="bibr" target="#b23">[24]</ref> projects the original Euclidean space of features to an angular space, introducing an angular margin for larger interclass variance. <ref type="bibr" target="#b42">[43]</ref> directly adds a cosine margin penalty to the target logit, showing better performance than <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr" target="#b7">[8]</ref> further improved the discriminative power of face recognition models, stabilising the training process. As these losses boosted face recognition models performance, in this work, we choose to adopt the ArcFace loss <ref type="bibr" target="#b7">[8]</ref> and adapt it for emotion recognition. To the best of our knowledge, this is the first time that such a loss designed for face recognition, is used in the context of emotion recognition. Our final contribution in this paper is the design of 2 networks trained with the ArcFace loss. After training them on Aff-Wild2, we re-trained them on each of the examined databases. Our results outperformed all state-of-the-art networks, illustrating: i) the richness of Aff-Wild2 (providing it with the ability to be used as robust prior for network pre-training) and ii) that the ArcFace loss can be used in the emotion recognition field, yielding state-of-the-art results. In fact, this is the very first proof of the effectiveness of additive angular margin in emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Aff-Wilddatabase</head><p>Aff-Wild2 is described next, presenting the new collected dataset and its properties, the generated partition sets, their distributions and the annotation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collected dataset and properties</head><p>We extend the Aff-Wild database <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref>, by collecting a new dataset consisting of 260 YouTube videos, with 1,413,000 frames and a total length of 13 hours and 5 minutes. The videos have been collected using the Youtube video sharing website. All of the collected videos are in MP4 format, with a frame rate of 30, provided under the CC licence. Keywords for retrieving the videos were selected from the 2-D Emotion Wheel, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The new videos have wide range in subjects': age (from babies to elderly people); ethnicity (caucasian/hispanic/latino/asian/black/african american); profession (e.g. actors, athletes, politicians, journalists); head pose; illumination conditions; occlussions; emotions. <ref type="figure" target="#fig_0">Figure 1</ref> shows frames of Aff-Wild2 verifying the above described ranges.</p><p>These videos show subjects who: react on a surprise, on something that brings them happiness or fulfillment, on flirting or rejection, on important political issues, on funny or mean tweets; are stand-up comedians; give a really interesting speech in ceremonies; are taking an oral exam; are giving lectures on depression, or other serious disorders; are performing passive, boring, apathetic, intense activities, etc.</p><p>Four experts annotated the new dataset in terms of valence and arousal, as in the case of Aff-Wild. We then concatenated the Aff-Wild database with the new dataset, forming Aff-Wild2. In total, Aff-Wild2 consists of 558 videos with 2,786,201 frames, showing both subtle and extreme human behaviours in real-world settings. The total number of subjects is 458; 279 of which are males and 179 females.</p><p>Two more tasks were implemented, in which we annotated parts of Aff-Wild2 with AUs and Exprs. In the first, three very experienced annotators annotated 63 videos, with 397,800 frames and a total length of 3 hours and 41 mins, in terms of AUs 1,2,4,6,12,15,20,25 -described in <ref type="figure" target="#fig_1">Figure 2</ref>. These videos contain 31 male and 31 female subjects. In the second, three experts annotated 84 videos consisting of 403,758 frames, with a total length of 3 hours and 45 mins, in terms of the 7 basic expressions. The videos show 42 male and 42 female subjects. Consequently, Aff-Wild2 contains 3 datasets (VA, AU, Expr); each contains annotations for a respective behavior task (preliminary work regarding the 3 sets can be found in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>). <ref type="table" target="#tab_0">Table  1</ref> summarizes the attributes and properties of the three annotated sets of Aff-Wild2.   <ref type="figure" target="#fig_2">Figure 3</ref> shows the 2D VA histogram of the new dataset, which was added to Aff-Wild. <ref type="figure" target="#fig_2">Figure 3</ref> shows the distribution of the seven emotion categories in Aff-Wild2. <ref type="table" target="#tab_1">Table 2</ref> shows the distribution of the activated AUs. We note that the Expr Set of images can be extended to also contain AU annotations, according to <ref type="table" target="#tab_0">Table 1</ref> of <ref type="bibr" target="#b9">[10]</ref>. However, this is out of the scope of the current paper.  Annotation Four experts have performed the VA set annotation, using the method proposed in <ref type="bibr" target="#b5">[6]</ref>. Valence and arousal values range continuously in [-1,1]. The final label values are the mean of those four annotations. The mean inter-annotation correlation is 0.63 for valence and 0.60 for arousal. For the AU set, three experts have performed the annotation. For the Expr set, three more experts performed the annotation. In both cases, agreement between the annotators has not always been 100%. We only kept the annotations, on which all experts agree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head><p>Two pre-processing steps, on the visual and audio modalities, have been applied to generate the input data for DNN based emotion analysis. We developed the following deep network architectures for emotion recognition: i) CNN, single-/multitask; ii) CNN-RNN multi-task; iii) CNN-RNN multi-modal (A/V) and multi-task; iv) new ArcFace networks with respective loss function, as described below.</p><p>Visual Modality Pre-Processing The SSH detector <ref type="bibr" target="#b29">[30]</ref> based on the ResNet and trained on the WiderFace dataset <ref type="bibr" target="#b45">[46]</ref> was used to extract face bounding boxes from all images. Also, 5 facial landmarks (two eyes, nose and two mouth corners) were extracted and used to perform similarity transformation (for face alignment). After that we obtain the cropped faces which are then resized to dimension 96 ? 96 ? 3.</p><p>The pixel intensities are normalized to take values in [-1,1].</p><p>Audio Modality Pre-Processing The audio signal (mono) is sampled at 44, 100Hz. Then spectrograms are extracted; spectrogram frames are computed over a 33ms</p><p>window with 11ms overlap. The resulting intensity values are normalized in [-1,1] to be consistent with the visual modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Single-&amp; Multi-Task</head><p>We employ 3 state-of-the-art networks, SphereFace-20 <ref type="bibr" target="#b23">[24]</ref>, VGGFace <ref type="bibr" target="#b31">[32]</ref>, and Inception ResNet <ref type="bibr" target="#b36">[37]</ref> (denoted as Inc.ResNet). We train these networks to perform one behavior task (VA estimation, AU detection, or Expr classification), or jointly perform all 3 tasks. We call the multi-task VGGFACE network, MT-VGG. The predictions for all tasks are pooled from the same feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-RNN Multi-Task</head><p>As shown in the experimental section, MT-VGG has the best performance; thus we construct a CNN-RNN multi-task network, based on MT-VGG. In more detail, a 2-layer GRU with 128 cells each is stacked on top of the first fc layer of MT-VGG for capturing the temporal dynamics; the output layer is on top of the GRU. We call this network MT-VGG-RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-RNN Multi-Modal (A/V) &amp; Multi-Task</head><p>To handle both video and audio modalities, we use a feature level fusion strategy in our developed deep learning model, that we illustrate in <ref type="figure" target="#fig_3">Figure 4</ref>. This model consists of two identical streams that extract features directly from raw input images and spectrograms, respectively. Each stream consists of a MT-VGG-RNN, described above, without the output layer. The features from the two streams are concatenated, forming a 256-dimensional feature vector that is passed through a 2-layer GRU layer with 128 units in each layer, in order to fuse the information of the audio and visual streams. The output layer follows on top of it. We call this network A/V-MT-VGG-RNN. </p><formula xml:id="formula_0">L CCE = E[?log e p p + log ? 7 i=1 e p i ]<label>(1)</label></formula><formula xml:id="formula_1">L BCE = E[? ? 17 i=1 (t i ? log p i + (1 ? t i ) ? log (1 ? p i ))]<label>(2)</label></formula><formula xml:id="formula_2">L CCC = 1 ? 0.5 ? (? a + ? v ), with ? a,v = 2s xy ? [s 2 x + s 2 y + (x ??) 2 ]<label>(3)</label></formula><p>where L CCE is the categorical cross entropy loss, L BCE is the binary cross entropy loss, p p is the prediction of positive class, p i is the prediction of AU i , t i ? {0, 1} is the label of AU i , ? a,v is the Concordance Correlation Coefficient (CCC) of arousal/valence, s x and s y are the variances of arousal/valence labels and predicted values respectively and s xy is the corresponding covariance value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ArcFace Loss Function &amp; Networks</head><p>Next, we focus on Expr recognition and introduce a new loss function. The softmax cross-entropy loss is modified as follows:</p><formula xml:id="formula_3">L = ?1 N N ? i=1 log e W T y i x i ? 7 j=1 e W T j x i = ?1 N N ? i=1 log e Wy i ? x i ?cos ?y i ? 7 j=1 e W j ? x i ?cos ? j x i =s, W j = 1 ====== ?1 N N ? i=1 log e s?cos ?y i ? 7 j=1 e s?cos ? j<label>(4)</label></formula><p>where the embedding feature x i ? R d denotes the deep feature of the i-th sample belonging to the y i -th class, W j ? R d denotes the j-th column of the weight W ? R d?7 , N is the batch size, ? j is the angle between weight W j and feature x i , W j is fixed to 1 by l 2 normalization, x i is fixed by l 2 normalization and re-scaled to s. From eq.4, it can be seen that the embedding features are distributed around each feature centre on the hypersphere. In our case, we adopt the ArcFace loss, where an angular margin penalty m between x i and W y i is added to simultaneously enhance the intra-class compactness and inter-class discrepancy (eq.4: ? y i ? ? ? y i + m). m is equal to the geodesic distance margin penalty in the normalised hypersphere. We refer the interested reader to <ref type="bibr" target="#b7">[8]</ref> for more details and explanation of this loss.</p><p>Next, we develop two networks to account for this loss. The first CNN architecture, called Multi-Task-ArcFace-Residual (MT-ArcRes) uses residual units and is depicted in <ref type="figure" target="#fig_4">Fig.5</ref>; 'bn' stands for batch normalization, the convolution layer is in the format: filter height ? filter width conv., number of output feature maps; the stride is equal to 2, everywhere; the fc layer is the embedding layer; the output layer provides the seven expresion class logits (W T j x i , j = 1..7). The second network is called Multi-Task-Arcface-VGG (MT-ArcVGG); the difference with MT-ArcRes is that the rectangular area in the Figure contains VGGFace's layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Study</head><p>The experimental study consists of two parts. In the first, we train MT CNNs, CNN-RNNs &amp; Multi-Modal CNN-RNNs, for VA, AU and Expr Recognition on the Aff-Wild2; then, we test the networks on 10 different databases, showing that Aff-Wild2 and the Multi-Task networks provide the best pre-trained framework for a large variety of emotion recognition settings. In the second, focusing on expression recognition, we first train ArcFace networks with Aff-Wild2 and then re-train them with each expression database; we evaluate them, achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details &amp; Settings</head><p>Specific details about hyperparameters of the developed architectures can be found in <ref type="table" target="#tab_2">Table 3</ref>. All experiments in this paper are implemented in TensorFlow, on a Tesla V100 32GB GPU, using Adam optimizer (with default values) or SGD with momentum (0.9) in the ArcFace Networks' case. Additional details follow: I) CNN Single-&amp; Multi-Task: The networks have first been pre-trained for VA estimation on the Aff-Wild database, then the output layer is discarded and substituted by a new one for single-or multi-task, depending on the network type. Then they are trained end-to-end on Aff-Wild2. II) CNN-RNN Multi-Task: The CNN part is initialized with the weights of the CNN MT-VGG. Then the whole architecture is trained end-to-end on Aff-Wild2. III) CNN-RNN Multi-Modal (A/V) &amp; Multi-Task: Training is divided in two phases: first the audio/visual streams are trained independently and then the audiovisual network is trained end-to-end. To train each stream individually, we follow the same procedure as in the CNN-RNN Multi-Task case. Once the single streams are trained, they are used for initializing the corresponding streams in the multi-stream architecture. Finally, the entire audiovisual network is trained end-to-end. IV) ArcFace Networks: Both networks are first trained on Aff-Wild2. Then, they are re-trained end-to-end on each of the examined databases. During testing we keep the feature embedding layer, discarding the output layer. For all training images, we extract features from the embedding layer and split them in 7 clusters. Then, for each test image, we compute its distance (based on cosine similarity) from all cluster centers and assign it to the center for which this distance is minimum. Databases <ref type="table" target="#tab_3">Table 4</ref> shows the databases used in our experiments along with their properties. BP4DS and BP4D+ datasets correspond to the ones used in the FERA 2015 <ref type="bibr" target="#b40">[41]</ref> and 2017 <ref type="bibr" target="#b41">[42]</ref> Challenges, respectively. All databases are in-the-wild, apart from DISFA, BP4DS, BP4D+, which are spontaneous. Let us note that for the Af-fectNet, BP4DS and BP4D+ databases, the test set is not released; thus we report the performances on the validation set, which we use for testing. Evaluation Metrics CCC, defined in eq.3 is used for VA estimation, as it has been the evaluation criterion in all related Challenges <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>. The usual F1 score is adopted for evaluation of AU detection and Expr classification. Exceptions are the RAF-DB and FER2013, in which the mean diagonal value of the confusion matrix and the accuracy metric, respectively, are the default performance measures. <ref type="table" target="#tab_4">Table 5</ref> presents the results of different CNN Single-and Multi-task (ST-and MT-) networks in a crossdatabase setting (networks are trained on Aff-Wild2 and tested on AffectNet, RAF-DB, FER2013 and IMFDB). The MT-VGG has the best performance for both VA and Expr recognition. In <ref type="table" target="#tab_4">Table 5</ref>, we also compare MT-VGG's performance with that of the state-of-the-art in each of the tested databases (the results shown are taken from the respective papers). It can be seen that MT-VGG beats the state-of-the-art in all databases, illustrating the excellent cross-performance of the generated framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on static databases for VA &amp; Expr Recognition</head><p>Only, in expression recognition in AffectNet, the obtained performance is lower to the state-of-the-art.  <ref type="table" target="#tab_5">Table 6</ref> presents the results of CNN, CNN-RNN multi-task, single-and multi-modal networks in a crossdatabase setting, testing on Aff-Wild, Aff-Wild2 and AFEW-VA databases. It can be seen that the MT-VGG-RNN (trained on the visual modality) displays a better performance than the MT-VGG, for VA and Expr Recognition, in all databases. Moreover, MT-VGG-RNN performs best for valence estimation when trained with the visual modality, whereas performs best for arousal when trained with the audio modality. This is because audio tends to have thematic constancy. Consider, for example, two fight sequences in a movie, one being a flashy fight scene and the other a one-sided fight with a person being injured. In both cases, arousal can be high due to loud and pronounced music, but valence will be positive in the former and negative in the latter sequence. It can also be seen that the A/V-MT-VGG-RNN outperforms the MT-VGG-RNN, illustrating that the A/V combination improves network performance, in valence, arousal and expression estimation. <ref type="table" target="#tab_5">Table 6</ref> compares the performance of the proposed networks with the state-of-the-art in the examined databases . It is evident that MT-VGG and MT-VGG-RNN outperform the respective state-of-the-art.  <ref type="table" target="#tab_6">Table 7</ref> compares the performance between MT-VGG and state-of-the-art networks in a cross-database setting among Emotionet, DISFA, BP4DS and BP4D+; all reported results are for the common AUs between the testing database and Aff-Wild2. It is clear that MT-VGG outperforms the winner <ref type="bibr" target="#b8">[9]</ref> of Emotionet 2017 Challenge, the baseline <ref type="bibr" target="#b40">[41]</ref> and the winner <ref type="bibr" target="#b48">[49]</ref> of FERA 2015, the baseline <ref type="bibr" target="#b41">[42]</ref> of FERA 2017 and the fine-tuned VGG (FVGG) and R-TI method of <ref type="bibr" target="#b22">[23]</ref>. Apart from Emotionet, in all other cases, there is a boost in performance. MT-VGG displays a slightly worse performance than the winner <ref type="bibr" target="#b37">[38]</ref> of FERA 2017. From the above Tables, it is evident that Aff-Wild2 constitutes a very rich database for deep network training and further testing on very different and diverse emotion databases; the presented cross-database results validate our network developments. <ref type="table" target="#tab_7">Table 8</ref> presents a performance comparison between: i) MT-VGG (trained on Aff-Wild2), ii) a fine-tuned MT-VGG (FT-MT-VGG; pre-trained on Aff-Wild2, then re-trained on each of the examined databases), iii) the two networks trained with the ArcFace loss (MT-ArcRes, MT-ArcVGG) on Aff-Wild2 and re-trained on each of the examined databases, iv) the state-of-the-art in these databases (whose results are taken from the respective papers). The FT-MT-VGG outperforms the state-of-the-art in all databases, apart from RAF-DB, where DLP-CNN performs better, however, trained with a locality preserving loss function. <ref type="table" target="#tab_7">Table 8</ref> also compares this network's performance to the performance of MT-ArcRes and MT-ArcVGG networks, trained with the ArcFace loss function. These networks outperform all others, including DLP-CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on video databases for VA &amp; Expr Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results for AU Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results with ArcFace Loss for Expr Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present the first, largest, in-the-wild, A/V database, called Aff-Wild2, that is annotated for VA, AUs and Exprs. We build and train multi-task and multi-modal CNNs and CNN-RNNs on Aff-Wild2 and test their performances on 10 databases, beating the state-of-the-art. We further train two new networks on Aff-Wild2, adopting the ArcFace loss function, and then re-train them on a variety of expression databases; the results improve the existing state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Frames of Aff-Wild2, showing subjects of different ethnicities, age groups, emotional states, head poses, illumination conditions and occlusions for a better loss function than the categorical cross entropy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The 2D Emotion Wheel (left); the AUs annotated in Aff-Wild2 (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>2D VA Histogram of the new data added to Aff-Wild (left); Histogram of the seven basic expressions in Aff-Wild2 (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>A/V-MT-VGG-RNN: the Multi-Modal and Multi-Task developed model Standard Loss Functions The objective function minimized during training of the multi-task networks is the sum of the individual task losses:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The MT-ArcRes network that has been trained with the ArcFace loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>General Attributes of Aff-Wild2; in the VA set, top row refers to the new dataset, while bottom row refers to Aff-WildPartition Sets and DistributionsEach set (VA, AU, Expr) is split into three subsets: training, validation and test. Partitioning is done in a subject independent manner, in the sense that a person can appear only in one of those three subsets. In the VA set, the resulting training, validation and test subsets consist of 350, 70 and 138 videos respectively. In the AU set, the respective subsets consist of 42, 7 and 14 videos respectively. In the Expr set, the corresponding subsets consist of 51, 11 and 22 videos respectively.</figDesc><table><row><cell cols="4">Aff-Wild2 # frames # videos # annotators</cell><cell>Video Length</cell><cell>Mean Resolution</cell></row><row><cell>VA set</cell><cell>1, 413, 000 1, 373, 201</cell><cell>260 298</cell><cell>4 8</cell><cell>0.03 ? 26.22 mins 0.10 ? 14.47 mins</cell><cell>1450 ? 900 607 ? 359</cell></row><row><cell>AU set</cell><cell>397, 800</cell><cell>63</cell><cell>3</cell><cell>0.03 ? 26.22 mins</cell><cell>1500 ? 900</cell></row><row><cell>Expr set</cell><cell>403, 758</cell><cell>84</cell><cell>3</cell><cell>0.04 ? 26.22 mins</cell><cell>1350 ? 800</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Distribution of AU annotations in Aff-Wild2</figDesc><table><row><cell>Action Unit #</cell><cell>AU 1</cell><cell cols="2">AU 2 AU 4</cell><cell cols="5">AU 6 AU 12 AU 15 AU 20 AU 25</cell></row><row><cell>Total Number of Activated AUs</cell><cell>86,677 43.9 %</cell><cell>4,166 2.1%</cell><cell>56,327 28.5%</cell><cell>25,226 12.8%</cell><cell>35,675 18.1%</cell><cell>3,340 1.7%</cell><cell>5,695 2.9%</cell><cell>9,048 4.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Network Configurations: ST= Single Task, MT=Multi-Task , 10 ?5 ], best : 10 ?4 [10 ?4 , 10 ?6 ], best : 10 ?5 [10 ?3 , 10 ?6 ], best : 10 ?5 [10 ?4 , 10 ?5 ], best : 10 ?4</figDesc><table><row><cell></cell><cell cols="2">ST-&amp; MT-CNN</cell><cell>MT-CNN-RNN</cell><cell>A/V-MT-CNN-RNN</cell><cell>MT-ArcRes / MT-ArcVGG</cell></row><row><cell cols="2">learning rate [10 ?4 batch size/seq.length</cell><cell>256 / -</cell><cell>10 / 90</cell><cell>5 / 90</cell><cell>300 / -</cell></row><row><cell>parameters</cell><cell cols="2">dropout=0.4</cell><cell>dropout=0.4</cell><cell>dropout=0.4</cell><cell>dropout=0.4, d ? {32, 512}, s ? {32, 64}, m ? {0.1, 0.5, 1, 1.5, 2, 2.5, 3}, best : 0.1/1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Properties of Databases used in our Experiments</figDesc><table><row><cell>Databases</cell><cell cols="9">AFEW-VA [21] AffectNet RAF-DB [22] FER2013 [13] IMFDB[36] Emotionet DISFA [27] BP4DS [51] BP4D+ [52]</cell></row><row><cell>Model of Affect</cell><cell>VA</cell><cell>VA, Expr</cell><cell>Expr</cell><cell>Expr</cell><cell>Expr</cell><cell>AUs</cell><cell>AUs</cell><cell>AUs</cell><cell>AUs</cell></row><row><cell># of videos</cell><cell>600</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54</cell><cell>1,640</cell><cell>5,463</cell></row><row><cell># of frames</cell><cell>30,050</cell><cell>450,000</cell><cell>15,200</cell><cell>35,887</cell><cell>34,512</cell><cell>50,000</cell><cell>261,630</cell><cell>222,573</cell><cell>967,570</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="10">: Cross-database evaluation (models trained on Aff-Wild2 and tested on</cell></row><row><cell cols="10">other databases) for VA and Expr on static databases: VA evaluation is shown as</cell></row><row><cell cols="9">(CCC V -CCC A ); single values correspond to expressions' performance metrics</cell><cell></cell></row><row><cell>Databases</cell><cell>MT-VGG</cell><cell>ST-VGG</cell><cell>MT-SphereFace</cell><cell>ST-SphereFace</cell><cell>MT-Inc. ResNet</cell><cell>ST-Inc. ResNet</cell><cell cols="3">AlexNet [29] VGGFACE[22] VGG[12]</cell></row><row><cell>FER2013</cell><cell>0.76</cell><cell>0.73</cell><cell>0.72</cell><cell>0.72</cell><cell>0.74</cell><cell>0.71</cell><cell>-</cell><cell>-</cell><cell>0.75</cell></row><row><cell>RAF-DB</cell><cell>0.61</cell><cell>0.57</cell><cell>0.53</cell><cell>0.52</cell><cell>0.57</cell><cell>0.55</cell><cell>-</cell><cell>0.58</cell><cell>-</cell></row><row><cell>IMFDB</cell><cell>0.42</cell><cell>0.39</cell><cell>0.39</cell><cell>0.38</cell><cell>0.4</cell><cell>0.39</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AffectNet</cell><cell>(0.61-0.46) 0.54</cell><cell>(0.51-0.42) 0.52</cell><cell>(0.5-0.43) 0.5</cell><cell>(0.5-0.4) 0.51</cell><cell>(0.52-0.45) 0.52</cell><cell>(0.5-0.42) 0.51</cell><cell>(0.6-0.34) 0.58</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Cross-database evaluation for VA and Expr on video databases: VA evaluation is shown as (CCC V -CCC A ); single values correspond to F1 score</figDesc><table><row><cell>Databases</cell><cell>MT-VGG</cell><cell>MT-VGG-RNN visual modality</cell><cell>MT-VGG-RNN audio modality</cell><cell>A/V-MT-VGG-RNN</cell><cell>best CNN [19, 20]</cell><cell>AffWildNet [19, 20]</cell></row><row><cell>Aff-Wild</cell><cell>(0.56-0.35)</cell><cell>(0.60-0.45)</cell><cell>(0.51-0.47)</cell><cell>(0.62-0.49)</cell><cell>(0.51-0.33)</cell><cell>(0.57-0.43)</cell></row><row><cell>Aff-Wild2</cell><cell>(0.38-0.3) 0.4</cell><cell>(0.40-0.33) 0.43</cell><cell>(0.34-0.36) 0.43</cell><cell>(0.42-0.38) 0.46</cell><cell>(0.33-0.25) -</cell><cell>(0.35-0.28) -</cell></row><row><cell cols="2">AFEW-VA (0.58-0.53)</cell><cell>(0.6-0.6)</cell><cell>-</cell><cell>-</cell><cell>(0.49-0.52)</cell><cell>(0.52-0.56)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Cross-database evaluation for AU Detection: evaluation metric is F1 score</figDesc><table><row><cell>Databases</cell><cell cols="2">MT-VGG MT-VGG-RNN</cell><cell>[9]</cell><cell cols="6">[49] [41] [38] [42] FVGG[23] R-T1[23]</cell></row><row><cell>Aff-Wild2</cell><cell>0.42</cell><cell>0.44</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Emotionet</cell><cell>0.52</cell><cell>-</cell><cell>0.51</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DISFA</cell><cell>0.61</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.52</cell><cell>0.60</cell></row><row><cell>BP4DS</cell><cell>0.66</cell><cell>-</cell><cell>-</cell><cell cols="2">0.54 0.53</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BP4D+</cell><cell>0.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.51 0.34</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Retrained multi-task networks with ArcFace loss, for expression recognition Databases MT-ArcRes MT-ArcVGG FT-MT-VGG MT-VGG AlexNet<ref type="bibr" target="#b28">[29]</ref> DLP-CNN<ref type="bibr" target="#b21">[22]</ref> VGG<ref type="bibr" target="#b11">[12]</ref> </figDesc><table><row><cell>AffectNet</cell><cell>0.63</cell><cell>0.62</cell><cell>0.59</cell><cell>0.54</cell><cell>0.58</cell><cell>-</cell><cell>-</cell></row><row><cell>RAF-DB</cell><cell>0.75</cell><cell>0.76</cell><cell>0.71</cell><cell>0.61</cell><cell>-</cell><cell>0.74</cell><cell>-</cell></row><row><cell>IMFDB</cell><cell>0.55</cell><cell>0.56</cell><cell>0.51</cell><cell>0.42</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FER2013</cell><cell>0.8</cell><cell>0.79</cell><cell>0.78</cell><cell>0.76</cell><cell>-</cell><cell>-</cell><cell>0.75</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Viktoriia Sharmanska for our fruitful conversations during preparation of this work. The work of S. Zafeiriou has been partially funded by the EPSRC Fellowship Deform (EP/S010203/1). The work of Dimitrios Kollias was funded by a Teaching Fellowship of Imperial College London.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The mug facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Aifanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Papachristou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Delopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR&apos;16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR&apos;16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mahnob mimicry database: A database of naturalistic human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Bilakhia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (1)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Describing the emotional states that are expressed in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randolph R Cornelius</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
	<note>Speech communication</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Martin Sawey, and Marc Schr?der. &apos;feeltrace&apos;: An instrument for recording perceived emotion in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susie</forename><surname>Savvidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edelle</forename><surname>Mcmahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA tutorial and research workshop (ITRW) on speech and emotion</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Handbook of cognition and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dalgleish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mick</forename><surname>Power</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial action recognition using very deep networks for highly imbalanced class distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinguo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichuan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1454" to="1462" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Facial action coding system (facs). A human face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local learning with deep and handcrafted features for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="64827" to="64836" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">Luc</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Aff-wild2: Extending the aff-wild database for affect recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07770</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A multi-task learning &amp; generation framework: Valence-arousal, action units &amp; primary expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07771</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training deep neural networks with different datasets in-the-wild: The emotion recognition paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognition of affect in the wild using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1972" to="1979" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="907" to="929" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Afew-va database for valence and arousal estimation in-the-wild. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farnaz</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zara</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The japanese female facial expression (jaffe) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeru</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miyuki</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Gyoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Budynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of third international conference on automatic face and gesture recognition</title>
		<meeting>third international conference on automatic face and gesture recognition</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database. Affective Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S Mohammad Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludo</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introducing the recola multimodal corpus of remote collaborative and affective interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sonderegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Avec 2017-real-life depression, and affect recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Mozgai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Schmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evidence of convergent validity on the dimensions of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1152</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Indian movie face database: a benchmark for face recognition under wide variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Setty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moula</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Beham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothi</forename><surname>Gudavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menaka</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhesyam</forename><surname>Vaddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidyagouri</forename><surname>Hemadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Karure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">View-independent facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuangao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="878" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ying-Li Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Induced disgust, happiness and surprise: an addition to the mmi facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect</title>
		<meeting>3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fera 2017-addressing head pose in the third facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>S?nchez-Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?szl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The dictionary of affect in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cm Whissel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory, research and experience</title>
		<editor>r. Plutchik and H. Kellerman</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic face and gesture recognition, 2006. FGR 2006. 7th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A highresolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference On</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Discriminant multi-label manifold embedding for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An?l</forename><surname>Y?ce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aff-wild: Valence and arousal&apos;inthe-wild&apos;challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a highresolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umur</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
