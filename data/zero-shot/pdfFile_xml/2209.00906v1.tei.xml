<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-Dependent Noisy Label Learning via Graphical Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Garg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Felix</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-Dependent Noisy Label Learning via Graphical Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Noisy labels are unavoidable yet troublesome in the ecosystem of deep learning because models can easily overfit them. There are many types of label noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with IDN being the only type that depends on image information. Such dependence on image information makes IDN a critical type of label noise to study, given that labelling mistakes are caused in large part by insufficient or ambiguous information about the visual classes present in images. Aiming to provide an effective technique to address IDN, we present a new graphical modelling approach called InstanceGM, that combines discriminative and generative models. The main contributions of InstanceGM are: i) the use of the continuous Bernoulli distribution to train the generative model, offering significant training advantages, and ii) the exploration of a state-of-the-art noisy-label discriminative classifier to generate clean labels from instancedependent noisy-label samples. InstanceGM is competitive with current noisy-label learning approaches, particularly in IDN benchmarks using synthetic and real-world datasets, where our method shows better accuracy than the competitors in most experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The latest developments in deep neural networks (DNNs) have shown outstanding results in a variety of applications ranging from computer vision <ref type="bibr" target="#b33">[31]</ref> to natural language processing <ref type="bibr" target="#b50">[48]</ref> and medical image analysis <ref type="bibr" target="#b49">[47]</ref>. Such success is strongly reliant on highcapacity models, which in turn, require a massive amount of correctly-annotated data for training <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b69">67]</ref>. Annotating a large amount of data is, however, arduous, costly and time-consuming, and therefore is often done via crowdsourcing <ref type="bibr" target="#b58">[56]</ref> that generally produces low-quality annota-* arpit.garg@aiml.team tions. Although that brings down the cost and scales up the process, the trade-off is the mislabelling of the data, resulting in a deterioration of deep models' performance <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b37">35]</ref> due to the memorisation effect <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b72">70]</ref>. This has, therefore, motivated the research of novel learning algorithms to tackle the label noise problem where data might have been mislabelled.</p><p>Early work in label noise <ref type="bibr" target="#b19">[17]</ref> was carried out under the assumption that label noise was instance-independent (IIN), i.e., mislabelling occurred regardless of the information about the visual classes present in images. In IIN, we generally have a transition matrix that contains a predefined probability of flipping between pairs of labels (e.g., any image showing a cat has a high priori probability of being mislabelled as a dog and low a priori probability of being mislabelled as a car). This type of noise can also be divided into two sub-types: symmetric, where a true label is flipped to another label with equal probability across all classes, and asymmetric, where a true label is more likely to be mislabeled into one of some particular classes <ref type="bibr" target="#b19">[17]</ref>. Nevertheless, the IIN assumption is impractical for many realworld datasets because we can intuitively argue that mislabellings mostly occur because of insufficient or ambiguous information about the visual classes present in images. As a result, recent studies have gradually shifted their focus toward the more realistic scenario of instance-dependent noise (IDN), where label noise depends on both the true class label and the image information <ref type="bibr" target="#b64">[62]</ref>.</p><p>Many methods have been introduced to handle not only IIN, but also IDN problems. Those include, but are not limited to, sample selection <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b29">27,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b74">72]</ref> that detects clean and noisy labels and applies semi-supervised learning methods on the processed data, robust losses <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b48">46]</ref> that can work well with either clean or noisy labels, and probabilistic approaches <ref type="bibr" target="#b68">[66]</ref> that model the data generation process, including how a noisy label is created. Despite some successes, most methods are often demonstrated in IIN settings with simulated symmetric and asymmetric noise. However, their performance is degraded when eval-uated on IDN problems, which include real-world and synthetic datasets. Although there are a few studies focusing on the IDN setting <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b76">74]</ref>, their relatively inaccurate classification results suggest that the algorithms can be improved further.</p><p>In this paper, we propose a new method to tackle the IDN problem, called InstanceGM. Our method is designed based on a graphical model that considers the clean label Y as a latent variable and introduces another latent variable Z representing the image feature to model the generation of a label noise? and an image X. InstanceGM integrates generative and discriminate models, where the generative model is based on a variational auto-encoder (VAE) <ref type="bibr" target="#b30">[28]</ref>, except that we replace the conventional mean squared error (MSE) when modelling the likelihood of reconstructed images by a continuous Bernoulli distribution <ref type="bibr" target="#b42">[40]</ref> that facilitates the training process since it avoids tuning additional hyper-parameters. For the discriminative model, to mitigate the problem of only using clean label data during the training process, which is a common issue present in the similar graphical model methods <ref type="bibr" target="#b68">[66]</ref>, we rely on DivideMix <ref type="bibr" target="#b35">[33]</ref> that uses both clean and noisy-label data for training by exploring semi-supervised learning via MixMatch <ref type="bibr" target="#b6">[5]</ref>. Di-videMix is shown to be a reasonably effective discriminative classifier for our InstanceGM. In summary, the main contributions of the proposed method are:</p><p>? InstanceGM follows a graphical modelling approach to generate both the image X and its noisy label? with the true label Y and image feature Z as latent variables. The modelling is associated with the continuous Bernoulli distribution to model the generation of instance X to facilitate the training, avoiding tuning of additional hyper-parameters (see Remark 3).</p><p>? For the discriminative classifier of InstanceGM, we replace the commonly used co-teaching, which is a dual model that relies only on training samples classified as clean, with DivideMix <ref type="bibr" target="#b35">[33]</ref> that uses all training samples classified as clean and noisy.</p><p>? InstanceGM shows state-of-the-art results on a variety of IDN benchmarks, including simulated and real-world datasets, such as CIFAR10 and CI-FAR100 <ref type="bibr" target="#b32">[30]</ref>, Red Mini-ImageNet from Controlled Noisy Web Labels (CNWL) <ref type="bibr" target="#b67">[65]</ref>, ANIMAL-10N <ref type="bibr" target="#b55">[53]</ref> and CLOTHING-1M [64].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>As DNNs have been shown to easily fit randomly labelled training data <ref type="bibr" target="#b70">[68]</ref>, they can also overfit a noisylabel dataset, which eventually results in poor generalisation to a clean-label testing data <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b72">70]</ref>. Several studies have, therefore, been conducted to investigate supervised learning under the label noise setting, including robust loss function <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b60">58]</ref>, sample selection <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b61">59]</ref>, robust regularisation <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b62">60]</ref> and robust architecture <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b66">64]</ref>. Below, we review methods dealing with noisy labels, especially IDN, without the reliance on clean validation sets <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b59">57]</ref>.</p><p>Let us start with methods designed to handle "any" type of label noise, including IDN and IIN. An important technique for both of these label noise types is sample selection <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b61">59]</ref>, which aims to select clean-label samples automatically for training. Although it is well-motivated and often effective, it suffers from the cumulative error caused by mistakes in the selection process, mainly when there are numerous unclear classes in the training data. Consequently, sample selection methods often rely on multiple clean-label sample classifiers to increase their robustness against such cumulative error <ref type="bibr" target="#b35">[33]</ref>. In addition, semisupervised learning (SSL) <ref type="bibr">[4,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b29">27,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b74">72]</ref> have also been integrated with sample selection and multiple cleanlabel classifiers to enable the training from clean and noisylabel samples. In particular, SSL methods use clean and noisy samples by treating them as labelled and unlabelled data, respectively, with a MixMatch approach <ref type="bibr" target="#b6">[5]</ref>. These methods above have been designed to handle "any" type of label noise, so they are usually assessed in synthetic IIN benchmarks and real-world IDN benchmarks.</p><p>Given that real-world datasets do not, in general, contain IIN, more recently proposed methods aim to address IDN benchmarks <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b75">73]</ref>. In these benchmarks, the task of differentiating between hard clean-labelled samples and noisy-label samples pose a major challenge. Such issue is noted by Song et al. <ref type="bibr" target="#b56">[54]</ref>, who state that the model performance in IDN can degrade significantly compared to other types of noises.</p><p>One direct way of addressing IDN problems relies on a graphical model approach that has random variables representing the observed noisy label, the image, and the latent clean label. This model also has a generative process to produce an image given the (clean and noisy) label information <ref type="bibr" target="#b34">[32]</ref>. Another approach examines a graphical model using a discriminative process <ref type="bibr" target="#b51">[49]</ref>, where the model attempts to explain the posterior probability of the observed noisy label by averaging the posterior probabilities of the clean class label. Yao et al. <ref type="bibr" target="#b68">[66]</ref> developed a new causal model to address IDN that also uses the same variables as the methods above plus a latent image feature variable, which relies on generative models to produce the image from the clean label and image feature, and to produce the noisy label from the image feature and clean label. That approach <ref type="bibr" target="#b68">[66]</ref>, however, did not produce competitive results compared with state of the art. We argue that the model's poor performance is mostly due to the co-teaching <ref type="bibr" target="#b19">[17]</ref> that is trained with a Y X Z? <ref type="figure" target="#fig_1">Figure 1</ref>. The proposed graphical model of the generation process that produces the observable (shaded nodes) data X and noisy la-bel? from hidden (non-shaded nodes) data representation Z and clean label Y . small set of samples classified as clean, which can inadvertently contain noisy-label samples -this is an issue that can cause a cumulative error, particularly in IDN problems.</p><p>Our work is motivated by the graphical model approaches mentioned above, that aim to address IDN problems. The main difference in our approach is the use of a more effective clean sample identifier that replaces co-teaching <ref type="bibr" target="#b68">[66]</ref> by DivideMix <ref type="bibr" target="#b35">[33]</ref>, which considers the whole training set, instead of only the samples classified as clean. Moreover, we propose a more effective training of the image generative model based on the continuous Bernoulli distribution <ref type="bibr" target="#b42">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition</head><p>We denote X as an observed random variable representing an image, Y as a latent random variable corresponding to the clean label of X, Z as a latent random variable denoting an image feature representation for of X, and? as the observed random variable for the noisy label. The training set is represented by</p><formula xml:id="formula_0">D = {(x i ,? i )} |D| i=1</formula><p>, where the image is represented by x ? X ? R H?W ?3 (with 3 color channels and size H ? W pixels) and the noisy label? ? Y ? {0, 1} |Y| denoted by a one-hot vector. In the conventional supervised learning, D is used to train a model f ? : X ? ? |Y|?1 (where ? |Y|?1 represents the probability simplex), parameterised by ? ? ?, that can predict the labels of testing images. The aim is to exploit the noisy data (X,? ) from a training set to infer a model f ? that can accurately predict the clean labels Y of data in a testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Probabilistic noisy label modelling</head><p>We follow a similar approach presented in <ref type="bibr" target="#b68">[66]</ref> to model the process that generates samples with noisy labels via the graphical model shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, where the clean label Y and image feature representation Z are latent variables. Under this modelling assumption, a noisy-label sample (x,?) can be generated as follows:</p><p>Remark 1 Conventionally, the process of generating data x in step 3 above is often modelled as a Bernoulli distribution or multivariate normal distribution, corresponding to the binary cross-entropy (BCE) or MSE reconstruction losses, respectively. Such modelling, however, leads to a pervasive error <ref type="bibr" target="#b42">[40]</ref> since the image pixels are in [0, 1] instead of {0, 1} (Bernoulli distribution) 1 or (??, +?) (multivariate normal distribution). We therefore adopt the continuous Bernoulli distribution <ref type="bibr" target="#b42">[40]</ref> which has a support in [0, 1] to correctly model this image generation process.</p><p>Note that the parameters of the continuous Bernoulli and categorical distributions are conditioned on Z, X and Y , and modelled as the outputs of two DNNs:</p><formula xml:id="formula_1">? = f ?x (z, y) and ? = f ?? (x, y),<label>(1)</label></formula><p>where f denotes the neural network, and ? x ,?? represent the network parameters. Following the convention in machine learning, we call f ?x (.) the decoder and f ?? (.) the noisy label classifier.</p><p>To solve the label noise problem that has data generated from the process above, we need to infer the posterior p(Z, Y |X,? ). However, due to the complexity of the graphical model in <ref type="figure" target="#fig_1">Fig. 1</ref>, exact inference for the posterior p(Z, Y |X,? ) is intractable, and therefore, the estimation must rely on an approximation. Motivated by <ref type="bibr" target="#b68">[66]</ref>, we employ variational inference to approximate the true posterior p(Z, Y |X,? ) by a variational "posterior" q(Z, Y |X,? ). Such posterior can be obtained by minimising the following Kullback-Leibler (KL) divergence:</p><formula xml:id="formula_2">min q KL q(Z, Y |X,? ) || p(Z, Y |X,? ) ,<label>(2)</label></formula><p>where the variational posterior q(Z, Y |X,? ) can be factorised following the product rule of probability. We assume that the posterior of the clean label Y is independent from the noisy label? , given the instance X: q(Y |X,? ) = q(Y |X). In addition, the variational posterior of feature representation is independent from the noisy label given the clean label and input data:</p><formula xml:id="formula_3">q(Z|X,? , Y ) = q(Z|X, Y ).</formula><p>The variational posterior of interest can, therefore, be written as: The objective function in <ref type="formula" target="#formula_2">(2)</ref> can then be expanded as:</p><formula xml:id="formula_4">q(Z, Y |X,? ) = q(Z|X,? , Y ) q(Y |X,? ) = q(Z|X, Y ) q(Y |X).<label>(3)</label></formula><formula xml:id="formula_5">L (vi) = E q(Z|X,Y )q(Y |X) [? ln p(X|Z, Y )] + E q(Y |X) ? ln p(? |X, Y ) + KL [q(Y |X)||p(Y )] + E q(Y |X) [KL [q(Z|X, Y )||p(Z)]] . (4) Remark 2</formula><p>The objective function L (vi) in (4) shares similarity with the loss in variational auto-encoder <ref type="bibr" target="#b30">[28]</ref>. In particular, the first two terms in (4) are analogous to the reconstruction loss, while the remaining terms are analogous to the KL loss that regularises the deviation between the posterior q and its prior.</p><p>To optimise the objective in (4), both the posteriors q(Z|X, Y ) and q(Y |X) and priors p(Z) and p(Y ) must be specified. We assume q(Z|X, Y ) to be a multivariate normal distribution with a diagonal covariance matrix and q(Y |X) to be a categorical distribution:</p><formula xml:id="formula_6">q(Z|X = x,? =?) = N Z; ?(x,?), diag(? 2 (x,?) q(Y |X = x) = Cat(Y ; ?(x)),<label>(5)</label></formula><p>where the parameters of these distributions are modelled as the outputs of two DNNs. Hereafter, we call the network that models q(Y |X) the clean label classifier, and the model q(Z|X,? ), the encoder.</p><p>For the priors, we follow the convention in generative models, especially VAE, to assume p(Z) as a standard normal distribution, while p(Y ) is a uniform distribution.</p><p>Given such assumptions, we can minimise the loss function L (vi) in (4) w.r.t. the parameters of the two classifiers, the encoder and decoder in <ref type="formula" target="#formula_6">(5)</ref> and <ref type="bibr" target="#b2">(1)</ref>. The obtained clean label classifier that models q(Y |X) will then be used as the final classifier to evaluate data in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3</head><p>Optimising the objective function in (4) often requires the definition of hyper-parameters to weight the KL divergences <ref type="bibr" target="#b17">[15]</ref>. However, such weighting mechanism depends on the estimation of the KL divergences weights that is usually achieved with a grid-search using a validation set, making solutions dependent on the dataset. The reason for such weighting mechanism lies at the log-likelhoods used as reconstruction losses. For example, ? ln p(X|Z, Y ) is simply replaced by the corresponding loss functions, such as MSE, without taking the normalisation constants of those likelihood functions into account, resulting in an incorrect balance between reconstruction loss and regularisation. In </p><formula xml:id="formula_7">D = {(x i ,? i )} |D| i=1 : noisy dataset 3:</formula><p>T : total number of epochs <ref type="bibr">4:</ref> ? : threshold to decide clean or noisy samples used in DivideMix 5:</p><formula xml:id="formula_8">q 1 (Y |X), q 2 (Y |X) ? WARMUP(D)</formula><p>Warm-up training of 2 clean-label classifiers on noisy dataset <ref type="bibr">6:</ref> for e = 1 : T do 7:</p><formula xml:id="formula_9">L 1 , U 1 , L 2 , U 2 ? CO-DIVIDE(D, q 1 (Y |X), q 2 (Y |X), ? ) 8:</formula><p>Apply Gaussian mixture model on loss values and filter out clean and noisy with a threshold on the likelihood 9:</p><p>L 1:2 are labelled sets (mostly clean) 10:</p><p>U 1:2 are unlabelled sets (mostly noisy) <ref type="bibr" target="#b13">11</ref>:</p><formula xml:id="formula_10">L (dm) (1) ? DIVIDEMIX LOSS(L 1 , U 1 , q 1 (Y |X), q 2 (Y |X))</formula><p>Calculate training loss in DivideMix 12:</p><formula xml:id="formula_11">L (dm) (2) ? DIVIDEMIX LOSS(L 2 , U 2 , q 2 (Y |X), q 1 (Y |X))</formula><p>13:</p><p>for k = 1:2 do Calculate loss on each one of the 2 models 14:</p><p>for each (x i ,? i ) ? L k do 15:</p><p>Compute each instance loss: L</p><formula xml:id="formula_12">(vi) i ? VARIATIONAL-FREE ENERGY(x i ,? i , q k , p k ) 16:</formula><p>q k is the variational posterior <ref type="bibr">17:</ref> p k denotes prior and data generation <ref type="bibr">18:</ref> Compute average loss: L (vi)</p><formula xml:id="formula_13">(k) = 1 /|L k | |L k | i=1 L (vi) i 19: Update model parameters by minimizing L (k) = L (vi) (k) + L (dm) (k)</formula><p>Eq. Compute L (vi) as the sum of the above terms as specified in Eq. (4) <ref type="bibr">28:</ref> return L (vi) this paper, we propose the use of the correct form of the loglikelihood, namely the continuous Bernoulli distribution for p(X|Z, Y ) and categorical distribution for p(? |X, Y ), with their normalisation constants. Hence, we no longer need the weighting of the KL divergences, making our proposed method simpler to train. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Practical implementation</head><p>In practice, the small loss hypothesis is often used to effectively identify the clean samples in a training set <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b35">33]</ref>. However, naively implementing such hypothesis using a single model might accumulate error due to sample selection bias. One way to avoid such scenario is to train two models simultaneously where each model is updated using only the clean samples selected by the other model. In this paper, we integrate a similar approach into our modelling presented in Section 3.2 to solve the label noise problem. <ref type="bibr" target="#b3">2</ref> More detailed information mentioned in Appendix B</p><p>In particular, we propose to train two models in parallel, resulting in four classifiers (two for the clean label classifier q(Y |X) and the other two for noisy labels p(? |X, Y )), two encoders q(Z|X, Y ) and two decoders p(X|Z, Y ).</p><p>In CausalNL <ref type="bibr" target="#b68">[66]</ref>, co-teaching is used as a way to integrate the small loss hypothesis to regularise the clean label classifiers. Co-teaching might, however, limit the capability of the modelling since it only uses samples classified as clean and ignores the other samples classified as noisy. In addition, co-teaching is initially designed for IIN problems, while our focus is on IDN problems. Hence, we propose to integrate DivideMix <ref type="bibr" target="#b35">[33]</ref>, a method based on the small loss hypothesis as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. This method starts with a warmup stage, and utilizes all training samples after classifying them as clean and noisy (co-divide) using a twocomponent Gaussian mixture model (GMM). The training samples are used by MixMatch [5] -a semi-supervised classification technique that considers clean samples as labelled and noisy samples as unlabeled. DivideMix shows a rea-sonable efficacy for IDN problems, as shown in <ref type="table">Table 1</ref>.</p><p>Remark 4 Other instance-dependent methods similar to DivideMix <ref type="bibr" target="#b35">[33]</ref>, such as Contrast-to-Divide <ref type="bibr" target="#b74">[72]</ref>, ELR+ <ref type="bibr" target="#b37">[35]</ref>, can also be integrated into our proposed framework. The reason that DivideMix is used is due to its remarkable performance, especially on the IDN setting, and its publicly available implementation.</p><p>In general, the loss function for training the proposed model consists of two losses: one is the loss L (vi) from the graphical modelling in <ref type="bibr">(4)</ref>, and the other is the loss to train DivideMix <ref type="bibr" target="#b35">[33,</ref><ref type="bibr">Eq. (12)</ref>], denoted as L (dm) . The whole loss is represented as:</p><formula xml:id="formula_14">L = L (vi) + L (dm)<label>(6)</label></formula><p>and the training procedure is summarised in Algorithm 1 and depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show the results of extensive experiments on two standard benchmark datasets with IDN, CI-FAR10 <ref type="bibr" target="#b32">[30]</ref> and CIFAR100 <ref type="bibr" target="#b32">[30]</ref> at various noise rates 3 , and three real-world datasets, ANIMAL-10N <ref type="bibr" target="#b55">[53]</ref>, Red Mini-Imagenet from CNWL <ref type="bibr" target="#b26">[24]</ref> and CLOTHING-1M <ref type="bibr" target="#b66">[64]</ref>. In Section 4.1, we explain all datasets mentioned above. In Section 4.2, we discuss all models and their parameters. We compare our approach with state-of-the-art models in IDN benchmarks and real-world datasets in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In both CIFAR10 and CIFAR100, there are 50k training images and 10k testing images with each images of size 32 ? 32 ? 3 pixels, where CIFAR10 consists of 10 classes, CIFAR100 has 100 classes and both datasets are class-balanced. As CIFAR10 and CIFAR100 datasets do not include label noise by default, we added IDN with noise rates in {0.2, 0.3, 0.4, 0.45, 0.5} following the setup proposed by Xia et al. <ref type="bibr" target="#b64">[62]</ref>.</p><p>Red Mini-Imagenet from CNWL <ref type="bibr" target="#b26">[24]</ref> is a real-world dataset where images and their corresponding labels are crawled from internet at various controllable label noise rates. This dataset is proposed to study real-world noise in controlled settings. In this work, we focus on Red Mini-ImageNet since it shows a realistic type of label noise. Red Mini-ImageNet has 100 classes, with each class containing 600 images sampled from the ImageNet dataset <ref type="bibr" target="#b53">[51]</ref>. The images are resized to 32?32 pixels from the original size of 84 ? 84 to have a fair comparison with <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b67">65]</ref>. The noise rates vary from 0% to 80%, but we use the rates 20%, 40%, 60% and 80% to be consistent with the literature <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b68">66]</ref>. <ref type="bibr" target="#b4">3</ref> Performance degradation at high IDN is presented in Appendix C. ANIMAL-10N is another real-world dataset proposed by Song et al. <ref type="bibr" target="#b55">[53]</ref>, which contains 10 animals with 5 pairs having similar appearances (e.g., wolf and coyote, hamster and guinea pig, etc.). The estimated rate of label noise is 8%. There are 50k training images 10k test images. No data augmentation is used, hence the setup is identical to the one proposed in <ref type="bibr" target="#b55">[53]</ref>.</p><p>CLOTHING-1M <ref type="bibr" target="#b66">[64]</ref> is a real-world dataset that comprises 1 million training apparel images taken from 14 categories of online shopping websites. The labels in this dataset are generated from surrounding texts, with an estimated noise of 38.5%. Due to the inconsistency in image sizes, we follow the standard setup in the literature <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b35">33]</ref> and resize the images to 256 ? 256 pixels. This dataset additionally includes 50k, 14k, and 10k manually validated clean training, validation, and testing data, respectively. During training, the clean training and validation sets are not used and only the clean testing set is used for assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>All the methods are implemented in PyTorch <ref type="bibr" target="#b47">[45]</ref>. For the baseline model DivideMix, all the default hyperparameters are considered as mentioned in original paper by Li et al. <ref type="bibr" target="#b35">[33]</ref>. All hyperparameter values mentioned below are from CausalNL <ref type="bibr" target="#b68">[66]</ref> and DivideMix <ref type="bibr" target="#b35">[33]</ref> unless otherwise specified. The size of the latent representation Z is fixed at 25 for CIFAR10, CIFAR100 and Red Mini-Imagenet, 64 for ANIMAL-10N, and 100 for CLOTHING-1M. For CI-FAR10, CIFAR100 and Red Mini-Imagenet, we used nonpretrained PreaAct-ResNet-18 (PRN18) <ref type="bibr" target="#b23">[21]</ref> as an encoder. VGG-19 is used as an encoder for ANIMAL-10N, following SELFIE <ref type="bibr" target="#b55">[53]</ref> and PLC <ref type="bibr" target="#b73">[71]</ref>. For CLOTHING-1M, we used ImageNet-pretrained ResNet-50. Clean data is not used for training.</p><p>The training of the model used stochastic gradient descent (SGD) for DivideMix stage with momentum of 0.9, batch size of 64 and an L2 regularisation whose parameter is 5 ? 10 ?4 . Additionally, Adam is used to train the VAE part of the model. The training runs for 300 epochs for CIFAR10, CIFAR100, Red Mini-Imagenet and ANIMAL-10N. The learning rate is 0.02 which is reduced to 0.002 at half of the number of training epochs. The WarmUp stage lasts for 10 epochs for CIFAR10, 30 for CIFAR100, ANIMAL-10N and Red Mini-Imagenet. For CLOTHING-1M, the WarmUp stage lasts 1 epoch with batch size of 32, and training runs for 80 epochs and learning rate of 0.01 decayed by a factor of 10 after the 40 th epoch .</p><p>For CIFAR10, CIFAR100 <ref type="bibr" target="#b32">[30]</ref>, Red Mini-Imagenet <ref type="bibr" target="#b26">[24]</ref> and ANIMAL-10N <ref type="bibr" target="#b55">[53]</ref>, the encoder has a similar architecture as CausalNL <ref type="bibr" target="#b68">[66]</ref>, with 4 hidden convolutional layers and feature maps containing 32, 64, 128 and 256 <ref type="table">Table 1</ref>. Test accuracy (%) of different methods on CIFAR10 and CIFAR100 <ref type="bibr" target="#b32">[30]</ref> under various IDN noise rates. Most results are extracted from <ref type="bibr" target="#b68">[66]</ref>, while results with * are reported in their respective papers. Results taken from kMEIDTM <ref type="bibr" target="#b11">[10]</ref> are presented with ? .  <ref type="table">Table 2</ref>. Test accuracy (%) for Red Mini-Imagenet (CNWL) <ref type="bibr" target="#b26">[24]</ref>.</p><p>Other model results are as presented in FaMUS <ref type="bibr" target="#b67">[65]</ref> and Prop-Mix <ref type="bibr" target="#b14">[12]</ref>. We presented our proposed results with our proposed InstanceGM and with inclusion of self-supervision <ref type="bibr" target="#b9">[8]</ref> in proposed algorithm (InstanceGM-SS). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Baselines and Measurements</head><p>In this section, we compare our proposed InstanceGM on baseline IDN benchmark datasets in Section 4.3.1, and we also validate our proposed model on various real-world noisy datasets in Section 4.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Instance-Dependent Noise Benchmark Datasets</head><p>The comparison between our InstanceGM and recently proposed approaches on CIFAR10 and CIFAR100 IDN benchmarks is shown in <ref type="table">Table 1</ref>. Note that the proposed approach achieves considerable improvements in both datasets at various IDN noise rates ranging from 20% to 50%. Given that CausalNL represents the main reference for our method, it is important to compare the performance of the two approaches. For CIFAR10, our method is roughly 15% better in all noise rates, and for CIFAR100, our method is between 38% and 45% better. Compared to the current state-of-theart methods in this benchmark (kMEIDTM <ref type="bibr" target="#b11">[10]</ref> and Di-videMix <ref type="bibr" target="#b35">[33]</ref>), our method is around 2% better in CIFAR10 and between 2% to almost 20% better in CIFAR100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Real-world Noisy Datasets</head><p>In <ref type="table">Tables 2 to 4</ref>, we present the results on ANIMAL-10N, Red Mini-Imagenet and CLOTHING-1M, respectively. In general the results show that InstanceGM outperforms or is competitive with the present state-of-the-art models for large-scale web-crawled datasets and small-scale humanannotated noisy datasets. <ref type="table">Table 3</ref> reports the classification accuracy on ANIMAL-10N. We can observe that In-stanceGM achieves slightly better performance than all <ref type="table">Table 3</ref>. Test accuracy (%) of different methods evaluated on ANIMAL-10N <ref type="bibr" target="#b55">[53]</ref> where only noisy data are used to train models. Other models' results are as presented in Nested-CE <ref type="bibr" target="#b10">[9]</ref>, and results with * are reported in their respective papers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Accuracy (%)  <ref type="table">Tables 2 and 4</ref>, demonstrating its ability to handle real-world IDN problems. In particular, <ref type="table">Table 2</ref> shows the results on Red Mini-Imagenet using two set-ups: 1) without pre-training (top part of the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>We show the ablation study of our proposed method on CIFAR10 <ref type="bibr" target="#b32">[30]</ref>, under IDN noise rate of 0.5 and ANIMAL-10N <ref type="bibr" target="#b55">[53]</ref>. On <ref type="table">Table 5</ref>, the performance of CausalNL <ref type="bibr" target="#b68">[66]</ref> is relatively low, which can be explained by the small number of clean samples used by co-teaching <ref type="bibr" target="#b19">[17]</ref>, and the use of MSE for image reconstruction loss <ref type="bibr" target="#b6">5</ref> . We argue that replac- <ref type="table">Table 5</ref>. This ablation study shows the test accuracy % on CI-FAR10 under IDN at noise rate 0.5. First, we show the result of CausalNL <ref type="bibr" target="#b68">[66]</ref>. Second, we show the result of CausalNL <ref type="bibr" target="#b68">[66]</ref> with Co-teaching <ref type="bibr" target="#b19">[17]</ref> replaced by DivideMix <ref type="bibr" target="#b35">[33]</ref> (without Continuous Bernoulli reconstruction). Then at last we show the results of our proposed algorithm InstanceGM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Accuracy (%)</p><p>CausalNL <ref type="bibr" target="#b68">[66]</ref> 78.63 CausalNL [66] + DivideMix <ref type="bibr" target="#b35">[33]</ref> 88.62 * w/o continuous Bernoulli InstanceGM 95.90 <ref type="table">Table 6</ref>. This ablation study shows the test accuracy % on ANIMAL-10N using various architectures (without selfsupervision), including ResNet <ref type="bibr" target="#b22">[20]</ref>, VGG <ref type="bibr" target="#b54">[52]</ref> and Con-vNeXt <ref type="bibr" target="#b41">[39]</ref> with InstanceGM. <ref type="table">Table 3</ref>, reported the results of VGG <ref type="bibr" target="#b54">[52]</ref> to provide a fair comparison with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Test Accuracy (%)</head><p>InstanceGM with ResNet <ref type="bibr" target="#b22">[20]</ref> 82.2 InstanceGM with VGG <ref type="bibr" target="#b54">[52]</ref> 84.6 InstanceGM with ConvNeXt <ref type="bibr" target="#b41">[39]</ref> 84.7</p><p>ing co-teaching <ref type="bibr" target="#b19">[17]</ref> by DivideMix <ref type="bibr" target="#b35">[33]</ref> will improve classification accuracy because it allows the use of the whole training set. To demonstrate that, we take CausalNL <ref type="bibr" target="#b68">[66]</ref> and replace its co-teaching by DivideMix, but keep the MSE reconstruction loss -this model is named CausalNL + Di-videMix (w/o continuous Bernoulli). Note that this allows a ? 10% accuracy improvement from CausalNL, but the use of MSE reconstruction loss can still limit classification accuracy. Hence, by replacing the MSE loss by the continuous Bernoulli loss for image reconstruction, we notice a further ? 7% accuracy improvement. For ANIMAL-10N <ref type="bibr" target="#b55">[53]</ref>, we test InstanceGM with various backbone networks (VGG <ref type="bibr" target="#b54">[52]</ref>, ResNet <ref type="bibr" target="#b22">[20]</ref>, and Con-vNeXt <ref type="bibr" target="#b41">[39]</ref>) and the results are displayed in <ref type="table">Table 6</ref>. Due to the architectural differences, ConvNeXt <ref type="bibr" target="#b41">[39]</ref> performed best on our proposed algorithm, but for a fair comparison with the other models, we use the VGG backbone <ref type="bibr" target="#b54">[52]</ref> results in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented an instance-dependent noisy label learning algorithm method, called InstanceGM. In-stanceGM explores generative and discriminative models <ref type="bibr" target="#b68">[66]</ref>, where for the generative model, we replace the usual MSE image reconstruction loss by the continuous Bernoulli reconstruction loss <ref type="bibr" target="#b42">[40]</ref> that improves the training process, and for the discriminative model, we replace co-teaching by DivideMix <ref type="bibr" target="#b35">[33]</ref> to enable the use of clean and noisy samples during training. We performed extensive experiments on various IDN benchmarks, and our results on CIFAR10, CIFAR100, Red Mini-Imagenet, ANIMAL-10N outperform the results of state-of-the-art methods, particularly in high noise rates and are competitive for CLOTHING-1M. The ablation study clearly shows the importance of the new continuous Bernoulli reconstruction loss <ref type="bibr" target="#b42">[40]</ref> and DivideMix <ref type="bibr" target="#b35">[33]</ref>, with both improving classification accuracy from CausalNL <ref type="bibr" target="#b68">[66]</ref>.</p><p>if ? ? = 0.5 2 otherwise.</p><p>Note that one could also use the Beta distribution whose support space is also [0, 1]. The advantage of using the continuous Bernoulli distribution is the simplicity since we need only one parameter per pixel, while the Beta distribution requires double the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results on CIFAR10 at High IDN Levels</head><p>We investigated the performance of InstanceGM on high IDN levels including 0.7, 0.8 and 0.9. We provided the test classification accuracy on CIFAR10 on <ref type="table">Table 7</ref>. The competing model results are from <ref type="bibr" target="#b28">[26]</ref>. Our InstanceGM shows superior results even in such high noise rate problems. Note that all results at these high noise rate problems are not good but the performance degradation for InstanceGM is lower, compared to the other models. <ref type="table">Table 7</ref>. Test accuracy (%) for CIFAR10 at high IDN rates. All the mentioned results of other methods are as presented in the paper <ref type="bibr" target="#b28">[26]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The proposed InstanceGM trains the Classifiers to output clean labels for instance-dependent noisy-label samples. We first warmup our two classifiers (Classifier-{11,12}) using the classification loss, and then with classification loss we train the GMM to separate clean and noisy samples with the semi-supervised model MixMatch<ref type="bibr" target="#b6">[5]</ref> from the DivideMix<ref type="bibr" target="#b35">[33]</ref> stage. Additionally, another set of encoders (Encoder-{1,2}) are used to generate the latent image features as depicted in the graphical model fromFig. 1. Furthermore, for image reconstruction, the decoders (Decoder-{1,2}) are used by utilizing the continuous Bernoulli loss, and another set of classifiers (Classifier-{21,22}) helps to identify the original noisy labels using the standard cross-entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Graphical model approach for learning with label noise 1: procedure INSTANCEGM(D, T, ? ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(6) 20 : 22 :loss 25 :</head><label>202225</label><figDesc>return q 1 (Y |X) clean-label classifier 21: function VARIATIONAL-FREE ENERGY(x,?, q, p) Calculate loss in Eq. (4) Sample y ? q(Y |X = x i ) Sample a clean label from its variational posterior 23: Sample z ? q(Z|X = x,? =?) Sample a feature representation from its variational posterior 24: Compute the 1st term in Eq. (4): ? ln p(X = x|Z = z, Y = y) image reconstruction Compute the 2nd term in Eq. (4): ? ln p(? =?|X = x, Y = y) noisy-label cross-entropy loss 26:Compute the remaining terms in Eq.(4)27:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>convolutional layers, and the feature maps contain 32, 64, 128, 256 and 512 features. The decoder networks have 5 transposed-convolutional layers and the feature maps have 512, 256, 128, 64 and 32 features.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Noise rate</cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell>CE [65]</cell><cell>47.36</cell><cell>42.70</cell><cell>37.30</cell><cell>29.76</cell></row><row><cell>MixUp [69]</cell><cell>49.10</cell><cell>46.40</cell><cell>40.58</cell><cell>33.58</cell></row><row><cell>DivideMix [33]</cell><cell>50.96</cell><cell>46.72</cell><cell>43.14</cell><cell>34.50</cell></row><row><cell>MentorMix [24]</cell><cell>51.02</cell><cell>47.14</cell><cell>43.80</cell><cell>33.46</cell></row><row><cell>FaMUS [65]</cell><cell>51.42</cell><cell>48.06</cell><cell>45.10</cell><cell>35.50</cell></row><row><cell>InstanceGM</cell><cell>58.38</cell><cell>52.24</cell><cell>47.96</cell><cell>39.62</cell></row><row><cell cols="2">With self-supervised learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PropMix [12]</cell><cell>61.24</cell><cell>56.22</cell><cell>52.84</cell><cell>43.42</cell></row><row><cell cols="2">InstanceGM-SS 4 60.89</cell><cell>56.37</cell><cell>53.21</cell><cell>44.03</cell></row><row><cell cols="5">features. In the decoding stage, we use 4 hidden layer</cell></row><row><cell cols="5">transposed-convolutional network and the feature maps</cell></row><row><cell cols="5">have 256, 128, 64 and 32 features. In Red Mini-Imagenet,</cell></row><row><cell cols="5">we use a similar architecture as CIFAR100 with and without</cell></row><row><cell cols="5">self-supervision [8]. For CLOTHING-1M [64], we use en-</cell></row><row><cell>coder networks with 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table )</head><label>)</label><figDesc></figDesc><table><row><cell>, and 2) with</cell></row><row><cell>self-supervised (SS) pre-training (bottom part of the table).</cell></row><row><cell>The SS pre-training is based on DINO [8] with the un-</cell></row><row><cell>labelled Red Mini-Imagenet dataset, allowing a fair com-</cell></row><row><cell>parison with PropMix [12], which uses a similar SS pre-</cell></row><row><cell>training. Without SS pre-training, our InstanceGM is sub-</cell></row><row><cell>stantially superior to recently proposed approaches. With</cell></row><row><cell>SS pre-training, results show that InstanceGM can improve</cell></row><row><cell>its performance, allowing us to achieve state-of-the-art re-</cell></row><row><cell>sults on Red Mini-Imagenet.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Except for black and white images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Implementation details are present in Appendix A</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">line 80 in https://github.com/a5507203/IDLN/blob/main/causalNL.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">tanh ?1 (1?2? ? ) 1?2? ?</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The appendix is organised as follows:</p><p>? Appendix A presents a detailed description of InstanceGM-SS and experimental details of self-supervision. Appendix A.1 contains the experimental details for the self-supervision method DINO, and Appendix A.2 contains the experimental details of InstanceGM with self-supervision (InstaceGM-SS).</p><p>? Appendix B shows the motivation behind the use of the continuous Bernoulli distribution.</p><p>? Appendix C shows the results on high IDN rates for CIFAR10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-supervision and experimental details</head><p>In addition to training the proposed method from scratch, we also adapt self-supervised learning to pre-train the feature extractor part in the classifier q(Y |X), denoted as InstanceGM-SS in <ref type="table">Table 2</ref>. In particular, we employ DINO <ref type="bibr" target="#b9">[8]</ref> to selfsupervisedly learn a feature extractor using the unlabelled data from the training set of Red Mini-Imagenet (DINO is a self-supervision method that uses self-distillation). Such integration allows our proposed method to be fairly compared with other label noise learning approaches that rely on self-supervision, such as PropMix <ref type="bibr" target="#b14">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Experimental details of self-supervision DINO</head><p>We trained the self-supervised model on Red Mini-Imagenet for 500 epochs on PreAct-ResNet-18 (PRN18). We used the same set of hyper-parameters as provided by DINO. The method follows the teacher-student setting where the weights of the teacher network are exponentially weighted averaged from the student network <ref type="bibr" target="#b21">[19]</ref>. It includes the teacher model temperature for warmup as 0.04 and 0.07 for training, and warmup teacher epochs as 50. The L2 weight decay regularisation is 0.000001, and batch size is 51 per gpu. The initial learning rate is set to 0.3 and the minimum learning rate is set to 0.0048, with the training warm up number of epochs set as 10. In addition,DINO needs various augmented views of the input image. It includes multi-crop strategy <ref type="bibr" target="#b8">[7]</ref> with high-resolution global and low-resolution local views. The two versions of global crops views are considered with scale values of 0.14 and 1. Moreover, the six different local crops views are considered having scale values of 0.05 and 0.14, with teacher momentum as 0.996.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experimental details of InstanceGM-SS</head><p>When we use the self-supervised trained classifier for InstanceGM-SS, we slightly change the settings to train the Red Mini-Imagenet, and results could be find in <ref type="table">Table 2</ref>. In particular, we use the self-supervised PreAct-ResNet-18 as a classifier with the latent representation Z of size 25. We train the network for 80 epochs with the learning rate reduced by 10 after 50 epochs. The warmup stage is reduced to 15 epochs. Otherwise, the previous settings for Red Mini-ImageNet without self-supervision are kept the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation of using continuous Bernoulli distribution</head><p>To explain the motivation behind the use of the continuous Bernoulli likelihood for image reconstruction, we refer to the variational inference technique. In particular, we denote x as an observable variable, e.g., input images, while z as a hidden (or latent) variable. For simplicity, we assume that both x and z are scalars. In variational inference, e.g. VAE, the objective is to maximise the evidence lower bound (ELBO) or minimise the variational-free energy w.r.t. ? -the parameter of the variational posterior q ? (z|x): min</p><p>where p(z) is the prior of z (for example, a standard Gaussian distribution N (0, I)), ? ? R + is a re-weighting factor. In theory, ? = 1, and we use ? here to explain common practice in VAE which is described in the following. The second term in <ref type="bibr" target="#b8">(7)</ref> could be evaluated with a closed-form formula for some simple cases of q ? (z|x) and p(z) or approximated using Monte Carlo sampling. Thus, we would focus on the explanation of the first term -often known as reconstruction loss. Depending on how p ? (x|z) is modelled, we could have different reconstruction losses, as explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Gaussian likelihood</head><p>If p ? (x|z) is a Gaussian distribution: p ? (x|z) = N (x; ?(z), ? 2 (z))), the negative log-likelihood term in <ref type="bibr" target="#b8">(7)</ref> can be written as:</p><p>The correct form of the reconstruction loss in (8) contains two terms including a "weighted" MSE term. However, common practice simply replaces the whole ? ln p ? (x|z) by (x ? ?(z)) 2 , resulting in an incorrect formula. As a result, it requires to fine-tune ? to some small value to balance the contributions of the first and second terms in <ref type="bibr" target="#b8">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Bernoulli likelihood</head><p>If p ? (x|z) is a Bernoulli distribution: p ? (x|z) = B(? ? (z)) where x ? {0, 1} and ? ? (z) ? [0, 1], then the negative log-likelihood in <ref type="formula">(7)</ref> is:</p><p>resulting in the binary cross-entropy loss (BCE) <ref type="bibr" target="#b15">[13]</ref>.</p><p>Simply implementing the reconstruction loss as BCE results in the pervasive error since the input x must be in {0, 1} <ref type="bibr" target="#b42">[40]</ref>, which is applicable for black and white images only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Continuous Bernoulli likelihood</head><p>For colour images, although one can model p ? (x|z) as a Gaussian distribution shown in <ref type="bibr" target="#b9">(8)</ref>, it might be a suboptimal choice since the support of the Gaussian distribution is un-bounded, while image data is bounded. Thus, we use the continuous Bernoulli distribution to model p ? (x|z) <ref type="bibr" target="#b42">[40]</ref> since the continuous Bernoulli distribution is supported in [0, 1] with only one parameter:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2. sample a representation from its prior: z ? p(Z), 3. sample an input data from its continuous Bernoulli distribution:x ? CB</title>
		<imprint/>
	</monogr>
	<note>X; ?(z, y)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">sample the corresponding noisy label from its categorical distribution:? ? Cat</title>
		<imprint/>
	</monogr>
	<note>? ; ?(x, y)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxinder</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Byeonghu Na, Kyungwoo Song, and Il-Chul Moon. From noisy prediction to true label: Noisy prediction calibration via generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesun</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonho</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active label cleaning for improved dataset quality under resource constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?lanie</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kerem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruthi</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Confidence scores make instancedependent label-noise learning possible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="825" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting co-teaching with compression regularization for label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan Ak</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2688" to="2692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiong</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance-dependent label-noise learning with manifoldregularized transition matrix estimation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised learning with side information for noisy labeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangzeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="306" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Propmix: Hard sample filtering and proportional mixup for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Filipe R Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On denoising autoencoders trained to minimise binary crossentropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08487</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep selflearning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep bilevel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An information fusion approach to learning with instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FINE samples for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhwan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Resolving training biases via influence-based data relabeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linpeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>University of Toronto</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating a kernel fisher discriminant in the presence of label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="306" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I S?nchez. A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud Arindra Adiyoso</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20331" to="20342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding instance-level label noise: Disparate impacts and treatments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6725" to="6735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Guo</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="6226" to="6236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A ConvNet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The continuous Bernoulli: fixing a pervasive error in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Loaiza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Ganem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6543" to="6553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Can gradient clipping mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3D object detection for autonomous driving: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="108796" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hetegcn: Heterogeneous graph convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ragesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Bairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Lingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Vikas C Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">How does early stopping help generalization against label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust learning by self-transition for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yooju</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1490" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yooju</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">IMAE for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Openset label noise can improve robustness against inherent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lue</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchunzi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Sample selection with uncertainty of losses for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00445</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Part-dependent label noise: Towards instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Faster meta update strategy for noise-robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Instance-dependent label-noise learning under a structural causal model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dual T: Reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7260" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning noise transition matrix from only noisy labels via total variation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yivan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12501" to="12512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning with feature-dependent label noise: A progressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Contrast to divide: Self-supervised pre-training for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1657" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A second-order approach to learning with instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Clusterability as an alternative to anchor points when learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
