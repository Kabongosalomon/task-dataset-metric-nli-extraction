<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLRNet: Cross Layer Refinement Network for Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabu</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLRNet: Cross Layer Refinement Network for Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane is critical in the vision navigation system of the intelligent vehicle. Naturally, lane is a traffic sign with high-level semantics, whereas it owns the specific local pattern which needs detailed low-level features to localize accurately. Using different feature levels is of great importance for accurate lane detection, but it is still underexplored. In this work, we present Cross Layer Refinement Network (CLRNet) aiming at fully utilizing both highlevel and low-level features in lane detection. In particular, it first detects lanes with high-level semantic features then performs refinement based on low-level features. In this way, we can exploit more contextual information to detect lanes while leveraging local detailed lane features to improve localization accuracy. We present ROIGather to gather global context, which further enhances the feature representation of lanes. In addition to our novel network design, we introduce Line IoU loss which regresses the lane line as a whole unit to improve the localization accuracy. Experiments demonstrate that the proposed method greatly outperforms the state-of-the-art lane detection approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Lane detection is an important yet challenging task in computer vision, which requires the network to predict lanes in an image. Detecting lanes can benefit many applications, such as autonomous driving and the Advanced Driver Assistance System (ADAS), which helps intelligent vehicles localize themselves better and drive safer.</p><p>Benefiting from the effective feature representation of CNN, many approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> have obtained promising performance. However, there are still some challenges for detecting accurate lanes. Lane has high-level semantics, whereas it owns the specific local pattern which needs detailed low-level features to localize accurately. How to utilize different feature levels effectively in CNN remains a problem. As we can see in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, the landmark and * Equal contribution. lane line have different semantics, but they share the similar feature (e.g., the long white line). It is hard to distinguish them without high-level semantics and global context. On the other hand, the locality is also essential since lane is long and thin with the simple local pattern. We show the detection result of high-level features in <ref type="figure" target="#fig_0">Fig 1(b)</ref>, though the lane is detected, its location is not precise. Thus, the low-level and high-level information are complementary for accurate lane detection. Previous works either model local geometry of lanes and integrate them into global results <ref type="bibr" target="#b19">[20]</ref> or construct a fully-connected layer with global features to predict lanes <ref type="bibr" target="#b18">[19]</ref>. These detectors have demonstrated the importance of local or global features for lane detection, but they don't take advantage of both features, yielding inaccurate detection performance. Another common problem in lane detection is no visual evidence for the presence of lanes. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, the lane is occupied by the car while in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>, the lane is hard to recognize due to the extreme lighting condition. In the literature, SCNN <ref type="bibr" target="#b16">[17]</ref> and RESA <ref type="bibr" target="#b32">[33]</ref> propose a message-passing mechanism to gather global context, but these methods perform pixel-wise prediction and don't take lane as a whole unit. Thus their performances lag behind many state-of-the-art detectors.</p><p>In this paper, we propose a new framework, Cross Layer Refinement Network (CLRNet), which fully utilizes lowlevel and high-level features for lane detection. Specifically, we first perform detection in high semantic features to coarsely localize lanes. Then, we perform refinement based on fine-detail features to get more precise locations.</p><p>Progressively refining the location of lane and feature extraction leads to high accuracy detection results. To solve the problem of non-visual evidence of lane, we introduce ROIGather to capture more global contextual information by building the relation between the ROI lane feature and the whole feature map. Moreover, we define the IoU of lane lines and propose the Line IoU (LIoU) loss to regress the lane as a whole unit and considerably improve the performance compared with standard loss, i.e., smooth-l 1 loss.</p><p>We demonstrate the effectiveness of our method on three lane detection benchmarks, i.e., CULane <ref type="bibr" target="#b16">[17]</ref>, Tusimple <ref type="bibr" target="#b25">[26]</ref>, and LLAMAS <ref type="bibr" target="#b1">[2]</ref>. The experiment results show our method achieves state-of-the-art accuracy on all datasets. The main contributions can be summarized as follows:</p><p>? We demonstrate low-level and high-level features are complementary for lane detection, and we propose a novel network architecture (CLRNet) to fully utilize low-level and high-level features for lane detection.</p><p>? We propose ROIGather to further enhance the representation of lane features by gathering global context, which can also be plugged into other networks.</p><p>? We propose Line IoU (LIoU) loss tailored for lane detection, regressing the lane as the whole unit and considerably improving the performance.</p><p>? To better compare the localization accuracy of different detectors, we also adopt the new mF1 metrics. We demonstrate the proposed method greatly outperforms other state-of-the-art approaches on three lane detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>According to the representation of lane, current CNNbased lane detection can be divided into three categories: segmentation-based method, anchor-based method, and parameter-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Segmentation-based methods</head><p>Modern algorithms typically adopt a pixel-wise prediction formulation, i.e., treat lane detection as a semantic segmentation task. SCNN <ref type="bibr" target="#b16">[17]</ref> proposes a message-passing mechanism to address no visual evidence problem, which captures the strong spatial relationship for lanes. SCNN significantly improves the lane detection performance, but the method is slow for real-time application. RESA <ref type="bibr" target="#b32">[33]</ref> proposes a real-time feature aggregation module, enabling the network to gather the global feature and improve performance. In CurveLane-NAS <ref type="bibr" target="#b27">[28]</ref>, they use neural architecture search (NAS) to find a better network for capturing accurate information to benefit the detection of curve lanes. However, the NAS is extremely expensive computationally and costs huge GPU hours. These segmentationbased methods are ineffective and time-consuming since they perform pixel-wise prediction on the whole image and don't consider lanes as a whole unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anchor-based methods</head><p>Anchor-based methods in lane detection can be divided into two classes, e.g., line anchor-based methods and row anchor-based methods. Line anchor-based methods adopt predefined line anchors as references to regress accurate lanes. Line-CNN <ref type="bibr" target="#b7">[8]</ref> is the pioneering work to use line anchors in lane detection. LaneATT <ref type="bibr" target="#b23">[24]</ref> proposes a novel anchor-based attention mechanism that aggregates global information. It achieves state-of-the-art results and shows both high efficacy and efficiency. SGNet <ref type="bibr" target="#b21">[22]</ref> introduces a novel vanish-point guided anchor generator and adds multiple structural guidance to improve performance. As for the row anchor-based method, it predicts the probable cell for each predefined row on images. UFLD <ref type="bibr" target="#b18">[19]</ref> first proposes a row anchor-based lane detection method and adopts lightweight backbones to achieve high inference speed. Albeit simple and fast, its overall performance is not good. CondLaneNet <ref type="bibr" target="#b11">[12]</ref> introduces a conditional lane detection strategy based on conditional convolution and row anchorbased formulation, i.e., it first locates start points of lane lines then performs row anchor-based lane detection. However, start points are hard to recognize in some complex scenarios, which results in relatively inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Parameter-based methods</head><p>Different from points regression, parameter-based methods model the lane curve with parameters and regress these parameters to detect lanes. PolyLaneNet <ref type="bibr" target="#b24">[25]</ref> adopts a polynomial regression problem and achieves high efficiency. LSTR <ref type="bibr" target="#b12">[13]</ref> takes road structures with camera pose into account to model the lane shape, then introduces the transformer to lane detection task to get the global feature. Parameter-based methods have fewer parameters to regress, but they are sensitive to the predicted parameters, e.g., the error prediction on high-order coefficient may cause shape change of lanes. Though parameter-based methods have fast inference speed, they still struggle to achieve higher performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Lane Representation</head><p>Lane Prior. Lanes are thin and long with strong shape priors, thus a predefined lane prior can help the network better localize lanes. In common object detection, objects are represented by rectangular boxes. Nevertheless, the box is not appropriate for the representation of the long line. Following <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b23">[24]</ref>, we use equally-spaced 2D-points as lane representation. Specifically, lane is expressed as a sequence of points, i.e., P = {(x 1 , y 1 ), ? ? ? , (x N , y N )}. The y coordinate of points is equally sampled through image vertically, i.e., y i = H N ?1 * i, where H is image height. Accordingly, the x coordinate is associated with the respective y i ? Y . In our paper, we call this representation Lane Prior. Each lane prior will be predicted by the network and consists of four components: (1) foreground and background probabilities.</p><p>(2) the length of lane prior. (3) the start point of the lane line and the angle between the x-axis of the lane prior (termed as x, y, and ?). (4) The N offsets, i.e., the horizontal distance between the prediction and its ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross Layer Refinement</head><p>Motivation. In neural networks, deep high-level features strongly respond to entire objects with more semantic meanings, while the shallow low-level features are with more local contextual information. Allowing lane objects to access high-level features can help exploit more useful context information, e.g., to distinguish lane lines or landmarks. In the meantime, fine-detail features help detect lanes with high localization accuracy. In object detection <ref type="bibr" target="#b8">[9]</ref>, it builds the feature pyramid to leverage the pyramidal shape of a ConvNet's feature hierarchy and assigns different scales of objects to different pyramid levels. However, it is hard to directly assign a lane to only one level since high-level and low-level features are both critical for lanes. Inspired by Cascade RCNN <ref type="bibr" target="#b2">[3]</ref>, we can assign lane objects to all levels and detect lanes sequentially. In particular, we can detect lanes with high-level features to localize lanes coarsely. Based on the detected lanes, we can refine them with more detailed features.</p><p>Refinement structure. Our goal is to leverage a ConvNet's pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout. We take ResNet <ref type="bibr" target="#b5">[6]</ref> as the backbone and use {L 0 , L 1 , L 2 } to denote feature levels generated by FPN. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our cross layer refinement starts from the highest level L 0 and gradually approaches L 2 . We use {R 0 , R 1 , R 2 } to denote the corresponding refinements. Then we can build a sequence of refinements</p><formula xml:id="formula_0">P t = P t?1 ? R t (L t?1 , P t?1 ),<label>(1)</label></formula><p>where t = 1, ? ? ? , T , T is the total number of refinements. Our method performs detection from highest level layer with high semantics. P t is the parameter of lane prior (start point coordinate x, y and angle ?), which is learnable inspired by <ref type="bibr" target="#b22">[23]</ref>. For the first layer L 0 , the P 0 is uniformly distributed on image plane. The refinement R t takes the P t as input to get the ROI lane features and then performs two FC layers to get the refined parameter P t . Progressively refining the lane prior and feature extraction is important for the success of cross layer refinement. Note that, our method is not limited to FPN structure, only using ResNet <ref type="bibr" target="#b5">[6]</ref> or adopting PAFPN <ref type="bibr" target="#b13">[14]</ref> is also suitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ROIGather</head><p>Motivation. After we assign lane priors to each feature map, we can get features of lane priors with ROIAlign <ref type="bibr" target="#b4">[5]</ref>. However, the contextual information of these features is still not sufficient. In some cases, the lane instance may be occupied or blurred with extreme lighting conditions. Thus there is no local visual evidence for the presence of lane. To determine whether a pixel belongs to a lane, we need to look at nearby features. Some recent studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref> also indicate that the performance could be improved if making sufficient use of long-range dependencies. Thus, we can gather more useful contextual information to better learn lane feature.</p><p>To this end, we add convolutions along the lane prior. In this way, each pixel in the lane prior can gather information of nearby pixels, and occupied parts can be reinforced from that information. Moreover, we build relations between features of lane priors and the whole feature map. Thus, it can exploit more contextual information to learn better feature representations.</p><p>ROIGather structure. To gather the global context for features of lane priors, we first compute the attention <ref type="bibr" target="#b26">[27]</ref> matrix W between ROI lane prior feautre (X p ) and the global feature map (X f ), which is written as:</p><formula xml:id="formula_1">W = f ( X T p X f ? C ),<label>(2)</label></formula><p>where f is a normalize function sof tmax. The aggregated feature is written as:</p><formula xml:id="formula_2">G = WX T f .<label>(3)</label></formula><p>The output G reflects the bonus of X f to X p which is selected from all locations of X f . Finally, we add the output to the original input X p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Line IoU loss</head><p>Motivation. As discussed above, the lane prior consists of discrete points needed to be regressed with its ground truth. The commonly used distance loss like smooth-l 1 can be used to regress these points. However, this kind of loss takes points as separate variables, which is an oversimplified assumption <ref type="bibr" target="#b30">[31]</ref>, resulting in less accurate regression. In contrast to distance loss, Intersection over Union (IoU) can take the lane prior as a whole unit to regress and it is tailored for evaluation metric <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. In our work, we derive an easy and effective algorithm to compute the Line IoU (LIoU) loss.</p><p>Formula. We introduce Line IoU loss starting from the definition of the line segment IoU, which is the ratio of interaction over union between two line segments. For each point in the predicted lane as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we first extend it (x p i ) with a radius e into a line segment. Then IoU can be calculated between the extended line segment and its ground truth, which is written as:</p><formula xml:id="formula_3">IoU = d O i d U i = min(x p i + e, x g i + e) ? max(x p i ? e, x g i ? e) max(x p i + e, x g i + e) ? min(x p i ? e, x g i ? e) ,<label>(4)</label></formula><p>where x p i ? e, x p i + e are the extended points of x p i , x g i ? e, x g i + e are the corresponding ground truth points. Note that, d O i can be negative, which can make it feasible to optimize in case of non-overlapping line segments.</p><p>Then LIoU can be considered as the combination of infinite line points. To simplify the expression and make it easy to compute, we transform it into a discrete form,</p><formula xml:id="formula_4">LIoU = N i=1 d O i N i=1 d U i .<label>(5)</label></formula><p>Then, the LIoU loss is defined as</p><formula xml:id="formula_5">L LIoU = 1 ? LIoU,<label>(6)</label></formula><p>where ?1 ? LIoU ? 1, when two lines overlay perfectly, then LIoU = 1, LIoU converges to -1 when two lines are far away. Our Line IoU loss exhibits two advantages: (1) It is simple and differentiable, which is very easy to implement parallel computations. <ref type="bibr">(</ref>2) It predicts the lane as a whole unit, which helps improve the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Infercence Details</head><p>Positive samples selection. During training, each ground truth lane is assigned with one or more predicted lanes dynamically as positive samples, which is inspired by <ref type="bibr" target="#b3">[4]</ref>. In particular, we first sort the predicted lanes based on the assigning cost, which is defined as:</p><formula xml:id="formula_6">C assign = w sim C sim + w cls C cls , C sim = (C dis ? C xy ? C theta ) 2 .<label>(7)</label></formula><p>Here C cls is the focal cost <ref type="bibr" target="#b9">[10]</ref> between predictions and labels. C sim is the similarity cost between predicted lanes and ground truth. It consists of three parts, C dis means the average pixel distance of all valid lane points, C xy means the distance of start point coordinates, C theta means the difference of the theta angle, they are all normalized to [0, 1]. w cls and w sim are weight coefficients of each defined component. Each ground truth lane is assigned with a dynamic number (top-k) of predicted lanes based on C assign .</p><p>Training Loss. Training loss consists of classification loss and regression loss. The regression loss is only performed on the assigned samples. The overall loss function is defined as:</p><formula xml:id="formula_7">L total = w cls L cls + w xytl L xytl + w LIoU L LIoU . (8)</formula><p>L cls is the focal loss between predictions and labels, L xytl is the smooth-l 1 loss for the start point coordinate, theta angle and lane length regression, L LIoU is the Line IoU loss between the predicted lane and ground truth. Optionally, we can add an auxiliary segmentation loss following <ref type="bibr" target="#b18">[19]</ref>. It is only used in the training period and has no cost in inference.</p><p>Inference. We set a threshold with a classification score to filter the background lanes (low score lane priors), and we use nms to remove high-overlapped lanes following <ref type="bibr" target="#b23">[24]</ref>. Our method can also be nms-free if we use the one-to-one assignment, i.e., set the top-k = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on two widely used lane detection benchmark datasets: CULane <ref type="bibr" target="#b16">[17]</ref> and Tusimple <ref type="bibr" target="#b25">[26]</ref> and one recently released benchmark (LLAMAS <ref type="bibr" target="#b1">[2]</ref>).</p><p>CULane <ref type="bibr" target="#b16">[17]</ref> is a large scale challenging dataset for lane detection. It contains nine challenging categories, such as crowded, night, cross, etc. The CULane dataset consists of 100,000 images for train, validation, and test sets. All the images have 1640 ? 590 pixels.</p><p>LLAMAS <ref type="bibr" target="#b1">[2]</ref> is also a large scale lane detection dataset with over 100k images. The lane markers in LLAMAS are automatically annotated with highly-accurate maps. Since the label of test set is not public, we upload the detection result to the website of LLAMAS benchmark for testing.</p><p>Tusimple <ref type="bibr" target="#b25">[26]</ref> lane detection benchmark is one of the most widely used datasets in lane detection. It contains only highway scenes with 3268 images for training, 358 for validation, and 2782 for testing. All have 1280 ? 720 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We adopt the ResNet <ref type="bibr" target="#b5">[6]</ref> and DLA <ref type="bibr" target="#b29">[30]</ref> as our pre-trained backbones. All input images are resized to 320 ? 800. For data augmentation, similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>, we use random affine transformation (translation, rotation, and scaling), random horizontal flips. In the optimizing process, we use AdamW <ref type="bibr" target="#b15">[16]</ref> optimizer with an initial learning rate of 1e-3 and cosine decay learning rate strategy <ref type="bibr" target="#b14">[15]</ref> with power set to 0.9. We train 15 epochs, 70 epochs, 20 epochs for CULane, Tusimple, and LLAMAS, respectively. Our network is implemented based on Pytorch with 1GPU to run all the experiments. We set the number points of lane prior N = 72, and the sampled number N p = 36. The resized H, W in ROIGather are 10, 25, respectively, channel C = 64. The extended radius e in LIoU is 15. The coefficients of assigning cost are set as w cls = 1 and w sim = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metric</head><p>We adopt the F1-measure as evaluation metric for CU-Lane <ref type="bibr" target="#b16">[17]</ref> and LLAMAS <ref type="bibr" target="#b1">[2]</ref>. Intersection-over-union (IoU) is calculated between predictions and ground truth. Predicted lanes whose IoU are larger than a threshold (0.5) are considered as true positives (TP). The F 1 is defined as:</p><formula xml:id="formula_8">F 1 = 2 ? P recision ? Recall P recision + Recall .</formula><p>Following COCO <ref type="bibr" target="#b10">[11]</ref> detection metric, we also report a new metric mF1 to better compare the localization performance of algorithms. It is defined as mF1 = (F1@50 + F1@55 + ? ? ? + F1@95)/10, where F1@50, F1@55, ? ? ? , F1@95 are F1 metrics when IoU thresholds are 0.5, 0.55, ? ? ? , 0.95 respectively. This is a break from the tradition which will reward detectors with better localization results. For Tusimple <ref type="bibr" target="#b25">[26]</ref> dataset, the evaluation formula is  <ref type="table">Table 1</ref>. State-of-the-art results on CULane. For a fairer comparison, we remeasure the FPS of the source code available detectors using one NVIDIA 1080Ti GPU on the same machine, * means FPS on TensorRT. In addition, we also evaluation these detectors to report the mF1, F1@50, F1@75. For "Cross" category , only false positives are shown. The reported metric of these categories is based on F1@50.</p><formula xml:id="formula_9">Accuracy = clip C clip clip S clip ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the state-of-the-art results</head><p>Performance on CULane. We show the results of our method on the CULane lane detection benchmark dataset and compare them with other popular lane detection methods. As illustrated in <ref type="table">Table 1</ref>, our proposed method achieves a new state-of-the-art on CULane with an 80.47 F1@50 measure. The ResNet18 version of our method achieves 79.58 F1@50, which is even higher than Cond-LaneNet (ResNet101) while getting 1.4 points higher than CondLaneNet (ResNet18). In particular, we surpass Cond-LaneNet (ResNet18) by 3.4% mF1, which indicates our method better regresses lanes with high localization accuracy. Comparing line anchor-based method LaneATT, our ResNet18 version surpasses 7.88 % mF1 and 4.45 % F1@50, respectively. In the meantime, CLRNet can achieve 206 FPS in one NVIDIA 1080Ti GPU with TensorRT, which is efficient for real-time lane detection. We show the qualitative results on the CULane dataset in <ref type="figure" target="#fig_5">Fig. 6</ref>. Segmentation-based methods like RESA don't predict the lane as a whole unit, which can not preserve the smoothness of lanes. CondLaneNet only predicts one start point of the lane as the proposal, it is easy to miss some lane instances. Our method can predict continuous and smooth lanes in these challenging scenarios, which demonstrates our method can definitely gather global context and has a strong ability to detect accurate lanes.</p><p>Performance on LLAMAS. The result on the LLAMAS dataset is shown in <ref type="table">Table 2</ref>. Our method outperforms Poly-LaneNet <ref type="bibr" target="#b24">[25]</ref> and LaneATT <ref type="bibr" target="#b23">[24]</ref>   <ref type="table">Table 2</ref>. State-of-the-art results on LLAMAS. Additionally, we rerun the evaluation for these methods with source code and trained models to get the mF1, F1@50, F1@75.</p><p>F1@50 respectively on the test set, which is significant improvement. Although LaneAF <ref type="bibr" target="#b0">[1]</ref> achieves 96.90 F1@50 in the valid dataset, its inference speed is slow (near 20FPS), which makes it hard for deployment. Moreover, our method achieves near 2 points mF1 higher than LaneAF, which demonstrates our method is more accurate in localization.</p><p>Performance on Tusimple. <ref type="table" target="#tab_4">Table 3</ref> shows the performance comparison with state-of-the-art approaches. The performance difference between different methods on this dataset is very small, which shows the result in this dataset seems to be saturated (high value) already. Our method achieves a new start-of-the-art in terms of F1 score and surpasses the previous state-of-the-art with a 0.6% F1 score. This significant improvement manifests the effectiveness of our method.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>To validate the effectiveness of different components of the proposed method, we conducted several experiments on the CULane dataset to show the performance.</p><p>Overall Ablation Study. To analyze the importance of each proposed method, we report the overall ablation studies in <ref type="table" target="#tab_5">Table 4</ref>. We gradually add LIoU loss, Cross Layer Refinement, and ROIGather on the ResNet18 baseline. LIoU loss improves the mF1 from 51.90 to 52.80. This result validates that the localization accuracy is much im- proved. Moreover, the refinement further improves the mF1 to 54.74. Results in mF1, F1@50, F1@70, and F1@90 are consistently improved, which validates that leveraging high-level and low-level semantic features to detect lanes is useful and yields consistent improvements. ROIGather further improves mF1 by 0.5%, which validates rich global context can enhance the representation of lane features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis for ROIGather. To further demonstrate how</head><p>ROIGather works in the network, we visualize the attention map (Eq. 2) in <ref type="figure" target="#fig_4">Fig. 5</ref>. It shows attention weight between the ROI feature of the lane prior (orange line) and the whole feature map. The brighter the color is, the larger the weight value is. Notably, the proposed ROIGather can (i) effectively gather global context with rich semantic information, (ii) capture the feature of foreground lanes even under occlusion. More quantitative results are in Appendix.</p><p>Ablation study on Cross Layer Refinement. Ablation studies of Cross Layer Refinement are shown in <ref type="table">Table 5</ref>. We first implement the detector with only one layer to perform refinement. As we can see from the result (setting R 0 , R 1 , R 2 ), these three refinements get similar results. R 2 gets relatively high F1@90 while the F1@50 is relatively low, indicating low-level features help regress lanes accurately. However, it may cause false detection due to losing high semantic information. We select the better result R 0 and gradually add more refinements. As R 0 ? R 0 shows,   <ref type="table">Table 5</ref>. Ablation studies of on different refinement methods. Ri is the refinement method discussed in Sec. 3.2. ADD means add all features with refinement iteration=3 for a fairer comparison.</p><p>it gets slightly improvement. Other fusion feature methods like adding all features still cannot give an improvement. Adopting the refinement from R 0 to R 2 is much better than others, which validates our cross layer refinement can utilize high-level and low-level features better.</p><p>Ablation Study on Line-IoU Loss. Ablation studies of Line IoU loss are shown in <ref type="table">Table 6</ref>. We first turn loss weight to select the best regression weight of smooth-l 1 . We observe that the regression loss of smooth-l 1 is much larger than classification loss when regression weight is 1.5. Results show decreasing weight to 0.5 is relatively better. In contrast, LIoU loss is more stable and improves the performance by near 1 point mF1. To be more specific, the improvements mostly come from high overlapping metrics, like F1@80 and F1@90. These experimental results validate that our Line IoU loss can achieve better performance and make the model better converged. We show the proposed Line IoU loss can also improve the performance of LaneATT <ref type="bibr" target="#b23">[24]</ref>, details can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Weight mF1 F1@50 F1@60 F1@70 F1@80 F1@90  <ref type="table">Table 6</ref>. Ablation studies of Line IoU loss on CULane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present Cross Layer Refinement Network (CLRNet) for lane detection. CLRNet can exploit high-level features to predict lanes while leveraging localdetailed features to improve localization accuracy. To solve the no visual evidence for the presence of lane, we propose ROIGather to enhance the representation of lane features by building relations with all pixels. To regress lane as a whole unit, we propose Line IoU loss tailored for lane detection, which considerably improves the performance compared with standard loss, i.e., smooth-l 1 loss. Our method is evaluated on three lane detection benchmark datasets, i.e., CULane, LLamas, and Tusimple. Experiments show our proposed method outperforms current state-of-the-art lane detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustrations of hard cases for lane detection. (a) The detection result of low-level features. It mistakes landmark as lane due to losing global context. (b) The detection result of high-level features. It predicts inaccurate localization of the lane. (c) The case that lane is almost occupied by the car. (d) The case that lane is blurred by the extreme lighting condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed CLRNet. (a) The network generates feature maps from FPN [9] structure. Subsequently, each lane prior will be refined from high-level features to low-level features. (b) Each head will exploit more contextual information for lane prior features. (c) Classification and regression of lane priors. The proposed Line IoU loss helps further improve the regression performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of Line IoU. Line IoU (interaction over union) can be calculated by integrating the IoU of the extended segment in terms of sampled xi position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Latency vs. F1-score of state-of-the-art methods on CULane and Tusimple benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of attention weight in ROIGather. It shows attention (Eq. 2) between the ROI feature of the lane prior and the whole feature map. The orange line is the correspond lane prior. The red regions corresponds to high score in attention weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization results of UFLD, RESA, LaneATT, CondLane and our method on CULane testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The ROIGather module is lightweighted and easy to implement. It takes feature map and lane priors as input, each lane prior has N points. For each lane prior, we follow ROIAlign<ref type="bibr" target="#b4">[5]</ref> to get the ROI feature of lane prior (X p ? R C?Np ). Unlike ROIAlign for bounding box, we uniform sample N p points from the lane prior and use bilinear interpolation to compute the exact values of input features at these locations. For ROI features of L 1 , L 2 , we concatenate the ROI features of previous layers to enhance feature representations. Convolutions are performed on the extracted ROI features to gather nearby features for each lane pixel. To save memory, we use fully-connected to further extract the lane prior feature (X p ? R C?1 ). The feature map is resized to X f ? R C?H?W and flattened to X f ? R C?HW . Detail settings are in Sec. 4.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where C clip , S clip are the number of correct points and the number of ground truth points of a image respectively.</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone mF1 F1@50 F1@75</cell><cell>FPS</cell><cell cols="10">GFlops Normal Crowded Dazzle Shadow No line Arrow Curve Cross Night</cell></row><row><cell>SCNN [17]</cell><cell cols="2">VGG16 38.84</cell><cell>71.60</cell><cell>39.84</cell><cell>7.5</cell><cell>328.4</cell><cell>90.60</cell><cell>69.70</cell><cell>58.50</cell><cell>66.90</cell><cell>43.40</cell><cell>84.10</cell><cell>64.40</cell><cell>1990</cell><cell>66.10</cell></row><row><cell>RESA [33]</cell><cell>ResNet34</cell><cell>-</cell><cell>74.50</cell><cell>-</cell><cell>45.5</cell><cell>41.0</cell><cell>91.90</cell><cell>72.40</cell><cell>66.50</cell><cell>72.00</cell><cell>46.30</cell><cell>88.10</cell><cell>68.60</cell><cell>1896</cell><cell>69.80</cell></row><row><cell>RESA [33]</cell><cell cols="2">ResNet50 47.86</cell><cell>75.30</cell><cell>53.39</cell><cell>35.7</cell><cell>43.0</cell><cell>92.10</cell><cell>73.10</cell><cell>69.20</cell><cell>72.80</cell><cell>47.70</cell><cell>88.30</cell><cell>70.30</cell><cell>1503</cell><cell>69.90</cell></row><row><cell>FastDraw [18]</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.3</cell><cell>-</cell><cell>85.90</cell><cell>63.60</cell><cell>57.00</cell><cell>69.90</cell><cell>40.60</cell><cell>79.40</cell><cell>65.20</cell><cell>7013</cell><cell>57.80</cell></row><row><cell>E2E [29]</cell><cell>ERFNet</cell><cell>-</cell><cell>74.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.00</cell><cell>73.10</cell><cell>64.50</cell><cell>74.10</cell><cell>46.60</cell><cell>85.80</cell><cell>71.90</cell><cell>2022</cell><cell>67.90</cell></row><row><cell>UFLD [19]</cell><cell cols="2">ResNet18 38.94</cell><cell>68.40</cell><cell>40.01</cell><cell>282</cell><cell>8.4</cell><cell>87.70</cell><cell>66.00</cell><cell>58.40</cell><cell>62.80</cell><cell>40.20</cell><cell>81.00</cell><cell>57.90</cell><cell>1743</cell><cell>62.10</cell></row><row><cell>UFLD [19]</cell><cell>ResNet34</cell><cell>-</cell><cell>72.30</cell><cell>-</cell><cell>170</cell><cell>16.9</cell><cell>90.70</cell><cell>70.20</cell><cell>59.50</cell><cell>69.30</cell><cell>44.40</cell><cell>85.70</cell><cell>69.50</cell><cell>2037</cell><cell>66.70</cell></row><row><cell>PINet [7]</cell><cell cols="2">Hourglass 46.81</cell><cell>74.40</cell><cell>51.33</cell><cell>25</cell><cell>-</cell><cell>90.30</cell><cell>72.30</cell><cell>66.30</cell><cell>68.40</cell><cell>49.80</cell><cell>83.70</cell><cell>65.20</cell><cell>1427</cell><cell>67.70</cell></row><row><cell>LaneATT [24]</cell><cell cols="2">ResNet18 47.35</cell><cell>75.13</cell><cell>51.29</cell><cell>153</cell><cell>9.3</cell><cell>91.17</cell><cell>72.71</cell><cell>65.82</cell><cell>68.03</cell><cell>49.13</cell><cell>87.82</cell><cell>63.75</cell><cell>1020</cell><cell>68.58</cell></row><row><cell>LaneATT [24]</cell><cell cols="2">ResNet34 49.57</cell><cell>76.68</cell><cell>54.34</cell><cell>129</cell><cell>18.0</cell><cell>92.14</cell><cell>75.03</cell><cell>66.47</cell><cell>78.15</cell><cell>49.39</cell><cell>88.38</cell><cell>67.72</cell><cell>1330</cell><cell>70.72</cell></row><row><cell>LaneATT [24]</cell><cell cols="2">ResNet122 51.48</cell><cell>77.02</cell><cell>57.50</cell><cell>20</cell><cell>70.5</cell><cell>91.74</cell><cell>76.16</cell><cell>69.47</cell><cell>76.31</cell><cell>50.46</cell><cell>86.29</cell><cell>64.05</cell><cell>1264</cell><cell>70.81</cell></row><row><cell>LaneAF [1]</cell><cell cols="2">ERFNet 48.60</cell><cell>75.63</cell><cell>54.53</cell><cell>24</cell><cell>22.2</cell><cell>91.10</cell><cell>73.32</cell><cell>69.71</cell><cell>75.81</cell><cell>50.62</cell><cell>86.86</cell><cell>65.02</cell><cell>1844</cell><cell>70.90</cell></row><row><cell>LaneAF [1]</cell><cell cols="2">DLA34 50.42</cell><cell>77.41</cell><cell>56.79</cell><cell>20</cell><cell>23.6</cell><cell>91.80</cell><cell>75.61</cell><cell>71.78</cell><cell>79.12</cell><cell>51.38</cell><cell>86.88</cell><cell>72.70</cell><cell>1360</cell><cell>73.03</cell></row><row><cell>SGNet [22]</cell><cell>ResNet18</cell><cell>-</cell><cell>76.12</cell><cell>-</cell><cell>117</cell><cell>-</cell><cell>91.42</cell><cell>74.05</cell><cell>66.89</cell><cell>72.17</cell><cell>50.16</cell><cell>87.13</cell><cell>67.02</cell><cell>1164</cell><cell>70.67</cell></row><row><cell>SGNet [22]</cell><cell>ResNet34</cell><cell>-</cell><cell>77.27</cell><cell>-</cell><cell>92</cell><cell>-</cell><cell>92.07</cell><cell>75.41</cell><cell>67.75</cell><cell>74.31</cell><cell>50.90</cell><cell>87.97</cell><cell>69.65</cell><cell>1373</cell><cell>72.69</cell></row><row><cell>FOLOLane [20]</cell><cell>ERFNet</cell><cell>-</cell><cell>78.80</cell><cell>-</cell><cell>40</cell><cell>-</cell><cell>92.70</cell><cell>77.80</cell><cell>75.20</cell><cell>79.30</cell><cell>52.10</cell><cell>89.00</cell><cell>69.40</cell><cell>1569</cell><cell>74.50</cell></row><row><cell>CondLane [12]</cell><cell cols="2">ResNet18 51.84</cell><cell>78.14</cell><cell>57.42</cell><cell>173</cell><cell>10.2</cell><cell>92.87</cell><cell>75.79</cell><cell>70.72</cell><cell>80.01</cell><cell>52.39</cell><cell>89.37</cell><cell>72.40</cell><cell>1364</cell><cell>73.23</cell></row><row><cell>CondLane [12]</cell><cell cols="2">ResNet34 53.11</cell><cell>78.74</cell><cell>59.39</cell><cell>128</cell><cell>19.6</cell><cell>93.38</cell><cell>77.14</cell><cell>71.17</cell><cell>79.93</cell><cell>51.85</cell><cell>89.89</cell><cell>73.88</cell><cell>1387</cell><cell>73.92</cell></row><row><cell>CondLane [12]</cell><cell cols="2">ResNet101 54.83</cell><cell>79.48</cell><cell>61.23</cell><cell>47</cell><cell>44.8</cell><cell>93.47</cell><cell>77.44</cell><cell>70.93</cell><cell>80.91</cell><cell>54.13</cell><cell>90.16</cell><cell>75.21</cell><cell>1201</cell><cell>74.80</cell></row><row><cell>CLRNet (ours)</cell><cell cols="2">ResNet18 55.23</cell><cell>79.58</cell><cell cols="2">62.21 119/206*</cell><cell>11.9</cell><cell>93.30</cell><cell>78.33</cell><cell>73.71</cell><cell>79.66</cell><cell>53.14</cell><cell>90.25</cell><cell>71.56</cell><cell>1321</cell><cell>75.11</cell></row><row><cell>CLRNet (ours)</cell><cell cols="2">ResNet34 55.14</cell><cell>79.73</cell><cell cols="2">62.11 103/156*</cell><cell>21.5</cell><cell>93.49</cell><cell>78.06</cell><cell>74.57</cell><cell>79.92</cell><cell>54.01</cell><cell>90.59</cell><cell>72.77</cell><cell>1216</cell><cell>75.02</cell></row><row><cell cols="3">CLRNet (ours) ResNet101 55.55</cell><cell>80.13</cell><cell>62.96</cell><cell>46/74*</cell><cell>42.9</cell><cell>93.85</cell><cell>78.78</cell><cell>72.49</cell><cell>82.33</cell><cell>54.50</cell><cell>89.79</cell><cell>75.57</cell><cell>1262</cell><cell>75.51</cell></row><row><cell>CLRNet (ours)</cell><cell cols="2">DLA34 55.64</cell><cell>80.47</cell><cell>62.78</cell><cell>94/151*</cell><cell>18.5</cell><cell>93.73</cell><cell>79.59</cell><cell>75.30</cell><cell>82.51</cell><cell>54.58</cell><cell>90.62</cell><cell>74.13</cell><cell>1155</cell><cell>75.37</cell></row></table><note>A predicted lane is a correct one if more than 85% predicted lane points are within 20 pixels the ground truth. Tusimple dataset also reports the rate of false positive (FP) and false negative (FN), where F P = Fpred N pred , F N = Mpred Ngt .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>State-of-the-art results on TuSimple. Additionally, F1 was computed using the official source code.</figDesc><table><row><cell cols="7">LIoU Refinement ROIGather mF1 F1@50 F1@75 F1@90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>51.90</cell><cell>78.37</cell><cell>58.32</cell><cell>14.43</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>52.80</cell><cell>78.27</cell><cell>59.50</cell><cell>16.54</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>54.74</cell><cell>78.91</cell><cell>61.77</cell><cell>20.09</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>55.23</cell><cell>79.58</cell><cell>62.21</cell><cell>20.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Effects of each component in our method. Results are reported on CULane.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Situ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan M</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12040</idno>
		<title level="m">Laneaf: Robust multi-lane detection with affinity fields</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised labeled lane marker dataset generation using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younkwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzeen</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witold</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Condlanenet: a top-to-down lane detection framework based on conditional convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05003</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Endto-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11582" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="276" to="291" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV 16</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focus on local: Detecting lane marker from bottom up via key point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14122" to="14130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Structure guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05403</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Real-time attention-guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="294" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6150" to="6156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark/" />
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lanesensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV 16</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1006" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Resa: Recurrent feature-shift aggregator for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3547" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scaloss: Side and corner aligned loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
