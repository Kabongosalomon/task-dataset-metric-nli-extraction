<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Many-Speakers Single Channel Speech Separation with Optimal Permutation Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Dovrat</surname></persName>
							<email>shaked.dovrat@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<email>wolf@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Many-Speakers Single Channel Speech Separation with Optimal Permutation Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech separation</term>
					<term>single channel</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single channel speech separation has experienced great progress in the last few years. However, training neural speech separation for a large number of speakers (e.g., more than 10 speakers) is out of reach for the current methods, which rely on the Permutation Invariant Training (PIT). In this work, we present a permutation invariant training that employs the Hungarian algorithm in order to train with an O(C 3 ) time complexity, where C is the number of speakers, in comparison to O(C!) of PIT based methods. Furthermore, we present a modified architecture that can handle the increased number of speakers. Our approach separates up to 20 speakers and improves the previous results for large C by a wide margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single channel speech separation is a fundamental problem in the speech process, which has seen tremendous advances in the last years. The main neural architectures can be divided into two categories: (i) Spectral based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and (ii) time domain based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Currently, the latter category leads with respect to the obtained accuracy.</p><p>Since the order of the speakers at the output of the neural network is arbitrary, a permutation invariant training is performed. Most of the neural architecture for speech separation use the permutation invariant training (PIT) loss <ref type="bibr" target="#b6">[7]</ref> or its extension to the utterance level (uPIT) <ref type="bibr" target="#b7">[8]</ref>. Both variants have a computational complexity of O(C!), where C is the number of the speakers. As a result, it is not feasible to run PIT on more than ten speakers.</p><p>In this work, following <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we propose a novel method to train a large number of speakers with a lower complexity of O(C 3 ), by using the Hungarian algorithm. The Hungarian algorithm is able to find the optimal permutation in terms of the minimal sum of pairwise losses, thus matching between pairs of output-and target-signals. In order to enable the separation network to deal with a large number of speakers, we further introduce an architecture that combines two distinct approaches to separation networks, LSTM and dilated convolutional layers.</p><p>In our experiments, our method separates up to 20 speakers, which, as far as we can ascertain, is twice the number tackled by any existing method. Moreover, we show that our method improves the previous state of the art separation results for separating 5 and 10 speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Signal channel speech separation was explored using classical approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and, more recently, using deep learning *Equal contribution methods. In <ref type="bibr" target="#b12">[13]</ref> an LSTM neural network with a phase sensitive loss function was introduced. An improvement in SDR was demonstrated on the CHiME-2 <ref type="bibr" target="#b14">[14]</ref> dataset. In <ref type="bibr" target="#b15">[15]</ref> a neural separation network with a clustering-based embedding was introduced, presenting results for the separation of two speakers and introducing the WSJ-2mix dataset that was extensively used by followup work. This work was further extended in <ref type="bibr" target="#b16">[16]</ref> by extracting an embedding of spectrogram segments and estimating a mask for the separation part. Results were provided for two and three speakers and an SDR improvement of 10.3 dB and 7.1 dB for WSJ-2mix and WSJ-3mix was obtained. In <ref type="bibr" target="#b17">[17]</ref>, the neural separator network was introduced. Attractor points in the embedding space were used to obtain the timefrequency bins for each speaker. The improvement on the WSJmix dataset was by 5.49%.</p><p>Luo et al. <ref type="bibr" target="#b3">[4]</ref> introduced TasNet, which is a time domain encoder-decoder neural architecture for the single channel speech separation problem. They show a results of 11.1 SDR improvement for WSJ-2mix dataset over the state of the art. Wang et al. <ref type="bibr" target="#b2">[3]</ref> proposed a neural architecture that separates the speakers in both the time and the frequency domains simultaneously. They presented an improvement of 13.2 SDR on the WSJ-2mix dataset. The work of <ref type="bibr" target="#b3">[4]</ref> further improved the architecture and introduced ConvTasNet <ref type="bibr" target="#b18">[18]</ref>, which employed a dilated convolutional neural network, showing an SDR improvement 15.6 dB for the WSJ-2mix dataset.</p><p>Another improvement with LSTM network was introduced in <ref type="bibr" target="#b19">[19]</ref>, where the dual-path recurrent neural network (DPRNN) architecture was employed to model extremely long sequences. They showed SDR improvement of 18.08 dB on WSJ-2mix dataset. In <ref type="bibr" target="#b4">[5]</ref> a separation network with M ulCat blocks was introduced. The proposed method also removed the masking sub-network, leading to an improvement of 20.12 dB to WSJ-2mix dataset. Furthermore, the WSJ-mix dataset was extended to include mixtures of 5 speakers, where the SDR improvement was 10.6 dB. <ref type="bibr" target="#b20">[20]</ref> combined DPRNN and TasNet and for the WSJ-5mix dataset they showed an SDR improvement of 10.41dB, and 11.14dB SDR improvement for online remixing. Zeghidour et al. <ref type="bibr" target="#b5">[6]</ref> introduced a neural separation network that infers a representation to each speaker, by performing clustering, and used it to separate the mixture. They show an SDR improvement of 22.2 dB for WSJ-2mix dataset.</p><p>Since our work builds upon the M ulCat network architecture <ref type="bibr" target="#b4">[5]</ref>, we will recap its major components. The network consists of an encoder, a separation module and a decoder. The encoder and decoder are simple 1D convolutions. The separation module starts with a chunking module which cuts the signal into chunks in time. Then, a series of doubled M ulCat blocks is applied. During training, a multi-scale loss is employed-after each doubled M ulCat block the activations are reconstructed by the decoder into audio signals and fed into the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2104.08955v4 [cs.SD] 7 Nov 2021</head><p>The method also uses the Scale-Invariant Signal-to-Noise Ratio (SI-SNR) loss, which is a slight improvement to the traditional SDR loss.</p><p>Another line of work for speech separation uses beamformers and introduces an extension of the minimum variance distortionless response (MVDR) <ref type="bibr" target="#b21">[21]</ref>. A neural beamformer was introduced in <ref type="bibr" target="#b22">[22]</ref> and further improved in <ref type="bibr" target="#b23">[23]</ref> for the speech separation problem. A follow up work introduced the linearly constrained minimum variance (LCMV) beamformer <ref type="bibr" target="#b24">[24]</ref>.</p><p>In <ref type="bibr" target="#b25">[25]</ref> a SinkPIT loss is introduced. They proposed a variant of the PIT loss, which is based on Sinkhorn's matrix balancing algorithm. They reduce the complexity of the PIT loss from O(C!) to O(kC 2 ), where k is set to 200. It is important to note that the chosen permutation is only an approximation of the optimal permutation. In another work, a probabilistic-PIT loss which considers the output permutation as discrete latent random variable was introduced <ref type="bibr" target="#b27">[26]</ref>. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref> noted that it is possible to use the Hungarian algorithm to find the best permutation for source separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hungarian Algorithm</head><p>The linear sum assignment problem (also known as the assignment problem) is the task of assigning C agents to do C tasks, such that each agent is assigned to exactly one task, and the total cost for the agents performing the tasks is minimal. In other words, given a C-by-C matrix of costs, M , one for each agenttask pair, find a permutation ? of the agents, such that the sum of costs of paired agents and tasks is minimal:</p><formula xml:id="formula_0">? = argmin ??? C n i=1 M i,?(i)<label>(1)</label></formula><p>A naive solution is to iterate over all C! possible permutations. Fortunately, an optimal and polynomial-time algorithm that solves the assignment problem was proposed in 1955 by Harold Kuhn <ref type="bibr" target="#b28">[27]</ref>, reviewed in 1957 by James Munkres <ref type="bibr" target="#b29">[28]</ref> and is mostly known by the name the Hungarian Algorithm. The initial time complexity of the algorithm was O(C 4 ) and it was modified later on to a time complexity of O(C 3 ) <ref type="bibr" target="#b30">[29]</ref>.</p><p>Simply put, the algorithm starts by trying to find an obvious permutation. If that fails, it goes on to make modifications to the input matrix, in order to find a valid and optimal permutation. The number of the modification iterations needed to find the solution is indicative of how well the permutation fits the data compared to alternative permutations. In other words, the more iterations needed until convergence, the less significant is the optimal permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This work extends the work in <ref type="bibr" target="#b4">[5]</ref>, through a number of new contributions. First, we introduce the Hungarian Loss which replaces the PIT loss and gives an optimal solution to the permutation issue with a much lower time complexity, O(C 3 ), which allows to train separation networks for many speakers. Second, we introduce a new network architecture that uses stacked dilated convolutions before each pair of M ulCat blocks of <ref type="bibr" target="#b4">[5]</ref>. The overall method, is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hungarian Loss</head><p>A single-channel speech separation network takes an audio signal that contains a mixture of C speakers speaking and outputs C audio signals, each optimized to contain a separate speaker.</p><p>During training, the network outputs the separated audio signals in an arbitrary order. Thus, in order to compute a meaningful loss, an alignment, i.e. a permutation, needs to be recovered between the outputs of the network and the separated target signals. One way to find the right permutation is to iterate over all possible C! permutations and choose the one which gives the lowest mean loss value on the pairs (i.e. PIT). The computational cost of PIT is unnoticeable when C is small, in comparison to the other parts of the network. However, it makes training on a large number of speakers impossible. For instance, for 20 speakers PIT needs to check 20! ? 2.4 ? 10 18 different permutations).</p><p>To address this issue, we formulate the task of finding the permutation which minimizes the loss function as a linear sum assignment problem. Given the C output signals and C target signals, we calculate the pairwise loss value,? (si,?j), on every pair of output (?j) and target (si) signals, which gives an Cby-C matrix of losses, M . Next, we assign each output with a unique target and vice-versa. Such assignment is equivalent to choosing C elements of the matrix, such that each chosen element is in a different row and column from all others. An optimal assignment minimizes the sum of values of the chosen elements. The PIT loss can then be viewed as a brute-force solution to this problem, iterating over all possible solutions:</p><formula xml:id="formula_1">(s,?) = min ??? C 1 C C i=1? (si,? ?(i) )<label>(2)</label></formula><p>By running the Hungarian Algorithm on M , we efficiently find the optimal permutation in polynomial time instead of the brute-force, factorial-time PIT. Note that the assignment algorithm does not need to be differential, since we find a permutation of the targets by which we calculate the loss, meaning that the process is separate from the backwards calculation of gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>We shift our focus to solving the task of separating mixtures of many (C ? 10) speakers. As C increases, the task of separating the mixtures becomes more challenging. Thus, we propose a new and suitable network architecture.</p><p>For this end, we modify the M ulCat-based architecture [5] by adding stacked dilated convolutions before each pair of M ulCat blocks. In addition, we increased some of the network's hyper-parameters to achieve a larger capacity needed for the harder tasks. The dilated convolutions scheme is borrowed from <ref type="bibr" target="#b18">[18]</ref>: We use 8 1-D Conv blocks stacked on top of each other, with dilation factors 2 i?1 for i ? {1, 2, ..., 8}. This corresponds to a single column in the separation module of <ref type="bibr" target="#b18">[18]</ref>.</p><p>The rest of the model is in accordance with <ref type="bibr" target="#b4">[5]</ref>, i.e. using the same encoder, chunking, M ulCat blocks, decoder and SI-SNR loss. We also adopt the multi-scale loss scheme, which applies the loss function after each double MulCat block, instead of just the last. We tuned some hyper-parameters of the architecture to accompany the harder tasks when C is large: N , number of features, was increased from 128 to 256. L, the encoder's kernel size was increased from 8 to 16. H, the number of hidden units in the LSTMs was increased from 128 to 256 and finally R, the number of double M ulCat blocks was increased from 6 to 7. A similar hyper-parameters adjustment was done in <ref type="bibr" target="#b25">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Comparison to state of the art We show results on datasets derived from WSJ corpus <ref type="bibr" target="#b31">[30]</ref> and LibriSpeech <ref type="bibr" target="#b32">[31]</ref>. For WSJ,  we use the 5-speaker mix, introduced in <ref type="bibr" target="#b4">[5]</ref>, which uses the same procedure as in <ref type="bibr" target="#b15">[15]</ref>, i.e. 30 hours of speech from the training set si tr s were used to create the training and validation sets. The five speakers were randomly chosen and combined with random SNR values between 0 ? 5 dB. The test set is created from si et s and si dt s with 16 speakers, that differ from the speakers of the training set. For LibriSpeech, we use the LibriMix <ref type="bibr" target="#b33">[32]</ref> datasets with mixes of 5, 10, 15 and 20 speakers. LibriMix offers mixtures of 2 and 3 speakers from LibriSpeech, and we used the given scripts to create mixtures of 5, 10, 15 and 20 speakers. Some modifications to the script needed to be done. These can be found online*. We used Lib-riMix's given parameters to get a sample rate of 8Khz, clean mixtures (no noise added) and each sample is cut in length, according to the minimal length sample in the mixture. We also use the augmentation process as in <ref type="bibr" target="#b25">[25]</ref>.</p><p>A separate model is trained for each dataset, with the corresponding number of output channels. Training was done using the Adam optimizer <ref type="bibr" target="#b34">[33]</ref>, with batch size 32 and a learning rate of 1e ? 3 which was multiplied by 0.95 every two epochs. During training, each sample is cut into 4-second segments. Table 1 compares the results of our model to other models, us-*https://github.com/ShakedDovrat/LibriMix ing the SI-SDRi metrics. As can be seen, our model outperforms the other methods by a large margin. For Libri-5mix and Libri-10mix, we improve the previous results by 1.89dB and 1.33dB respectively. Interestingly, the previous state of the art results for Libri5M ix is obtained with M ulCut <ref type="bibr" target="#b4">[5]</ref>, whereas for Libri10M ix it is obtained by <ref type="bibr" target="#b25">[25]</ref>, which uses SinkPIT. This is due to the fact that the PIT loss in M ulCut for 10 speakers is prohibitive. For WSJ-5mix, the SDR of our method improves by more than 2dB over the previous method. We are the first to present results for the Libri15M ix and Libri20M ix datasets and running previous work on it is not practical. Sample results are shared online https: //shakeddovrat.github.io/hungarian/. Hungarian loss vs. alternative losses In order to show the benefits of using the Hungarian algorithm as opposed to PIT, we show the training duration of an epoch of each dataset, depicted in <ref type="table" target="#tab_1">Table 2</ref>. As shown, on a 5 speaker mix both methods take about the same time. However, for C = 10, the Hungarian method is about 9 times faster on our model. For C ? 15, the Hungarian method is still fast, while PIT is unusable as it failed to complete a single epoch after days of running. To our knowledge, the SinkPIT can run on 20 speakers but only find an approximation to the optimal permutation, whereas our method is both optimal and fast (the source code was not published).</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we present the average number of iterations per example that were performed by the Hungarian algorithm for the various datasets. We can observe two phenomena: (i) As the training proceeds and the neural network improves the separation performance, the average number of iterations is decreased. (ii) The average number of iterations is lower when the number of speakers in the mixture is lower. Both of these observations came from the fact that the Hungarian algorithm needs no iterations to converge when the outputs or the network are mixed or noisy.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we plot the mel spectrogram for typical samples as in <ref type="bibr" target="#b25">[25]</ref>. This is shown for a mixture of 10 speakers, from the Libri10M ix dataset. As can be seen in speakers 4 and 8 our method provide a cleaner mel spectrogram, Furthermore, the SI-SDRi difference is 4.9 dB and 2.4 dB for speakers 4 and 8 respectively (there is a significant improvement in all speakers).</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we plot the pairwise SI-SDR negative matrix M sorted in descending order, in comparison to the results of the SinkPIT system <ref type="bibr" target="#b25">[25]</ref>. Evidently, the entropy in our method  is lower, especially in the last rows, i.e., it has much less confusion compared to the baseline method.</p><p>Ablation study We run ablation analysis in order to understand the contribution of each component of our method. The results are summarized in <ref type="table" target="#tab_2">Table 3</ref>, where -C means without the 1D convolutions and -H means using the PIT loss instead of the Hungarian loss. As can be seen, adding the dilated convolution to the architecture improves the performance. Moreover, without the Hungarian algorithm, it is practically impossible to train on 15 or more speakers. For 10 speakers, using PIT drastically diminishes performance due to longer training time. On the other hand, for 5 speakers, the PIT and Hungarian have similar performance since they both found the optimal permutation in a similar runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we provide a method for single channel sound separation for a large number of sources. Our method is the first work to show that one can separate a mixture of 20 speakers from a single channel recording. Our solution is based on the Hungarian algorithm, which efficiently finds the optimal permutation from the C! possible permutations and on a new network architecture that adds stacked 1-D convolutions and added capacity to the state of the art architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 re-   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(i) The proposed Hungarian Loss. M , a C-by-C matrix of SI-SNR losses between output and target pairs is computed. M is fed into the Hungarian algorithm, which efficiently finds the optimal permutation of target signals, ?. (ii) The proposed separation network architecture. The novel components are the added Conv blocks and the Hungarian loss, which replaces the PIT loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The average number of iterations per example performed by the Hungarian algorithm. The average is decreasing as training progresses, which indicates that the separation is improving. This also means that the runtime of the Hungarian algorithm is getting even shorter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Separation results for a mixture of 10 speakers. First row: mel spectrogram of ground truth signals. Second row: mel spectrogram of SinkPIT signals. Third row: mel spectrogram of the outputs of our method. The last row shows the SI-SDR improvement for the SinkPIT method and our method. The x-axis is sorted by the SI-SDRi in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The pairwise SI-SDR matrix M , sorted in descending order. For the same input sample as inFigure 3. (a) Our method. (b) SinkPIT<ref type="bibr" target="#b25">[25]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>SDR improvement performance of various models versus number of speakers and datasets. 'X' results indicated simulation that failed to complete a single epoch, due to high complexity PIT loss. xMix is LibriMix with x speakers.</figDesc><table><row><cell>Model</cell><cell cols="5">5Mix 10Mix 15Mix 20Mix WSJ-5</cell></row><row><cell>ConvTasNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.8</cell></row><row><cell>DPRNN [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.6</cell></row><row><cell>MulCat [5]</cell><cell cols="2">10.83 4.74</cell><cell>X</cell><cell>X</cell><cell>10.6</cell></row><row><cell>TasTas [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.14</cell></row><row><cell>SinkPIT [25]</cell><cell cols="2">9.39 6.45</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="2">12.72 7.78</cell><cell>5.66</cell><cell>4.26</cell><cell>13.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Training run times of PIT and Hungarian algorithm in minutes per epoch. 'X' indicates simulation that failed to complete a single epoch, due to long run times of the PIT loss.</figDesc><table><row><cell>Dataset</cell><cell>#Spkrs</cell><cell>#Perms</cell><cell cols="2">PIT Hungarian</cell></row><row><cell>WSJ-5mix</cell><cell>5</cell><cell>120</cell><cell>65</cell><cell>62</cell></row><row><cell>Libri-5Mix</cell><cell>5</cell><cell>120</cell><cell>139</cell><cell>140</cell></row><row><cell>Libri-10Mix</cell><cell>10</cell><cell>? 3.6e6</cell><cell>462</cell><cell>52</cell></row><row><cell>Libri-15Mix</cell><cell>15</cell><cell>? 1.3e12</cell><cell>X</cell><cell>36</cell></row><row><cell>Libri-20Mix</cell><cell>20</cell><cell>? 2.4e18</cell><cell>X</cell><cell>29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation analysis -SDR Performance for LibriMix and WSJ-mix datasets. xMix is LibriMix with x speakers. "-C" means without the 1D convolutions and "-H" means using PIT instead of the Hungarian loss. 'X' indicates simulation that failed to complete a single epoch, due to long run times of the PIT loss.</figDesc><table><row><cell>Model</cell><cell cols="5">5Mix 10Mix 15Mix 20Mix WSJ-5</cell></row><row><cell cols="2">Ours-C-H 11.10</cell><cell>4.47</cell><cell>X</cell><cell>X</cell><cell>12.18</cell></row><row><cell>Ours-H</cell><cell>12.53</cell><cell>4.84</cell><cell>X</cell><cell>X</cell><cell>13.07</cell></row><row><cell>Ours-C</cell><cell>11.19</cell><cell>5.89</cell><cell>5.14</cell><cell>4.10</cell><cell>12.10</cell></row><row><cell>Ours</cell><cell>12.72</cell><cell>7.78</cell><cell>5.66</cell><cell>4.26</cell><cell>13.22</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>search and innovation programme (grant ERC CoG 725974). We thank Hideyuki Tachibana for the helpful discussion. The contribution of Eliya Nachmani is part of a Ph.D. thesis research conducted at Tel Aviv University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A spectral-change-aware loss function for dnn-based speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6870" to="6874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="7164" to="7175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Serialized output training for end-to-end overlapped speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Monaural source separation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single-channel speech presence probability estimation and noise tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Audio Source Separation and Speech Enhancement</title>
		<imprint>
			<biblScope unit="page" from="87" to="106" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech dereverberation using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Chazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="390" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="708" to="712" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The second &apos;chime&apos;speech separation and recognition challenge: Datasets, tasks and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nesta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matassoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02173</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Toward the pre-cocktail party problem with tastas +</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03692</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multichannel eigenspace beamforming in a reverberant noisy environment with multiple interfering speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1071" to="1086" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speaker location and microphone spacing invariant acoustic modeling from raw multichannel waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fasnet: Low-latency adaptive beamforming for multi-microphone audio processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13387</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Global and local simplex representations for multichannel source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laufer-Goldshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Talmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="914" to="928" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards listening to 10 people simultaneously: An efficient permutation invariant training of audio source separation using sinkhorn&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probabilistic permutation invariant training for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khorram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01768</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On some techniques useful for solution of transportation network problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="194" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
