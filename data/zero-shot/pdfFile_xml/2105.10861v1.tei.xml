<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RST Parsing from Scratch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Tung</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technological University ? Salesforce Research Asia ? Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">A-STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technological University ? Salesforce Research Asia ? Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">A-STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<email>srjoty@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technological University ? Salesforce Research Asia ? Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">A-STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technological University ? Salesforce Research Asia ? Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">A-STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
							<email>xlli@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technological University ? Salesforce Research Asia ? Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">A-STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? ?</forename><forename type="middle">?</forename><surname>Nanyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technological University ? Salesforce Research Asia ? Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">A-STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RST Parsing from Scratch</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. With extensive experiments on the standard English RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a document, the clauses, sentences and paragraphs are logically connected together to form a coherent discourse. The goal of discourse parsing is to uncover this underlying coherence structure, which has been shown to benefit numerous NLP applications including text classification <ref type="bibr" target="#b18">(Ji and Smith, 2017)</ref>, summarization <ref type="bibr" target="#b14">(Gerani et al., 2014)</ref>, sentiment analysis <ref type="bibr" target="#b4">(Bhatia et al., 2015)</ref>, machine translation evaluation <ref type="bibr" target="#b22">(Joty et al., 2017)</ref> and conversational machine reading <ref type="bibr">(Gao et al., 2020)</ref>.</p><p>Rhetorical Structure Theory or <ref type="bibr">RST (Mann and Thompson, 1988)</ref>, one of the most influential theories of discourse, postulates a hierarchical discourse structure called discourse tree (DT). The leaves of a DT are clause-like units, known as elementary discourse units <ref type="bibr">(EDUs)</ref>. Adjacent EDUs and higher-order spans are connected hierarchically through coherence relations (e.g., Contrast, Expla-nation). Spans connected through a relation are categorized based on their relative importancenucleus being the main part, with satellite being the subordinate one. <ref type="figure">Fig. 1</ref> exemplifies a DT spanning over two sentences and six EDUs. Finding discourse structure generally requires breaking the text into EDUs (discourse segmentation) and linking the EDUs into a DT (discourse parsing).</p><p>Discourse parsers can be singled out by whether they apply a bottom-up or top-down procedure. Bottom-up parsers include transition-based models <ref type="bibr" target="#b10">(Feng and Hirst, 2014;</ref><ref type="bibr" target="#b1">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b5">Braud et al., 2017;</ref><ref type="bibr" target="#b40">Wang et al., 2017)</ref> or globally optimized chart parsing models <ref type="bibr" target="#b37">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b21">Joty et al., 2013</ref><ref type="bibr" target="#b20">Joty et al., , 2015</ref>. The former constructs a DT by a sequence of shift and reduce decisions, and can parse a text in asymptotic running time that is linear in number of EDUs. However, the transition-based parsers make greedy local decisions at each decoding step, which could propagate errors into future steps. In contrast, chart parsers learn scoring functions for sub-trees and adopt a CKY-like algorithm to search for the highest scoring tree. These methods normally have higher accuracy but suffer from a slow parsing speed with a complexity of O(n 3 ) for n EDUs. The top-down parsers are relatively new in discourse <ref type="bibr" target="#b27">(Lin et al., 2019;</ref><ref type="bibr" target="#b44">Zhang et al., 2020;</ref><ref type="bibr" target="#b24">Kobayashi et al., 2020)</ref>. These methods focus on finding splitting points in each iteration to build a DT. However, the local decisions could still affect the performance as most of the methods are still greedy.</p><p>Like most other fields in NLP, language parsing has also undergone a major paradigm shift from traditional feature-based statistical parsing to end-toend neural parsing. Being able to parse a document end-to-end from scratch is appealing for several key reasons. First, it makes the overall development procedure easily adaptable to new languages, domains and tasks by surpassing the expensive feature engineering step that often requires more time and domain/language expertise. Second, the lack of an explicit feature extraction phase makes the training and testing (decoding) faster.</p><p>Because of the task complexity, it is only recently that neural approaches have started to outperform traditional feature-rich methods. However, successful document level neural parsers still rely heavily on handcrafted features <ref type="bibr" target="#b42">Yu et al., 2018;</ref><ref type="bibr" target="#b44">Zhang et al., 2020;</ref><ref type="bibr" target="#b24">Kobayashi et al., 2020)</ref>. Therefore, even though these methods adopt a neural framework, they are not "end-to-end" and do not enjoy the above mentioned benefits of an end-to-end neural parser. Moreover, in existing methods (both traditional and neural), discourse segmentation is detached from parsing and treated as a prerequisite step. Therefore, the errors in segmentation affect the overall parsing performance <ref type="bibr" target="#b37">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b19">Joty et al., 2012)</ref>.</p><p>In view of the limitations of existing approaches, in this work we propose an end-to-end top-down document level parsing model that:</p><p>? Can generate a discourse tree from scratch without requiring discourse segmentation as a prerequisite step; rather, it generates the EDUs as a by-product of parsing. Crucially, this novel formulation facilitates solving the two tasks in a single neural model. Our formulation is generic and works in the same way when it is provided with the EDU segmentation.</p><p>? Treats discourse parsing as a sequence of splitting decisions at token boundaries and uses a seq2seq pointer network <ref type="bibr" target="#b39">(Vinyals et al., 2015)</ref> to model the splitting decisions at each decoding step. Importantly, our seq2seq parsing model can adopt beam search to widen the search space for the highest scoring tree, which to our knowledge is also novel for the parsing problem.</p><p>? Does not rely on any handcrafted features, which makes it faster to train or test, and easily adaptable to other domains and languages.</p><p>? Achieves the state of the art (SoTA) with an F 1 score of 46.6 in the Full (label+structure) metric for end-to-end parsing on the English RST Discourse Treebank, which outperforms many parsers that use gold EDU segmentation. With gold segmentation, our model achieves a SoTA F 1 score of 50.2 (Full), outperforming the best existing system by 2.1 absolute points. More imporantly, it does so without using any handcrafted features (not even part-of-speech tags).</p><p>We make our code available at https://ntunlpsg.github.io/project/rst-parser</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Assuming that a document has already been segmented into EDUs, following the traditional approach, the corresponding discourse tree (DT) can be represented as a set of labeled constituents.</p><formula xml:id="formula_0">C := {((i t , k t , j t ), r t )|i t ? k t &lt; j t } m t=1 (1)</formula><p>where m = |C| is the number of internal nodes in the tree and r t is the relation label between the discourse unit containing EDUs i t through k t and the one containing EDUs k t + 1 through j t .</p><p>Traditionally, in RST parsing, discourse segmentation is performed first to obtain the sequence of EDUs, which is followed by the parsing process to assemble the EDUs into a labeled tree. In other words, traditionally discourse segmentation and parsing have been considered as two distinct tasks that are solved by two different models.</p><p>On the contrary, in this work we take a radically different approach that directly starts with parsing the (unsegmented) document in a top-down manner and treats discourse segmentation as a special case of parsing that we get as a by-product. Importantly, this novel formulation of the problem allows us to solve the two problems in a single neural model. Our parsing model is generic and also works in the same way when it is fed with an EDU-segmented text. Before presenting the model architecture, we first formulate the problem as a splitting decision problem at the token level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parsing as a Splitting Decision Problem</head><p>We reformulate the discourse parsing problem from Eq. (1) as a sequence of splitting decisions at token boundaries (instead of EDUs). Specifically, the input text is first prepended and appended with the special start (&lt;sod&gt;) and end (&lt;eod&gt;) tokens, respectively. We define the token-boundary as the indexed position between two consecutive tokens. For example, the constituent spanning "But he added :" in <ref type="figure">Fig. 2</ref> is defined as (0, 4).</p><p>Following the standard practice, we convert the discourse tree by transforming each multi-nuclear constituent into a hierarchical right-branching binary sub-tree. Every internal node in the resulting binary tree will have a left and a right constituent, allowing us to represent it by its split into the left and right children. Based on this, we define the Boundary-based splitting representation when EDUs are provided S edu = {(0, 44) ) 4, (4, 44) ) 25, (4, 25) ) 17,(25, 44) ) 37, (25, 37) ) 33} Boundary-based splitting representation for end-to-end parsing S = {(0, 44) ) 4, (0, 4) ) 4, (4, 44) ) 25, (4, 25) ) 17, (4, 17) ) 17, (17, 25) ) 25, (25, 44) ) 37, (25, 37) ) 33, (25, 33) ) 33, (33, 37) ) 37, (37, 44) ) 44} <ref type="figure">Figure 1</ref>: A discourse tree for two sentences in the RST discourse treebank. The internal nodes (e.g., Attribution, Contrast) denote the coherence relations and the edge labels reflect the nuclearity of the child span. Below the tree, we show the sequence of splitting decisions S edu when EDUs are provided and S when EDUs are not provided (end-to-end parsing). The bold splitting decision represents the final split of the span, forming an EDU. <ref type="figure">Figure 2</ref>: Relation between token-boundary (above) and token (below) representations. A token-boundary position k is located between the tokens at k and k + 1.</p><p>parsing as a set of splitting decisions S at tokenboundaries by the following proposition:</p><p>Proposition 1 Given a binarized discourse tree for a document containing n tokens, the tree can be converted into a set of token-boundary splitting decisions S = {(i, j) ) k|i &lt; k ? j} such that the parent constituent (i, j) either gets split into two child constituents (i, k) and (k, j) for k &lt; j, or forms a terminal EDU unit for k = j, i.e., the span will not be split further (i.e., marks segmentation).</p><p>Notice that S is a generalized formulation of RST parsing, which also includes the decoding of EDUs as a special case (k = j). It is quite straightforward to change this formulation to the parsing scenario, where discourse segmentation (sequence of EDUs) is provided. Formally, in that case, the tree can be converted into a set of splitting decisions S edu = {(i, j) ) k|i &lt; k &lt; j} such that the constituent (i, j) gets split into two constituents (i, k) and (k, j) for k &lt; j, i.e., we simply omit the special case of k = j as the EDUs are given. In other words, in our generalized formulation, discourse segmentation is just one extra step of parsing, and can be done top-down end-to-end.</p><p>An example of our formalism of the parsing problem is shown in <ref type="figure">Fig. 1</ref> for a discourse tree spanning over two sentences (44 tokens); for simplicity, we do not show the relation labels corresponding to the splitting decisions (marked by )).</p><p>Since each splitting decision corresponds to one and only one internal node in the tree, it guarantees that the transformation from the tree to S (and S edu ) has a one-to-one mapping. Therefore, predicting the sequence of such splitting decisions is equivalent to predicting the discourse tree (DT).</p><p>Seq2Seq Parsing Model. In this work, we adopt a structure-then-label framework. Specifically, we factorize the probability of a DT into the probability of the tree structure and the probability of the relations (i.e., the node labels) as follows:</p><formula xml:id="formula_1">P ? (DT |x) = P ? (S, L|x) = P ? (L|S, x)P ? (S|x)<label>(2)</label></formula><p>where x is the input document, and S and L respectively denote the structure and labels of the DT. This formulation allows us to first infer the best tree structure (e.g., using beam search), and then find the corresponding labels.</p><p>As discussed, we consider the structure prediction problem as a sequence of splitting decisions to generate the tree in a top-down manner. We use a seq2seq pointer network <ref type="bibr" target="#b39">(Vinyals et al., 2015)</ref> to model the sequence of splitting decisions <ref type="figure">(Fig. 3</ref>). We adopt a depth-first order of the decision sequence, which showed more consistent <ref type="figure">Figure 3</ref>: Our discourse parser along with a few decoding steps for a given document. The input to the decoder at each step is the representation of the span to be split. We predict the splitting point using the biaffine function between the corresponding decoder state and the token-boundary encoder representations. The figure is for end-to-end parsing, where each EDU-corresponding span points to its right edge to mark the EDU. The coherence relations between the left and right spans are assigned using a label classifier after the (approximately) optimal tree structure is formed using beam search. performance in our preliminary experiments than other alternatives, such as breath-first order.</p><p>First, we encode the tokens in a document x = (x 0 , . . . , x n ) with a document encoder and get the token-boundary representations (h 0 , . . . , h n ). Then, at each decoding step t, the model takes as input an internal node (i t , j t ), and produces an output y t (by pointing to the token boundaries) that represents the splitting decision (i t , j t ) ) k t to split it into two child constituents (i t , k t ) and (k t , j t ). For example, the initial span (0, 44) in <ref type="figure">Fig. 1</ref> is split at boundary position 4, yielding two child spans (0, 4) and (4, 44). If the span (0, 4) is given as an EDU (i.e., segmentation given), the splitting stops at (0, 4), thus omitted in S edu <ref type="figure">(Fig. 1)</ref>. Otherwise, an extra decision (0, 4) ) 4 ? S needs to be made to mark the EDUs for end-to-end parsing. With this, the probability of S can be expressed as:</p><formula xml:id="formula_2">P ? (S|x) = y t ?S P ? (yt|y&lt;t, x) = |S| t=1 P ? (it, jt) ) kt|((i, j) ) k)&lt;t, x</formula><p>This end-to-end conditional splitting formulation is the main novelty of our method and is in contrast to previous approaches which rely on offlineinferred EDUs from a separate discourse segmenter. Our formalism streamlines the overall parsing process, unifies the neural components seamlessly and smoothens the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>In the following, we describe the components of our parsing model: the document encoder, the boundary and span representations, the decoding process through the decoder and the label classifier. Document Encoder. Given an input document of n words x = (x 1 , . . . , x n ), we first add &lt;sod&gt; and &lt;eod&gt; markers to the sequence. After that, each token x i in the sequence is mapped into its dense vector representation e i as:</p><formula xml:id="formula_3">e i = [e char i , e word i ]</formula><p>, where e char i , and e word i are respectively the character and word embeddings of token x i . For word embedding, we experiment with (i) randomly initialized, (ii) pretrained static embeddings ,e.g., GloVe <ref type="bibr" target="#b36">(Pennington et al., 2014)</ref>). To represent the character embedding of a token, we apply a character bidirectional LSTM i.e., Bi-LSTM <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997)</ref> or pretrained contextualized embeddings, e.g., XLNet <ref type="bibr" target="#b41">(Yang et al., 2019)</ref>. The token representations are then passed to a sequence encoder of a three-layer Bi-LSTM to obtain their forward f i and backward b i contextual representations.</p><p>Token-boundary Span Representations. To represent each token-boundary position k between token positions k and k + 1, we use the fencepost representation <ref type="bibr" target="#b8">(Cross and Huang, 2016)</ref>:</p><formula xml:id="formula_4">h k = [f k ; b k+1 ]<label>(3)</label></formula><p>where f k and b k+1 are the forward and backward LSTM hidden vectors of positions k and k + 1 respectively, and [?; ?] is the concatenation operation. Then, to represent the token-boundary span (i, j), we use the linear combination of the two endpoints i and j as: where W 1 and W 2 are trainable weights. These span representations will be used as input to the decoder or the label classifier. <ref type="figure" target="#fig_0">Fig. 4</ref> illustrates an example boundary span representation.</p><formula xml:id="formula_5">hi,j = W1hi + W2hj<label>(4)</label></formula><p>The Decoder. Our model uses a unidirectional LSTM as the decoder. At each decoding step t, the decoder takes as input the corresponding span (i, j) (i.e., h i,j ) and its previous LSTM state d t?1 to generate the current state d t and then the biaffine function <ref type="bibr" target="#b9">(Dozat and Manning, 2017)</ref> is applied between d t and all the encoded token-boundary representations (h 0 , h 1 , . . . , h n ) as follows:</p><formula xml:id="formula_6">d t = MLP d (dt) h i = MLP h (hi)<label>(5)</label></formula><formula xml:id="formula_7">s i t = d t T W dh h i + h i T w h (6) a i t = exp(s i t ) n i=0 exp(s i t )</formula><p>for i = 0, . . . , n</p><p>where each MLP operation comprises a linear transformation with LeakyReLU activation <ref type="bibr" target="#b30">(Maas et al., 2013)</ref> to transform d i and h i into equal-sized vectors d t , h i ? IR d , and W dh ? IR d?d and w h ? IR d are respectively the weight matrix and weight vector for the biaffine function. The resulting biaffine scores s i t are then fed into a softmax layer to acquire the pointing distribution a i t ? [0, 1] n+1 for the splitting decision. During inference, when decoding the tree at step t, we only examine the "valid" splitting points between i and j, and we look for k such that i &lt; k ? j.</p><p>Label Classifier. We perform label assignment after decoding the entire tree structure. Each assignment takes into account the splitting decision that generated it since the label represents the relation between the child spans. Specifically, for a constituent (i, j) that was split into two child constituents (i, k) and (k, j), we determine the coherence relation between them as follows:</p><formula xml:id="formula_9">h l ik = MLP l ([hi; h k ]); h r kj = MLPr([h k ; hj]) (8) P ? (l|(i, k), (k, j)) = softmax (h l ik ) T W lr h r kj +(h l ik ) T W l + (h r kj ) T Wr + b (9) l * (i,k),(k,j) = arg max l?L P ? (l|(i, k), (k, j))<label>(10)</label></formula><p>where L is the total number of labels (i.e., coherence relations with nuclearity attached); each of MLP l and MLP r includes a linear transformation with LeakyReLU activation to transform the left and right spans into equal-sized vectors h l ik , h r kj ? IR d ; W lr ? IR d?L?d , W l ? IR d?L , W r ? IR d?L are the weights and b is a bias vector.</p><p>Training Objective. Our parsing model is trained by minimizing the total loss defined as:</p><formula xml:id="formula_10">L(?e, ? d , ? l ) = Ls(?e, ? d ) + L l (?e, ? l )<label>(11)</label></formula><p>where structure L s and label L l losses are crossentropy losses computed for the splitting and labeling tasks respectively, and ? e , ? d and ? l denote the encoder, decoder and labeling parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Complete Discourse Parsing Models</head><p>Having presented the generic framework, we now describe how it can be easily adapted to the two parsing scenarios: (i) end-to-end parsing and (ii) parsing with EDUs. We also describe the incorporation of beam search for inference.</p><p>End-to-End Parsing. As mentioned, previous work for end-to-end parsing assumes a separate segmenter that provides EDU-segmented texts to the parser. Our method, however, is an end-to-end framework that produces both the EDUs as well as the parse tree in the same inference process. To guide the search better, we incorporate an inductive bias into our inference based on the finding that most sentences have a well-formed subtree in the document-level tree <ref type="bibr" target="#b37">(Soricut and Marcu, 2003)</ref>, i.e., discourse structure tends to align with the text structure (sentence boundary in this case); for example, <ref type="bibr" target="#b11">Fisher and Roark (2007)</ref>; <ref type="bibr" target="#b21">Joty et al. (2013)</ref> found that more than 95% of the sentences have a well-formed subtree in the RST discourse treebank.</p><p>Our goal is to ensure that each sentence corresponds to an internal node in the tree. This can be achieved by a simple adjustment in our inference. When decoding at time step t with the span (i t , j t ) as input, if the span contains M &gt; 0 sentence boundaries within it, we pick the one that Algorithm 1 Discourse Tree Inference (end-to-end) Input: Document length n; boundary encoder states: (h0, h1, . . . , hn); sentence boundary set SB ; label scores: P (l|(i, k), (k, j)), 0 ? i &lt; k ? j ? n, l ? L, initial decoder state st.</p><formula xml:id="formula_11">Output: Parse tree DT ST = [(0, n)] stack of spans S = [] while ST = ? do (i, j) = pop(ST ) Current span to split a, st = dec(st, hi,j ) a: split prob. dist. if (i, j) ? SB = ? then k = arg max i&lt;k?j &amp; k?SB a else k = arg max i&lt;k?j a end if if j ? 1 &gt; k &gt; i + 1 then push(ST , (k, j)) push(ST , (i, k)) else if j ? 1 &gt; k = i + 1 then push(ST , (k, j)) else if k = j ? 1 &gt; i + 1 then push(ST , (i, k)) end if if k = j then push(S((i, k, j)) end if end while DT = [((i, k, j), argmax l P (l|(i, k)(k, j))?(i, k, j) ? S]</formula><p>has the highest pointing score (Eq. 7) among the M alternatives as the split point k t . If there is no sentence boundary within the input span (M = 0), we find the next split point as usual. In other words, sentence boundaries in a document get the chance to be split before the token boundaries inside a sentence. This constraint is indeed similar to the 1S-1S (1 subtree for 1 sentence) constraint of <ref type="bibr" target="#b21">Joty et al. (2013)</ref>'s bottom-up parsing, and is also consistent with the property that EDUs are always within the sentence boundary. Algorithm 1 illustrate the end-to-end inference algorithm.</p><p>Parsing with EDUs. When segmentation information is provided, we can have a better encoding of the EDUs to construct the tree. Specifically, rather than simply taking the token-boundary representation corresponding to the EDU boundary as the EDU representation, we adopt a hierarchical approach, where we add another Bi-LSTM layer (called "Boundary LSTM") that connects EDU boundaries (a figure of this framework is in the Appendix). In other words, the input sequence to this LSTM layer is (h 0 , . . . , h m ), where h 0 = h 0 , h m = h n and h j ? {h 1 , . . . , h n?1 } such that h j is an EDU boundary. For instance, for the example in <ref type="figure">Fig. 1</ref>, the input to the Boundary LSTM layer is (h 0 , h 4 , h 17 , h 25 , h 33 , h 37 , h 44 ).</p><p>This hierarchical representation facilitates better modeling of relations between EDUs and higher order spans, and can capture long-range dependencies better, especially for long documents. List of empty beam items init_input_span= [(0, m), (0, 0), . . . , (0, 0)] m-2 paddings (0,0) init_tree= [(0, 0, 0), (0, 0, 0), . . . , (0, 0, 0)] m-1 elements beam[0] = (0, s, init_input_span, init_tree)</p><p>Initialize first item (log-prob,state,input_span,tree)</p><formula xml:id="formula_12">for t = 1 to L d do for (logp, s, input-span, tree) ? beam[t ? 1] do (i, j) = input-span[t ? 1] Current span to split a, s = decoder-step(s, hi,j ) a: split prob. dist. for (k, p k ) ? top-B(a) and i &lt; k &lt; j do curr-input-span = input-span curr-tree = tree curr-tree[t ? 1] = (i, k, j) if k &gt; i + 1 then curr-input-span[t] = (i, k) end if if j &gt; k + 1 then curr-input-span[t + j ? k ? 1] = (k, j) end if push (logp + log(p k ), s , curr-input-span, curr-tree) to beam[t] end for end for prune beam[t]</formula><p>Keep top-B highest score trees end for logp*, s * , ip * , S * = arg max logp beam[L d ] S * : best structure</p><formula xml:id="formula_13">DT = [(i, k, j, arg max l P ? (l|(i, k), (k, j)) ?(i, k, j) ? S * ]</formula><p>Incorporating Beam Search. Previous work <ref type="bibr" target="#b27">(Lin et al., 2019;</ref><ref type="bibr" target="#b44">Zhang et al., 2020)</ref> which also uses a seq2seq architecture, computes the pointing scores over the token or span representations only within the input span. For example, for an input span (i, j), the pointing scores are computed considering only (h i , . . . , h j ) as opposed to (h 1 , . . . , h n ) in our Eq. 7. This makes the scales of the scores uneven across different input spans as the lengths of the spans vary. Thus, such scores cannot be objectively compared across sub-trees globally at the full-tree level. In addition, since efficient global search methods like beam search cannot be applied properly with non-uniform scores, these previous methods had to remain greedy at each decoding step. In contrast, our decoder points to all the encoded token-boundary representations in every step (Eq. 7). This ensures that the pointing scores are evenly scaled, allowing fair comparisons between the scores of all candidate sub-trees. Therefore, our method enables the effective use of beam search through highly probable candidate trees. Algorithm 2 illustrates the beam search inference when EDUs are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct our experiments on discourse parsing with and without gold segmentation. We use the standard English RST Discourse Treebank or RST-DT <ref type="bibr" target="#b28">(Lynn et al., 2002)</ref> for training and evaluation.</p><p>It consists of 385 annotated Wall Street Journal news articles: 347 for training and 38 for testing. We randomly select 10% of the training set as our development set for hyper-parameter tuning. Following prior work, we adopted the same 18 courser relations defined in <ref type="bibr" target="#b7">(Carlson and Marcu, 2001)</ref>. For evaluation, we report the standard metrics Span, Nuclearity, Relation and Full F1 scores, computed using the standard Parseval <ref type="bibr" target="#b35">(Morey et al., , 2018</ref> and <ref type="bibr">RST-Parseval (Marcu, 2000)</ref> metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parsing with Gold Segmentation</head><p>Settings. Discourse parsing with gold EDUs has been the standard practice in many previous studies. We compare our model with ten different baselines as shown in <ref type="table">Table 1</ref>. We report most results from <ref type="bibr" target="#b35">Morey et al. (2018)</ref>; <ref type="bibr" target="#b44">Zhang et al. (2020)</ref>; <ref type="bibr" target="#b24">Kobayashi et al. (2020)</ref>, while we reproduce <ref type="bibr" target="#b42">Yu et al. (2018)</ref> using their provided source code.</p><p>For our model setup, we use the encoder-decoder framework with a 3-layer Bi-LSTM encoder and 3-layer unidirectional LSTM decoder. The LSTM hidden size is 400, the word embedding size is 100 for random initialization, while the character embedding size is 50. The hidden dimension in MLP modules and biaffine function for structure prediction is 500. The beam width B is 20. Our model is trained by Adam optimizer (Kingma and Ba, 2015) with a batch size of 10000 tokens. Our learning rate is initialized at 0.002 and scheduled to decay at an exponential rate of 0.75 for every 5000 steps. Model selection for testing is performed based on the Full F1 score on the development set. When using pretrained word embeddings, we use the 100D vectors from GloVe <ref type="bibr" target="#b36">(Pennington et al., 2014)</ref>. For pretrained model, we use the XLNet-base-cased version <ref type="bibr" target="#b41">(Yang et al., 2019)</ref>. <ref type="bibr">1</ref> The pretrained models/embeddings are kept frozen during training. <ref type="table">Table 1</ref>, we see that our model with GloVe (static) embeddings achieves a Full F1 score of 46.8, the highest among all the parsers that do not use pretrained models (or contextual embeddings). This suggests that a BiLSTMbased parser can be competitive with effective modeling. The model also outperforms the one proposed by <ref type="bibr" target="#b44">Zhang et al. (2020)</ref>, which is closest to ours in terms of modelling, by 3.9%, 4.1%, 2.4% and 2.5% absolute in Span, Nuclearity, Relation <ref type="table">Table 1</ref>: Parsing results with gold segmentation. The sign + denotes that systems use handcrafted features such as lexical, syntactic, sentence/paragraph boundary features and so on, * denotes that systems use external cross-lingual features and ? means that systems use pretrained models. and Full, respectively. More importantly, our system achieves such results without relying on external data or features, in contrast to previous approaches. In addition, by using XLNet-base pretrained model, our system surpasses all existing methods (with or without pretraining) in all four metrics, achieving the state of the art with 2.9%, 4.0%, 2.4% and 2.1% absolute improvements. It also reduces the gap between system performance and human agreement. When evaluated with the RST-Parseval <ref type="bibr" target="#b32">(Marcu, 2000)</ref> metric, our model outperforms the baselines by 0.6%, 1.4% and 1.8% in Span, Nuclearity and Relation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From the results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">End-to-end Parsing</head><p>For end-to-end parsing, we compare our method with the model proposed by <ref type="bibr" target="#b44">Zhang et al. (2020)</ref>. Their parsing model uses the EDU segmentation from <ref type="bibr" target="#b25">Li et al. (2018)</ref>. Our method, in contrast, predicts the EDUs along with the discourse tree in a unified process ( ?2.3). In terms of model setup, we use a setup identical to the experiments with gold segmentation ( ?3.1). <ref type="table" target="#tab_2">Table 2</ref> reports the performance for documentlevel end-to-end parsing. Compared to <ref type="bibr" target="#b44">Zhang et al. (2020)</ref>, our model with GloVe embeddings yields 1.5%, 2.9%, 2.4% and 2.5% absolute gains in Span, Nuclearity, Relation and Full F1 scores, respectively. Furthermore, the model with XLNet   achieves even better performance and outperforms many models that use gold segmentation <ref type="table">(Table 1)</ref>.</p><p>EDU Segmentation Results. Our end-to-end parsing method gets an F1 score of 96.30 for the resulting EDUs. Our result rivals existing SoTA segmentation methods -92.20 F1 of <ref type="bibr" target="#b25">Li et al. (2018)</ref> and 95.55 F1 of <ref type="bibr" target="#b27">Lin et al. (2019)</ref>. This shows the efficacy of our unified framework for not only discourse parsing but also segmentation. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To further understand the contributions from the different components of our unified parsing framework, we perform an ablation study by removing selected components from a network trained with the best set of parameters.</p><p>With Gold Segmentation. <ref type="table" target="#tab_3">Table 3</ref> shows two ablations for parsing with gold EDUs. We see that both beam search and boundary LSTM (hierarchical encoding as shown in <ref type="figure">Fig. 7)</ref> are important to the model. The former can find better tree structure by searching a larger searching space. The latter, meanwhile, connects the EDU-boundary representations, which enhances the model's ability to capture long-range dependencies between EDUs. <ref type="bibr">2</ref> We could not compare our segmentation results with the DISRPT 2019 Shared Task <ref type="bibr" target="#b43">(Zeldes et al., 2019)</ref> participants. We found few inconsistencies in the settings. First, in their "gold sentence" dataset, instead of using the gold sentence, they pre-process the text with an automatic tokenizer and sentence segmenter. Second, in the evaluation, under the same settings, they do not exclude the trivial BeginSegment label at the beginning of each sentence which we exclude in evaluating our segmentation result (following the RST standard).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Span Nuc Rel Full   End-to-end Parsing. For end-to-end parsing, <ref type="table" target="#tab_5">Table 4</ref> shows that the sentence boundary constraint ( ?2.3) is indeed quite important to guide the model as it decodes long texts. Since sentence segmentation models are quite accurate, they can be employed if ground truth sentence segmentation is not available. We also notice that pretraining (GloVe) leads to improved performance.</p><p>Error Analysis. We show our best parser's (with gold EDUs) confusion matrix for the 10 most frequent relation labels in <ref type="figure" target="#fig_1">Fig. 5</ref>. The complete matrix with the 18 relations is shown in Appendix <ref type="figure" target="#fig_2">(Fig. 8)</ref>.</p><p>The imbalanced relation distribution in RST-DT affects our model's performance to some extent. Also semantic similar relations tend to be confused with each other. <ref type="figure">Fig. 6</ref> shows an example where our model mistakenly labels Summary as Elaboration. However, one could argue that the relation Elaboration is also valid here because the parenthesized text brings additional information (the equivalent amount of money). We show more error examples in the Appendix <ref type="figure" target="#fig_3">(Fig. 9 -11)</ref>, where our parser la-   <ref type="table" target="#tab_7">Table 5</ref> compares the parsing speed of our models with a representative non-neural (Feng and Hirst, 2014) and neural model <ref type="bibr" target="#b42">(Yu et al., 2018)</ref>. We measure speed empirically using the wall time for parsing the test set. We ran the baselines and our models under the same settings (CPU: Intel Xeon W-2133 and GPU: Nvidia GTX 1080 Ti).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parsing Speed</head><p>With gold-segmentation, our model with GloVe embeddings can parse the test set in 19 seconds, which is up to 11 times faster than <ref type="bibr" target="#b10">(Feng and Hirst, 2014)</ref>, and this is when their features are precomputed. The speed gain can be attributed to (i) to the efficient GPU implementation of neural modules to process the decoding steps, and (ii) the fact that our model does not need to compute any handcrafted features. With pretrained models, our parser with gold segmentation is about 2.4 times faster than <ref type="bibr" target="#b42">(Yu et al., 2018)</ref>. Our end-to-end parser that also performs segmentation is faster than the baselines that are provided with the EDUs. Nonetheless, we believe there is still room for speed improvement by choosing a better network, like the Longformer (Beltagy et al., 2020) which has an O(1) parallel time complexity in encoding a text, compared to the O(n) complexity of the recurrent encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Discourse analysis has been a long-established problem in NLP. Prior to the neural tsunami in NLP, discourse parsing methods commonly em-ployed statistical models with handcrafted features <ref type="bibr" target="#b37">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b15">Hernault et al., 2010;</ref><ref type="bibr" target="#b10">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b20">Joty et al., 2015)</ref>. Even within the neural paradigm, most previous studies still rely on external features to achieve their best performances <ref type="bibr" target="#b40">Wang et al., 2017;</ref><ref type="bibr" target="#b6">Braud et al., 2016</ref><ref type="bibr" target="#b5">Braud et al., , 2017</ref><ref type="bibr" target="#b42">Yu et al., 2018)</ref>. These parsers adopt a bottom-up approach, either transition-based or chart-based parsing.</p><p>Recently, top-down parsing has attracted more attention due to its ability to maintain an overall view of the input text. Inspired by the Stack-Pointer network <ref type="bibr" target="#b29">(Ma et al., 2018)</ref> for dependency parsing, <ref type="bibr" target="#b27">Lin et al. (2019)</ref> first propose a seq2seq model for sentence-level parsing. <ref type="bibr" target="#b44">Zhang et al. (2020)</ref> extend this to the document level. <ref type="bibr" target="#b24">Kobayashi et al. (2020)</ref> adopt a greedy splitting mechanism for discourse parsing inspired by <ref type="bibr" target="#b38">Stern et al. (2017)</ref>'s work in constituency parsing. By using pretrained models/embeddings and extra features (e.g., syntactic, text organizational features), these models achieve competitive results. However, their decoder infers a tree greedily.</p><p>Our approach differs from previous work in that it can perform end-to-end discourse parsing in a single neural framework without needing segmentation as a prerequisite. Our model can parse a document from scratch without relying on any external features. Moreover, it can apply efficient beam search decoding to search for the best tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel top-down end-to-end method for discourse parsing based on a seq2seq model. Our model casts discourse parsing as a series of splitting decisions at token boundaries, which can solve discourse parsing and segmentation in a single model. In both end-to-end parsing and parsing with gold segmentation, our parser achieves state-of-the-art, surpassing existing methods by a good margin, without relying on handcrafted features. Our parser is not only more effective but also more efficient than the existing ones.</p><p>This work leads us to several future directions. Our short-term goal is to improve the model with better architecture and training mechanisms. For example, joint training on discourse and syntactic parsing tasks could be a good future direction since both tasks are related and can be modeled within our unified conditional splitting framework. We also plan to extend our parser to other languages. <ref type="figure">Figure 7</ref> shows first few decoding steps with final parsers with EDUs. <ref type="figure">Figure 7</ref>: Our discourse parser along with the first few decoding steps. When EDUs are given, we use a hierarchical EDU encoding (Boundary LSTM) to model EDU boundary representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Error Analysis</head><p>We show our best parser's (with gold EDUs) confusion matrix for all relation labels in <ref type="figure" target="#fig_1">Fig. 5. Fig. 9</ref> -11 present examples where our parser falsely labels a Condition as Background, Temporal as Joint and Explanation as Elaboration.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of token-boundary span encoder. The figure lays out an example representation for the boundary at 2 and the representation of the token-boundary span (0, 44), which corresponds to the whole document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Confusion matrix for the 10 most frequent relations on the RST-DT test set. The vertical axis represents true and horizontal axis represents predicted relations. The relations are: Elaboration (EL), Attribution (AT), Joint (JO), Same-Unit (SA), Contrast (CONT), Background (BA), Explanation (EX), Cause (CA), Temporal (TEM), Condition (COND).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Confusion matrix for relation labels on the RST-DT test set. The vertical axis represents true and horizontal axis represents predicted relations. The relations are: Joint (JO), Elaboration (EL), Temporal (TEM), Contrast (CONT), Cause (CA), Same-Unit (SA), Manner-Means (MA), Attribution (AT), Condition (COND), Comparison (COMP), Background (BA), Enablement (EN), Explanation (EX), Evaluation (EV), Summary (SU), Topic-Comment (T-CM), Topic-Change (T-C) and TextualOrganization (T-O).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Our system incorrectly labels a Condition as Background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Our system incorrectly labels a Temporal as Joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Our system incorrectly labels a Explanation as Elaboration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2 Discourse Tree Inference with Beam Search (with gold EDUs) Input: Number of EDUs in document m; beam width B; EDU boundarybased encoder states: (h0, . . . , hm); label scores: P ? (l|(i, k), (k, j), 0 ? i &lt; k &lt; j ? m, l ? {1, . . . , L}, initial decoder state s. Output: Parse tree DT L d = m ? 1 Decoding length beam = array of L d items</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>End-to-end parsing performance.</figDesc><table><row><cell>Model</cell><cell>Span Nuc</cell><cell>Rel</cell><cell>Full</cell></row><row><cell>Final model</cell><cell cols="3">71.1 59.6 47.7 46.8</cell></row><row><cell>Beam search</cell><cell cols="3">70.1 58.1 46.8 45.8</cell></row><row><cell cols="4">Boundary LSTM 68.5 55.5 46.1 44.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation test of our models with gold EDUs. Beam</figDesc><table><row><cell>search indicates the full model with greedy decoding (beam</cell></row><row><cell>width 1), while Boundary LSTM is the full model with greedy</cell></row><row><cell>decoding and no LSTM connection between EDU-boundary</cell></row><row><cell>representations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation test of our end-to-end model. GloVe is the full model with randomized word embeddings, while Sentence guidance is the full model with randomized word embeddings and without sentence guidance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figure 6: An error example where our system incorrectly labels a Summary as Elaboration.</figDesc><table><row><cell>System</cell><cell cols="2">Gold Seg. End-to-End Time (s) Speedup</cell></row><row><cell>(Feng and Hirst, 2014)</cell><cell>210</cell><cell>1.0x</cell></row><row><cell>(Yu et al., 2018)</cell><cell>79</cell><cell>2.7x</cell></row><row><cell>Our parser (Glove)</cell><cell>19</cell><cell>11.1x</cell></row><row><cell>Our parser (XLNet)</cell><cell>33</cell><cell>6.4x</cell></row><row><cell>Our parser (GloVe)</cell><cell>45</cell><cell>4.7x</cell></row><row><cell>Our parser (XLNet)</cell><cell>60</cell><cell>3.5x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The wall time for parsing the RST-DT test set.</figDesc><table><row><cell>bels a Condition as Background, Temporal as Joint</cell></row><row><cell>and Explanation as Elaboration. As we can see, all</cell></row><row><cell>these relations are semantically close and arguably</cell></row><row><cell>interchangeable.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our initial attempt with BERT did not offer significant gain as BERT is not explicitly designed to process long documents and has a limit of maximum 512 tokens.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 6 Parsing with EDUs</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parseval</forename><surname>Metric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Morey</surname></persName>
		</author>
		<idno>Agreement 78.7 66.8 57.1 55.0</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eisenstein</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rst-Parseval</forename><surname>Metric</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Marcu</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References Iz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2212" to="2218" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-lingual RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="292" to="304" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view and multi-task training of RST discourse parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1903" to="1913" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Discourse tagging reference manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<idno>ISI-TR- 545</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California Information Sciences Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The utility of parse-derived features for automatic discourse segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seeger</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<pubPlace>Irwin King, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discern: Discourse-aware entailment reasoning network for conversational machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2439" to="2449" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Nejat</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1168</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hilda: A discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural discourse structure for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A novel discriminative framework for sentence-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="904" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CODRA: A novel discriminative framework for rhetorical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00226</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="435" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining Intra-and Multisentential Rhetorical Parsing for Document-level Discourse Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<ptr target="Bul-garia.ACL" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL&apos;13</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL&apos;13<address><addrLine>Sofia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discourse structure in machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00298</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="683" to="722" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Top-down rst parsing utilizing granularity levels in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Artificial Intelligence for the American (AAAI)</title>
		<meeting>the 2020 Conference on Artificial Intelligence for the American (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8099" to="8106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segbot: A generic neural text segmentation model with pointer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/579</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4166" to="4172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discourse parsing with attention-based hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified linear-time framework for sentence-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathyusha</forename><surname>Jwalapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4190" to="4200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rst discourse treebank (rst-dt) ldc2002t07. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlson</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stackpointer networks for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1130</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1403" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The rhetorical parsing of unrestricted texts: a surface-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="448" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How much progress have we made on RST discourse parsing? a replication study of recent results on the RST-DT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017</title>
		<meeting>the 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1319" to="1324" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A dependency perspective on RST discourse parsing and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00314</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="235" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A two-stage parsing method for text-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="184" to="188" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transition-based neural RST parsing with implicit syntax features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The DIS-RPT 2019 shared task on elementary discourse unit segmentation and connective detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zeldes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debopam</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><forename type="middle">Galani</forename><surname>Maziero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Iruskieta</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-2713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Discourse Relation Parsing and Treebanking</title>
		<meeting>the Workshop on Discourse Relation Parsing and Treebanking<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A top-down neural architecture towards text-level parsing of discourse rhetorical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6386" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
