<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
							<email>wangweiming@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we tackle the problem of category-level 9D pose estimation in the wild, given a single RGB-D frame. Using supervised data of real-world 9D poses is tedious and erroneous, and also fails to generalize to unseen scenarios. Besides, category-level pose estimation requires a method to be able to generalize to unseen objects at test time, which is also challenging. Drawing inspirations from traditional point pair features (PPFs), in this paper, we design a novel Category-level PPF (CPPF) voting method to achieve accurate, robust and generalizable 9D pose estimation in the wild. To obtain robust pose estimation, we sample numerous point pairs on an object, and for each pair our model predicts necessary SE(3)-invariant voting statistics on object centers, orientations and scales. A novel coarse-to-fine voting algorithm is proposed to eliminate noisy point pair samples and generate final predictions from the population. To get rid of false positives in the orientation voting process, an auxiliary binary disambiguating classification task is introduced for each sampled point pair. In order to detect objects in the wild, we carefully design our sim-to-real pipeline by training on synthetic point clouds only, unless objects have ambiguous poses in geometry. Under this circumstance, color information is leveraged to disambiguate these poses. Results on standard benchmarks show that our method is on par with current state of the arts with realworld training data. Extensive experiments further show that our method is robust to noise and gives promising results under extremely challenging scenarios. Our code is available on https://github.com/qq456cvb/CPPF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimation of 3D position, orientation and scale of novel objects, namely, category-level 9D pose estimation in the wild, is of great importance in many fields, such as robotics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> and human-object interactions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. There are many prior works exploring this direction, but with limitations, though. Some past works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> have explored the instance-level 6D pose estimation. However, they require exact object models and their sizes beforehand, which is often not realizable in real-world scenarios. NOCS <ref type="bibr" target="#b31">[32]</ref> introduces normalized object coordinate space to give a consistent representation across intra-class objects. Although it is able to achieve category-level pose estimations, it requires real-world pose annotations, which are tedious and limited by size. Besides, the 3D object scales predicted by NOCS are simple heuristics and prone to object occlusions, which is inevitable in real world. We doubt if one can leverage a sim-to-real approach that generalize 9D pose estimations from synthetic objects to real world, since ground-truth pose annotations in real-world scenarios are hard to acquire. Gao et al. <ref type="bibr" target="#b9">[10]</ref> tries to solve this problem via comparing the appearance of objects of synthetic objects and real objects, and finds a pose that minimize the difference. Though this method does not require real 9D pose labels, it is erroneous and inferior to NOCS in terms of orientation error, due to the domain gap between synthetic and real RGB images.</p><p>Embracing these challenges, in this paper, we propose Category-level PPF (CPPF) that votes for category-level 9D poses. We draw some inspirations from traditional point pair features (PPFs), and formulate the problem of pose estimation as a voting process, where each point pair would generate several offsets or relative angles towards groundtruth 9D poses. Next, the pose with the most votes is cast as our final prediction. In contrast to traditional instancelevel PPFs, where each pair is matched against an offline database, our method is much faster and able to generalize to unseen objects. To overcome the difficulty in orientation voting, where false positives are generated, an auxiliary binary classification task is introduced.</p><p>In order to segment out the point cloud of a real-world target object, we leverage an off-the-shelf or fine-tuned instance segmentation model. We argue that instance segmentation labels are much cheaper and easier to annotate than 9D poses.</p><p>Besides, we develop a two-stage coarse-to-fine voting method, to robustly estimate the object pose when predictions of instance segmentation are not accurate. This method could eliminate noisy point pairs that do not contribute to the voting of object pose. Furthermore, our additional experiments show that when only object bounding boxes are available, our method could still achieve robust and appealing results.</p><p>We evaluate our method on the publicly available realworld dataset released by Wang et al. <ref type="bibr" target="#b31">[32]</ref> on categorylevel object pose estimation. Results show that our method beats sim-to-real state of the arts and is comparable to realworld training methods. In addition, we show that when only bounding box detections are provided, our method still gives decent 9D pose predictions. To further evaluate the generalization ability of our method to real-world scenarios, we directly apply our method on SUN RGB-D <ref type="bibr" target="#b26">[27]</ref> dataset with zero-shot transfer, which contains much more diverse and complex scenes. Our method also outperforms baselines by a large margin. In summary, our contribution is:</p><p>? We propose a novel category-level voting scheme to extract 9D pose of objects. An auxiliary task is introduced to remove the ambiguity in orientation voting.</p><p>A coarse-to-fine voting algorithm is proposed to eliminate noisy point pairs with robust pose predictions.</p><p>? We introduce a novel sim-to-real pipeline with carefully designed point pair features to achieve generalizable sim-to-real transfer, with synthetic models only.</p><p>? Extensive experiments show that our sim-to-real method is on par with current state-of-the-art meth-ods, which utilize real-world training data. Besides, our model is robust to segmentation errors and could give accurate pose predictions with only bounding box detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Pose Estimation</head><p>Instance-Level Pose Estimation There are many pose detection methods that requires only point clouds. Drost et al. <ref type="bibr" target="#b8">[9]</ref> propose point pair features to match against an object database using a voting scheme. Later, some researchers improves upon Drost's work: Hinterstoisser et al. <ref type="bibr" target="#b11">[12]</ref> leverage a smart sampling scheme to restrict the searching range; Vidal et al. <ref type="bibr" target="#b30">[31]</ref> improve the matching process by considering neighborhoods that potentially affected due to noise. It also improves the post-processing step such that the retrieved pose is more consistent with the observed camera view. Shi et al. <ref type="bibr" target="#b25">[26]</ref> propose a method on generating object poses from stable geometric groups.</p><p>There are also many works taking RGB(-D) images as input. Kehl et al. <ref type="bibr" target="#b15">[16]</ref> extend the popular SSD paradigm to cover the full 6D pose space. Branchmann et al. <ref type="bibr" target="#b2">[3]</ref> learn to classify each pixel into a set of normalized coordinates and then generates a set of candidates by RANSAC. Grabner et al. <ref type="bibr" target="#b10">[11]</ref> render depth images from 3D models using the predicted poses and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. Rios et al. <ref type="bibr" target="#b24">[25]</ref> use a discriminative learning approach to match the object pose in images against a database. DeepIM <ref type="bibr" target="#b18">[19]</ref> leverages a FlowNet to output relative pose between real and rendered image patches. The pose is refined in an iterative way. Kehl et al. <ref type="bibr" target="#b16">[17]</ref> learn to auto-encode RGB-D patches and match them in a codebook to vote for the final pose. Gao et al. <ref type="bibr" target="#b9">[10]</ref> directly regress 6D poses from object point clouds. These methods lack the ability to generalize to unseen objects, and most of them <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> do not scale well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category-Level Pose Estimation</head><p>Recently, a few works focus on category-level object estimation, where an unseen object's pose is to be detected. NOCS <ref type="bibr" target="#b31">[32]</ref> learns to regress objects' normalized coordinates establishing 2D-3D relationships, so that object poses can be solved in closed form. However, it requires real-world pose annotations for training. SPD <ref type="bibr" target="#b27">[28]</ref> improves the predictions of canonical object models by deforming categorical shape priors. Then, CASS <ref type="bibr" target="#b4">[5]</ref> use a variational auto-encoder to capture poseindependent features, along with pose-dependent ones, to directly predict the 6D poses. FS-Net <ref type="bibr" target="#b5">[6]</ref> proposes a decoupled rotation mechanism that uses two decoders to decode the category-level rotation information. For translation and size estimation, it uses a residual estimation network. Du-alPoseNet <ref type="bibr" target="#b20">[21]</ref> leverages two parallel decoders either make a pose prediction explicitly, or implicitly do so by reconstructing the input point cloud in its canonical pose. The explicit prediction is then refined with the implicit one. Chen et al. <ref type="bibr" target="#b6">[7]</ref> propose to render synthetic models and compare the appearance with real images under different poses. This method, though achieves sim-to-real transfer, is inferior to our method, due to the domain gap between synthetic and real RGB images. Their method, however, is prone to occlusion and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sim-to-Real Transfer</head><p>Sim-to-Real is a common strategy in many fields like object reconstruction, pose estimation and reinforcement learning for robots. ShapeHD <ref type="bibr" target="#b33">[34]</ref> and MarrNet <ref type="bibr" target="#b32">[33]</ref> render realistic ShapeNet <ref type="bibr" target="#b3">[4]</ref> models for 3D object reconstruction from a single RGB image. PoseCNN <ref type="bibr" target="#b34">[35]</ref> renders different objects into random background to synthesis images for training of object poses. Many works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref> follow this paradigm, and training on synthetic RGB images have been a common practice in object pose estimation. In reinforcement learning, domain adaption methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> are usually leveraged, and visual/physical realistic simulation of real environment <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref> plays an important role. Most existing methods explore the domain transfer in color space, while few works <ref type="bibr" target="#b29">[30]</ref> focus on the domain gap between synthetic and real point clouds. This is due to the fact that in real scenarios, objects are often occluded with noisy backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries: Point Pair Features</head><p>In this section, we briefly discuss the original point pair features (PPFs) proposed by Drost et al. <ref type="bibr" target="#b8">[9]</ref>, which can be leveraged for instance-level retrieval.</p><p>Given two points p 1 and p 2 with normals n 1 and n 2 , set d = p 2 ? p 1 and define the so-called point pair features F :</p><formula xml:id="formula_0">F (p 1 , p 2 ) = (?d? 2 , ?(n 1 , d), ?(n 2 , d), ?(n 1 , n 2 )), (1)</formula><p>In the offline phase, the global model description is created. Such a global model description contains all the precalculated PPFs for the object of interest.</p><p>In the online phase, a set of reference points in the scene is selected. All other points in the scene are paired with the reference points to create point pair features. These features are matched to the model features contained in the global model description, and a set of potential matches is retrieved. Every potential match votes for an object pose by using an efficient voting scheme where the pose is parametrized relative to the reference point. Specifically, for each match, the pose can be retrieved by aligning the PPF in the scene to that in the offline database. For more details, we refer the reader to Drost et al. <ref type="bibr" target="#b8">[9]</ref>. Though Drost PPF has been successful in many scenarios, it can not do a category-level pose estimation, and it is not scalable as the number of objects goes large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>To address the problem raised by instance-level PPFs, we propose Category-level PPF (CPPF) -a brand-new method for detecting category-level object poses. Compared with Drost PPF, our method is free of an offline database. Instead, we directly predict the necessary statistics to vote for object centers, orientations and scales, using a neural network with augmented point pair features as the input.</p><p>We assume the target object is first segmented out by some off-the-shelf or fine-tuned instance segmentation model. The segmentation does not need to be exact, and we will introduce our coarse-to-fine voting process to robustly estimate 9D poses when the segmentation is inaccurate. Furthermore, in Section 6.2, we show that instance segmentation can be replaced by coarse bounding box masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Point Pair Voting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Voting for Centers</head><p>Denote the object center as o, for each point pair p 1 and p 2 , we predict the following two offsets:</p><formula xml:id="formula_1">? = ? ? ? p 1 o ? ? ?? ? p 1 p 2 ? ? ?? ? p 1 p 2 ? 2 ,<label>(2)</label></formula><formula xml:id="formula_2">? = ?o ? (p 1 + ? ? ?? ? p 1 p 2 ? ? ?? ? p 1 p 2 ? 2 )? 2 ,<label>(3)</label></formula><p>Notice that these two offsets are invariant to rotations and translations, because for arbitrary rotation matrix R ? SO(3) and translation t ? R 3 , the new offsets ? ? and ? ? are:</p><formula xml:id="formula_3">? ? = R ? ? ? ? p 1 o ? R ? ? ?? ? p 1 p 2 ?R ? ? ?? ? p 1 p 2 ? 2 = ?,<label>(4)</label></formula><formula xml:id="formula_4">? ? = ?(R ? o + t) ? (R ? p 1 + t + ? R ? ? ?? ? p 1 p 2 ?R ? ? ?? ? p 1 p 2 ? 2 )? 2 = ?.<label>(5)</label></formula><p>The proof is simple and omitted, observing that both inner product and L2 norm are invariant to rotations. Once ? and ? are fixed, the object center is determined up to one degree-of-freedom ambiguity. Specifically, the object center will lie on a circle, with center c</p><formula xml:id="formula_5">= p 1 + ? ? ? ?? ? p1p2 ? ? ?? ? p1p2?2</formula><p>and radius ?, demonstrated in <ref type="figure" target="#fig_1">Figure 2a</ref>.</p><p>Inspired by Canonical Voting <ref type="bibr" target="#b37">[38]</ref>, we can enumerate every 2? K degree and generate multiple votes along the circle. Though there is a circle ambiguity for a single pair, when there are enough pairs, the ground truth location will emerge with the largest vote count, shown in <ref type="figure" target="#fig_1">Figure 2b</ref>. Symmetry in Objects Another nice property of our voting scheme is that symmetric objects are naturally handled without any special treatment. Previous methods like NOCS <ref type="bibr" target="#b6">[7]</ref> require a special treatment of symmetric objects because of the ambiguity of the normalized space. They map the same input features to different outputs due to symmetry. In contrast, in our model, input features of symmetric point pairs are exactly the same (due to the SE(3) invariance of PPF), and the output offsets for these pairs are also identical, so that our model learns a proper functional mapping. This is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Voting for Orientations</head><p>Denoting the up orientation as e 1 and right orientation as e 2 , we predict the following two relative angles:</p><formula xml:id="formula_6">? = e 1 ? ? ?? ? p 1 p 2 ? ? ?? ? p 1 p 2 ? 2 (6) ? = e 2 ? ? ?? ? p 1 p 2 ? ? ?? ? p 1 p 2 ? 2 .<label>(7)</label></formula><p>These two angles are also invariant to arbitrary rotations.  <ref type="figure">Figure 4</ref>. (a) Once ? is predicted, the candidate orientation vector will lie on a cone with one degree of freedom. (b) For two point pairs p1, p2 and p1, p3, we generate candidate votes and count them into bins, the final prediction is the one with the most votes.</p><p>Analogous to center voting, there is also an ambiguity of one degree of freedom, shown in <ref type="figure">Figure 4a</ref>. Likewise, we also generate a set of proposals for each point pair with a constant degree interval, and then select the predicted orientation as the one with the largest voting count. Because the orientation is continuous, in practice, we uniformly enumerate a set of orientations from unit sphere. For each orientation, we count the number of vote candidates that fall into a fixed solid angle around the orientation. The orientation with the largest count is identified as the final prediction. This is illustrated in <ref type="figure">Figure 4b</ref>.</p><p>Removing the Ambiguity of Orientations Unfortunately, for orientations, a fake peak with opposite direction sometimes appears when the object is symmetric in structure. When the relative angle ? is about ? 2 for a majority of point pairs, the opposite orientation to ground-truth (i.e., -e 1 ) also receives a lot of point votes, giving a false positive.</p><p>To eliminate false positives, an auxiliary task is introduced. For each point pair p 1 , p 2 , we calculate p 1 's normal n 1 (normal ambiguity removed by ensuring n 1 ? ? ?? ? p 1 p 2 &lt; 0). Then we do a binary classification on the following two auxiliary variables:</p><formula xml:id="formula_7">? = 1, if n 1 ? e 1 &gt; 0 0, otherwise ,<label>(8)</label></formula><formula xml:id="formula_8">? = 1, if n 1 ? e 2 &gt; 0 0, otherwise .<label>(9)</label></formula><p>Taking the up orientation as an example, we show how these auxiliary variables can be used to remove the ambiguity in the opposite direction. During inference, for each point pair,? is predicted by our neural network. Denote the orientation candidate after voting as? 1 , we calculate two additional statistics? ? = n 1 ?? 1 and ?? ? = ?n 1 ?? 1 for each pair. Then,? ? and ?? ? are compared with?. If the summation of CrossEntropy(? ? ,?) from all point pairs is smaller than CrossEntropy(?? ? ,?). we keep? 1 ; other-wise, we flip the sign of? 1 and set? 1 := ?? 1 . We will verify the usefulness of this task in our ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Voting for Scales</head><p>Denoting the category-level average bounding box scales as s ? R 3 and the bounding box scale of a particular instance as s ? R 3 , we predict the following statistic:</p><formula xml:id="formula_9">? = log(s) ? log(s).<label>(10)</label></formula><p>During inference, ? is first averaged among sampled point pairs, and then the predicted scale can be retrieved as? = exp(?) * s, where * is the point-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Coarse-to-Fine Voting Process</head><p>In previous sections, we describe the overall voting process for 9D poses (i.e., translation, rotation, scale). However, the generated object pose (especially orientation) might be inaccurate due to noisy points when the instance segmentation is not precise. In order to filter out these noisy points, we propose a coarse-to-fine voting algorithm. Specifically, we first vote for object centers with all the points and then back-trace these votes, keeping only the points that generate enough votes close to the voted object center. Once the noisy points are removed, we vote for the object pose again with the filtered points. A formal description of this algorithm is illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sim-to-Real Transfer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Transfer with Depth Maps</head><p>One big advantage of our method is that we only need to train on the synthetic models, and then generalize to realworld scenarios. We achieve this by using the depth map during both training and testing phases, and only local point features are leveraged as input. We find that depth or point clouds are much more accurate in sim-to-real generalizations. In contrast, color information is harder to transfer in real-world scenarios, because light conditions are really hard to tune in order to generalize. Color information is only leveraged when there are several ambiguous poses that cannot be distinguished from point clouds.</p><p>Rendering Synthetic Models through Realistic Self-Occlusion For each category, we choose several synthetic topology-correct models from ShapeNetCore55 similar to that in NOCS <ref type="bibr" target="#b6">[7]</ref>. Then we use OpenGL <ref type="bibr" target="#b23">[24]</ref> to render each model's depth map from a sampled perspective. All points from the back faces get culled in order to simulate selfocclusion. Notice that compared with NOCS, our method does not need to choose a random background and paste synthetic models onto it. The only requirement is the synthetic model itself. Use neural network to predict ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Generate candidate center votes o <ref type="bibr" target="#b0">(1)</ref> , . . . , o (N ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Accumulate candidate votes into G by discretization. 8: end for 9: Outputt = argmax(G) as the predicted translation. <ref type="bibr" target="#b9">10</ref> Use neural network to predict ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Generate candidate center votes o <ref type="bibr" target="#b0">(1)</ref> , . . . , o (N ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>for i = 1, . . . , N do <ref type="bibr">15:</ref> if ?o (i) ?t? &lt; ? then <ref type="bibr">16:</ref> Add p 1 and p 2 to P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>end if <ref type="bibr">18:</ref> end for 19: end for 20: Initialize discretized orientation grids E 1 and E 2 . 21: Initialize ? acc = 0. 22: for each sampled point pair p 1 and p 2 in P do <ref type="bibr">23:</ref> Use neural network to predict ?, ? and ?. <ref type="bibr">24:</ref> ? acc + = ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>Generate candidate orientation votes e with constant interval. <ref type="bibr">26:</ref> Accumulate candidate votes into E 1 and E 2 27: end for 28: Output? = exp( ?acc |P| ) * s. 29: Output? 1 = argmax(E 1 ) and? 2 = argmax(E 2 ) as the orientation with the most votes.</p><p>Voxelization and Random Jittering Another problem of sim-to-real transfer is the different sampling density in simulated and real scenarios. To mitigate the domain gap, during both training and testing, we voxelize input point clouds with a predefined resolution to obtain a constant sampling density. Besides, we observe that both simulated and real point clouds have a grid artifact. This is due to the rasterization of image pixels. To solve this issue, we randomly jitter the simulated and real point clouds, leading to an improvement on the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Use Color Information to Disambiguate Poses</head><p>For most categories, our rendered depth images (i.e., point clouds) generalize well to real-world. However, for laptop, there are two ambiguous poses. The laptop lid and keyboard base are hard to discriminate with point clouds only, even for humans. To solve this problem, we train an additional network that takes RGB inputs to segment the lid and keyboard. The training data for this network only contains rendered synthetic laptop images with Blender <ref type="bibr" target="#b7">[8]</ref>, so that our model is still free of real training images. When testing, we calculate the normal of laptop keyboard by RANSAC plane detection on the predicted segmentation. If the voting result is inconsistent with this normal, we replace it with the normal, otherwise the result is unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>In practice, we convert the scalar regression problem into a multi-class classification problem by using a list of anchors, and find that this gives us a better result. We use 32 bins for translation and 36 bins for rotation. In both training and inference stage, we uniformly sample a fixed number of points (20,000 in training, 100,000 in testing) per model/image and predict their corresponding statistics (i.e., ?, ?, ?, ?, ?). In the center voting process, the accumulation 3D grid has a resolution of 0.4 cm except for laptop which is 1 cm. The range of 3D grid is the tightest axisaligned bounding box of input. In the orientation voting process, the orientation grid has a resolution of 1.5 degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Network Architecture</head><p>We use SPRIN <ref type="bibr" target="#b36">[37]</ref>, which is an SO(3) invariant network, and we modified input features to make it SE(3) invariant. The input to the network is the point pair and the set of k nearest neighbors of each point. Denoting k neighbors of point p as {p <ref type="bibr" target="#b0">(1)</ref> , ? ? ? , p (k) }, the sides and angles of triangles formed by 1 k k 1 p (n) , p (n) and p are fed to SPRIN to extract rotation invariant point embeddings. Besides, the normal for each point is also estimated and leveraged from its k nearest neighbors. The network profession is illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. We train each category separately, with Adam optimizer, using learning rate 1e-3, for 200 epochs. The batch size is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we evaluate our method on two datasets. NOCS REAL275 <ref type="bibr" target="#b31">[32]</ref> and SUN RGB-D <ref type="bibr" target="#b26">[27]</ref>. Both datasets provide RGB-D frames with annotated 9D bounding boxes.</p><p>Metrics We follow NOCS <ref type="bibr" target="#b31">[32]</ref> to report both intersection over union and 6D pose average precision. Intersection over union (IoU) is calculated between the predicted and groundtruth bounding boxes with threshold of 50%, while 6D pose average precision is calculated by measuring the average precision of object instances for which the error is less than m cm for translation and n ? for rotation. We follow NOCS to set a detection threshold of 10% bounding box overlap between prediction and ground truth to ensure that most objects are included in the evaluation. Notice that the original 3D box mAP computation code provided by NOCS is buggy. Instead, we use the correct code from Objectron [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">NOCS REAL275 with Instance Mask</head><p>Wang et al. <ref type="bibr" target="#b31">[32]</ref> captures 8K real RGB-D frames (4300 for training, 950 for validation and 2750 for testing) of 18 different real scenes (7 for training, 5 for validation, and 6 for testing) using a Structure Sensor. We use the 2750 testing scenes for evaluation. We use the instance segmentation masks from NOCS <ref type="bibr" target="#b31">[32]</ref> for a fair comparison.</p><p>Comparisons to State of the Arts We compare our method with a set of real-world training methods: NOCS <ref type="bibr" target="#b31">[32]</ref>, CASS <ref type="bibr" target="#b4">[5]</ref>, SPD <ref type="bibr" target="#b27">[28]</ref>, FS-Net <ref type="bibr" target="#b5">[6]</ref>, Dual-PoseNet <ref type="bibr" target="#b20">[21]</ref>; and a set of methods requiring synthetic training data only: Chen et al. <ref type="bibr" target="#b6">[7]</ref>, Gao et al. <ref type="bibr" target="#b9">[10]</ref>. The original mAP results reported by Chen et al. <ref type="bibr" target="#b9">[10]</ref> uses the IoU matches computed by NOCS <ref type="bibr" target="#b31">[32]</ref> which is potentially unfair, we fix this by using the matches computed by Chen et al.'s <ref type="bibr" target="#b6">[7]</ref> method itself. We also augment Gao et al.'s <ref type="bibr" target="#b9">[10]</ref> method to additionally regress 3D box scales. NOCS <ref type="bibr" target="#b31">[32]</ref>, Chen et al. <ref type="bibr" target="#b6">[7]</ref>, Gao et al. <ref type="bibr" target="#b9">[10]</ref> and DualPoseNet <ref type="bibr" target="#b20">[21]</ref>'s results are given by running the official code provided by the authors, while the others are borrowed from the original papers. Notice that DualPoseNet <ref type="bibr" target="#b20">[21]</ref> uses its own instance segmentation masks other than those provided by NOCS, which may result in a higher mAP than the actual.</p><p>The results are given in <ref type="table" target="#tab_1">Table 1</ref>. We see that our method achieves an mAP of 16.9, 44.9 and 50.8 for (5 ? , 5 cm), (10 ? , 5 cm) and (15 ? , 5 cm) respectively. It outperforms the best sim-to-real baseline by 9.1, 27.8 and 24.3, which is a quite large margin. Our method is also comparable to those methods that are trained on real-world pose annotations. More detailed analysis and comparison is illustrated in <ref type="figure" target="#fig_7">Figure 6</ref>. Some qualitative comparisons are given in <ref type="figure" target="#fig_8">Figure 7</ref>. This experiment shows that our proposed method generalize well to real-world data with only synthetic training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">NOCS REAL275 with Bounding Box Masks</head><p>Though our method does not require pose annotations for real-world data, it does need to first segment out the target object with a real-world trained instance segmentation model. Can we relax this requirement? Thanks to our robust coarse-to-fine voting algorithm 1 which filters out noisy points, we find that when only bounding boxes are given, our method still achieves significant results. Notice that current state of the arts all require pixel-wise instance segmentation as input. Quantitative results are given in <ref type="table" target="#tab_1">Table 1</ref>. Qualitative pose predictions are shown in <ref type="figure" target="#fig_9">Figure 8</ref>.</p><p>Moreover, we also tried to get rid of detection priors completely for bowls, more details in the supplementary.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">SUN RGB-D in the Wild</head><p>SUN RGB-D <ref type="bibr" target="#b26">[27]</ref> is a scene understanding benchmark which provides 58,657 9D bounding boxes with accurate object orientations for 10,000 images. We use all the chairs in validation split for evaluation, which contains 2,699 images. In order to make the problem more challenging, we randomly rotate the SUN RGB-D scenes while the original point clouds are aligned with gravity. In addition, we require that all the algorithms cannot see any training data but use existing instance segmentation models (i.e., trained on MSCOCO <ref type="bibr" target="#b21">[22]</ref> but not fine-tuned on SUN RGB-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Results</head><p>We compare our method with two baselines: direct back-projection and Gao et al. <ref type="bibr" target="#b9">[10]</ref>. Direct back-projection is a simple baseline that directly back project the detected instance into an axis-aligned bounding box, while Gao et al. <ref type="bibr" target="#b9">[10]</ref> is the same as in Section 6.1.</p><p>Since this is an extremely difficult task, we only evaluate orientation errors along the up axis. Results are listed in <ref type="table">Table 2</ref>. More results are given in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Ablation Study and Running Time</head><p>In this section, we conduct various ablation studies on our model. Results are reported on REAL275 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Point Pair Samples and Size of Discrete Orientation Bins</head><p>As the number of pair samples increase, the voting results for orientation and translation become more accurate, while getting saturated for 100,000 point pairs. The size of discrete orientation bins also decides the accumulation accuracy during the voting process. Quantitative results are given in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Auxiliary Task for Disambiguating Poses The auxiliary classification task helps our model get rid of potentially flipped orientations, and <ref type="table" target="#tab_4">Table 4</ref> verifies this.</p><p>Whether to use Coarse-to-Fine Voting Process Recall, in Algorithm 1, we filter out point pairs that do not con-   tribute to the proposed object center. This makes our model robust to the noisy points from the imperfect instance segmentation. Quantitative results are given in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Regression vs. Classification Direct regression on relevant statistics are worse than classification. This may due to the fact that regression does not constrain the value into a valid range and produces more noisy outputs. Quantitative results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Running Time Analysis It takes 171ms, 229ms and 13ms per image for the voting of centers, orientations and scales respectively on a single 1080Ti GPU. Our model is efficient thanks to the highly parallelized voting process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a category-level voting algorithm to predict 9D poses in the wild. To overcome the difficulty of false positives during the voting step, an auxiliary orientation classification task is introduced. Our model is trained on synthetic objects and generalizes well to real scenes. Results show that our method is superior to previous sim-to-real methods, even with bounding box masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Direct 9D Pose Estimation and Segmentation</head><p>In our experiments, we show that our model can work with both segmentation and bounding box masks. Can we locate objects with our voting scheme directly (i.e., without any preprocessing instance detection pipeline)? For some categories like laptop, this is hard since the laptop base can be mixed with the floor in view of point clouds. However, for bowls, this is possible, where the pose estimation is indeed zero-shot without seeing any real-world data during the whole pipeline. We modify Algorithm 1 by sampling point pairs from the whole scene, while keeping the coarseto-fine procedure. We also use a threshold to generate candidate object locations instead of a simple argmax (line 9 in Algorithm 1).</p><p>Zero-shot instance segmentation Surprisingly, as a byproduct, our method is able to infer the instance-level mask without even seeing any segmentation labels. This is done by counting the contribution of each point, where any point that votes more than v times within a small radius ? of the true object center (line 15 in Algorithm 1), is considered lying on the target object. Qualitative 9D pose and segmentation results are given in <ref type="figure">Figure 9</ref>. Quantitative results are listed in <ref type="table" target="#tab_5">Table 5</ref>. <ref type="figure">Figure 9</ref>. Our 9D pose prediction and instance segmentation results on NOCS REAL275 test dataset. Notice that we do not leverage existing instance segmentation models, and the segmentation is generated by our voting model, which is trained on synthetic objects only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Quantitative Results on SUN RGB-D Dataset</head><p>We conduct more zero-shot pose estimation experiments on SUN RGB-D, with the instance masks annotated by SUN RGB-D. Notice that our model is trained solely on ShapeNet synthetic models, and then directly tested on SUN RGB-D frames. Results are listed in <ref type="table">Table 6</ref>. Qualitative results are given in <ref type="figure" target="#fig_0">Figure 10</ref>.  <ref type="table">Table 6</ref>. Zero-shot pose estimation results using instance masks provided by SUN RGB-D. Rotation error is evaluated along the gravity axis. <ref type="figure" target="#fig_0">Figure 10</ref>. Some successful pose estimations on SUN RGB-D. Notice that our model is trained on synthetic ShapeNet objects only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Ablation Studies</head><p>In our sim-to-real pipeline, point clouds are first voxelized and jittered in order to generate the same distribution for both synthetic and real objects. <ref type="table" target="#tab_6">Table 7</ref> gives the ablation studies on these two techniques, where we see that both voxelization and random jittering helps improve the final detection result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed results on each category</head><p>We plot detailed comparisons of each category on NOCS REAL275 test set. Rotation AP is given in <ref type="figure" target="#fig_0">Figure 11</ref> and Translation AP is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. Our model achieves the best result on most categories, and outperforms previous self-supervised methods by a large margin.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An overview of our proposed voting scheme. During training, for each sampled point pairs on synthetic models, we train a set of voting targets with a SE(3) invariant neural network. When testing, objects are first segmented out, and then for each object we randomly sample some point pairs to vote for the final 9D poses (translation, orientation and scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Offset prediction in center voting. Take a bowl as an example, our model predicts ? = ? ? ? ? p1c?2 and ? = ? ? ? co?2, where c is the perpendicular foot on ? ?? ? p1p2 opposite o. Once ? and ? are fixed, object center would possibly lie on the red dash circle. (b) Center voting scheme. For each point pair, candidate centers are generated on the dash circle for an interval of 2? K .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Handling symmetric objects. Here, we take the birdeye view of a bowl for better illustration. The three symmetric point pairs around the rim share exactly the same input PPF features and output offsets (? and ?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>: Initialize point pair pool P = {}. 11: for each sampled point pair p 1 and p 2 in point cloud do 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Our network architecture. For each input point pair, we first extract SE(3) invariant embeddings for each point. Then the two embeddings are concatenated with the original PPF feature (Equation 1), and fed into multi-layer perceptions to predict final outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Quantitative comparisons on NOCS REAL275 test dataset. Our method achieves state-of-the-art performance among pure sim-to-real methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparisons on NOCS REAL275 test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Our 9D pose prediction given only bounding box masks on NOCS REAL275 test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Rotation AP for each category on NOCS REAL275 test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Translation AP for each category on NOCS REAL275 test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Coarse-to-Fine Voting Algorithm. Input: object point cloud {p 1 , . . . , p N }.2: Output: translationt, orientation? 1 ,? 2 and scale?.3:  Initialize empty 3D grids G.4:  for each sampled point pair p 1 and p 2 in point cloud do</figDesc><table /><note>1:5:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of various methods. Syn.(O) means synthetic ShapeNet objects only; while Syn.(O+B) means ShapeNet models rendered with real backgrounds. Real means the real-world training data provided by NOCS. The best using realworld training data is marked blue, and the best using synthetic training data is marked red.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Training Data</cell><cell cols="2">3D 25 3D 50</cell><cell>5 ? 5 cm</cell><cell>10 ? 5 cm</cell><cell>15 ? 5 cm</cell></row><row><cell>NOCS [32]</cell><cell cols="2">Syn.(O+B) + Real 74.4</cell><cell>27.8</cell><cell>9.8</cell><cell cols="2">24.1 34.9</cell></row><row><cell>CASS [5]</cell><cell>Syn.(O+B) + Real</cell><cell>-</cell><cell>-</cell><cell cols="2">23.5 58.0</cell><cell>-</cell></row><row><cell>SPD [28]</cell><cell>Syn.(O+B) + Real</cell><cell>-</cell><cell>-</cell><cell cols="2">21.4 54.1</cell><cell>-</cell></row><row><cell>FS-Net [6]</cell><cell>Syn.(O+B) + Real</cell><cell>-</cell><cell>-</cell><cell cols="2">28.2 60.8</cell><cell>-</cell></row><row><cell>DualPoseNet [21]</cell><cell>Syn.(O) + Real</cell><cell>82.3</cell><cell cols="4">57.3 36.1 67.8 76.3</cell></row><row><cell>Chen et al. [7]</cell><cell>Syn.(O)</cell><cell>15.5</cell><cell>1.3</cell><cell>0.7</cell><cell>3.6</cell><cell>9.1</cell></row><row><cell>Gao et al. [10]</cell><cell>Syn.(O)</cell><cell>68.6</cell><cell>24.7</cell><cell>7.8</cell><cell cols="2">17.1 26.5</cell></row><row><cell>Ours w/ bbox mask</cell><cell>Syn.(O)</cell><cell>73.7</cell><cell cols="4">27.2 12.4 35.3 41.2</cell></row><row><cell>Ours w/ inst. mask</cell><cell>Syn.(O)</cell><cell>78.2</cell><cell cols="4">26.4 16.9 44.9 50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 2. mAP results on SUN RGB-D datasets in the wild. Our method achieves the best performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">3D 10 3D 25</cell><cell>20 ? 10 cm</cell><cell>40 ? 20 cm</cell><cell>60 ? 30 cm</cell></row><row><cell cols="2">Back-projection 20.2</cell><cell>4.4</cell><cell>0.0</cell><cell>0.6</cell><cell>5.8</cell></row><row><cell>Gao et al. [10]</cell><cell>22.2</cell><cell>6.0</cell><cell>0.0</cell><cell>1.0</cell><cell>7.0</cell></row><row><cell>Ours</cell><cell>24.7</cell><cell>8.3</cell><cell>0.8</cell><cell>10.7</cell><cell>17.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Effects of sample number and orientation bin size.</figDesc><table><row><cell>Number of</cell><cell>Orientation</cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell></cell></row><row><cell>Point Pairs</cell><cell>Bin Size ( ? )</cell><cell cols="2">3D 25 3D 50</cell><cell>5 ? 5 cm</cell><cell>10 ? 5 cm</cell><cell>15 ? 5 cm</cell></row><row><cell>100,000</cell><cell>1</cell><cell>78.3</cell><cell cols="4">24.3 13.2 38.2 45.7</cell></row><row><cell>100,000</cell><cell>2</cell><cell>78.5</cell><cell cols="4">25.9 13.4 46.6 52.2</cell></row><row><cell>100,000</cell><cell>4</cell><cell>78.4</cell><cell>26.2</cell><cell>7.0</cell><cell cols="2">42.7 53.5</cell></row><row><cell>10,000</cell><cell>1.5</cell><cell>68.5</cell><cell>21.7</cell><cell>8.6</cell><cell cols="2">27.2 34.1</cell></row><row><cell>60,000</cell><cell>1.5</cell><cell>78.1</cell><cell cols="4">25.8 15.6 42.8 49.1</cell></row><row><cell>200,000</cell><cell>1.5</cell><cell>78.4</cell><cell cols="4">26.6 17.3 45.5 51.3</cell></row><row><cell>100,000</cell><cell>1.5</cell><cell>78.2</cell><cell cols="4">26.4 16.9 44.9 50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation results on NOCS REAL275 test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell></cell></row><row><cell></cell><cell cols="2">3D 25 3D 50</cell><cell>5 ? 5 cm</cell><cell>10 ? 5 cm</cell><cell>15 ? 5 cm</cell></row><row><cell>No Aux. Classification</cell><cell>69.2</cell><cell cols="4">23.9 12.0 31.7 36.8</cell></row><row><cell cols="2">No Coarse-to-Fine Voting 75.6</cell><cell>22.3</cell><cell>9.7</cell><cell cols="2">27.4 32.6</cell></row><row><cell>Regression</cell><cell>73.3</cell><cell>14.7</cell><cell>1.8</cell><cell cols="2">12.7 24.2</cell></row><row><cell>Ours (full)</cell><cell>78.2</cell><cell cols="4">26.4 16.9 44.9 50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Zero-shot 9D pose estimation without detection priors. We report the mAP results for bowls in real-world scenarios with only synthetic training data.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">mAP (%)</cell><cell></cell><cell></cell></row><row><cell cols="3">3D25 3D50</cell><cell>5 ? 5 cm</cell><cell>10 ? 5 cm</cell><cell>15 ? 5 cm</cell></row><row><cell>Bowl</cell><cell>43.0</cell><cell>6.9</cell><cell>0.8</cell><cell>10.1</cell><cell>22.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mAP (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>20 ?</cell><cell>40 ?</cell><cell>60 ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10 cm</cell><cell>20 cm</cell><cell cols="2">30 cm</cell></row><row><cell cols="2">Bathtub</cell><cell>10.9</cell><cell>38.8</cell><cell>49.8</cell><cell></cell></row><row><cell cols="2">Bookshelf</cell><cell>0.0</cell><cell>1.0</cell><cell>6.4</cell><cell></cell></row><row><cell>Bed</cell><cell></cell><cell>0.0</cell><cell>0.8</cell><cell>3.4</cell><cell></cell></row><row><cell>Sofa</cell><cell></cell><cell>0.0</cell><cell>1.7</cell><cell>10.5</cell><cell></cell></row><row><cell cols="2">Table</cell><cell>0.5</cell><cell>6.9</cell><cell>17.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Ablation results on NOCS REAL275 test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">3D25 3D50</cell><cell>5 ? 5 cm</cell><cell>10 ? 5 cm</cell><cell>15 ? 5 cm</cell></row><row><cell>No Voxelization</cell><cell>76.7</cell><cell>21.6</cell><cell>11.9</cell><cell>37.2</cell><cell>45.6</cell></row><row><cell>No Jittering</cell><cell>77.1</cell><cell>24.6</cell><cell>16.2</cell><cell>43.1</cell><cell>49.7</cell></row><row><cell>Ours (full)</cell><cell>78.2</cell><cell>26.4</cell><cell>16.9</cell><cell>44.9</cell><cell>50.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Objectron: A large scale dataset of object-centric videos in the wild with pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Ahmadyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4243" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Linlin Shen, and Ales Leonardis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Category level object pose estimation via neural analysis-by-synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender Online Community</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
			<publisher>Ieee</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">6d object pose regression via supervised learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Lauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3643" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d pose estimation and 3d model retrieval for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3022" to="3031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going further with point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bop challenge 2020 on 6d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="577" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sim-to-real via simto-sim: Data-efficient robotic grasping via randomizedto-canonical adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12627" to="12637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robotic picking in dense clutter via domain invariant learning from synthetic dense cluttered rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">103901</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OpenGL programming guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Woo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Addison-Wesley</publisher>
			<biblScope unit="volume">478</biblScope>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminatively trained templates for 3d object detection: A real time scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyes</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2048" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stablepose: Learning 6d object poses from geometrically stable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15222" to="15231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shape prior deformation for categorical 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marcelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A method for 6d pose estimation of free-form rigid objects using point pair features on range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chyi-Yeu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Llad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mart?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2678</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Marrnet: 3d shape reconstruction via 2.5 d sketches. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning shape priors for single-view 3d completion and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sim-to-real transfer of accurate grasping with eyein-hand observations and continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03303</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prin/sprin: On extracting point-wise rotation invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Canonical voting: Towards robust oriented bounding box detection in 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
