<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WEIGHTED SPEECH DISTORTION LOSSES FOR NEURAL-NETWORK-BASED REAL-TIME SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Braun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandan</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harishchandra</forename><surname>Dubey</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WEIGHTED SPEECH DISTORTION LOSSES FOR NEURAL-NETWORK-BASED REAL-TIME SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Real-time speech enhancement</term>
					<term>recurrent neu- ral networks</term>
					<term>loss function</term>
					<term>speech distortion</term>
					<term>mean opinion score</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates several aspects of training a RNN (recurrent neural network) that impact the objective and subjective quality of enhanced speech for real-time single-channel speech enhancement. Specifically, we focus on a RNN that enhances short-time speech spectra on a single-frame-in, single-frame-out basis, a framework adopted by most classical signal processing methods. We propose two novel mean-squared-error-based learning objectives that enable separate control over the importance of speech distortion versus noise reduction. The proposed loss functions are evaluated by widely accepted objective quality and intelligibility measures and compared to other competitive online methods. In addition, we study the impact of feature normalization and varying batch sequence lengths on the objective quality of enhanced speech. Finally, we show subjective ratings for the proposed approach and a state-of-the-art real-time RNN-based method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech enhancement (SE) algorithms aim at improving speech quality and intelligibility of speech signals degraded by additive noise <ref type="bibr" target="#b0">[1]</ref>, in order to improve human or machine interpretation of speech. Typical SE applications are hearing aids, automatic speech recognition, and audio/video communications in noisy environments. Most SE methods apply a spectral suppression gain or filter to the noisy speech signal in a time-frequency domain <ref type="bibr" target="#b1">[2]</ref>. In recent supervised learning methods using deep neural networks (DNNs), a DNN is typically set up to estimate this time-varying gain function <ref type="bibr" target="#b2">[3]</ref> from one or more sets of features derived from noisy speech.</p><p>Online processing capability is an attractive feature of a SE algorithm and required for real-time communication applications. Although most classical SE methods have to accommodate their approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> for fulfilling causality, many DNN-based methods in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> do not enforce this constraint. Several DNN-based approaches report high-quality enhancement using generous look-ahead <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, but their performance for decreasing lookahead is not well investigated. Nevertheless, DNN-based systems are preferred over classical methods for their ability to accurately suppress transient noise. In this work, we study real-time speech enhancement with recurrent neural network (RNN). Recent works involving RNNs demonstrated promising results <ref type="bibr" target="#b9">[10]</ref>, even at very low signal-to-noise ratio (SNR) scenarios <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Yangyang performed this work as an intern while at Microsoft Research.</p><p>A key challenge in designing a SE algorithm for audio/video communication is to preserve perceived (subjective) speech quality to the best extent possible while suppressing the noise. In classical literature, optimizing such a compound global objective can be done by solving a constrained objective function <ref type="bibr" target="#b12">[13]</ref>. Alternatively, one can optimize a simpler objective such as the (log) mean-squared error (MSE) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> and employ post-processing modules such as residual noise removal <ref type="bibr" target="#b6">[7]</ref> and gain limiting <ref type="bibr" target="#b14">[15]</ref>. By contrast, one major benefit of the deep learning framework is the relative ease to incorporate complex learning objectives one believes would drive the enhanced speech towards better quality and intelligibility. Methods along this line of thought include learning multiple objectives from heterogeneous features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, jointly optimizing the final goal and its sub-targets (e.g. speech-presence probability) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, and directly optimizing towards an objective measure of speech quality or intelligibility <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The latter seems a promising way to improve objective quality, although both models have to incorporate the standard MSE due to the band limitation of each objective measure. <ref type="bibr" target="#b20">[21]</ref> reported that a simple perceptually weighted wide-band MSE alone does not improve objective speech quality or intelligibility, suggesting that the MSE is still a reliable learning objective for wide-band speech enhancement.</p><p>In this paper, we propose a DNN-based online speech enhancement system for real-time applications. First, we will discuss features and normalization techniques that would facilitate pattern learning with a RNN. We then describe a compact RNN that produces a gain function from a single noisy frame. Next, we introduce two simple MSE-based loss functions with separate control of speech distortion and noise reduction. During the evaluation, we thoroughly examine the effect of error weighting on the subjective and objective speech quality and intelligibility measures. Furthermore, we discuss how the objective metrics are affected by different feature normalization techniques and training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM FORMULATION</head><p>We assume the microphone signals to be described in the short-time Fourier transform (STFT) domain by In real-time processing, G[t, k] shall depend only on the past and present information of input, and is given by</p><formula xml:id="formula_0">X[t, k] = S[t, k] + N [t, k],<label>(1)</label></formula><formula xml:id="formula_1">G[t, k] = n(g(f (|X[l, k]|)); ?), l ? t,<label>(3)</label></formula><p>where f is a transform function applied on the STFTM of the noisy signal, g is a normalization function, and n is a DNN whose adaptable parameters are together denoted by ?. Finally, noisy phase of X[t, k] is applied to |?[t, k]| to obtain the enhanced signal. In the following sections, we will review the state-of-the-art methods, before discussing our choices for f and g, the architecture of n, two learning objectives for ?, and further considerations in training that we believe will impact the quality of enhanced speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STATE-OF-THE-ART ONLINE NOISE REDUCTION</head><p>Classical online SE methods typically seek for the optimal gain function by optimizing some objective functions in a statistical sense. One of the most effective methods in this category assumes the clean and noise STFT are uncorrelated, complex Gaussian distributions, and solves for G[t, k] by minimizing the MSE between clean and enhanced STFTM <ref type="bibr" target="#b5">[6]</ref> or log-STFTM <ref type="bibr" target="#b13">[14]</ref>. Although more advanced noise and speech-presence probability models could be incorporated to improve speech quality and prevent musical noise <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, retaining speech quality while removing highly non-stationary noise is still a challenging task.</p><p>In recent DNN-based methods, statistical assumptions about the distribution of noisy and clean STFTM are typically dropped, while the minimum MSE (MMSE) objective becomes a loss function for which a DNN optimizes by stochastic gradient descent. One of the most popular loss functions has been the MSE between clean and enhanced STFTM</p><formula xml:id="formula_2">L( G; S, X) = mean(|| S ? G X|| 2 2 ),<label>(4)</label></formula><p>where A denotes |A[t, k]| in vector form, and is the element-wise product. A competitive method proposed recently <ref type="bibr" target="#b9">[10]</ref> estimates the optimal gain function of a smoothed energy contour using a RNN and interpolates the spectral details by pitch filtering. Experiments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref> report strong objective and subjective speech quality from enhanced speech produced by this RNNoise system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature representation</head><p>Selecting appropriate features and normalization is important for successfully training a DNN. We consider two basic features in STFTM and log-power spectra (LPS), and apply global, frequencydependent (FD), and frequency-independent (FI) normalization, respectively, to train our network. The STFTM used in all our systems is computed based on a 32 ms Hamming window with 75% overlap between frames and a 512-point discrete Fourier transform. The LPS is taken with the natural logarithm and floored at -120 dB, i. e.,</p><formula xml:id="formula_3">fLP S (|X[t, k]|) = log(max(|X[t, k]| 2 , 10 ?12 ))<label>(5)</label></formula><p>We explore three types of normalization to be each individually combined with either STFTM or LPS mentioned above. First, we consider global normalization, in which case each frequency bin is standardized by its mean and standard deviation accumulated from a training set:</p><formula xml:id="formula_4">gG(f (|X[t, k]|)) = f (|X[t, k]|) ? ? f (x) [k] /? f (x) [k]<label>(6)</label></formula><p>Second, we consider online FD mean and variance normalization, in which case the running mean and variance are smoothed by a decaying exponential:</p><formula xml:id="formula_5">? f (x) [t, k] = c ? f (x) [t ? 1, k] + (1 ? c) f (|X[t, k]|) (7) ? 2 f (x) [t, k] = c ? 2 f (x) [t ? 1, k] + (1 ? c) f (|X[t, k]|) 2 (8) gF D (f (|X[t, k]|)) = f (|X[t, k]|) ? ? f (x) [t, k] ? 2 f (x) [t, k] ? ? 2 f (x) [t, k]<label>(9)</label></formula><p>where c = exp(??t/? ), ?t is the frame shift in seconds (8 milliseconds in our setting), and ? is a time constant that controls the adaptation speed. The idea is that the normalized spectra will facilitate the recurrent neural network learning long-term patterns. Finally, we also have the FI online normalization, in which case the mean and variance from each frequency are averaged and applied to all frequencies. This method retains the relative dynamics across frequency bins, but might pose a more challenging learning task to the learning machine. In all our experiments apart from the feature experiments, we use FD online normalization with ? = 3s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning machine</head><p>Our learning machine that takes in one frame of noisy speech spectra and outputs one frame of magnitude gain function is based on the gated recurrent unit (GRU) <ref type="bibr" target="#b22">[23]</ref>. GRUs are preferred over long shortterm memory (LSTM) <ref type="bibr" target="#b23">[24]</ref> given their computational efficiency and superior performance in real-time SE tasks <ref type="bibr" target="#b21">[22]</ref>. We stack three GRU layers followed by a fully-connected (FC) output layer with sigmoid activation to predict the gain function G[t, k].</p><p>It is worth mentioning that we do not apply convolution layers as often done in other related work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> because of the relatively arbitrary process involved in choosing the amount of frequency span and filter taps. Previous studies <ref type="bibr" target="#b25">[26]</ref> have shown that a na?ve convolution layer applied on past and present input noisy frames did not improve objective quality of enhanced speech. Instead, we explore the temporal modeling capability of the network by training with sequences of different lengths, features, and loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss functions</head><p>We use three loss functions to train our system. First, we use the regular MSE between clean and enhanced STFTM in <ref type="bibr" target="#b3">(4)</ref>. To obtain a better control of the loss, we propose to separate the error into speech distortion and noise reduction terms</p><formula xml:id="formula_6">Lspeech = mean(|| SSA ? ( G S)SA|| 2 2 )<label>(10)</label></formula><formula xml:id="formula_7">Lnoise = mean(|| G N || 2 2 )<label>(11)</label></formula><p>where subscript SA denotes a subset of frames where speech is active. In our experiments, we adopted a simple energy-based framelevel voice activity detector operating on the power spectra of clean utterances. The short-time speech energy is accumulated between 300 Hz and 5000 Hz and smoothed over 3 frames by a movingaverage filter. Finally, a frame is decided to be voiced above a threshold of 30 dB below the peak energy of the whole utterance.</p><p>As the estimated gain approaches all-pass, the speech distortion error is minimized and the noise error is maximized, and vice versa. Therefore, we can control the relative importance of speech distortion to noise reduction with a fixed-weighted loss,  where ? is a constant in range [0, 1]. We notice a similar loss has been developed independently and termed two-component loss (2CL) <ref type="bibr" target="#b26">[27]</ref>. Next, we discuss an extension to this fixed weighting. In classical speech enhancement literature, the suppression rule is often adapted based on the SNR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>. Specifically, suppression should be limited at high SNR to avoid artifacts, and be aggressive at low SNR. Motivated by this principle, our second SNR-weighted loss adjusts ? in (12) using the global SNR of each utterance</p><formula xml:id="formula_8">L( G; SSA, N ) = ?Lspeech + (1 ? ?)Lnoise,<label>(12)</label></formula><formula xml:id="formula_9">? = SNR SNR + ? ,<label>(13)</label></formula><p>where SNR = || S|| 2 2 /|| N || 2 2 and ? is a constant. Note that d?/d[10log 10 (SNR)] is maximized when SNR = ?. In this way, ? controls the global SNR at which a fixed amount of deviation would cause the maximum drift in speech distortion weighting. Furthermore, ? also indicates the global SNR, where the two loss terms are equally weighted. We illustrate this in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The proposed method is depicted in a flow diagram shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. During training, both clean speech and noise are required for computing the weighted loss. The trained model enhances the noisy STFTM one frame at a time and utilizes the noisy phase for reconstructing the enhanced speech waveform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS AND DISCUSSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Corpora &amp; Experimental setup</head><p>We train and evaluate all DNN-based systems using a dataset synthesized from publicly available speech and noise corpus using the MS-SNSD dataset <ref type="bibr" target="#b21">[22]</ref> and toolkit 1 . 14 diverse noise types are se-1 https://github.com/microsoft/MS-SNSD We performed a comparative study of proposed methods with three baselines based on several objective speech quality and intelligibility measures and subjective tests. Specifically, we include perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b27">[28]</ref>, short-time objective intelligibility (STOI) <ref type="bibr" target="#b28">[29]</ref>, cepstral distance (CD), and scaleinvariant signal-to-distortion ratio (SI-SDR) <ref type="bibr" target="#b29">[30]</ref> for objective evaluation of enhanced speech in time, spectral, and cepstral domains. We conducted a subjective listening test using a web-based subjective framework presented in <ref type="bibr" target="#b21">[22]</ref>. Each clip is rated with a discrete rating between 1 (very poor speech quality) and 5 (excellent speech quality) by 20 crowd-sourced listeners. Training and qualification are ensured before presenting test clips to these listeners. The mean of all 20 ratings is the mean opinion score (MOS) for that clip. We also removed obvious spammers who consistently selected the same rating throughout the MOS test. Our subjective test complements the other objective assessments, thus providing a balanced benchmark for evaluation of studied noise reduction algorithms.</p><p>We compare our proposed methods with three baseline methods. We used a classical enhancer, which is a slightly optimized implementation of the MMSE log-spectral amplitude (LSA) estimator <ref type="bibr" target="#b13">[14]</ref> described in <ref type="bibr" target="#b30">[31]</ref>. DNN-based baselines include the improved RNNoise (RNNoiseI) <ref type="bibr" target="#b21">[22]</ref> and a RNN (RNNoise257) that replicates the network architecture of RNNoise <ref type="bibr" target="#b9">[10]</ref> but operates on 257-point spectra, is trained on (4), and does not have the originally proposed post-processing component. RNNoise257 realizes a system with a comparable number of parameters as the proposed methods.</p><p>In the next section, we discuss the impact of feature normalization and training on various sequence lengths on the objective quality of enhanced speech. Then, we explore the optimal weighting for the proposed fixed-weighted and SNR-weighted loss functions. Finally, we compare the subjective and objective quality of enhanced speech produced by our systems to several competitive online methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results &amp; Discussions</head><p>We want to evaluate how training with long or short sequences affects temporal modeling in the RNN. Although long sequences are expected to help deal with long-term noise patterns, it might also potentially degrade speech that is only short-term stationary. <ref type="table" target="#tab_1">Table 1</ref> summarizes this impact of sequence lengths on objective speech quality. For each setting, we adjust the number of sequences in a minibatch so that one batch always contains one minute of noisy  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MOS (mean ? std.) Noisy 2.63 ? 0.03 RNNoiseI <ref type="bibr" target="#b21">[22]</ref> 3.26 ? 0.03 Proposed (? = 0.05) 3.93 ? 0.03 Proposed (? = 0.1)</p><p>3.92 ? 0.03 Proposed (? = 0.2)</p><p>3.74 ? 0.03 Proposed (? = 0.3)</p><p>3.65 ? 0.03 speech. We observe a noticeable improvement in performance as each segment increases to 5 seconds, beyond which the improvement starts to diminish. We do not show the result for the feature test due to space limitation, but overall there is little difference between all normalized variants of STFTM and LPS features, while no normalization results in degradation. In general, we recommend FD online normalization due to its invariance to varying signal levels. We also suggest using segments that are no less than 5-second long each during training. The effect of speech distortion weighting is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, where ? or ? are changed to search the optimal points for each objective measure. Curiously, only STOI and CD agree on the same coefficient in both cases, while both PESQ and SI-SDR suggest smaller weight on speech distortion. The optimal SNR weights for all metrics are concentrated around 20 dB, meaning that the speech distortion weight should only rapidly increase when the noisy signal is relatively clean. Overall, a fixed weighting is slightly better than SNR weighting in all metrics.</p><p>During experiments, we notice that even though our systems trained on MSE (e.g. row 4 in <ref type="table" target="#tab_1">Table 1</ref>) could achieve similar objective measures compared to those trained on the proposed weighted losses <ref type="bibr" target="#b11">(12)</ref>, the corresponding subjective quality of systems trained on the weighted loss is a lot better. The most noticeable improvement of systems trained on our loss functions, especially with small ?, is that the estimated gain function is much more frequency-selective than systems trained on regular MSE, resulting in higher noise suppression, especially at high SNRs. To testify this, we present the result of the online subjective listening test in <ref type="table" target="#tab_2">Table 2</ref>. Not only did all our selected systems significantly outperform the improved RN-Noise (RNNoiseI) trained on MSE presented in <ref type="bibr" target="#b21">[22]</ref>, we were surprised that the listening test subjects preferred a rather low setting for the speech distortion weight ?. This trend is mispredicted by all objective measures as well as the authors' subjective preference of about ? = 0.35. We observed noticeable speech distortion as ? goes below 0.35, while noise became more suppressed. It is evident that more detailed investigations are required in future work to shed more light on speech distortion and noise reduction preferences for different groups of listeners. Finally, we report the objective evaluation from each baseline method, the noisy reference, and an oracle Wiener filter as upper bound in <ref type="table" target="#tab_3">Table 3</ref>. The selected system from our method is trained using fixed speech distortion weighting with ? = 0.35, which we believe strikes a good balance between speech distortion and noise reduction. Although this setup might not be the most preferred for human listeners, it can be easily tuned to different applications. It is nevertheless important to show that it outperforms all tested classical or DNN-based methods in all objective metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we proposed and evaluated a real-time speech enhancement approach based on a compact recurrent neural network trained with a simple MSE-based speech distortion weighted loss function. We show the impact of various feature normalization techniques and sequence lengths on the objective quality of enhanced speech. We also demonstrate how to control the amount of speech distortion with fixed-weighted and SNR-weighted coefficients in the loss function. Both objective and subjective tests show that our method outperforms other competitive online methods. In the future, we will explore time-varying speech distortion weighting and its influence on subjective and objective speech quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Selected SNR-weighted speech distortion weighting. Horizontal line marks equal weighting of Lspeech and Lnoise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Flow diagram of the proposed system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Effect of fixed weighting and SNR weighting on objective speech quality and intelligibility measures. Black dashed vertical lines indicate the optimal coefficient for each metric. Note that the optimal points coincide for STOI and CD at ? = 0.65 and ? = 18.2 dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Effect of sequence lengths in a one-minute minibatch.</figDesc><table><row><cell cols="2">Length (s) SI-SDR (dB)</cell><cell>CD</cell><cell cols="2">STOI (%) PESQ (MOS)</cell></row><row><cell>1</cell><cell>13.7</cell><cell>3.78</cell><cell>90.1</cell><cell>2.58</cell></row><row><cell>2</cell><cell>13.7</cell><cell>3.80</cell><cell>90.3</cell><cell>2.57</cell></row><row><cell>5</cell><cell>14.1</cell><cell>3.72</cell><cell>90.5</cell><cell>2.59</cell></row><row><cell>10</cell><cell>14.1</cell><cell>3.73</cell><cell>90.7</cell><cell>2.64</cell></row><row><cell>20</cell><cell>14.0</cell><cell>3.73</cell><cell>90.6</cell><cell>2.64</cell></row><row><cell cols="5">lected for training, while samples from 9 noise types not included</cell></row><row><cell cols="5">in the training set are used for evaluation. Our test set includes</cell></row><row><cell cols="5">challenging and highly non-stationary noise types such as munch-</cell></row><row><cell cols="5">ing, multi-talker babble, keyboard typing, etc. All audio clips are</cell></row><row><cell cols="5">resampled to 16 kHz. The training set consists of 84 hours each of</cell></row><row><cell cols="5">clean speech and noise while 18 hours (5500 clips) of noisy speech</cell></row><row><cell cols="5">constitute the evaluation set. All speech clips are level-normalized</cell></row><row><cell cols="5">on a per-utterance basis, while each noise clip is scaled to have one</cell></row><row><cell cols="5">of the five global SNRs from {40,30,20,10,0} dB. During the train-</cell></row><row><cell cols="5">ing of all DNN-based systems described below, we randomly select</cell></row><row><cell cols="5">an excerpt of clean speech and noise, respectively, before mixing</cell></row><row><cell cols="3">them to create the noisy utterance.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Subjective MOS from 5500 clips and 20 ratings per clip.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of objective metrics with baseline online SE systems. Refer to text for details about each setup.</figDesc><table><row><cell>Method</cell><cell cols="2"># Param. SI-SDR</cell><cell>CD</cell><cell>STOI</cell><cell>PESQ</cell></row><row><cell></cell><cell></cell><cell>(dB)</cell><cell></cell><cell>(%)</cell><cell>(MOS)</cell></row><row><cell>Noisy</cell><cell>-</cell><cell>9.81</cell><cell>4.56</cell><cell>88.0</cell><cell>2.22</cell></row><row><cell>LSA [14, 31]</cell><cell>-</cell><cell>6.10</cell><cell>4.64</cell><cell>84.7</cell><cell>2.33</cell></row><row><cell>RNNoiseI [22]</cell><cell>61.2 K</cell><cell>10.4</cell><cell>3.83</cell><cell>88.0</cell><cell>2.55</cell></row><row><cell>RNNoise257</cell><cell>2.64 M</cell><cell>13.0</cell><cell>3.88</cell><cell>89.3</cell><cell>2.56</cell></row><row><cell>Proposed 0.35</cell><cell>1.26 M</cell><cell>14.3</cell><cell>3.83</cell><cell>90.7</cell><cell>2.65</cell></row><row><cell>Wiener</cell><cell>Oracle</cell><cell>20.5</cell><cell>2.13</cell><cell>98.1</cell><cell>3.82</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: theory and practice</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Speech Enhancement</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Noise estimation by minima controlled recursive averaging for robust speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berdugo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="15" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech enhancement for nonstationary noise environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berdugo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2403" to="2418" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hybrid DSP/deep learning approach to realtime full-band speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 20th International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional recurrent neural network for real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3229" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A priori SNR estimation based on a recurrent neural network for robust speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3274" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Residual noise control using a parametric multichannel wiener filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Habets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum mean-square error log-spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="445" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient musical noise suppression for speech enhancement system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Esch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple-target deep learning for LSTM-RNN based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hands-free Speech Communications and Microphone Arrays (HSCMA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiobjective learning and mask-based post-processing for deep neural network based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTER-SPEECH 2015</title>
		<imprint>
			<biblScope unit="page" from="1508" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech Denoising with Deep Feature Losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2723" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mart?n-Do?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptually guided speech enhancement using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5074" to="5078" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech enhancement in multiplenoise conditions using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Florencio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTER-SPEECH 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3738" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Scalable Noisy Speech Dataset and Online Subjective Test Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTERSPEECH 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1816" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutionalrecurrent neural networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2401" to="2405" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Experiments on deep learning for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Components loss for neural networks in mask-based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elshamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05087</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SDRhalf-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sound capture and processing: practical approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Tashev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
