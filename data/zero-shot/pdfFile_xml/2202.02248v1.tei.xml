<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Backpropagation Neural Tree</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ojha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Reading</orgName>
								<address>
									<settlement>Reading</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Nicosia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center of System Biology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical &amp; Biotechnological Sciences</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Backpropagation Neural Tree</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stochastic gradient descent</term>
					<term>RMSprop</term>
					<term>Backpropagation</term>
					<term>Minimal Architec- ture</term>
					<term>Neural networks</term>
					<term>Neural trees</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel algorithm called Backpropagation Neural Tree (BNeuralT), which is a stochastic computational dendritic tree. BNeuralT takes random repeated inputs through its leaves and imposes dendritic nonlinearities through its internal connections like a biological dendritic tree would do. Considering the dendritic-tree like plausible biological properties, BNeuralT is a single neuron neural tree model with its internal sub-trees resembling dendritic nonlinearities. BNeuralT algorithm produces an ad hoc neural tree which is trained using a stochastic gradient descent optimizer like gradient descent (GD), momentum GD, Nesterov accelerated GD, Adagrad, RMSprop, or Adam. BNeuralT training has two phases, each computed in a depth-first search manner: the forward pass computes neural tree's output in a post-order traversal, while the error backpropagation during the backward pass is performed recursively in a pre-order traversal. A BNeuralT model can be considered a minimal subset of a neural network (NN), meaning it is a "thinned" NN whose complexity is lower than an ordinary NN. Our algorithm produces high-performing and parsimonious models balancing the complexity with descriptive ability on a wide variety of machine learning problems: classification, regression, and pattern recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-driven learning is a hypothesis (trained model) search from a hypothesis-space that fits input data to its target output as good as possible (a low error on test data). A learning algorithm like neural networks (NNs) parameter optimization via backpropagation is the effort to find such a hypothesis <ref type="bibr" target="#b39">(Rumelhart et al., 1986)</ref>. We propose a new study of ad hoc neural trees generation and their optimization via our recursive backpropagation algorithm to find such a hypothesis.</p><p>Hence, we propose a new algorithm called Backpropagation Neural Tree (BNeuralT).</p><p>A tree of BNeuralT is like a biological dendritic tree <ref type="bibr" target="#b47">(Travis et al., 2005;</ref><ref type="bibr" target="#b26">Mel, 2016</ref>) that processes repeated inputs connected to a single neuron <ref type="bibr">(Beniaguev et al., 2020;</ref><ref type="bibr" target="#b14">Jones and Kording, 2021)</ref> through dendritic nonlinearities <ref type="bibr" target="#b24">(London and H?usser, 2005)</ref>. Structurally, BNeuralT model is a stochastic computational dendritic tree that takes random repeated inputs through its leaves and imposes dendritic nonlinearities through its internal nodes like a biological dendritic tree would do <ref type="bibr" target="#b47">(Travis et al., 2005;</ref><ref type="bibr" target="#b14">Jones and Kording, 2021)</ref>. Hence, considering the plausible dendritictree-like biological properties, BNeuralT is a single neuron neural tree model with its internal nodes resembling dendritic nonlinearities.</p><p>Structurally, BNeuralT, being a tree, is a minimal subset of a (highly sparse) NN whose complexity is comparatively low <ref type="bibr" target="#b34">(Poirazi et al., 2003b)</ref>. This means that a NN with a very high dropout [a network regularization technique <ref type="bibr" target="#b44">(Srivastava et al., 2014)</ref>] prior to its training can be similar to BNeuralT, except BNeuralT has dedicated paths from input to output as opposed to sparse NN that has shared connections between nodes. Hence, we aim to gauge the performance of ad hoc neural trees trained using stochastic gradient descent (SGD) optimizers like gradient descent (GD), momentum gradient descent (MGD) <ref type="bibr" target="#b35">(Qian, 1999)</ref>, Nesterov accelerated gradient descent (NAG) <ref type="bibr">(Bengio et al., 2013)</ref>, adaptive gradient (Adagrad) <ref type="bibr" target="#b5">(Dean et al., 2012)</ref>, root-mean-square gradient propagation (RMSprop) <ref type="bibr" target="#b46">(Tieleman and Hinton, 2012)</ref>, and adaptive moment estimation (Adam) <ref type="bibr" target="#b19">(Kingma and Ba, 2015)</ref>.</p><p>Operationally, an expression-tree with its operator (node) being neural nodes (i.e., an operator is an activation function), edges being neural weights, and leaves being inputs make a neural tree architecture, where the tree's architecture itself can be optimized <ref type="bibr" target="#b3">(Chen et al., 2005;</ref><ref type="bibr" target="#b42">Schmidt and Lipson, 2009</ref>). The tree's edges (parameters) optimization is straightforward using a gradientfree method <ref type="bibr" target="#b38">(Rios and Sahinidis, 2013;</ref><ref type="bibr" target="#b17">Kennedy and Eberhart, 1995)</ref> where the tree is assumed a target function <ref type="bibr" target="#b29">(Ojha et al., 2017)</ref>. However, its gradient-based optimization is non-trivial, especially because the error-backpropagation through the tree data structure is recursive to traverse. Our proposed BNeuralT algorithm does a two-phase computation of a neural tree in a depth-first search manner: the forward pass computes neural tree's outputs in a post-order traversal, while the error backpropagation during the backward pass is performed recursively in a pre-order traversal.</p><p>We trained ad hoc neural trees in an online (example-by-example) and a mini-batch mode on a variety of learning problems: classification, regression, and pattern recognition. For classification and pattern recognition problems, BNeuralT has its root node's children (nodes at tree depth one) strictly dedicated to each target class, and the root node decides the winner class on receiving input data. BNeuralT dedicates its root as the output node for a regression problem.</p><p>We evaluated BNeuralT's convergence process on six SGD optimizers and analyzed BNeuralT's complexity against its convergence accuracy. Each training version was compared with a similar training version of a multi-layer perceptron (MLP) algorithm (i.e., an input-hidden-output NN architecture) and classification and regression algorithms such as decision tree (DT) <ref type="bibr">(Breiman et al., 1984)</ref>, random forest (RF) <ref type="bibr">(Breiman, 2001)</ref>, single and multi-objective versions of a heterogeneous flexible neural tree (HFNT S and HFNT M ) <ref type="bibr" target="#b29">(Ojha et al., 2017)</ref>, multi-output neural tree (MONT) <ref type="bibr" target="#b30">(Ojha and Nicosia, 2020)</ref>, Gaussian process (GP) <ref type="bibr" target="#b37">(Rasmussen and Williams, 2006)</ref>, na?ve Bayes classifier (NBC) <ref type="bibr" target="#b27">(Mitchell, 1997)</ref>, and support vector machine (SVM) <ref type="bibr" target="#b4">(Cortes and Vapnik, 1995;</ref><ref type="bibr" target="#b1">Chang and Lin, 2011;</ref><ref type="bibr" target="#b7">Fan et al., 2008)</ref>. The results on all problems indicate the success of our BNeuralT algorithm that produces high-performing and parsimonious models balancing the complexity and descriptive ability with a minimal training hyperparameters setup.</p><p>Our contribution is an innovative Recursive Backpropagation Neural Tree algorithm that ? takes inspiration from biological dendritic trees to solve a wide class of machine learning problems through a single neuron tree-like model performing dendritic nonlinearities through its internal nodes and resembling a highly sparse neural network.</p><p>? generates low complexity and high accuracy models. Therefore, we have designed a learning system capable of producing minimal and sustainable neural trees that have fewer parameters to produce more compact and, therefore, sustainable neural models able to reduce CPU time and, consequently, CO 2 emissions for machine learning applications.</p><p>? shows that the sigmoidal dendritic nonlinearity of any stochastic ad hoc neural tree structure can solve machine learning problems with high accuracy, and any such structure excels to genetically optimized neural tree structures, NNs, and other learning algorithms. This paper presents relevant related work in Sec. 2. BNeuralT model's architecture and properties are described in Sec. 3. Secs. 4.1 and 4.2 outline the hyperparameter settings and experiment versions. The performance of BNeuralT on machine learning problems is summarized in Sec. 5 and discussed in Sec. 6, followed by conclusions in Sec. 7. Source code of BNeuralT algorithm and pre-trained models are available at https://github.com/vojha-code/BNeuralT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>We review the works defining neural tree architectures and training processes. The early definition of neural trees appeared in <ref type="bibr" target="#b41">(Sakar and Mammone, 1993;</ref><ref type="bibr" target="#b43">Sirat and Nadal, 1990)</ref>, where the tree's "root-to-leaf " path is represented as a neural network (NN). Such a tree makes its decision through leaf nodes, and its internal nodes are NNs (or neural nodes). <ref type="bibr" target="#b15">Jordan and Jacobs (1994)</ref> proposed a hierarchical mixture expert model that performs construction of a binary tree structure where the model hierarchically combines the outputs of expert networks (feed-forward NNs at the terminal) though getting networks (feed-forward NNs at non-terminal) and propagates computation from "leaf-to-root" and where each NN uses the whole input features set.</p><p>In contrast, our model is purely a single network (tree) structure representation, whereas a hierarchical mixture expert model is a hierarchical combination of several (preferably small) networks. Therefore, unlike hierarchical mixture expert model, our model is a subset of a NN where "leaf-to-root" has a specific information processing path. In fact, considering plausible inspiration from biological computational dendritic tree <ref type="bibr" target="#b47">(Travis et al., 2005;</ref><ref type="bibr" target="#b26">Mel, 2016;</ref><ref type="bibr" target="#b34">Poirazi et al., 2003b)</ref>, our model behaves as a single neuron model <ref type="bibr" target="#b14">(Jones and Kording, 2021)</ref>.</p><p>Our proposed BNeuralT algorithm generates an m-ary tree structure stochastically and assigns edge weights randomly. BNeuralT's each leaf node (terminal node) takes a single input variable from a set of all available variables (data features). Therefore, in a generated tree, some features could remain unused by the model leading to only select features responsible for the prediction.</p><p>Moreover, tree's each neural node (non-terminal node) takes a weighted summation of its child's output. Hence, a BNeuralT model potentially performs an input dimension reduction and propagates the computation from leaf to root.</p><p>A recent work of <ref type="bibr" target="#b45">Tanno et al. (2019)</ref> demonstrates neural tree as an arrangement of convolution layers and linear classifier as a learning model resembling a decision tree-like classifier where the incoming inputs at the nodes are inferred through the so-called router, processed through tree edges (transformers), and classified through leaf (solver) nodes. In contrast, our model takes image pixels as its inputs. A leaf-to-root as a neural tree definition appeared in <ref type="bibr" target="#b49">(Zhang et al., 1997;</ref><ref type="bibr" target="#b3">Chen et al., 2005)</ref>, where the tree's leaf nodes are designated inputs, internal nodes are neural nodes, and edges are weights. Such types of neural trees have been subjected to structure optimization <ref type="bibr" target="#b3">(Chen et al., 2005;</ref><ref type="bibr" target="#b29">Ojha et al., 2017)</ref> and parameter optimization via gradient-free optimization techniques like particle swarm optimization <ref type="bibr" target="#b2">(Chen et al., 2007)</ref> and differential evolution <ref type="bibr" target="#b29">(Ojha et al., 2017)</ref>. <ref type="bibr" target="#b49">Zhang et al. (1997)</ref> demonstrated that a neural tree could be evolved as a subset of an MLP.</p><p>Their effort was to evolve a neural tree using genetic programming and optimize parameters using a genetic algorithm. <ref type="bibr" target="#b21">Lee et al. (2016)</ref> focused on implementing pooling layers within a convolutional NN as a tree structure. However, our approach is to generate and train ad hoc neural trees using our proposed recursive backpropagation algorithm. To the best of our knowledge and review, this is the first and novel attempt to generate and train ad hoc neural trees using our recursive error-backpropagation algorithm. Our motivation is to avoid any prior assumptions on network architecture and complicated hyperparameter settings. <ref type="bibr" target="#b44">Srivastava et al. (2014)</ref> proposed dropout technique that suggests randomly dropping neurons from a large NN. This creates "thinned" NN instances during training and prevents a NN from overfitting. Our proposed BNeuralT randomly generates a tree architecture, which can be considered a sparse NN in a similar sense with rather a higher dropout. Also, the branching and pruning of the tree branches in BNeuralT are performed at the tree generation stage, where a branch is probabilistically pruned by generating a leaf node at a depth lower than terminals. 4 3 Backpropagation neural tree 3.1 Problem statement Let X ? R d be an instance-space and Y = {c 1 , . . . , c r } be a set of r labels such that a label y ? Y is assigned to an instance x ? X . Therefore, for a training set of N instance-label</p><formula xml:id="formula_0">pairs S = (x i , y i ) N i=1 , we induce a classifier G(X , w) that reduces classification cost L Error (G) = 1 /N N i=1 (? i = y i ), where? i is a predicted output class on an input instance x i = x i 1 , x i 2 , . . . , x i d</formula><p>labeled with the target class y i ? {c 1 , . . . , c r }. Additionally, when an instance x ? X is associated with a continuous variable y ? R rather than a set of discrete class labels, then G(X , w) is a predictor that for a training set of instance-output pairs S reduces a prediction cost like mean</p><formula xml:id="formula_1">squared error (MSE) L MSE (G) = 1 /N N i=1 (? ? y i ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Backpropagation neural tree algorithm</head><p>Backpropagation neural tree (BNeuralT) takes a tree-like architecture whose root node is a decision node, and leaf nodes are inputs. For classification learning problems, BNeuralT has strictly dedicated nodes at level-1 (child nodes of the root) of the tree to represent classes. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> is an example of a classification neural tree where root's each immediate child is a subtree dedicated to a class, and the root node only decides a winner class? = argmax{c 1 , . . . , c r } for an instance-label pair (x, y). For regression learning problems, BNeuralT is a regression neural tree whose root node decides the tree's predicted output?</p><formula xml:id="formula_2">= ?( child i=1 w i v i + b v 0 ), where ?(?)</formula><p>is an activation function yielding a value in [0, 1], w i is a edge weight, v i is the activation of i-th child, and b v 0 is the root's bias (cf. <ref type="figure" target="#fig_0">Fig. 1(b)</ref>).</p><p>BNeuralT, denoted as G is an m-ary rooted tree with its one node designated as the root node, and each node takes at least m ? 2 child nodes except for a leaf node that takes no child node. Hence, for a tree depth p, BNeuralT takes n ? [(m p+1 ? 1)/(m ? 1)] nodes (including the number of internal nodes |V | and the leaf nodes |T |). Thus, BNeuralT can be defined as a union of internal and leaf nodes G</p><formula xml:id="formula_3">= V ? T = v j 1 , v j 2 , . . . , v j K ? {t 1 , t 2 , . . . , t L } where k-th node v j k ? V</formula><p>is an internal node and receives 2 ? j ? m inputs from its child nodes. The k-th leaf node t k ? T has no child, and it has a designated input x i ? {x 1 , x 2 , . . . , x d }. <ref type="figure" target="#fig_0">Fig. 1</ref> is an example of classification (left) and regression (right) trees. All internal nodes (shaded in gray) of the tree are neural nodes and may behave like the nodes of a NN. That is, a neural node computes a weighted summation z of inputs and squashes that using an activation function ?(z), e.g., sigmoid : ?(z) = 1 /(1 + e z ) or ReLU : ?(z) = max(0, z). We installed sigmoid or ReLU functions as BNeuralT's neural nodes, which can be any other activation function like tanh. The trainable parameters w are the edges and the bias weights of the nodes. The number of nodes n in a tree grows as per O(m p ). The number of edges (n ? 1) is proportional to the growth of n, so is the number of tree's trainable parameters w.</p><p>x 1</p><p>x 3</p><p>x 3</p><p>x 4</p><p>x 2 takes three immediate children: v1, v3, and v4, each respectively designated to a class c1, c2, and c3. The internal nodes (shaded in gray) are neural nodes and take an activation function ?(?) and leaf nodes are inputs. Each designated output class has its subtree. This tree takes its input from the set {x1, x2, . . . , x5}. The link w v j i between nodes are neural weights. (b) A neural tree example for a regression problem has one output node v0.</p><formula xml:id="formula_4">x 2 v 3 0 v 2 1 v 3 4 v 3 5 x 1 x 3 x 3 v 3 2 c 1 c 3 root node neural node input node b v1 b v4 b v2 b v5 bias output class v0 (b) Regression neural tree</formula><p>Complexity of BNeuralT. A BNeuralT model resembles an expression tree, and its computation is a depth-first-search post-order or pre-order traversal where each node needs to be visited at least once. Hence, the worst-case time complexity of BNeuralT is O(n), n being the number of nodes. In a BNeuralT model, each internal node has a bias, each leaf has an input, and each edge has a weight. Therefore, the space requirement of a BNeuralT model is 2|V | + |T |, i.e., two times internal nodes plus leaf nodes, which will grow proportional to the growth of tree's total nodes n = |V | + |T |. Thus, BNeuralT's worst-case space complexity is O(n).</p><p>Biologically plausible neural computation of BNeuralT. A typical <ref type="bibr">NN uses McCulloch and Pitts (1943)</ref> neurons. Such a neuron operates on a weighted sum of inputs and processes the sum via a nonlinear threshold function. Such neural computation considers that the dendrites (synaptic inputs) of a neuron are summed at "soma," thereby exciting a neuron, i.e., providing it a firing strength <ref type="bibr" target="#b25">(McCulloch and Pitts, 1943;</ref><ref type="bibr" target="#b12">Hodgkin and Huxley, 1952;</ref><ref type="bibr" target="#b33">Poirazi et al., 2003a)</ref>. However, the biological behavior of dendrites shows that dendrites themselves impose nonlinearity on their synaptic inputs before summing at "soma" <ref type="bibr" target="#b24">(London and H?usser, 2005;</ref><ref type="bibr" target="#b10">Hay et al., 2011)</ref>. This dendritic nonlinearity is possibly a sigmoidal nonlinearity <ref type="bibr" target="#b34">(Poirazi et al., 2003b)</ref>. Additionally, the synaptic connections in a fully connected NN are symmetric, whereas biological dendritic connections are asymmetric <ref type="bibr" target="#b26">(Mel, 2016;</ref><ref type="bibr" target="#b47">Travis et al., 2005;</ref><ref type="bibr" target="#b8">Farhoodi and Kording, 2018)</ref> [cf.  accuracy. <ref type="bibr" target="#b14">Jones and Kording (2021)</ref> considered the biologically asymmetric morphology of "dendritic tree" and its repeated synaptic inputs to a neuron to show the computational capability of a single neuron for solving machine learning problems.  <ref type="figure" target="#fig_2">Fig. 2(a)</ref>), and it has a stochastic m-ary rooted tree-like structure (cf. <ref type="figure" target="#fig_2">Fig. 2(c)</ref>). Thus, through BNeuralT we investigate the ability of a single neuron with sigmoidal nonlinearity (and linear when using ReLU) in its dendritic connections on three machine-learning problems: multi-class classification, regression, and pattern recognition.</p><p>Stochastic gradient descent (SGD) training. BNeuralT's trainable parameters w are iteratively optimized by a stochastic gradient descent (SGD) method (cf. Algorithm 1) that at an iteration j requires gradient ?w j ? Gradient? w L(x j , G w j ) computation (cf. Algorithm 2) and weight update as per w j ? w j?1 + ??w j . The weight update w j ? w j?1 + ??w j in line number 7 of Algorithm 1 is a simple GD method, where other similar optimizers like MGD, NAG, Adagrad, RMSprop, or Adam can also be used. <ref type="table">Table 1</ref> details the expressions of weight updates for these optimizers.</p><p>Error-backpropagation in BNeuralT. Our proposed recursive error-backpropagation in BNeu-ralT algorithm has two computation phases: forward pass and backward pass (cf. <ref type="figure" target="#fig_4">Fig. 3</ref>). Both work in a depth-first search manner. Since a tree data structure is algorithmically recursive to traverse through, both forward pass and backward (error-backpropagation) pass take place in a recursive manner. The forward pass computation produces the output for a tree in a post-order traversal manner (cf. <ref type="figure" target="#fig_4">Fig. 3(left)</ref>). That is, each leaf node propagates its input through dendrite Algorithm 1 Stochastic gradient descent (SGD) 1: procedure SGD(G w , S) SGD(?, ?) takes a neural tree G w and training data S 2:</p><formula xml:id="formula_5">w 0 ? G w initial weights 3: for epoch i &lt; epoch max do 4:</formula><p>S ? Shuffle(S) function Shuffle returns a randomly shuffle dataset 5:</p><p>for an instance x j ? S do 6: end for 10: end procedure <ref type="table">Table 1</ref>: Gradient descent versions to replace line number 7 in Algorithm 1. Symbols ?, ?, ?, ?1, and ?2 are constants (hyperparameter) of respective algorithms. Symbol vj and wj show previous momentum and weights, respectively, and ?wL(xj, Gw j ) shows gradient of loss L over input xj and w of tree G.</p><formula xml:id="formula_6">?w j ? Gradient ? w L(x j , G w j ) computed</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Expression</head><p>MGD <ref type="bibr" target="#b35">(Qian, 1999)</ref> </p><formula xml:id="formula_7">v j ? ?v j?1 + ?? w L(x j , G w j ) w j ? w j?1 ? v j Nesterov accelerated GD (Bengio et al., 2013) v j ? ?v j?1 + ?? w L(x j , G w j ?w j?1 ) w j ? w j?1 ? v j Adagrad (Dean et al., 2012) v j ? v j?1 + ? w L(x j , G w j ) 2 w j ? w j?1 ? ? / ? vj + ? w L(x j , G w j )</formula><p>RMSprop <ref type="bibr" target="#b46">(Tieleman and Hinton, 2012</ref>)</p><formula xml:id="formula_8">v j ? (1 ? ?)v j?1 + ?? w L(x j , G w j ) 2 w j ? w j?1 ? ( ? / ? vj + ) ? w L(x j , G w j )</formula><p>Adam <ref type="bibr" target="#b19">(Kingma and Ba, 2015)</ref> </p><formula xml:id="formula_9">m j ? ?1mj?1 + (1 ? ?1)?wL(xj, Gw j ) / 1 ? ? j 1 v j ? ?2vj?1 + (1 ? ?2)?wL(xj, Gw j ) 2 / 1 ? ? j 2 w j ? w j?1 ? ? /( ? vj + )m j</formula><p>(edge) to its parent node, and subsequently, each internal node, after computing received inputs from its child nodes, propagates activation to their respective parent node. Finally, the root node computes the tree's output.</p><p>The backward pass computes the gradient of the error with respect to edge weights. The backward pass computes gradient ? for each internal node and propagates it back to each edge depth-by-depth. Hence, the backward pass is a pre-order traversal of the tree (cf. <ref type="figure" target="#fig_4">Fig. 3(right)</ref>).</p><p>That is gradient ? computed at the root node flow backward to its child node until it reaches leaf nodes. [((4 5) ? 2) ((6 7)?3)] ? 1 1 ? [(2 ? (4 5)) (3 ? ( <ref type="formula">6 7)</ref>)] bckward pass: pre-order  h j = N ? h j current node's activation is the incoming signal for weight w jk 8:</p><formula xml:id="formula_10">b v 5 bias input b v j b v k y = h k v j v i ? j x 4 x 3 x 2 x 3 h j h i h k w jk ? k ?w b v k ? i w ij ?w ij ?w l0 j k w l0 j k w l0 i j ?w l0 i j ?w l0i ?w l1i w l0i w l1i w b v j w b v i ?w jk ?w b v i w b v k backward phase: depth-first search (pre-order) forward phase: depth-first search (post-order) x i e =? i ? y i k j ? j = h j (1 ? h j )? k w jk ? k = (? i ? y i )? i (1 ?? i ) w jk w ij? i h j ?w jk = ? k h j ?w ij = ? j x i b k = 1 b j = 1 w b k w b j ?w b k = ? k ?w b j = ? j d i j=1 (w i j h i j + b v j ) h i 1 h i j h i d i h j ? b v j v j</formula><p>? k = N ? N P arent ? ? gradient back-propagated from the parent node 9:</p><p>N ? ?w jk = ? k h j w jk is the weight between node N j and its parent node N k 10:</p><p>N ? ?w b j = N ? ? j bias weight is set to current node's gradient ? 11:</p><p>else if N ? T ype ? Leaf node then 12: </p><formula xml:id="formula_11">x i = N ? x N ? x ? {x 1 , x 2 , . . . , x d } is an input attribute 13: ? j = N ? N P arent ? ? gradient back-propagated from parent to child 14: N ? ?w ij = ? j x i w ij is</formula><formula xml:id="formula_12">1: procedure Compute ? (y,?, N ? G, G)</formula><p>y is a target,? is a prediction, and N is the current (or entry) node of tree G w jk = N ? w jk edge (weight) between current node N j and parent node N k 8: which respectively have <ref type="bibr">14, 13, 33, 8, 30, 4, 13, 18, and 9 input attributes; 2, 2, 2, 2, 2, 3, 3, 4, and 7 target classes; and 690, 270, 351, 768, 569, 150, 178, 846, and 214</ref> examples. For regression problems, we select Baseball (Bas), Daily Electricity Energy (Dee), Diabetes (Dia), Friedman (Frd), and Miles Per Gallon (Mpg), which respectively have <ref type="bibr">16, 6, 10, 5, and 6 input attributes and 337, 365, 442, 1200, and 392</ref> examples. Each regression dataset has one target.</p><formula xml:id="formula_13">N ? ? j = h j (1 ? h j )? k w</formula><p>These datasets are available at <ref type="bibr">(Bache and Lichman, 2013;</ref><ref type="bibr" target="#b16">Keel, 2011</ref>). These problems are significantly different not only in terms of the number of classes and examples but also in terms of their attribute types and range. This differing nature of these problems poses significant variations in difficulty for one algorithm to excel on all problems <ref type="bibr" target="#b48">(Wolpert, 1996)</ref>.</p><p>Both classification and regression learning datasets were normalized using min-max normalization between 0 and 1. Each dataset was randomly shuffled and partitioned into training (80%) and test (20%) sets for each instance of the experiment. For a pattern recognition problem, we select the MNIST dataset <ref type="bibr" target="#b20">(LeCun et al., 2020)</ref>, which has 60, 000 training examples and 10, 000 test examples labeled with a set of 10 handwritten characters, and this dataset was normalized by dividing gray-scale pixel value by 255.</p><p>BNeuralT hyperparameters. We repeated experiments 30 times (independently) for each classification and regression problem. In each run, we generated ad hoc BNeuralTs (stochastically generated tree structures) for each dataset with a maximum tree depth p = 5; max child per node m = 5, and branch pruning factor P [leaf p &lt; p] ? {0.4, 0.5} which is a probability of a leaf node being generated at a depth lower than the tree height p. A higher leaf generation probability (e.g., 0.5) at internal nodes means that tree height terminates earlier than its predefined depth, which means a tree will be generated with fewer parameters. A lower leaf generation probability (e.g., 0.4) means a deeper tree structure with more parameters.</p><p>For classification problems, BNeuralT's output neural node was argmax node (i.e., winner takes all node), whereas, for regression problems, it was sigmoid activation function. Its internal nodes were sigmoid activation (or ReLU for some trial experiment versions). For pattern recognition (MNIST dataset), BNeuralT models were generated on an ad hoc basis via setting maximum tree depth p ? {5, 10}, and max child per node m ? {10, 15} with P [leaf p &lt; p] ? {0.4, 0.5} and a tree size threshold |G| ?MIN ? {1K, 20K}, where "K" is 1000.</p><p>Other algorithms hyperparameter. MLP architecture was a fixed three-layer architecture [inputs-hidden (100 nodes)-targets] for each dataset of classification and regression problems.</p><p>A SoftMax layer acted as the MLP classifier's output nodes, and an MLP regression had a sigmoid activation as its output node. The internal neural nodes in an MLP were sigmoid (or ReLU ) functions. Other algorithms HFNT S , HFNT M , MONT 3 , DT, RF, GP, NBC, SVM, and CARTs had their default setups as they are in their libraries <ref type="bibr" target="#b31">(Pedregosa et al., 2011)</ref> or in the literature <ref type="bibr" target="#b29">(Ojha et al., 2017;</ref><ref type="bibr" target="#b30">Ojha and Nicosia, 2020;</ref><ref type="bibr">Zharmagambetov et al., 2019)</ref>. A detailed list of hyperparameters of all algorithms is provided in <ref type="table" target="#tab_17">Supplementary Table A1</ref>.</p><p>SGD hyperparameters. BNeuralT and MLP algorithms take optimizers like GD, MGD, NAG, Adagrad, RMSprop, or Adam. The training parameters were learning rate ? = 0.1, momentum rate ? = 0.9, ? 1 = 0.9, ? 2 = 0.9, = 1e ?8 , training mode was stochastic (online), and training epochs were 500. Since the gradient descent computation was stochastic, both BNeuralT and MLP do the same number of forward-pass (function) evaluations, i.e., number training examples ? epochs. All six optimizers were used for training BNeuralT and MLP with an earlystopping restore-best strategy (or without an early-stopping for some trail experiments), whereas other algorithms take their own default optimizer <ref type="bibr" target="#b31">(Pedregosa et al., 2011)</ref>. While BNeuralT and MLP were trained in online mode (example-by-example training), other algorithms take only Loss functions. The loss function for BNeuralT training for classification and pattern recognition problems was a miss-classification rate L Error (G). MLP training on classification problems was best with categorical cross-entropy loss <ref type="bibr">(Bishop, 2006)</ref>. The training of other algorithms had default setups recommended in their libraries <ref type="bibr" target="#b31">(Pedregosa et al., 2011)</ref>. For regression problems, all algorithms were trained by reducing L MSE (?). The test metric for classification problems for all algorithms was a miss-classification rate L Error (?) and for regression problems, it was a regression fit L r 2 (?) = 1 ? N i=1 (yi ??i) 2 / N i=1 (yi ??) 2 (Nash-Sutcliffe model efficiency coefficient) which gives a value between [??, 1], where? is the mean of target y.</p><p>Forward pass computation time, ? . BNeuralT was implemented in Java 11, and MLP was implemented using TensorFlow and Keras libraries <ref type="bibr" target="#b18">(Keras, 2020)</ref>. Other algorithms were implemented using scikit-learn library <ref type="bibr" target="#b31">(Pedregosa et al., 2011)</ref> in Python 3.5. The forward pass computation time, ? is a wall-clock time on Windows 10 operating system with configuration x64 Intel i5-2400 CPU, 3.1GHz, 3101Mhz, 4 Cores, and 16GB physical memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BNeuralT and MLP experiment versions</head><p>We experimented with multiple versions of BNeuralT and MLP settings to bring out the best of both. We tried sigmoid and ReLU as the internal activation functions. We tried BNeuralT's branch pruning factor P [leaf p &lt; p] with 0.5 and 0.4.</p><p>The learning rates of the optimizers had two sets: (i) A flat learning rate ? = 0.1 for all optimizers. (ii) The learning rate recommended in the Keras library for respective optimizers, i.e., RMSprop, Adam, and Adagrad had ? = 0.001, and MGD, NAG, and GD had ? = 0.01. We call the library's recommended ? value a default learning rate. In addition, the SGD learning was tried "with" and "without" early-stopping (ES) strategies. We repeated each experiment for each dataset for 30 independent runs, and their average performance on test sets was evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BNeuralT performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Selection of the best performing setting</head><p>We selected the best performing setting based on the average test accuracy computed over 30 independent runs of BNeuralT, MLP, HFNT S , HFNT M , MONT 3 , DT, RF, GP, NBC, and SVM to report them in detail in this section. The best performing BNeuralT setting was the "ES training of BNeuralT having sigmoid nodes, 0.1 learning rate, and 0.4 leaf generation rate."</p><p>The best MLP setting was "ES training of MLP having sigmoid nodes and default learning rate." We found that HFNT S , HFNT M , DT, RF, GP, NBC, and SVM worked best with their recommended setting.</p><p>We found that collectively on all classification and regression datasets, BNeuralT with sigmoid nodes, 0.1 learning rate, and 0.4 leaf generation rate trained using RMSprop performed the best among all experiment versions of all algorithms. This setting produced an average accuracy of 83.2% across all datasets with an average of 222 trainable parameters. This same setting also performed the best across all classification datasets among all algorithms, i.e., it produced an average accuracy of 89.1% with an average of 261 trainable parameters. In fact, the top six best results over classification datasets were from BNeuralT settings. GP algorithm came 7th with an average classification accuracy of 86.79%. MLP with sigmoid node and ES training using 13 MGD optimizer with default learning performed 8th with an average accuracy of 86.78% with an average 1970 trainable parameters. MLP, however, performed slightly better on regression problems than the other algorithms. MLP with sigmoid node and ES training using NAG optimizer with default learning rate produced an average regression fit value of 0.775. Whereas BNeuralT with sigmoid nodes, 0.1 learning rate, and 0.4 leaf generation rate trained using RMSprop optimizer produced an average regression fit value of 0.727. It is important to note that this performance of BNeuralT comes with a much lower average trainable parameter. BNeuralT used only 152 trainable parameters compared to MLP that used 1041 parameters. This means BNeuralT's performance comes with an order magnitude less parameter than MLP on both classification and regression tasks. <ref type="table" target="#tab_5">Table 2</ref> reports the details of each algorithm's best-performing settings. However, exhaustive lists of 110 experiments, from which we selected these best performing settings, are provided in <ref type="table" target="#tab_3">Supplementary Tables A2 and A3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BNeuralT models summary</head><p>BNeuralT classification models summary. <ref type="table" target="#tab_5">Table 2 suggests</ref>  and green, blue, red, and black nodes, respectively indicate inputs, dendritic nonlinearities, root, and class nodes.</p><p>The average tree size of BNeuralT with P [leaf p &lt; p] = 0.5 and RMSprop was 119 (89% accuracy).</p><p>The average tree size of HFNT M , MONT 3 , and HFNT S algorithms were 29 (72.4% accuracy), 36 (83.1% accuracy), and 92 (78.9% accuracy), respectively. Since tree construction and forward pass computation are similar for BNeuralT, HFNT, and MONT algorithms, there is a trade-off between the model's compactness and accuracy. In fact, this produces a set of trade-off solutions (between accuracy and complexity). Along with this set of Pareto solutions, one can choose which is the best candidate solution for the given machine learning problem under examination: more accurate but less sustainable or a little less accurate but more robust and sustainable.</p><p>The forward pass computation time on a single example (in multiple of 10 ?6 seconds) of BNeu-ralT was 11.2 seconds, whereas MLP took 1288; DT, 3.1; <ref type="bibr">RF,</ref><ref type="bibr">485.4;</ref><ref type="bibr">GP,</ref><ref type="bibr">455.6;</ref><ref type="bibr">NBC,</ref><ref type="bibr">31.9;</ref><ref type="bibr">and SVM,</ref><ref type="bibr">16</ref>.1 seconds. DT was the fastest, and BNeuralT was the second-fastest. However, DT has a much lower accuracy (81.3%) than BNeuralT (89.1%). The time computation is difficult to compare as the algorithms were implemented in different programming languages <ref type="bibr" target="#b32">(Pereira et al., 2017)</ref>. BNeuralT was implemented in Java 11, and all other algorithms were implemented in Python 3.5. However, BNeuralT's performance on classification problems was clearly better among all algorithms. This is further evident from BNeuralT's collective average accuracy of all optimizers on all classification datasets was 86.1%. Whereas on all optimizers, MLP's average accuracy was 83.8%, tree algorithms had 80.4%, and other algorithms had 83.6% accuracy.</p><p>We selected BNeuralT's best optimizer RMSprop for the statistical significance test. This test was designed to examine whether the performance of BNeuralT's RMSprop is statistically significant than that of the other algorithms.  <ref type="table" target="#tab_7">Supplementary Table A4</ref> and <ref type="table">Table A5</ref> also favor BNeuralT's RMSprop.</p><p>BNeuralT regression models summary. MLP's Adam performed best for regression problems. MLP's Adam produced an average regression fit of 0.772 without dropout and 0.754 with dropout on all datasets. BNeuralT's RMSprop offered an average regression fit of 0.727, which differs only by 5.8% with the best MLP result. This performance of BNeuralT comes with the use of only 14.6% trainable parameters than the parameters used by the MLP (w = 1014). (Note that an MLP dropout model during its test phase uses all weights since dropout only regularizes weights by averaging gradient over epochs during the training phase <ref type="bibr" target="#b44">(Srivastava et al., 2014)</ref>.)</p><p>This suggests that BNeuralT is highly capable of learning data with very low complexity with a faster forward pass computation time. The structure of some select best performing BNeuralT regression models is shown in <ref type="figure">Fig. 6</ref>.</p><p>The average tree size of BNeuralT with P [leaf p &lt; p] = 0.5 and RMSprop for regression problems was 64 (L r 2 = 0.675). The average tree size of HFNT S and HFNT M algorithms were 127 (L r 2 = 0.562) and 90 (L r 2 = 0.567) respectively. Here, BNeuralT was able to perform accurately compared to genetically optimized HFNT algorithms with less complex models. <ref type="table" target="#tab_3">Table 3</ref>   The black node in a tree is its root node, class output nodes are in red, function nodes are in blue, and leaf nodes are in green. The link connecting nodes are neural weights.  We fed BNeuralT with pixels of MNIST character images since we do not use convolution in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The statistical tests in</head><p>BNeuralT. We aimed at generating varied BNeuralT models with varied trainable parameters length by varying tree size. We hoped that a low complexity (few parameters) BNeuralT model would perform competitively with a few reported state-of-the-art. Therefore, we compare BNeu-ralT performance to gauge its robustness not only on learning small-scale problems reported in <ref type="table" target="#tab_5">Table 2</ref> but on large-scale learning problems like MNIST. <ref type="table" target="#tab_7">Table 4</ref> summarizes BNeuralT models compared with the performances of tree-based state-of-the-art classification algorithms. In our few trials, BNeuralT does perform competitively with many state-of-the-art (cf. <ref type="table" target="#tab_7">Table 4</ref>).</p><p>The performance of BNeuralT is better than tree-alternating optimization (TAO) (Carreira-Perpinan and Tavallali, 2018), <ref type="bibr">CART (Breiman et al., 1984)</ref>, C5.0 <ref type="bibr" target="#b36">(Quinlan, 1993)</ref>, oblique classifier 1 (OC1) <ref type="bibr" target="#b28">(Murthy et al., 1993)</ref>, and generalized unbiased interaction detection and estimation (GUIDE) <ref type="bibr" target="#b23">(Loh, 2014)</ref> algorithms that worked on MNIST raw pixels inputs (Zharmagambetov et al., 2019) like BNeuralT (cf. <ref type="table" target="#tab_7">Table 4</ref>).</p><p>We compare BNeuralT with biologically plausible models of Jones and Kording (2021) that performed binary classification on MNIST's two classes (class 3 and class 5). This is, however, a trivial comparison as BNeuralT works on all classes and uses sigmoidal dendritic nonlinearities, whereas Jones and Kording (2021)'s models work on binary class and use Leaky ReLU as dendritic nonlinearities. They obtained an error rate of 7.8%, 3.65%, and 8.89% respectively with 1-tree (w = 2, 047), 32-tree (w = 65, 504), and A-32-tree (w = 65, 504) models. In contrast to Jones and Kording (2021)'s models, BNeuralT performs classification on all ten classes of MNIST pixels. Obviously, some classes are easier to learn than others (see <ref type="figure">Fig. 7</ref>), and training a binary classifier presents an entirely different difficulty level than a multi-class classification.</p><p>However, although a one-to-one comparison is not possible in such a scenario, it may be worth noting that BNeuralT obtained an error rate of 7.74% (w = 11, 987) and 6.08% (w = 23, 835) on all ten classes. Therefore, the sparse stochastic structure of BNeuralT (e.g., <ref type="figure">Fig. 8</ref>) stands competitive with the models of <ref type="bibr" target="#b14">Jones and Kording (2021)</ref>.</p><p>Moreover, BNeuralT models show a linear relation between trainable parameters and their accuracy (cf <ref type="table" target="#tab_7">. Table 4</ref>). Hence, BNeuralT models with relatively higher parameters and exhaustive hyperparameter tuning are able to produce efficient results. <ref type="figure">Fig. 7</ref> shows to hardest" to learn as c1, c6, c0, c7, c5, c4, c9, c2, c3, and c8 (cf. <ref type="figure">Fig. 7)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">BNeuralT convergence analysis</head><p>We evaluated average asymptotic convergence profiles of all six SGD optimizers for optimizing</p><p>BNeuralT on classification and regression problems (cf. <ref type="figure" target="#fig_0">Figs. 9, 10, and 11</ref>). For such an analysis, we recorded training and test accuracies of each training epoch. Since we ran algorithms for 30 independent instances, we analyzed the average trajectory of all 30 runs. In each run, an ad hoc BNeuralT architecture was generated, which could vary in tree size between a minimum "outputs ? 2" nodes to a maximum (m p+1 ? 1)/(m ? 1) nodes. Hence, BNeuralT architecture and trainable parameters varied stochastically at each instance of the experiment.</p><p>Such high entropy network architectures pose difficulties for SGDs to perform well consistently.</p><p>We, therefore, investigated BNeuralT models' accuracy against their architecture (number of parameters) (cf. <ref type="figure" target="#fig_0">Fig. 12</ref>).</p><p>BNeuralT classification models convergence. <ref type="figure" target="#fig_13">Fig. 9</ref>  Adagrad showed the most interesting convergence profile as initially, it had worse convergence among all optimizers, and while approaching higher epochs, it started rapidly improving its convergence. Thus, over an asymptotic behavior, Adagrad converged to a similar accuracy to that of RMSprop's accuracy. The optimizers NAG and MGD behave equivalently. Adam and GD were found sensitive to BNeuralT architecture (and trainable parameters).</p><p>BNeuralT regression models convergence. <ref type="figure" target="#fig_0">Fig. 10</ref>  BNeuralT and MLP settings convergence. In <ref type="figure" target="#fig_0">Fig. 11</ref>, we compare convergence of six optimizers for optimizing both BNeuralT and MLP on various settings. We show this comparison on "glass" and "miles per gallon" datasets as an example. (Supplementary shows convergence of all other datasets on various settings.) In <ref type="figure" target="#fig_0">Fig. 11</ref>, we observe that the learning rate 0.1 produces stable convergence for all optimizers. For learning rate 0.1, Adam does not converge as good as BNeuralT average convergence trajectory performance computed over 30 independent runs for six optimizers over nine classification problems. The x-axis, log 10 (epochs) has the range [0.0, 2.7] and is the training epochs 1 to 500. The y-axis, ? log 10 (LError(G)) has the range [0, 10] and is the log scale of the training and test accuracies. An error of 0.01 (accuracy 99%) on the ? log 10 (LError(G)) scale has a value of 2.0 and an accuracy of 90% has a value of 1.0. Thus, a higher value on the y-axis is better. Error bar is the standard deviation of ? log 10 (LError(G)) and it indicates stochasticity of the convergence that helps an optimizer escape local minima better. Thus, a larger length is better. For each data, training and test convergence pair are plotted for 500 epochs (on the log scale, 2.7). RMSprop, MGD, NAG, Adagrad, GD, and Adam are respectively indicated in blue, orange, green, red, purple, and brown colors, respectively, with symbols diamond, triangle, circle, downward triangle, and star. other algorithms (cf. <ref type="figure" target="#fig_0">Figs. 11(a) and 11(d)</ref>). However, Adam does converge when learning rate is 0.001 (cf. <ref type="figure" target="#fig_0">Figs. 11(b)</ref>, 11(e), 11(h), and 11(k)). For learning rate 0.001, Adagrad does not converge as good as others. It may be observed that Adagrad's adaptively decreasing learning rate property made the convergence very slow and it may require more epochs to converge (cf.</p><p>Figs. 11(b), 11(e), 11(h), and 11(k)).</p><p>Convergence of optimizers on ReLU. For ReLU activation function, Adagrad and GD being the slowest converging optimizers performed better than other faster converging optimizers like RMSprop, Adam, and NAG (cf. <ref type="figure" target="#fig_0">Figs. 11(c)</ref>, 11(f), 11(l) and 11(l)). In fact, when using ReLU activation function for regression problems, BNeuralT suffered from exploding gradient issues when using optimizers like GD, MGD, NAG, and Adam during some instances of runs of some datasets. Adagrad, however, remained unaffected by exploding gradient issue. This is due to its decreasing convergence speed. BNeuralT's performance with ReLU, due to its high sparsity,</p><p>was affected by exploding gradient effect more than the MLP, which showed more tolerance to exploding gradient effect due to its large number of parameters (cf. <ref type="figure" target="#fig_4">Supplementary Fig. A3</ref>).</p><p>Convergence of accuracy against trainable parameters. BNeuralT's tree size (proportional to trainable parameter) and test accuracy in <ref type="figure" target="#fig_0">Fig. 12</ref> suggest that RMSprop compared to other optimizers can optimize ad hoc structure better. We observed that the accuracy of BNeuralT increases with increasing tree size. However, accuracy dropped for some outliers in the connected scatter plot in <ref type="figure" target="#fig_0">Fig. 12</ref>. This was because many points were within a specific range. For classification problems, except for RMSprop, NAG was another better optimizer. For regression problems, along with RMSprop, Adagrad was another better performing optimizer.</p><p>BNeuralT's RMSprop optimizer showed rather more stable performance for stochastically varying architectures compared to other optimizers. For the pattern recognition MNIST dataset,</p><p>RMSprop optimizer was used, and it showed a linear increase in accuracy for increasing order of tree size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We designed and investigated a learning system called BNeuralT capable of solving three classes of machine learning problems: classification, regression, and pattern recognition. We assessed the capability of this neural tree algorithm as a single neuron model approximating computational dendritic tree-like behavior (cf. <ref type="figure" target="#fig_2">Figs. 2 and Fig. 4</ref>). This algorithm can also be considered a highly sparse NN trained using SGD optimizers. To train BNeuralT using SGDs, we designed a recursive backpropagation algorithm. Therefore, we broadly assessed three aspects of a learning system, i.e., its performance on (i) stochastically generated highly sparse models, (ii) sigmoid and ReLU functions and their dendritic interactions with internal nodes, and (iii) optimizers asymptotic convergence behavior. We had a diverse range of classification and regression problems and algorithms to compare BNeuralT's capabilities over these dimensions.  problems. Since optimizers had to optimize the same architecture in an instance, it may be the continuous-variable output in the case of regression problems has helped Adagrad perform better than the discrete variable output in classification problems.</p><p>The use of activation functions influenced the performances of SGD optimizers. The sigmoid function proved to be more efficient with RMSprop, NAG, and MGD. Whereas ReLU proved to be efficient with Adagrad. This may be related to Adagrad's slow convergence speed that avoided weights to explode too quickly compared to faster converging optimizers like RMSprop (cf. <ref type="figure" target="#fig_0">Fig. 11(a-b)</ref>, 11(d-e), 11(g-h)). This phenomenon of Adagrad may be confirmed since GD being the slowest converging SGD, was also found efficient when ReLU is used (cf. <ref type="figure" target="#fig_0">Fig. 11</ref>(c, f, and e)). Additionally, Adagrad converged better with a learning rate of 0.1 than 0.001 (e.g. <ref type="figure" target="#fig_0">Fig. 11(a-b)</ref>). This is because Adagrad was too slow at earlier epochs that prevented it from converging within a fixed number of training epochs.</p><p>BNeuralT is operationally similar to HFNT and MONT algorithms. The HFNT and MONT algorithms model structures were genetically optimized as opposed to BNeuralT structure. The better performance of BNeuralT compared to HFNT and MONT shows that the stochastic structure of BNeuralT has a high potential to solve machine learning problems (cf. <ref type="table" target="#tab_5">Table 2</ref>).</p><p>However, this performance comparison also shows that BNeuralT models can be further compacted because both HFNT and MONT on classification problems had smaller average tree sizes than BNeuralT. This confirms that optimization of structure made HFNT and MONT more compact, although their accuracies were slightly compromised. On regression problems, however, BNeuralT performed better than HFNT both in terms of tree size and regression fit.</p><p>BNeuralT's performance compared to MLP's (with and without dropout) models and genetically optimized HFNT and MONT models confirms Occam's razor principle of parsimony for machine learning model selection that the simple models possess better generalization capability than the complex models <ref type="bibr">(Blumer et al., 1987)</ref>. Indeed, it is similar to the sparsity of the biological brain that a sparse network generalizes better or as good as a dense network <ref type="bibr" target="#b9">(Friston, 2008;</ref><ref type="bibr" target="#b11">Herculano-Houzel et al., 2010;</ref><ref type="bibr" target="#b13">Hoefler et al., 2021)</ref>. Moreover, it has been argued that a dense network is often overparameterized, and only a minute fraction of it is required for generalization <ref type="bibr" target="#b6">(Denil et al., 2013)</ref>. Our result is in a similar line because BNeuralT, with only an average of 222 parameters, which is only 13.5% of parameters than that of MLP's average 1638 parameters, is able to generalize machine learning problems better or with similar accuracy than MLP.</p><p>Additionally, the sparsity and compactness of BNeuralT models reduce memory usage and CO 2 footprint as they require less memory and computational resources than dense networks.</p><p>The decision tree algorithms DT and RF (ensemble of DTs) computationally have dedicated paths from the root to leaves <ref type="bibr">(Breiman et al., 1984;</ref><ref type="bibr">Breiman, 2001)</ref>. BNeuralT computationally also has dedicated information processing paths but from leaves to root. Although these algorithms differ in how nodes propagate information, a performance comparison suggests that</p><p>BNeuralT has superior or competitive performances compared with DT and RF (cf. <ref type="table" target="#tab_5">Table 2</ref>).</p><p>This performance is noticeable since RF is an ensemble algorithm that, using bootstrapping, combines 100 DTs to construct a predictor <ref type="bibr">(Breiman, 2001)</ref>. Hence, the better performance of a standalone randomly generated BNeuralT model shows its high capabilities. Especially when RF being an ensemble of many trees, is more complex than a small and compact BNeuralT tree.</p><p>Moreover, DTs are symbolic machine learning algorithms whose models offer inference ability as opposed to the black-box nature of NNs because of their ability to induce data using dedicated paths from the root to leaves. Likewise as shown in Figs. 5 and 6,</p><p>BNeuralT has dedicated information processing paths from leaves to root, and such paths related to particular subsets of inputs may be analyzed, which potentially may offer inference ability to BNeuralT.</p><p>Thus, BNeuralT models are potentially inferable as opposed to NNs. However, this is a challenging task since BNeuralT's nodes combine inputs and perform a nonlinear or linear transformation.</p><p>We assessed BNeuralT performance against GP, NBC, and SVM. These three algorithms take Gaussian kernels. That is, these algorithms have a powerful approach towards prediction. GP and NBC algorithms are robust and powerful algorithms if input data follow a normal distribution. Similarly, SVM uses Gaussian kernels to project input to high dimensions, increasing the separability of data points to help to classify them <ref type="bibr" target="#b4">(Cortes and Vapnik, 1995)</ref>. Better performance of BNeuralT compared to these algorithms on classification and regression problems (cf. The biologically plausible design of BNeuralT comes from its structural arrangement that takes random repeated input and has a computational dendritic tree-like organization with sigmoidal nonlinearities or ReLU linearity through its internal nodes <ref type="bibr" target="#b24">(London and H?usser, 2005)</ref>. The biologically plausible computational dendritic tree-like models 1-tree and 32-tree have a regular structural arrangement where repeated inputs are fed to a neuron systematically to form a tree structure <ref type="bibr" target="#b14">(Jones and Kording, 2021)</ref>. Whereas BNeuralT takes randomly generated inputs and takes a non-systematic stochastic approach to its tree construction (cf. <ref type="figure" target="#fig_2">Fig. 2)</ref>. Moreover,</p><p>BNeuralT works on multi-class classification; and 1-tree, 32-tree, and A-32-tree models work on binary classification <ref type="bibr" target="#b14">(Jones and Kording, 2021)</ref>.</p><p>BNeuralT's comparison with 1-tree, 32-tree, A-32-tree models, although limited, presents a noticeable performance. The error rate of BNeuralT on the MNIST dataset on all ten classes classification was 6.08% with 23835 parameters. The error rates of 1-tree, 32-tree, A-32-tree models on the binary classification of classes 3 and 5 of the MNIST dataset were reported as 7.8%, 3.65%, and 8.89%, respectively, and they had 2047, 65504, 65504 parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This result confirms BNeuralT's potential to produce capable learning systems, especially when</head><p>BNeuralT's structural randomness (cf. <ref type="figure" target="#fig_2">Fig. 2)</ref> is closer to the randomness (if any) of biological computational dendritic-tree <ref type="bibr" target="#b47">(Travis et al., 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We propose a new algorithm Backpropagation Neural Tree (BNeuralT). Our BNeuralT algorithm plausibly has a biological dendritic tree-like modeling capability. It has a single neuron-like model with sigmodal dendritic nonlinearities or rectified linear unit (ReLU) based dendritic linearity.</p><p>It uses random repeated inputs at the leaves of subtrees attached to a single neuron, which is the root of a tree. BNeuralT uses stochastic gradient descent (SGD) optimizers to optimize stochastically generated sparse tree structures that are potentially minimal subsets of neuron networks (NNs). We propose a recursive error backpropagation algorithm to apply SGDs to train trees that require pre-order and post-order traversal in a depth-first-search manner for their forward pass and backward pass computations.</p><p>The results showed that our stochastically generated biologically plausibly tree structure and recursive error backpropagation algorithm have the capacity to learn a wide variety of machine learning problems. Moreover, we show that any stochastically generated tree structures can learn machine learning problems with high accuracy, and structure optimization may only be required for making models more compact. However, there is a trade-off for compacting models as we found that making the models more compact means compromising on accuracy. Additionally, BNeuralT's strong performance compared to MLP's dropout regularization technique confirms that SGD training of any "a priori " arbitrarily "thinned network " (spares tree structures) has the potential to solve machine learning tasks with equivalent or better degree of accuracy.</p><p>The sigmoidal dendritic nonlinearities (sigmoid function used at tree's root and internal nodes) performed obviously better than a linear dendritic tree (sigmoid function used at tree's root and ReLU at internal nodes). However, the linear dendritic tree differed from the best performing nonlinear dendritic tree by only about 10% accuracy. Nevertheless, it was comparable with a few nonlinear dendritic tree models, especially with those trained with gradient descent (GD), momentum GD, and Adam. This shows that purely single node BNeuralT models might solve machine learning problems efficiently.</p><p>On MNIST (pixels) character classification dataset, BNeuralT, when loosely compared with A.1 Source code, scripts, pre-trained models, and data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 BNeuralT Code</head><p>BNeuralT algorithm source code repository: https://github.com/vojha-code/BNeuralT has the following items, including training and evaluation entry points for re-producing results in <ref type="table" target="#tab_5">Table 2</ref> and <ref type="table" target="#tab_7">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 MLP and Other Algorithms Code and Scripts</head><p>Scripts of MLP and other algorithms are available at:</p><p>https://github.com/vojha-code/BNeuralT/tree/master/source_mlp_tf A.1.3 Pre-trained Models (of <ref type="table" target="#tab_5">Table 2</ref> and The performance of BNeuralT and MLP of classification problems is shown in <ref type="figure" target="#fig_0">Figs. A1(a)</ref>, A2(a), A3(a), A4(a), A5(a), A6(a), and A7(a). The y-axis of each plot is "? log 10 (L Error (?))" that has the range [0, 10] and is the log scale of the training and test accuracies. An accuracy of 99% (an error of 0.01) on ? log 10 (L Error (?)) scale has a value of 2.0, and an accuracy of 90%</p><p>(an error 0.1) has a value of 1.0. Thus, a higher value on the y-axis is better. Error bar is the standard deviation of ? log 10 (L Error (?)) value. The performances of regression problems are shown in <ref type="figure" target="#fig_0">Figs. A1(</ref> <ref type="figure">and A7(b)</ref>. The y-axis of each plot is log 10 (L MSE (?)), and it is the training and test sets MSE on the log scale. An MSE 0.01 on the log scale has a value of ?2. Thus, a lower value is better. Error bar is the standard deviation of log 10 (L MSE (?)). In both classification and regression plots, a larger length of error bar shows higher stochasticity of an optimizer that indicates an optimizer's higher ability to skip local minima. Thus, it shows an optimizer's better convergence ability.</p><formula xml:id="formula_14">b), A2(b), A3(b), A4(b), A5(b), A6(b),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 BNeuralT Convergence Trajectories</head><p>Figs. A1, A2, and A3 are BNeuralT models with their leaf generation rate at lower tree depth set to 0.5, and they were trained with an early-stopping strategy. However, they varied in the following ways: <ref type="figure" target="#fig_0">Fig. A1</ref> has the sigmoid functions as its internal nodes. All six SGDs are trained with a learning rate of 0.1. <ref type="figure" target="#fig_2">Fig. A2</ref> has the sigmoid functions as its internal nodes. In this setting, the optimizers RMSprop, Adam, and Adagrad had a learning rate of 0.001, and optimizer MGD, NAG, and GD had a learning rate of 0.01. <ref type="figure" target="#fig_4">Fig. A3</ref> has the ReLU functions as its internal nodes. All six SGDs are trained with a learning rate of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figs. A1, A2, and A3 offer convergence profiles for both classification and regression problems for</head><p>BNeuralT setting with higher leaf generation rates, i.e., 0.5. For the higher leaf generation rate (smaller model size), BNeuralT with sigmoid node and 0.1 learning rate convergence is similar to the lower leaf generation rate (larger models). However, for the default learning rate (lower learning rate), BNeuralT convergence shows a different profile (cf. <ref type="figure" target="#fig_2">Fig. A2</ref>). For a lower learning rate, RMSprop (and Adam) is much slower at the beginning of training but converges very fast at the reminder of the training epochs. NAG and MGD show a monotonically increasing and stable learning profile. Interestingly, Adam (as evident from literature <ref type="bibr" target="#b19">(Kingma and Ba, 2015)</ref>) performed the best with a learning rate of 0.001, and as shown in <ref type="figure" target="#fig_13">Fig. 9</ref>, this was not the case where the learning rate was 0.1. Adam has a similar profile to RMSprop, but RMSprop produced better accuracy than Adam. Adagrad, with a lower learning rate, had worse performance among all six SGDs.</p><p>BNeuralT's performance with ReLU due to its high sparsity and loss nonlinearity show a decline in the model's performance (cf. <ref type="figure" target="#fig_4">Fig. A3</ref>). In fact, it seems to suffer from exploding gradient issues for some optimizers like GD, MGD, NAG, and Adam. Adagrad, however, remains unaffected by this issue when the ReLU function was used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 MLP Convergence Trajectories</head><p>Figs. A4, A5, A6, and A7 are MLP algorithm settings, each with sigmoid activation functions.</p><p>However, they varied as per early-stopping and learning rate usage strategy: <ref type="figure" target="#fig_5">Fig. A4 was trained</ref> with early-stopping and with a flat 0.1 learning rate for all optimizers. <ref type="figure" target="#fig_11">Fig. A5</ref> was trained with early-stopping, but RMSprop, Adam, and Adagrad had a learning rate of 0.001, and MGD, NAG, and GD had a learning rate of 0.01. <ref type="figure">Fig. A6</ref> was trained without early-stopping and with a flat 0.1 learning rate for all optimizers. <ref type="figure">Fig. A7 was trained</ref>  Considering the convergence profiles of different optimizers on MLP training for early stopping and learning rate 0.1, we observed the following: Adagrad with early stopping and higher initial learning rate offered better accuracy on both classification and regression problems. This is because Adagrad's small step at higher epochs allowed networks to find a better early stopping point than that of the other algorithms whose larger step size made networks converge to a premature early stopping point (cf. <ref type="figure" target="#fig_5">Figs. A4 and A6</ref>).</p><p>With an initial small learning rate of 0.001, Adagrad is too slow to converge within a predefined number of epochs (500). In this setting, Adam seems to have an appropriate small step size to converge to a proper early-stopping point. The performances of RMSprop, MGD, NAG, and GD are next to Adam's performance (cf. <ref type="figure" target="#fig_11">Figs. A5 and A7)</ref>.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Supplementary Tables</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Neural Trees: (a) A neural tree example of a three-class classification learning problem. The root node v 3 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Fig. 2(a)].Poirazi et al. (2003b) using a sparse two-layer NN analogous to a binary tree-like dendritic NN has shown the possibility of modeling a single neuron as a NN. The work of Beniaguev et al. (2020) demonstrated a proof of concept single neuron model as a synaptic integration and fire model capable of performing classification of two types of classes with a high degree of temporal 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>(b) shows Jones and Kording (2021)'s a single neuron computational model with repeated inputs as a binary tree structure. Unlike the work of Poirazi et al. (2003b), BNeuralT has asymmetric dendritic connections to a "single" neuron (Beniaguev et al., 2020; Jones and Kording, 2021). Jones and Kording (2021)'s dendritic tree has a systematic and regular binary-tree-like structure and solves a binary classification problem. Whereas BNeuralT's neuron is like the neuron of Travis et al. (2005) (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4 (left)shows the forward pass and backward pass computation labeled with variables of neural tree computation.Fig. 4 (right)shows the backpropagation of gradient from an output node to the inputs (leaf nodes). Algorithm 2 is a summary of error-backpropagation and Algorithm 3 shows recursive gradient computation for BNeuralT that facilitates error-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Forward pass (left) and backward pass (right) computation. The arrows show the direction of computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Left. Backpropagation Neural Tree. Output node v k yields output y using forward pass upon receiving inputs xi from leaf nodes. Each node is linked with an edge weight wij. The backward pass propagates error e = (y ??) back to input nodes to compute weight change ?w. Right. Backpropagation of error from an output node, k; to a hidden node, j; to an input node, i; and to bias inputs b k and bj. Dashed lines represent error backpropagation and computation ? and gradient ?w (cf. Algorithm 2) to find weight change ?w that help stochastic gradient descent (cf. Algorithm 1).Algorithm 2 Backpropagation computation of neural tree G 1: procedure Gradient ?wL(x, G)x and w are the inputs and trainable parameters 2:G ? ?Compute ? (y, G(x), N 0 ? G, G)Compute ? using Algorithm 3 for target y and prediction G(x) at tree's root N 0 3:for all nodes N in G ? do 4:if N ? T ype ? Output node then output node's bias weight update 5:N ? ?w b k = N ? ? k bias weight is set to current node's gradient ? 6:else if N ? T ype ? Internal (hidden) node then 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>T ype ? Output node then compute gradient ? k for output node N in G 3: N ? ? k = (? ? y)?(1 ??) gradient at sigmoid output node 4: else compute gradient at an internal (hidden) neural node 5: h j = N ? h j activation (value) of an internal (neural) node 6:? k = N ? N P arent ? ? k retrieve parent's gradient ? of the current node N 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>for all child(neural)  node N c of N in G do 11: G ? ?Compute ? (?, ?, N c ? G, G) call Compute ?, ?indicates unused argument We select a set of nine classification problems: Australia (Aus), Heart (Hrt), Ionosphere (Ion), Pima (Pma), Wisconsin (Wis), Iris (Irs), Wine (Win), Vehicle (Vhl), Glass (Gls),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>offline mode (epoch-by-epoch) training. For the pattern recognition problem (MNIST), we set a mini-batch training with a batch size of 128 examples. RMSprop was used as an optimizer, and BNeuralT was trained by varying learning rate ? ? {0.1, 0.01} and the number of epochs ? {10, 25, 50, 70}. The results of other algorithms on MNIST were collected from literature to compare performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>These variations produced five BNeuralT settings: (i) ES training of BNeuralT having sigmoid nodes, 0.1 learning rate, and 0.5 leaf generation rate; (ii) ES training of BNeuralT having sigmoid nodes, default learning rate, and 0.5 leaf generation rate; (iii) ES training of BNeuralT having ReLU nodes, 0.1 learning rate, and 0.5 leaf generation rate; (iv) ES training of BNeuralT having sigmoid nodes, 0.1 learning rate, and 0.4 leaf generation rate; and (v) without ES training of BNeuralT having sigmoid nodes, 0.1 learning rate, and 0.5 leaf generation rate. Multiple MLP settings were tried. Out of which, some best performing settings were: (i) ES training of MLP having sigmoid nodes and 0.1 learning rate; (ii) ES training of MLP having sigmoid nodes and default learning rate; (iii) ES training of MLP having sigmoid nodes, 0.1 learning rate, and L2-norm regularization; (iv) ES training of MLP having sigmoid nodes, default learning rate, and L2-norm regularization; and (v) experiment setting same as (i) but without ES; and (vi) experiment setting same as (ii) but without ES. Other trials were using dropout with and without early stopping. For each algorithm (BNeuralT, MLP, HFNT S , HFNT M , MONT 3 , DT, RF GP, NBC, and SVM), each optimizer (GD, MGD, NAG, Adagrad, RMSprop, and Adam), and each variation of hyperparameter setting, there were 110 experiments (cf. Tables A2 and A3 in Supplementary).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>that BNeuralT performance on both classification and regression problems is highly competitive with MLP and other algorithms. For example, the average performance of BNeuralT's RMSprop on all classification problems is 2.65% (average accuracy: 89.1%) higher than the nearest best performing non-BNeuralT algorithm. The best MLP model offered an average accuracy of 86.8%, and MLP with a 0.4 dropout rate using Adam produced an 85.9% accuracy. Other algorithms were as follows: HFNT S , 78.9%; HFNT M , 72.4%; MONT 3 , 83.1%; DT, 81.3%; RF, 86.4%; GP, 86.8%; NBC, 78.2%; and SVM, 84.1%. For this performance, BNeuralT uses only 13.25% (w = 261) trainable parameters than MLP's 1969 parameters. The structures of some select best performing BNeuralT classification models are shown in Fig. 5, where black edges indicate dendrites;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5 :</head><label>5</label><figDesc>Classification Trees. (a) -(i) show test accuracy and tree size of select high performing trees of datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>an example BNeuralT (20K) model's training convergence and MNIST character classification performance on a receiver operating characteristic curve plot, where BNeuralT (20K) model for all classes produces a very high sensitivity (true-positive rate) and very low specificity (low false-positive rate). Of all classes, we may arrange classes on the scale of hardness of learnability in the order of "easiest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 :</head><label>9</label><figDesc>Early-stopping training of BNeuralT having sigmoid nodes, 0.1 learning rate, and 0.4 leaf generation rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 :</head><label>10</label><figDesc>BNeuralT average convergence trajectory performance computed over 30 independent runs for six optimizers over five regression problems. The x-axis, log 10 (epochs) has the range [0.0, 2.7] and is the training epochs 1 to 500. The y-axis, log 10 (L MSE (G)) is the training and test sets mean square error (MSE) on the log scale. An MSE 0.01 on the log scale has a value of ?2.0. Thus, a lower value is better. Error bar is the standard deviation of log 10 (LMSE(G)) and it indicates stochasticity of the convergence that helps an optimizer escape local minima better. Thus, a larger length is better. For each dataset, training and test convergence pair are plotted for 500 epochs (on the log scale, 2.7). The optimizers RMSprop, MGD, NAG, Adagrad, GD, and Adam are respectively indicated in blue, orange, green, red, purple, and brown colors with respective symbols diamond, triangle, circle, down triangle, and star.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Comparison of convergence of optimizers for optimizing BNeuralT and MLP and optimizers convergence over varied learning rate settings and activation function usage. (a)-(f) Classification problem where y-axis is an error rate. (g)-(l) Regression problems where y-axis is an MSE. The x-axis, log 10 (epochs) has the range [0.0, 2.7] and is the training epochs 1 to 500. For MLP, early-stopping method show values for optimizes only upto the epochs where training stopped. Learning rate ? value def ault indicates that RMSprop, Adam, and Adagrad has a value of 0.001 as their learning rate and GD, NAG, and MGD has a value of 0.01. BNeuralT model's tree size |G| (x-axis) compared with its average (avg.) accuracy Lacc(G) = (1 ? LError(G)) (y-axis has range [0, 1]) on the training sets of nine classification problems, five regression problems, and one pattern recognition problem. For the pattern recognition problem, BNeuralT model's tree size |G| (xaxis) is in the multiple of 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>BNeuralT (a) classification (b) regression models having sigmoid nodes, 0.5 leaf generation rate, 0.1 learning rate, and early-stopping strategy setting. (a) RMSprop in blue is the fastest converging optimizer as it appears at the top line in plots. Adagrad in red shows slow convergence in the beginning and only rapidly improves at higher epochs. Adam and RMSprop have similar trajectories at earlier epochs, but Adam stays at local minima. (b) Adagrad and RMSprop both appear as bottom lines in graphs showing better convergence than others. BNeuralT (a) classification (b) regression models having sigmoid nodes, 0.5 leaf generation rate, default learning rate, and early-stopping strategy setting. (a) NAG in green is the fastest converging and most dominant optimizer in plots. Adagrad in red is the slowest. Adam and RMSprop have a similar trajectory, and both show rapid convergence at higher epochs. (b) RMSprop and NAG both appear as bottom lines in graphs showing better convergence than others. BNeuralT (a) classification (b) regression models having ReLU nodes, 0.5 leaf generation rate, 0.1 learning rate, and early-stopping strategy setting. (a) Adagrad in red is the fastest converging and most dominant optimizer in plots. The downward curve of other algorithms, except Adagrad, indicates that, when ReLU was used, not all structures could be trained. This is because of exploding gradient issue. This is because of exploding gradient issue. Hence the average trajectories show a downward curve (trajectory) in performance after certain epochs. This phenomenon is indicated with a sudden upward curve in plots (b) for regression. (b) Adagrad in red is the fastest converging and most dominant optimizer in plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>MLP (a) classification (b) regression with sigmoid nodes trained with 0.1 learning rate and earlystopping strategy. (a) Adagrad in red in this setting seems taking more iteration as red lines in test sets plots show more fluctuation and longer epochs and lines are on the upper side of plots. (b) NAG is green shows better performance as lines are lower in plots. MLP (a) classification (b) regression with sigmoid nodes trained with default learning rate and earlystopping strategy. (a) NAG and MGD, in green and orange in this setting, show better performance as on the test sets, and they are on the upper side of plots. Adam is competitively closer to NAG and MGD. (b) Adam, GD, and RMSprop in respective order show better performance as lines are lower in plots, and GD takes longer epochs to converge in early stopping. MLP (a) classification (b) regression with sigmoid nodes trained with 0.1 learning rate and without an early-stopping strategy. (a) Adagrad and GD in red and purple in this setting show more fluctuation as lines are on the upper side of plots. (b) GD and NAG in purple and green show better performance as lines are lower in plots. MLP (a) classification (b) regression with sigmoid nodes trained with default learning rate and without an early-stopping strategy. (a) Adam and NAG in brown and green in this setting show more fluctuation as lines are on the upper side of plots. (b) Adam, RMSprop, and NAG in brown, blue, and green show better performance as lines are lower in plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Biologically plausible neural computation using dendritic trees. Red circle represents a neuron (soma), black lines are dendrites, and the numbers indicate inputs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">4 3 3</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>1</cell><cell></cell><cell>1</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>2</cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>2</cell><cell></cell><cell>1</cell><cell cols="2">4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(c) BNeuralT</cell><cell></cell><cell></cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 presents</head><label>3</label><figDesc>Kolmogorov-Smirnov (KS) test results on two samples to examine the null hypothesis that there is no difference between the performance distributions of BNeuralT's RMSprop and other algorithms. The results show that for most datasets and most algorithms, the classification results of BNeuralT's RMSprop show statistical significance over other algorithms' performance as the null hypothesis of no difference is rejected in most cases. This was the case, despite using a restrictive Bonferroni correction to adjust p-values. Wilcoxon signed-rank test and Independent T-test in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>also suggest that BNeuralT's RMSprop performance distribution on regression problems compared with MLP's Adam is statistically insignificant only on the Friedman dataset. On all other regression datasets, BNeuralT's RMSprop performance is equally significant as other algorithms.</figDesc><table><row><cell>(a) australian (92%, 63)</cell><cell>(b) heart (96%, 173)</cell><cell>(c) ionosphere (99%, 60)</cell></row><row><cell>(d) pima (87%, 125)</cell><cell>(e) wiscosin (100%, 85)</cell><cell>(f) iris (100%, 86)</cell></row><row><cell>(g) wine (100%, 28)</cell><cell>(h) vehicle (82%, 155)</cell><cell>(i) glass (88%, 466)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>BNeuralT and other algorithms performance as per average (avg.) accuracy (1 ? LError(?)) and avg. regression fit (L r 2 (?)) on the test sets of 30 independent runs for nine classification and five regression learning problems. Both accuracy and regression fit take 1.0 as the best value. Trainable parameters (w) of BNeuralT and MLP are neural weights. The best accuracy among all algorithms is marked in bold. Both BNeuralT and MLP are trained on an online mode, whereas other algorithms take offline mode training. Average forward pass (single example prediction time) wall-clack time is ? ? e ?6 seconds (a lower value is better), where ? J and ? P respectively indicate time for Java 11 and Python 3.7. Symbol M0.4 indicates MLP with a dropout rate of 0.4, (Avg.) indicates the average performance of an optimizer over datasets, and [Avg.] indicates the average performance of optimizers on a dataset. The classifier results of models MONT3 and HFNT M are from<ref type="bibr" target="#b30">Ojha and Nicosia (2020</ref>). RF ? indicates random forest which is an ensemble model. .874 .789 .968 .947 .953 .671 .580 .832 .485 .748 .323 .653 .803 .602 MGD .886 .879 .935 .806 .980 .988 .980 .726 .687 .874 .585 .804 .434 .763 .849 .687 NAG .886 .878 .938 .808 .980 .987 .978 .731 .688 .875 .585 .804 .434 .757 .851 .686 Adagrad .872 .852 .907 .780 .974 .966 .981 .697 .638 .852 .621 .819 .432 .820 .851 .708 RMSprop .895 .897 .952 .822 .986 .992 .991 .750 .732 .891 .665 .837 .492 .776 .867 .727 Adam .875 .866 .870 .791 .982 .978 .974 .599 .657 .843 .579 .765 .360 .587 .825 .623 [Avg.] .880 .870 .913 .799 .978 .976 .976 .696 .663 .861 .587 .796 .412 .726 .841 .672 MLP (M) GD .870 .831 .880 .763 .979 .968 .973 .806 .614 .854 .697 .821 .481 .736 .844 .716 MGD .874 .831 .907 .774 .981 .977 .985 .853 .629 .868 .718 .826 .486 .804 .852 .737 NAG .873 .828 .902 .772 .980 .976 .984 .852 .640 .868 .718 .826 .485 .786 .851 .733 Adagrad .870 .827 .723 .670 .944 .736 .877 .462 .362 .719 .407 .584 .221 .571 .630 .482 RMSprop .874 .832 .872 .758 .980 .969 .991 .804 .605 .854 .718 .826 .486 .866 .866 .752 Adam .876 .833 .882 .774 .984 .972 .991 .826 .635 .863 .721 .829 .490 .943 .874 .772 [Avg.] .873 .830 .861 .752 .975 .933 .967 .767 .581 .838 .663 .785 .442 .784 .820 .699 M0.4Adam .871 .815 .912 .774 .971 .960 .973 .806 .645 .859 .707 .828 .491 .884 .862 .754 Trees HFNT S .824 .825 .871 .754 .973 .912 .918 .481 .544 .789 .576 .794 -0.062 .728 .775 .562 HFNT M. .689 .756 .484 RF ? .868 .809 .930 .752 .962 .959 .981 .745 .768 .864 .663 .829 .442 .871 .871 .735 [Avg.] .842 .791 .882 .745 .953 .923 .914 .579 .608 .804 .458 .605 .081 .601 .633 .476 Others GP .861 .820 .916 .769 .970 .960 .983 .843 .689 .868 .648 .820 .484 .724 .801 .695 NBC .797 .833 .882 .758 .930 .953 .969 .455 .457 .782 SVM .861 .841 .880 .769 .977 .926 .978 .762 .575 .841 .647 .838 .405 .912 .861 .733 [Avg.] .840 .831 .893 .765 .959 .946 .977 .687 .574 .830 .648 .829 .445 .818 .831 .714</figDesc><table><row><cell></cell><cell>Avg. classification accuracy [1 ? LError(?)]</cell><cell>Avg. regression fit L r 2 (?)</cell></row><row><cell cols="2">Algorithm Aus Hrt Ion Pma Wis Irs Win Vhl Gls (Avg.)</cell><cell>Bas Dee Dia Frd Mpg (Avg.)</cell></row><row><cell>GD</cell><cell>.862 .848</cell></row><row><cell>BNeuralT (G)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Kolmogorov-Smirnov (KS)  test on two samples: BNeuralT's RMSprop against all other algorithms for each data. The stat, pval, and post respectively indicate KS statistic, two-tailed p-value, and Bonferroni correction post-hoc adjusted p-value. The values are marked in bold where the null hypothesis that BNeuralT's RMSprop and other algorithms come from the same distribution is rejected as per Bonferroni correction. RMSprop stat .33 .60 .77 .67 .40 .50 .23 .60 .73  .30 .23 .20 .57 .20  Adam stat .37 .50 .53 .57 .40 .77 .30 .63 .70  .30 .40 .17 .50 .17  RF stat .43 .73 .40 .90 .80 .57 .47 .30 .33  .30 .20 .37 .77 .20  SVM stat .50 .53 .70 .67 .30 .70 .47 .20 .77  .37 .13 .50 .90  .Regression Trees. (a) -(e) show best performing tree structure of the respective dataset, their accuracy and tree size shown in brackets. The red node in a tree is the root node (output node), function nodes are blue, and leaf nodes are green. The link connecting nodes are neural weights.</figDesc><table><row><cell></cell><cell>BNeuralT's</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Regression</cell></row><row><cell></cell><cell>RMSprop vs.</cell><cell cols="9">Aus Hrt Ion Pma Wis Irs Win Vhl Gls</cell><cell cols="4">Bas Dee Dia Frd Mpg</cell></row><row><cell></cell><cell cols="10">GD stat .43 .63 .73 .70 .47 .67 .60 .63 .70</cell><cell cols="4">.27 .30 .23 .73 .43</cell></row><row><cell></cell><cell cols="2">pval .01</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="3">.24 .14 .39</cell><cell>0 .01</cell></row><row><cell></cell><cell cols="2">post .07</cell><cell cols="2">0 0</cell><cell cols="3">0 .03 0</cell><cell>0</cell><cell cols="2">0 0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0 .06</cell></row><row><cell></cell><cell cols="10">MGD stat .40 .63 .53 .60 .43 .47 .30 .87 .60</cell><cell cols="4">.30 .20 .20 .20 .37</cell></row><row><cell></cell><cell cols="2">pval .02</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .01</cell><cell cols="2">0 .14</cell><cell>0</cell><cell>0</cell><cell cols="4">.14 .59 .59 .59 .03</cell></row><row><cell></cell><cell cols="2">post .16</cell><cell cols="2">0 0</cell><cell cols="3">0 .07 .03</cell><cell>1</cell><cell cols="2">0 0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1 .31</cell></row><row><cell></cell><cell cols="10">NAG stat .43 .63 .63 .57 .43 .47 .33 .87 .53</cell><cell cols="4">.30 .20 .17 .33 .33</cell></row><row><cell></cell><cell cols="2">pval .01</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .01</cell><cell cols="2">0 .07</cell><cell>0</cell><cell>0</cell><cell cols="4">.14 .59 .81 .07 .07</cell></row><row><cell></cell><cell cols="2">post .07</cell><cell cols="2">0 0</cell><cell cols="4">0 .07 .03 .71</cell><cell cols="2">0 0</cell><cell>1</cell><cell>1</cell><cell cols="2">1 .64 .64</cell></row><row><cell>MLP</cell><cell cols="4">Adagrad stat .43 .67 pval .01 0 post .07 0 0 1 0</cell><cell cols="4">1 .87 0 0 0 0 0 1 .83 0 0 0</cell><cell cols="2">1 0 0 0 1 0</cell><cell>.83 0 0</cell><cell cols="3">1 .93 .93 0 0 0 0 0 0</cell><cell>1 0 0</cell></row><row><cell></cell><cell cols="2">pval .07</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .02</cell><cell cols="2">0 .39</cell><cell>0</cell><cell>0</cell><cell cols="3">.14 .39 .59</cell><cell>0 .59</cell></row><row><cell></cell><cell cols="2">post .71</cell><cell cols="2">0 0</cell><cell cols="3">0 .16 .01</cell><cell>1</cell><cell cols="2">0 0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell></cell><cell cols="10">Adam stat .30 .63 .67 .63 .47 .50 .17 .70 .63</cell><cell cols="3">.30 .17 .17</cell><cell>1 .20</cell></row><row><cell></cell><cell cols="2">pval .14</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .81</cell><cell>0</cell><cell>0</cell><cell cols="3">.14 .81 .81</cell><cell>0 .59</cell></row><row><cell></cell><cell>post</cell><cell>1</cell><cell cols="2">0 0</cell><cell cols="3">0 .03 .01</cell><cell>1</cell><cell cols="2">0 0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell></cell><cell cols="2">M 0.4 pval .03</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .02</cell><cell cols="2">0 .14</cell><cell>0</cell><cell>0</cell><cell cols="3">.14 .02 .81</cell><cell>0 .81</cell></row><row><cell></cell><cell cols="4">post .45 .01 0</cell><cell cols="3">0 .20 0</cell><cell>1</cell><cell cols="2">0 0</cell><cell cols="2">1 .19</cell><cell cols="2">1 .01</cell><cell>1</cell></row><row><cell></cell><cell cols="10">HFNT S stat .67 .70 .87 .70 .53 .53 .73 .93 .73</cell><cell cols="2">.47 .37</cell><cell></cell><cell>.40 .40</cell></row><row><cell></cell><cell>pval</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .03</cell><cell></cell><cell>.02 .02</cell></row><row><cell></cell><cell>post</cell><cell>0</cell><cell cols="2">0 0</cell><cell>0</cell><cell cols="2">0 0</cell><cell>0</cell><cell cols="2">0 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell cols="8">HFNT M stat .67 .87 .83 .87 .63 .53 .77</cell><cell cols="2">1 .87</cell><cell cols="2">.43 .40</cell><cell></cell><cell>.63 .43</cell></row><row><cell></cell><cell>pval</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">.01 .02</cell><cell></cell><cell>0 .01</cell></row><row><cell>Trees</cell><cell cols="10">post DT stat .90 .87 .70 .93 .87 .67 .90 .53 .33 0 0 0 0 0 0 0 0 0</cell><cell cols="4">1 .67 .90 .93 .77 .87 1 1 .85 .85</cell></row><row><cell></cell><cell>pval</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .07</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>post</cell><cell>0</cell><cell cols="2">0 0</cell><cell>0</cell><cell cols="2">0 0</cell><cell>0</cell><cell cols="2">0 .71</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell cols="2">pval .01</cell><cell cols="2">0 .02</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="3">0 .14 .07</cell><cell cols="3">.14 .59 .03</cell><cell>0 .59</cell></row><row><cell></cell><cell cols="2">post .09</cell><cell cols="2">0 .20</cell><cell>0</cell><cell cols="3">0 0 .03</cell><cell cols="2">1 .92</cell><cell>1</cell><cell cols="2">1 .41</cell><cell>0</cell><cell>1</cell></row><row><cell></cell><cell cols="10">GP stat .43 .63 .57 .67 .43 .50 .33 .83 .30</cell><cell cols="4">.40 .37 .17 .73 .90</cell></row><row><cell></cell><cell cols="2">pval .01</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .01</cell><cell cols="2">0 .07</cell><cell cols="2">0 .14</cell><cell cols="3">.02 .03 .81</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell cols="2">post .07</cell><cell cols="2">0 0</cell><cell cols="4">0 .07 .01 .71</cell><cell>0</cell><cell>1</cell><cell cols="2">.14 .31</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Other</cell><cell cols="8">NBC stat .93 .57 .77 .77 .93 .57 .50 pval 0 0 0 0 0 0 0 post 0 0 0 0 0 0 .01</cell><cell cols="2">1 .97 0 0 0 0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell></row><row><cell></cell><cell>pval</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0 .14</cell><cell>0</cell><cell cols="2">0 .59</cell><cell>0</cell><cell cols="2">.03 .96</cell><cell>0</cell><cell>0 .59</cell></row><row><cell></cell><cell cols="2">post .01</cell><cell cols="2">0 0</cell><cell>0</cell><cell cols="3">1 0 .03</cell><cell cols="2">1 0</cell><cell>.31</cell><cell cols="2">1 .01</cell><cell>0</cell><cell>1</cell></row></table><note>BNeuralT pattern recognition (MNIST) models summary. RMSprop optimizer was found robust and converging fastest for classification models (cf. Sec. 5.3). Hence, we train BNeuralT on the MNIST character classification dataset (LeCun et al., 2020) using RMSprop.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 presents</head><label>4</label><figDesc>MNIST (pixels) results compared with classification trees. BNeuralT performs the best among the reported trees that work on MNIST (pixels) for character classification.However, convolution of images has been proven efficient for image classification problems. For example, CapsNet<ref type="bibr" target="#b40">(Sabour et al., 2017)</ref>, a state-of-the-art algorithm on MNIST (convolution), has an error rate of 0.25, but it uses 8 million parameters. Compared to that, a BNeuralT on MNIST (pixels) used 23, 835 trainable parameters for an error rate of 6.08, and another model used 241, 999 trainable parameters for an error rate of 5.19. Obviously, there is a trade-off between the model's parameters size and accuracy. The performances of a varied range of other algorithms on MNIST dataset are available at<ref type="bibr" target="#b20">(LeCun et al., 2020)</ref>. Our goal is to use as much compact model as we can for high accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Test error rate % of ad hoc BNeuralT (G) models with a varied number of trainable parameters on the MNIST dataset. All models are trained for 70 epochs, and where denoted ? is for 25 epochs. The decision tree models are reported in(Zharmagambetov  et al., 2019). BNeuralT-20K (pixels) MNIST model (tree structure). This model has 3,664 function nodes (blue nodes), 16,507 leaf nodes (green nodes), and ten class nodes (red nodes in the inner circle), and the root node (in black) in the center. This model has 6,738 edges (gray lines connecting nodes). These lines also represent neural weights. Each blue node also has its bias. Edge weights and bias together make 23,835 tree's trainable parameters. This model has a test accuracy of 94% (an error rate of 6.08%).</figDesc><table><row><cell></cell><cell>Algorithms</cell><cell>Error(%)</cell></row><row><cell>BNeuralTs</cell><cell>BNeuralT-10K (pixels) BNeuralT-18K (pixels) BNeuralT-20K (pixels) BNeuralT-200K  ? (pixels)</cell><cell>7.74 6.58 6.08 5.19</cell></row><row><cell>Classification Trees</cell><cell>GUIDE (pixels, oblique split) OC1 (pixels, oblique split) GUIDE (pixels) CART-R (pixels) CART-P (pixels) C5.0 (pixels) TAO (pixels) TAO (pixels, oblique split)</cell><cell>26.21 25.66 21.48 11.97 11.95 11.69 11.48 5.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The optimizers RMSprop, MGD, NAG, Adagrad, GD, and Adam are respectively indicated in blue, orange, green, red, purple, and brown colors, respectively, with symbols diamond, triangle, circle, downward triangle, and star. For a few cases, convergence is linear to tree size, however for a few, high accuracy is achieved with smaller trees. For MNIST, RMSprop has 10-epoch of stochastic online training and shows a linear relation with tree size. In all tasks, the convergence of RMSprop (blue diamonds) is the best, followed by NAG (green squares) and MGD (yellow triangles). Adagrad (red circles) shows competitive performance with RMSprop (blue diamonds) for regression problems. Classification datasets have green and blueish hues because NAG and RMSprop are top optimizers, and for regression datasets, plots appear to be red and blueish hue because of RMSprop and Adagrad are top optimizers.Since BNeuralT resembles a highly sparse NN, its performance was assessed against MLP (and MLP with dropout rate similar to the probability of keeping nodes in BNeuralT) for their similar versions of SGD training. Six classification trees of BNeuralT, among all other algorithms and experiments, were top-performing models with a very low number of parameters. In fact,</figDesc><table><row><cell>BNeuralT performed better against MLP dropout on classification problems, and it statistically</cell></row><row><cell>had a similar performance on regression problems. This BNeuralT's performance against MLP's</cell></row><row><cell>dropout regularization technique confirms that</cell></row><row><cell>stochastic gradient descent training of any a priori arbitrarily "thinned" network</cell></row><row><cell>has the potential to solve machine learning tasks with equivalent or better degree of</cell></row><row><cell>accuracy than a fully connected symmetric and systematic NN architecture.</cell></row></table><note>We used six different SGD optimizers for optimizing BNeuralT and MLP. Each optimizer be- haved differently in terms of their asymptotic convergence depending on what problem they solve, how their learning rate behaved over the training epochs, and what activation function was used (cf. Figs. 9, 10, and 11). For example, with a 0.1 learning rate, RMSprop was the best among others for BNeuralT optimization over classification problems. For regression prob- lems, both RMSprop and Adagrad performed well. Adagrad, however, was slow on classification</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 2 )</head><label>2</label><figDesc>suggests that BNeuralT offers an efficient alternative to these algorithms as BNeuralT 29 does not make any assumption about data to generate a hypothesis (model) when fitting or classifying data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Hada, S. S., Carreira-Perpi??n, M.?. and Gabidolla, M. (2019), 'An experimental comparison of old and new decision tree algorithms', arXiv:1911.03054 .A SupplementarySupplementary has three Sections: Sec A.1 contains URLs of source code repositories, README, trained models, and links to data. Sec.A.2 offers convergence trajectories of BNeuralT and MLP.</figDesc><table><row><cell cols="4">Zharmagambetov, A., Sec.A.3 supplements Table 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">References</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bache,</cell><cell>K.</cell><cell>and</cell><cell>Lichman,</cell><cell>M.</cell><cell>(2013),</cell><cell>'UCI</cell><cell>machine</cell><cell>learning</cell><cell>repository',</cell></row><row><cell cols="8">https://archive.ics.uci.edu/ml/index.php (Accessed on: 01 Apr 2020).</cell><cell></cell><cell></cell></row><row><cell cols="10">Bengio, Y., Boulanger-Lewandowski, N. and Pascanu, R. (2013), Advances in optimizing recurrent net-</cell></row><row><cell cols="10">works, in 'IEEE International Conference on Acoustics, Speech and Signal Processing', IEEE, pp. 8624-</cell></row><row><cell>8628.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Beniaguev, D., Segev, I. and London, M. (2020), 'Single cortical neurons as deep artificial neural net-</cell></row><row><cell cols="4">works', bioRxiv p. 613141.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Bishop, C. M. (2006), Pattern recognition and machine learning, Springer.</cell><cell></cell><cell></cell></row><row><cell cols="10">Blumer, A., Ehrenfeucht, A., Haussler, D. and Warmuth, M. K. (1987), 'Occam's razor', Information</cell></row><row><cell cols="4">processing letters 24(6), 377-380.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Breiman, L. (2001), 'Random forests', Machine Learning 45(1), 5-32.</cell><cell></cell><cell></cell></row><row><cell cols="10">Breiman, L., Friedman, J., Stone, C. J. and Olshen, R. A. (1984), Classification and regression trees,</cell></row><row><cell cols="2">CRC Press.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1-tree and 32-tree biologically plausible dendritic tree algorithms, was found competitive. More- over, BNeuralT performed best among select tree-based classifiers for the classification of MNIST characters. On classification and regression problems, the overall performance of BNeuralT was better than some varied types of well-known algorithms: decision tree, random forest, Gaussian process, na?ve Bayes classifier, and support vector machine. Such a performance of BNeuralT came from a minimal hyperparameter setup. Therefore, this work shows that our newly designed learning algorithm generates high-performing and parsimonious (therefore sustainable) models balancing the complexity with descriptive ability.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 4 )</head><label>4</label><figDesc>Experiments produced pre-trained models and results are available at:</figDesc><table><row><cell>https://github.com/vojha-code/BNeuralT/tree/master/trained_models</cell></row><row><cell>A.1.4 Data</cell></row><row><cell>Classification and regression learning problems:</cell></row><row><cell>https://github.com/vojha-code/BNeuralT/tree/master/data</cell></row><row><cell>For these datasets, the exact sequence of all 30 independent runs for training and test can be</cell></row><row><cell>found in models files pre-trained models.</cell></row><row><cell>Pattern recognition problem (MNIST): http://yann.lecun.com/exdb/mnist/</cell></row><row><cell>A.2 Supplementary Figures</cell></row><row><cell>Figs. A1, A2, A3, A4, A5, A6, and A7 are the average training and test convergence perfor-</cell></row><row><cell>mance computed over 30 independent runs for six optimizers of BNeuralT and MLP over nine</cell></row><row><cell>classification and five regression datasets. The x-axis in Figs. A1, A2, A3, A4, A5, A6, and A7</cell></row><row><cell>are log 10 (epochs) that has range [0.0, 2.7] and is the training epoch range [1, 500]. Optimizers</cell></row><row><cell>RMSprop, MGD, NAG, Adagrad, GD, and Adam are indicated in blue, orange, green, red,</cell></row><row><cell>purple, and brown, respectively, with symbols diamond, triangle, circle, downward triangle, and</cell></row></table><note>star. In each plot is labeled with the name of the dataset and set type. For example, iris (train) and iris (test) represent training set and test set convergence of optimizers on iris data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Adam, however, does perform well with a default (0.001) learning rate. Adagrad shows poor converges with a slower learning rate as it does in the cases of BNeuralT. NAG and MGD, in the cases of both classification and regression problems, show a stable convergence profile.</figDesc><table><row><cell>without early-stopping, but the</cell></row><row><cell>optimizers RMSprop, Adam, and Adagrad had a learning rate of 0.001 and optimizer MGD,</cell></row><row><cell>NAG, and GD had a learning rate of 0.01.</cell></row><row><cell>The convergence profiles of the optimizers for MLP were not consistent, with one optimizer</cell></row><row><cell>outperforming all other optimizers across all datasets [cf. Figs. A4 (only upto ES epochs),</cell></row><row><cell>A5 (only upto ES epochs), A6 (full epochs), and A7 (full epochs)]. RMSprop for learning</cell></row><row><cell>rate 0.1 performed better on four datasets. Adagrad performed relatively consistent among all</cell></row><row><cell>optimizers. Adam performed worse in cases of regression problems with a learning rate of 0.1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A1 :</head><label>A1</label><figDesc>Hyperparameter definition and choices used during the experiments of the algorithms. BNeuralT and MLP parameters setting are listed in detail. Some of the basic settings as recommended in the library or literature of HFNT, DT/RF, GP, NCB, and SVM are listed in this table.</figDesc><table><row><cell></cell><cell>Parameter</cell><cell>Definition</cell><cell>Default Rang</cell><cell>Choices used</cell></row><row><cell></cell><cell>Scaling</cell><cell>Input-features scaling range.</cell><cell>[min, max],</cell><cell>[0, 1]</cell></row><row><cell>BNeuralT</cell><cell>Tree height Tree arity Tree edge P [leafp &lt; p]</cell><cell cols="3">Maximum depth (layers) of a tree model. {p ? Z &gt; 1} Maximum arguments of a node m. {m ? Z ? 2} Initialization of neural weights of tree [w l , wu], w l ? R, wu ? R [0, 1] {5, 10} {5, 10} Leaf generation at depth &lt; p P [leafp &lt; p] ? [0, 1] {0.4, 0.5}</cell></row><row><cell></cell><cell>Internal nodes</cell><cell>An activation function</cell><cell cols="2">{Sigmoid, ReLU, tanh} {Sigmoid, ReLU}</cell></row><row><cell>MLP</cell><cell cols="2">Layers Hidden nodes Activation nodes An activation function Number of hidden layers in architecture Number of nodes at hidden layer</cell><cell cols="2">1 100 {Sigmoid, ReLU, tanh} {Sigmoid, ReLU}</cell></row><row><cell></cell><cell>Learning rate</cell><cell>Optimizer's learning rate</cell><cell>? ? [0, 1]</cell><cell>{0.1, 0.01, 0.001}</cell></row><row><cell>Optimizers</cell><cell cols="2">Momentum rate Optimizers momentum rate Training epochs Condition for algorithm termination Early-stopping Stopping algorithm to avoid overfitting Method of early-stopping</cell><cell cols="2">? ? [0, 1] {Epochs ? Z &gt; 1} [0, Epochs] Fallback ? [True, False] True {0.9} 500 {50}</cell></row><row><cell></cell><cell>Algorithms</cell><cell>Algorithms for optimization</cell><cell cols="2">{GD, MGD, NAG, Adagrad, RMSprop,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Adam, L-BFGS (Liu and Nocedal, 1989)}</cell></row><row><cell></cell><cell>Tree setup</cell><cell cols="3">Height = 10, arity = 5, edge = [0, 1], leaf generation is attribute dependent</cell></row><row><cell>HFNT</cell><cell>Internal node Structure</cell><cell cols="3">Takes arbitrarily any function from {Gaussian, Sigmoid, tanh, fermi, etc.} Single and multi objective genetic programming with 50 population and 500 generations</cell></row><row><cell></cell><cell>Parameter</cell><cell cols="3">Differential evaluation with 100 population and 1000 generations, 0.9 crossover rate</cell></row><row><cell></cell><cell>Tree depth</cell><cell>Restriction max depth of the tree</cell><cell>{p ? Z &gt; 1}</cell><cell>Unbounded</cell></row><row><cell>DT/RF</cell><cell>Max leaf node Split Criteria</cell><cell>Restriction on max leaf node of a tree Strategy for splitting a decision node Decision criteria about an input</cell><cell>Classification</cell><cell>Unbounded "best" "gini index"</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Regression</cell><cell>MSE</cell></row><row><cell></cell><cell>Random forest</cell><cell>Data sampling method for tree generation</cell><cell></cell><cell>bootstrap</cell></row><row><cell></cell><cell>Estimators</cell><cell>The number of trees in the forest</cell><cell></cell><cell>100</cell></row><row><cell></cell><cell>Optimizer</cell><cell>Algorithm used for kernel optimization</cell><cell></cell><cell>L-BFGS</cell></row><row><cell></cell><cell cols="2">Training iteration Condition for algorithm termination</cell><cell>{Epochs ? Z &gt; 1}</cell><cell>100</cell></row><row><cell>GP</cell><cell>Classification</cell><cell>Method of multi-class classification</cell><cell>{1 vs rest, 1 vs 1}</cell><cell>{1 vs rest}</cell></row><row><cell></cell><cell>Kernel</cell><cell>Gaussian kernel used for learning</cell><cell>Classification</cell><cell>radial basis</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Regression</cell><cell>dot product +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>white noise</cell></row><row><cell></cell><cell>Kernel</cell><cell>Kernel used for learning</cell><cell></cell><cell>radial basis</cell></row><row><cell>SVM</cell><cell>Classification Penalty Training loss</cell><cell>Method of multi-class classification Regularization to avoid overfitting Loss function used during training</cell><cell>{1 vs rest, 1 vs 1 } {L1, L2}</cell><cell>{1 vs rest} L1 hinge loss</cell></row><row><cell></cell><cell cols="2">Training iteration Condition for algorithm termination</cell><cell>{Epochs ? Z &gt; 1}</cell><cell>1000</cell></row><row><cell cols="2">NBC</cell><cell>It uses Gaussian function</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table A2 :</head><label>A2</label><figDesc>Summary of top (based on overall performance) 55 of 110 experiments on all data, on all algorithms, and all settings . The values are an average of 30 independent runs of each setting. Names of experiments start with B indicate BNeuralT and M indicate MLP. The next letter is S or R indicates sigmoid and ReLU activation function, ESy indicates early-stopping and ESn indicates no early-stopping, Rn or Ry indicates no regularization or elastic net regularization; L indicates default learning rate or absence of L indicates a learning rate of 0.1, Dr indicates dropout; p4 or p5 indicates leaf generation rate 0.4 or 0.5, and optimizers GD, MGD, NAG, Adagrad, RMSprop, and Adam are indicated with G, M, N, A, R, D, respectively.</figDesc><table><row><cell>Overall</cell><cell>Acc</cell><cell>? Acc</cell><cell>w</cell><cell>Classification</cell><cell>r 2</cell><cell>?</cell><cell>w</cell><cell>Regression</cell><cell>Acc</cell><cell>? Acc</cell><cell>w</cell></row><row><cell>B-S-ESy-p4-R</cell><cell>.832</cell><cell>.15</cell><cell>222</cell><cell>B-S-ESy-p4-R</cell><cell>.891</cell><cell>.10</cell><cell>261</cell><cell>M-S-ESy-Rn-N</cell><cell>.775</cell><cell>.16</cell><cell>1041</cell></row><row><cell>M-S-ESy-Rn-N</cell><cell>.832</cell><cell>.14</cell><cell>1638</cell><cell>B-S-ESy-p5-R</cell><cell>.888</cell><cell>.11</cell><cell>157</cell><cell>M-S-ESy-Rn-L-D</cell><cell>.772</cell><cell>.16</cell><cell>1041</cell></row><row><cell>M-S-ESy-Rn-L-D</cell><cell>.831</cell><cell>.14</cell><cell>1638</cell><cell>B-S-ESy-p4-N</cell><cell>.875</cell><cell>.11</cell><cell>261</cell><cell>M-R-ESn-Rn-L-G</cell><cell>.761</cell><cell>.17</cell><cell>1041</cell></row><row><cell>M-S-ESy-Rn-G</cell><cell>.823</cell><cell>.14</cell><cell>1638</cell><cell>B-S-ESy-p4-M</cell><cell>.874</cell><cell>.11</cell><cell>261</cell><cell>M-S-ESy-Rn-M</cell><cell>.759</cell><cell>.22</cell><cell>1041</cell></row><row><cell>M-S-ESy-Rn-M</cell><cell>.822</cell><cell>.17</cell><cell>1638</cell><cell>B-S-ESy-p5-N</cell><cell>.871</cell><cell>.11</cell><cell>157</cell><cell>M-S-ESn-Rn-L-Dr-D</cell><cell>.754</cell><cell>.15</cell><cell>1041</cell></row><row><cell>M-S-ESn-Rn-L-Dr-D</cell><cell>.821</cell><cell>.14</cell><cell>1638</cell><cell>B-S-ESy-p5-M</cell><cell>.871</cell><cell>.11</cell><cell>157</cell><cell>M-S-ESy-Rn-L-R</cell><cell>.752</cell><cell>.16</cell><cell>1041</cell></row><row><cell>M-S-ESy-Rn-L-M</cell><cell>.821</cell><cell>.14</cell><cell>1638</cell><cell>GP</cell><cell>.868</cell><cell>.10</cell><cell></cell><cell>M-S-ESn-Rn-L-D</cell><cell>.749</cell><cell>.19</cell><cell>1041</cell></row><row><cell>M-S-ESy-Rn-L-N</cell><cell>.820</cell><cell>.14</cell><cell>1638</cell><cell>M-S-ESy-Rn-L-M</cell><cell>.868</cell><cell>.12</cell><cell>1970</cell><cell>M-S-ESn-Rn-L-R</cell><cell>.745</cell><cell>.17</cell><cell>1041</cell></row><row><cell>RF</cell><cell>.818</cell><cell>.14</cell><cell></cell><cell>M-S-ESy-Rn-L-N</cell><cell>.868</cell><cell>.11</cell><cell>1970</cell><cell>M-S-ESy-Rn-G</cell><cell>.744</cell><cell>.14</cell><cell>1041</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table A3 :</head><label>A3</label><figDesc>Summary of last (based on overall performance) 55 of 110 experiments on all data, on all algorithms, and all settings . The values are an average of 30 independent runs of each setting. Names of experiments start with B indicate BNeuralT and M indicate MLP. The next letter is S or R indicates sigmoid and ReLU activation function, ESy indicates early-stopping and ESn indicates no early-stopping, Rn or Ry indicates no regularization or elastic net regularization; L indicates default learning rate or absence of L indicates a learning rate of 0.1, Dr indicates dropout; p4 or p5 indicates leaf generation rate 0.4 or 0.5, and optimizers GD, MGD, NAG, Adagrad, RMSprop, and Adam are indicated with G, M, N, A, R, D, respectively.</figDesc><table><row><cell>Overall</cell><cell>Acc</cell><cell>? Acc</cell><cell>w</cell><cell>Classification</cell><cell>r 2</cell><cell>?</cell><cell>w</cell><cell>Regression</cell><cell>Acc</cell><cell>? Acc</cell><cell>w</cell></row><row><cell>B-S-ESn-M</cell><cell>.759</cell><cell>.19</cell><cell>128</cell><cell>B-S-ESy-p5-L-R</cell><cell>.825</cell><cell>.15</cell><cell>162</cell><cell>M-R-ESn-Rn-N</cell><cell>.650</cell><cell>.30</cell><cell>1041</cell></row><row><cell>B-S-ESy-p5-L-D</cell><cell>.751</cell><cell>.19</cell><cell>138</cell><cell>M-R-ESn-Rn-L-R</cell><cell>.824</cell><cell>.10</cell><cell>1946</cell><cell>B-S-ESn-M</cell><cell>.649</cell><cell>.22</cell><cell>80</cell></row><row><cell>B-S-ESy-p4-G</cell><cell>.750</cell><cell>.21</cell><cell>222</cell><cell>M-S-ESy-Rn-D</cell><cell>.821</cell><cell>.15</cell><cell>1970</cell><cell>B-S-ESn-R</cell><cell>.646</cell><cell>.21</cell><cell>80</cell></row><row><cell>B-S-ESy-p5-D</cell><cell>.747</cell><cell>.21</cell><cell>131</cell><cell>B-S-ESn-N</cell><cell>.821</cell><cell>.13</cell><cell>154</cell><cell>B-S-ESy-p5-N</cell><cell>.644</cell><cell>.22</cell><cell>85</cell></row><row><cell>M-R-ESn-Rn-L-D</cell><cell>.747</cell><cell>.24</cell><cell>1569</cell><cell>M-R-ESn-Rn-L-D</cell><cell>.820</cell><cell>.11</cell><cell>1946</cell><cell>M-R-ESn-Rn-L-D</cell><cell>.644</cell><cell>.32</cell><cell>1041</cell></row><row><cell>M-S-ESn-Rn-Dr-N</cell><cell>.744</cell><cell>.17</cell><cell>1638</cell><cell>B-S-ESn-M</cell><cell>.820</cell><cell>.13</cell><cell>154</cell><cell>B-S-ESy-p5-M</cell><cell>.643</cell><cell>.22</cell><cell>85</cell></row><row><cell>B-S-ESy-p5-L-N</cell><cell>.740</cell><cell>.22</cell><cell>138</cell><cell>B-S-ESn-A</cell><cell>.819</cell><cell>.14</cell><cell>154</cell><cell>M-S-ESy-Rn-Dr-N</cell><cell>.628</cell><cell>.64</cell><cell>1041</cell></row><row><cell>B-S-ESy-p5-L-M</cell><cell>.740</cell><cell>.22</cell><cell>138</cell><cell>M-R-ESn-Rn-L-N</cell><cell>.817</cell><cell>.11</cell><cell>1946</cell><cell>B-S-ESy-p4-D</cell><cell>.623</cell><cell>.21</cell><cell>152</cell></row><row><cell>B-S-ESy-p5-G</cell><cell>.737</cell><cell>.22</cell><cell>131</cell><cell>B-R-ESy-p5-R</cell><cell>.817</cell><cell>.14</cell><cell>150</cell><cell>B-S-ESy-p4-G</cell><cell>.602</cell><cell>.23</cell><cell>152</cell></row><row><cell>M-S-ESy-Rn-Dr-N</cell><cell>.733</cell><cell>.41</cell><cell>1638</cell><cell>M-R-ESn-Rn-L-M</cell><cell>.816</cell><cell>.11</cell><cell>1946</cell><cell>B-S-ESn-G</cell><cell>.582</cell><cell>.24</cell><cell>80</cell></row><row><cell>M-R-ESn-Rn-L-A</cell><cell>.732</cell><cell>.16</cell><cell>1569</cell><cell>DT</cell><cell>.813</cell><cell>.11</cell><cell></cell><cell>B-S-ESy-p5-L-M</cell><cell>.580</cell><cell>.24</cell><cell>95</cell></row><row><cell>B-S-ESn-G</cell><cell>.720</cell><cell>.21</cell><cell>128</cell><cell>B-R-ESy-p5-G</cell><cell>.802</cell><cell>.18</cell><cell>150</cell><cell>B-S-ESy-p5-L-N</cell><cell>.580</cell><cell>.24</cell><cell>95</cell></row><row><cell>M-S-ESn-Rn-Dr-M</cell><cell>.714</cell><cell>.27</cell><cell>1638</cell><cell>M-S-ESn-Rn-N</cell><cell>.798</cell><cell>.15</cell><cell>1970</cell><cell>B-S-ESy-p5-D</cell><cell>.579</cell><cell>.21</cell><cell>85</cell></row><row><cell>HFNT S</cell><cell>.708</cell><cell>.32</cell><cell></cell><cell>B-S-ESn-G</cell><cell>.797</cell><cell>.14</cell><cell>154</cell><cell>B-S-ESy-p5-G</cell><cell>.570</cell><cell>.24</cell><cell>85</cell></row><row><cell>B-R-ESy-p5-A</cell><cell>.706</cell><cell>.22</cell><cell>125</cell><cell>B-R-ESy-p5-A</cell><cell>.795</cell><cell>.16</cell><cell>150</cell><cell>HFNT M</cell><cell>.567</cell><cell>.54</cell><cell></cell></row><row><cell>B-S-ESn-D</cell><cell>.705</cell><cell>.21</cell><cell>128</cell><cell>B-S-ESy-p5-L-D</cell><cell>.795</cell><cell>.16</cell><cell>162</cell><cell>HFNT S</cell><cell>.562</cell><cell>.44</cell><cell></cell></row><row><cell>HFNT M</cell><cell>.704</cell><cell>.37</cell><cell></cell><cell>M-S-ESn-Rn-D</cell><cell>.792</cell><cell>.15</cell><cell>1970</cell><cell>B-S-ESn-D</cell><cell>.557</cell><cell>.23</cell><cell>80</cell></row><row><cell>DT</cell><cell>.695</cell><cell>.27</cell><cell></cell><cell>M-S-ESy-Rn-Dr-N</cell><cell>.791</cell><cell>.17</cell><cell>1970</cell><cell>B-R-ESy-p5-A</cell><cell>.544</cell><cell>.24</cell><cell>81</cell></row><row><cell>B-R-ESy-p5-R</cell><cell>.689</cell><cell>.25</cell><cell>125</cell><cell>M-S-ESy-Rn-Dr-D</cell><cell>.791</cell><cell>.17</cell><cell>1970</cell><cell>B-R-ESy-p5-N</cell><cell>.484</cell><cell>.25</cell><cell>81</cell></row><row><cell>B-R-ESy-p5-G</cell><cell>.683</cell><cell>.32</cell><cell>125</cell><cell>M-S-ESn-Rn-M</cell><cell>.791</cell><cell>.17</cell><cell>1970</cell><cell>DT</cell><cell>.484</cell><cell>.34</cell><cell></cell></row><row><cell>B-R-ESy-p5-N</cell><cell>.677</cell><cell>.24</cell><cell>125</cell><cell>M-S-ESy-Rn-Dr-M</cell><cell>.789</cell><cell>.16</cell><cell>1970</cell><cell>B-R-ESy-p5-M</cell><cell>.483</cell><cell>.25</cell><cell>81</cell></row><row><cell>B-R-ESy-p5-M</cell><cell>.676</cell><cell>.24</cell><cell>125</cell><cell>HFNT S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table A4 :</head><label>A4</label><figDesc>Wilcoxon signed-rank test on two samples: BNeuralT's RMSprop against all other algorithms for each data. The 'stat,' 'pval,' and 'post,' respectively indicate Wilcoxon signed-rank statistic, two-tailed p-value, and Bonferroni correction post hoc adjusted p-value. The values are marked in bold where the null hypothesis that BNeuralT's RMSprop and other algorithms have no difference is rejected.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alternating optimization of decision trees, with application to learning sparse oblique trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tavallali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1211" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flexible neural trees ensemble for stock index modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="697" to="703" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Time-series forecasting using flexible neural tree model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="235" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBLINEAR: a library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sampling neuron morphologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farhoodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">248385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical models in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1000211</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Models of neocortical layer 5b pyramidal cells capturing a wide range of dendritic and perisomatic active properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sch?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Segev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1002107</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectivity-driven white matter scaling and folding in primate cerebral cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herculano-Houzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kaas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="19008" to="19013" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A quantitative description of membrane current and its application to conduction and excitation in nerve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Hodgkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Huxley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="544" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00554</idno>
		<title level="m">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks&apos;</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Might a single neuron solve interesting machine learning problems through successive computations on its dendritic tree?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1554" to="1571" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">KEEL dataset repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keel</surname></persName>
		</author>
		<ptr target="https://sci2s.ugr.es/keel/datasets.php" />
		<imprint>
			<date type="published" when="2011-04" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICNN&apos;95-International Conference on Neural Networks</title>
		<meeting>of ICNN&apos;95-International Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Keras the sequential model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io/guides/sequentialmodel" />
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>3rd International Conference for Learning Representations (ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="464" to="472" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fifty years of classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="348" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dendritic computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?usser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="503" to="532" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Toward a simplified model of an active dendritic tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Mel</surname></persName>
		</author>
		<editor>G. Stuart, N. Spruston and M. H?usser</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="405" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OC1: A randomized induction of oblique decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salzberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beigel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>11th National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sn??el</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="909" to="924" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-objective optimisation of multi-output neural trees, in &apos;IEEE Congress on Evolutionary Computation (CEC)&apos;, IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nicosia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Energy efficiency across programming languages: how do energy, time, and memory relate?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saraiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in &apos;Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="256" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arithmetic of subthreshold synaptic summation in a model ca1 pyramidal cell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Mel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="977" to="987" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramidal neuron as two-layer neural network&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Mel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="999" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Derivative-free optimization: a review of algorithms and comparison of software implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Sahinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1247" to="1293" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Growing and pruning neural tree networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="299" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Solving iterated functions using genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers</title>
		<meeting>11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2149" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural trees: a new tool for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sirat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nadal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<title level="m">Adaptive neural trees, in &apos;Proc. 36th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6166" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude&apos;, Coursera: Neural Networks for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regional dendritic variation in neonatal human cortex: a quantitative golgi study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Travis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="277" to="287" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The lack of a priori distinctions between learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evolutionary induction of sparse neural trees&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>M?hlenbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="236" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
