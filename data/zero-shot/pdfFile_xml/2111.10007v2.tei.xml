<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FBNetV5: Neural Architecture Search for Multiple Tasks in One Run</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
							<email>zhanghang@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
							<email>xiaoliangdai@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Wang</surname></persName>
							<email>jialiangw@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
							<email>yingyan.lin@rice.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
							<email>vajdap@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FBNetV5: Neural Architecture Search for Multiple Tasks in One Run</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) has been widely adopted to design accurate and efficient image classification models. However, applying NAS to a new computer vision task still requires a huge amount of effort. This is because 1) previous NAS research has been over-prioritized on image classification while largely ignoring other tasks; 2) many NAS works focus on optimizing task-specific components that cannot be favorably transferred to other tasks; and 3) existing NAS methods are typically designed to be "proxyless" and require significant effort to be integrated with each new task's training pipelines. To tackle these challenges, we propose FBNetV5, a NAS framework that can search for neural architectures for a variety of vision tasks with much reduced computational cost and human effort. Specifically, we design 1) a search space that is simple yet inclusive and transferable; 2) a multitask search process that is disentangled with target tasks' training pipeline; and 3) an algorithm to simultaneously search for architectures for multiple tasks with a computational cost agnostic to the number of tasks. We evaluate the proposed FBNetV5 targeting three fundamental vision tasks -image classification, object detection, and semantic segmentation. Models searched by FBNetV5 in a single run of search have outperformed the previous stateof-the-art in all the three tasks: image classification (e.g., ?1.3% ImageNet top-1 accuracy under the same FLOPs as compared to FBNetV3), semantic segmentation (e.g., ?1.8% higher ADE20K val. mIoU than SegFormer with 3.6? fewer FLOPs), and object detection (e.g., ?1.1% COCO val. mAP with 1.2? fewer FLOPs as compared to YOLOX).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent breakthroughs in deep neural networks (DNNs) have fueled a growing demand for deploying DNNs in perception systems for a wide range of computer vision (CV) applications that are powered by various fundamental CV * Equal contribution. ? Work done while interning at Meta Reality Labs.  <ref type="figure" target="#fig_1">Figure 1</ref>. The architectures simultaneously searched in a single run of FBNetV5 outperforms the SotA performance in three tasks: ImageNet <ref type="bibr" target="#b14">[15]</ref> image classification, ADE20K <ref type="bibr" target="#b65">[65]</ref> semantic segmentation, and COCO <ref type="bibr" target="#b31">[32]</ref> object detection. tasks, including classification, object detection, and semantic segmentation. To develop real-world DNN based perception systems, the neural architecture design is among the most important factors that determine the achievable task performance and efficiency. Nevertheless, designing neural architectures for different applications is challenging due to its prohibitive computational cost, intractable design space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40]</ref>, diverse application-driven deployment requirements <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b61">61]</ref>, and so on.</p><p>To tackle the aforementioned challenges, the CV community has been exploring neural architecture search (NAS) to design DNNs for CV tasks. In general, the expectations for NAS are two-fold: First, to build better neural architectures with stronger performance and higher efficiency; and second, to automate the design process in order to reduce the human effort and computational cost for DNN design. While the former ensures effective real-world solutions, the latter is critical to facilitate the fast development of DNNs to more applications. Looking back at the progress of recent years, it is fair to say that NAS has met the first expectation in advanc-ing the frontiers of accuracy and efficiency, especially for image classification tasks. However, existing NAS methods still fall short of meeting the second expectation.</p><p>The reasons for the above limitation include the following. First, over the years the NAS community has been over fixated on benchmarking NAS methods on image classification tasks, driven by the commonly believed assumption that the best models for image classification are also the best backbones for other tasks. However, this assumption is not always true <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b64">64]</ref>, and often leads to suboptimal architectures for many non-classification tasks. Second, many existing NAS works focus on optimizing task-specific components that are not transferable or favorable to other tasks. For example, <ref type="bibr" target="#b45">[46]</ref> only searches for the encoder part within the encoder-decoder structure of segmentation tasks, while the optimal encoder is coupled with the decoder designs. <ref type="bibr" target="#b19">[20]</ref> is customized to RetinaNet <ref type="bibr" target="#b30">[31]</ref> in object detection tasks. As a result, NAS advances made for one task do not necessarily favor other tasks or help reduce the design effort. Finally, a popular belief in current NAS practice is that it is better for NAS to be "proxyless" and a NAS method should be integrated into the target tasks' training pipeline for directly optimizing the corresponding architectures based on the training losses of each target task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b63">63]</ref>. However, this makes NAS unscalable when dealing with many new tasks, since adding each new task would require nontrivial efforts to integrate the NAS techniques into the existing training pipeline of the target task. In particular, many popular NAS methods conduct search by training a supernet <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b63">63]</ref>, adding dedicated cost regularization to the loss function <ref type="bibr" target="#b15">[16]</ref>, adopting special initialization <ref type="bibr" target="#b63">[63]</ref>, and so on. These techniques often heavily interfere with the target task's training process and thus requires much engineering effort to re-tune the hyperparameters to achieve the desired performance.</p><p>In this work, we propose FBNetV5, a NAS framework, that can simultaneously search for backbone topologies for multiple tasks in a single run of search. As a proof of concept, we target three fundamental computer vision tasksimage classification, object detection, and semantic segmentation. Starting from a state-of-the-art image classification model, i.e., FBNetV3 <ref type="bibr" target="#b12">[13]</ref>, we construct a supernet consisting of parallel paths with multiple resolutions, similar to HRNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>. Based on the supernet, FBNetV5 searches for the optimal topology for each target task by parameterizing a set of binary masks indicating whether to keep or drop a building block in the supernet. To disentangle the search process from the target tasks' training pipeline, we conduct search by training the supernet on a proxy multitask dataset with classification, object detection, and semantic segmentation labels. Following <ref type="bibr" target="#b20">[21]</ref>, the dataset is based on ImageNet, with detection and segmentation labels generated by pretrained open-source models. To make the computational cost and hyper-parameter tuning effort agnostic to the number of tasks, we propose a supernet training algorithm that simultaneously search for task architectures in one run. After the supernet training, we individually train the searched task-specific architectures to uncover their performance.</p><p>Excitingly, in addition to requiring reduced computational cost and human effort, extensive experiments show that FB-NetV5 produces compact models that can achieve SotA performance on all three target tasks. On ImageNet <ref type="bibr" target="#b14">[15]</ref> classification, our model achieved 1.3% higher top-1 accuracy under the same FLOPs as compared to FBNetV3 <ref type="bibr" target="#b12">[13]</ref>; on ADE20K <ref type="bibr" target="#b65">[65]</ref> semantic segmentation, our model achieved 1.8% higher mIoU than SegFormer <ref type="bibr" target="#b60">[60]</ref> with 3.6? fewer FLOPs; on COCO <ref type="bibr" target="#b31">[32]</ref> object detection, our model achieved 1.1% higher mAP with 1.2? fewer FLOPs compared to YOLOX <ref type="bibr" target="#b18">[19]</ref>. It is worth noting that all our well-performing architectures are searched simultaneously in a single run, yet they beat the SotA neural architectures that are delicately searched or designed for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Neural Architecture Search for Efficient DNNs. Various NAS methods have been developed to design efficient DNNs, aiming to 1) achieve boosted accuracy vs. efficiency trade-offs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref> and 2) automate the design process to reduce human effort and computational cost. Early NAS works mostly adopt reinforcement learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b67">67]</ref> or evolutionary search algorithms <ref type="bibr" target="#b41">[42]</ref> which require substantial resources. To reduce the search cost, differentiable NAS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">57]</ref> was developed to differentiably update the weights and architectures. Recently, to deliver multiple neural architectures meeting different cost constraints, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b63">63]</ref> propose to jointly train all the sub-networks in a weightsharing supernet and then locate the optimal architectures under different cost constraints without re-training or finetuning. However, unlike our work, all the works above focus on a single task, mostly image classification, and they do not reduce the effort of designing architectures for other tasks.</p><p>Task-aware Neural Architecture Design. To facilitate designing optimal DNNs for various tasks, recent works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref> propose to design general architecture backbones for different CV tasks. In parallel, with the belief that each CV task requires its own unique architecture to achieve the task-specific optimal accuracy vs. efficiency trade-off, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref> develop dedicated search spaces for different CV tasks, from which they search for taskaware DNN architectures. However, these existing methods mostly focus on optimizing task-specific components of which the advantages are not transferable to other tasks. Recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> begin to focus on designing networks for multiple tasks in a unified search space and has shown promising results. However, they are designed to be "proxyless" and the search process needs to be integrated to downstream tasks's training pipeline. This makes it less scalable <ref type="figure">Figure 2</ref>. Overview of FBNetV5. We search backbone topologies for multiple tasks by training a supernet once on a multitask dataset. Each task has its own architecture distribution from which we sample task-specific architectures and train them using the existing training pipeline of the target tasks. Supernet configurations in Appendix A. Fusion module details in Appendix B. Search process in <ref type="bibr">Algorithm 4.</ref> to add new tasks, since it requires non-trivial engineering effort and compute cost to integrate NAS to the existing training pipeline of a target task. Our work bypasses this by using a disentangled search process, and we conduct search for multiple tasks in one run. This is computationally efficient and allows us to utilize target tasks' existing training pipelines with no extra efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present our proposed FBNetV5 framework that aims to reduce the computational cost and human effort required by NAS for multiple tasks. FBNetV5 contains three key components: 1) A simple yet inclusive and transferable search space (Section 3.1); 2) A search process equipped with a multitask learning proxy to disentangle NAS from target tasks' training pipelines (Section 3.2); and 3) a search algorithm to simultaneously produce architectures for multiple tasks at a constant computational cost agnostic to the number of target tasks (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space</head><p>To search for architectures for multiple tasks, we design the search space to meet three standards: 1) Simple and elegant: we favor simple search space over complicated ones; 2) Inclusive: the search space should include strong architectures for all target tasks; and 3) Transferable: the searched architectures should be useful not only for one model, but also transferable to a family of models.</p><p>Inspired by HRNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>, we extend a SotA classification model, FBNetV3 <ref type="bibr" target="#b12">[13]</ref>, to a supernet with parallel paths and multiple stages. Each path has a different resolution while blocks on the same path have the same resolution. This is shown in <ref type="figure">Figure 2</ref> (bottom-left). We divide an FBNetV3 into 4 partitions along the depth dimension, each partition outputs a feature map with a resolution down-sampled by 4, 8, 16, and 32 times, respectively. Stage 0 of the supernet is essentially the FBNetV3 model. For following stages, we use the last 2 layers of each partition to construct a block per stage. During inference, we first compute Stage 0 of the supernet, and then compute the remaining blocks by topological order. Similar to <ref type="bibr" target="#b53">[54]</ref>, we insert (lightweight) fusion modules (see Appendix B) between stages to fuse information from different paths (resolutions). A block-wise model configuration of the supernet can be found in Appendix A.</p><p>The aforementioned supernet contains blocks with varying significance to different tasks. By conventional wisdom, a classification architecture may only need blocks on the low-resolution paths, while segmentation or object detection would favor blocks with a higher resolution. Based on this, we search for network topologies, i.e., which blocks to select or skip for different tasks. Formally, for a supernet with P paths, S stages, and B = S ? P blocks, a candidate architecture can be characterized by a binary vector a ? {0, 1} B , where a i = 1 means to select block-i and a i = 0 means to skip and remove the corresponding connections from and to this block. More details about the implementation of fusion modules with skipped blocks are provided in Appendix B.</p><p>We believe that this search space is simple and elegant. It only contains binary choices for each of the B blocks. This is much simpler than other search space design that considers how to mix different types of operators (convolutions and transformers) together, or how to wire operators with complicated connections. Furthermore, the search space is inclusive. As a sanity check, the search space include most of the mainstream network topologies for CV tasks, e.g., 1) the simple linear topology for most of the classification models, 2) the U-Net <ref type="bibr" target="#b43">[44]</ref> and PANet <ref type="bibr" target="#b33">[34]</ref> topology for se-mantic segmentation, and 3) the Feature-Pyramid Networks (FPN) <ref type="bibr" target="#b29">[30]</ref> and BiFPN <ref type="bibr" target="#b49">[50]</ref> for object detection, as illustrated in Appendix C. The searched architecture topology is transferable. FBNet contains a series of models from small to large. We conduct search on a FBNet-A based supernet, and the topology can be transferred to other models. It is worth noting that transferring topology to models of different sizes, depths, and resolutions is also a common practice adopted by works such as FPN <ref type="bibr" target="#b29">[30]</ref> and BiFPN <ref type="bibr" target="#b49">[50]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Disentangled Search Process</head><p>A popular belief is that NAS should be proxyless and the search process should be integrated into each target task's training pipeline for achieving better results. However, implementing and integrating the search process to each target task's pipeline can require significant engineering effort. Moreover, many NAS techniques heavily interfere with the target task's training and thus requires much engineering effort to re-tune the hyperparameters.</p><p>To avoid the above limitations, we design a search process that is disentangled with target tasks' training pipeline. Specifically, we conduct search by training a supernet on a multitask dataset where each image is annotated with labels from all target tasks. Following <ref type="bibr" target="#b57">[57]</ref>, the supernet training jointly optimize the model weights and more importantly, task-specific architecture distributions (e.g., the SEG, DET, and CLS Arch. Prob. in <ref type="figure">Figure 2</ref>). The goal of the search process is to obtain a task-specific architecture distribution from which we can sample architectures for the target tasks. The searched models can then be trained using the existing training pipeline of the target tasks without the necessity of implementing the search process into the tasks' training pipeline or re-tune the existing hyper-parameters. The search process is shown in <ref type="figure">Figure 2</ref>.</p><p>As there is no large-scale multitask dataset publicly available, we follow <ref type="bibr" target="#b20">[21]</ref> to construct a pseudo-labeled dataset based on ImageNet. Specifically, we use 1) original Ima-geNet labels for classification, 2) open-source CenterNet2 <ref type="bibr" target="#b66">[66]</ref> pretrained on the COCO object detection dataset to generate pseudo detection labels, and 3) open-source Mask-Former <ref type="bibr" target="#b9">[10]</ref> pretrained on the COCO-stuff semantic segmentation dataset (171 classes) to generate pixel-wise segmentation labels. In addition, we follow <ref type="bibr" target="#b20">[21]</ref> to filter out object detection results with a confidence lower than 0.5, and set segmentation predictions whose maximum probability lower than 0.5 to be the "don't-care" category. As such, this dataset can easily extend to include more tasks by using open-source pretrained models to generate task-specific pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Search Algorithm</head><p>Our search algorithm is based on the differentiable neural architecture search <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">57]</ref> for low computational cost compared with other methods, such as sampling-based meth- ods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48]</ref>. For multiple tasks, a simple idea is to apply the conventional single-task NAS (Algorithm 1) T times for each task. To make this more scalable, we derive a novel search algorithm with a constant computational cost agnostic to the number of tasks (Algorithm 4). For better clarity, We introduce the derivation of the search algorithm in four steps corresponding to Algorithm 1, 2, 3, and 4, respectively. We summarize and compare the four search algorithms at each step in <ref type="table" target="#tab_0">Table 1</ref>. We visualize Algorithm 4 in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Differentiable NAS for a Single Task</head><p>We start from a typical differentiable NAS designed for a single task, which can be formulated as</p><formula xml:id="formula_0">min a?A,w t (a, w),<label>(1)</label></formula><p>where a is a candidate architecture in the search space A, w is the supernet's weight, and t (?) is the loss function of taskt that also considers the cost of architecture a. Following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">57]</ref>, the cost of an architecture can be defined in terms of FLOPs, parameter size, latency, energy, etc.</p><p>In our work, we search in a block-level search space. For block-b of the supernet, we have</p><formula xml:id="formula_1">y = a b f b (x) + (1 ? a b )x,<label>(2)</label></formula><p>where x, y are input and output of block </p><formula xml:id="formula_2">-b function f b (?). a b ? {0,</formula><formula xml:id="formula_3">E a?p? { t (a, w)},<label>(3)</label></formula><p>where a ? {0, 1} B is a random variable sampled from a distribution p ? , parameterized by ? ? [0, 1] B . For each block, we independently sample a b ? Bernoulli(? b ) from a Bernoulli distribution with an expected value of ? b . The probability of architecture a computes as</p><formula xml:id="formula_4">p ? (a) = B b=1 ? a b b (1 ? ? b ) (1?a b ) .<label>(4)</label></formula><p>Under this relaxation, we can jointly optimize the supernet's weight w and architecture parameter ? with stochastic gradient descent. Specifically, in the forward pass, we first sample a ? p ? , and compute the loss with input data x, weights w, and architecture a. Next, we compute gradient with respect to w and a. Since architecture a is a discrete random variable, we cannot pass the gradient directly to ?. Previous works have adopted the Straight-Through Estimator <ref type="bibr" target="#b1">[2]</ref> to approximate the gradient to ? as ?l t ?? ? ?l t ?a . Alternatively, Gumbel-Softmax <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b57">57]</ref> can also be used to estimate the gradient. We train w and ? jointly using SGD with learning rate ?, ? ? . After the training finishes, we sample architectures a from the trained distribution p ? and pass them to target task's training pipeline. This process is summarized in Algorithm 1. Forward pass to compute t (a, w, x) 5:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Differentiable NAS for a Single Task</head><formula xml:id="formula_5">Backward pass to compute ? t ?w , ? t ?a 6: Straight-Through Estimation ? t ?? ? ? t ?a 7: Gradient update w ? w ? ? ? t ?w , ? ? ? ? ? ? ? t</formula><p>?? . 8: end for 9: Sample a ? p ? for target task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Extending to Multiple Tasks</head><p>We are interested in searching architectures for multiple tasks, which can be formulated as</p><formula xml:id="formula_6">min a 1 ,??? ,a T ,w 1 ,???w T T t=1 t (a t , w t ).<label>(5)</label></formula><p>This is a rather awkward way to combine T independent optimization problems together. To simplify the problem, we first approximate Equation <ref type="formula" target="#formula_6">(5)</ref> as</p><formula xml:id="formula_7">min a 1 ,??? ,a T ,w T t=1 t (a t , w),<label>(6)</label></formula><p>where w is the weight of an over-parameterized supernet shared among all tasks, and a t is the architecture sampled for task-t. One concern of using Equation <ref type="formula" target="#formula_7">(6)</ref> to approximate Equation <ref type="formula" target="#formula_6">(5)</ref> is that in multitask learning, the optimization of different tasks may interfere with each other. We conjecture that in an over-parameterized supernet with large enough capacity, the interference is small and can be ignored. Also, unlike conventional multitask learning, our goal is not to train a network with multitask capability, but to find optimal architectures a t for each task. We conjecture that the task interference has limited impact on the search results.</p><p>Using the same relaxation trick as Equation <ref type="formula" target="#formula_3">(3)</ref>, we rewrite Equation <ref type="formula" target="#formula_7">(6)</ref> as</p><formula xml:id="formula_8">min ? 1 ,??? ,? T ,w T t=1 E a t ?p ? t { t (a t , w)},<label>(7)</label></formula><p>where a t are architectures sampled from a task-specific distribution p ? t parameterized by ? t . To solve this, we can slightly modify Algorithm 1 to reach Algorithm 2.</p><p>Algorithm 2 Differentiable NAS for multiple tasks <ref type="bibr">1:</ref> for iter = 1, ? ? ? , N do 2:</p><p>Sample a batch of data x 3:</p><p>for task t = 1, ? ? ? , T do 4:</p><p>Sample a t ? p ? t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Forward pass to compute t (a t , w, x)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Backward pass to compute ? t ?w , ? t ?a t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Accumulate</p><formula xml:id="formula_9">? w = ? w + ? t ?w 8: Straight-Through Estimation ? ? t ? ? t ?a t 9:</formula><p>end for 10:</p><formula xml:id="formula_10">Gradient update w ? w ? ?? w 11:</formula><p>Gradient update ? t ? ? t ? ? ? ? ? t for t = 1, ? ? ? , T 12: end for 13: Sample a t ? p ? t and for target task-t With Algorithm 2, we did not gain efficiency compared with running the Algorithm 1 for T times, since we need to compute T forward and backward passes in each iteration. With the same number of iterations, we end up with a T times higher compute cost. But in the next two sections, we show how we adopt importance sampling and REINFORCE to reduce the number of forward and backward passes to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Reducing T Forward Passes to 1</head><p>Reviewing Algorithm 2, the need to run multiple forward passes comes from lines 4 and 5 that for each task, we need to sample different architectures from different p ? t to estimate the expected task loss</p><formula xml:id="formula_11">E a t ?p ? t { t (a t , w)} under p ? t .</formula><p>Using Importance Sampling <ref type="bibr" target="#b37">[38]</ref>, we reduce T forward passes into 1. Instead of sampling T architectures from T distributions, we can just sample architectures once from a common proxy distribution q and let T tasks share the same architecture a in their the forward pass. Though not sampling from p ? t , we can still compute an unbiased estimation of the task loss expectation E a t ?p ? t { t (a t , w)} as</p><formula xml:id="formula_12">E a t ?p ? t { t (a t , w)} = E a?q { p ? t (a) q(a) t (a, w)} ? 1 N N i=1 p ? t (a i ) q(a i ) t (a i , w), with a i ? q.<label>(8)</label></formula><p>N is the number of architecture samples. q can be any distribution as long as it satisfies the condition that q(a) = 0 where p ? t (a) = 0. Equation <ref type="formula" target="#formula_12">(8)</ref> will always be an unbiased estimator. We empirically design q as a distribution that we first uniformly sample a task from {1, ? ? ? , T }, and sample the architecture from p(a) ? t . For any architecture a, its probability can be calculated as q(a) = 1/T t p(a) ? t with p(a) ? t computed by Equation <ref type="formula" target="#formula_4">(4)</ref>. Using importance sampling, we redesign the search algorithm as Algorithm 3 to reduce the number of forward passes from T to 1.</p><p>Algorithm 3 Reduce forward passes with Importance Sampling <ref type="bibr">1:</ref> for iter = 1, ? ? ? , N do 2:</p><p>Sample a batch of data x 3:</p><p>Sample a ? q 4:</p><p>Forward pass to compute y = f (a, w, x) 5:</p><p>for task t = 1, ? ? ? , T do 6:</p><p>Importance Sampling ? t ? p ? t (a)/q(a) 7:</p><p>Compute task loss t ? ? t ? t (a, w, y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Backward pass to compute ? t ?w , ? t ?a 9:</p><formula xml:id="formula_13">Accumulate ? w = ? w + ? t ?w 10: Straight-Through Estimation ? ? t ? ? t ?a 11: end for 12: Gradient update w ? w ? ?? w 13:</formula><p>Gradient update ? t ? ? t ? ? ? ? ? t for t = 1, ? ? ? , T 14: end for 15: Sample a t ? p ? t for target task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Reducing T Backward Passes to 1</head><p>Algorithm 3 only requires 1 forward pass but T backward passes. This ie because to optimize the architecture distribution for task-t, we need to run a backward pass to compute ? t /?a, which we use to estimate ? t /?? t and to update the task architecture parameter ? t . To avoid this, we use REINFORCE <ref type="bibr" target="#b56">[56]</ref> to estimate the gradient ? t /?? t as</p><formula xml:id="formula_14">? ? t E a?p ? t { t (a)} = ? ? t a?A p ? t (a) t (a) = a?A t (a)? ? t p ? t (a) = a?A t (a)p ? t (a)? ? t log p ? t (a) = E a?p ? t { t (a)? ? t log p ? t (a)} ? 1 N N i=1 t (a i )? ? t log p ? t (a i ), with a i ? p ? t .</formula><p>(9) N is the number of architecture samples. Given the definition of p ? t (a) in Equation <ref type="formula" target="#formula_4">(4)</ref>, we can easily derive ? ? t log p ? t (a), with its b-th element simply computed as</p><formula xml:id="formula_15">(? ? t log p ? t (a)) b = 1/(? a b b (1 ? ? b ) (1?a b ) )).<label>(10)</label></formula><p>Equation <ref type="formula">(9)</ref> is also referred to as the score function estimator of the true gradient ? t /?? t . The intuition is that for any sampled architecture a i , we score its gradient by the loss t (a i ), such that architectures that cause larger loss will be suppressed and vice versa. This technique is more often referred to as the policy gradient in Reinforcement Learning. For NAS, a similar technique is adopted by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b62">62]</ref> to search for classification models. Using Equation <ref type="formula">(9)</ref>, we no longer need to run back propogation to compute ? t /?? t for each task. We still need to compute the gradient to the supernet weights ? /?w, but we can first sum up the task losses t and run backward pass only once. This is summarized in Algorithm 4 and visualized in <ref type="figure">Figure 2</ref>. We discuss more important details of this algorithm in Appendix E. Note that we still have two for-loops in each iteration to compute the task loss ? t t from the network's prediction y and the gradient estimator for ? t /?? t , but their computational cost is negligible compared with the forward and backward passes. Forward pass y = f (a, w, x) <ref type="bibr">5:</ref> for task t = 1, ? ? ? , T do Backward pass to compute w ? w ? ? ? ?w 10:</p><p>for task t = 1, ? ? ? , T do 11:</p><p>REINFORCE ? t ? ? t ? ? ? t ? ? t log p ? t (a) <ref type="bibr">12:</ref> end for 13: end for 14: Sample a t ? p ? t for target task 4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>We implement the search process and target task's training pipeline in D2Go 1 powered by Pytorch <ref type="bibr" target="#b38">[39]</ref> and Detec-tron2 <ref type="bibr" target="#b58">[58]</ref>. For the search (training supernet) process, we build a supernet extended from an FBNetV3-A model as illustrated in Section 3.1. During search, we first pretrain the supernet on ImageNet <ref type="bibr" target="#b14">[15]</ref> with classification labels for 1100 epochs, mostly following a regular classification training recipe <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref>. More details are included in Appendix F.2. This step takes about 60 hours to finish on 64 V100 GPUs. Then we train the supernet on the multitask proxy dataset for 9375 steps using SGD with a base learning rate of 0.96. We decay the learning rate by 10x at step-3125. We set the initial sampling probability of all blocks to 0.5. We do not update the architecture parameters until step-6250. We set the architecture parameter's learning rate to be 0.01 of the weight's learning rate. It takes about 10 hours to finish when trained on 16 V100 GPUs. More details of the search implementation can be found in Appendix F.1. After the search, we sample the most likely architectures for each task.</p><p>For training the searched architectures, we mostly follow existing SotA training recipes for each task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b58">58]</ref>. See Appendix F.2 for details. For semantic segmentation, we follow MaskFormer <ref type="bibr" target="#b9">[10]</ref> and attach a modified lightweight MaskFormer head (dubbed Lite MaskFormer) to the searched backbone. For object detection, we use Faster R-CNN's <ref type="bibr" target="#b42">[43]</ref> detection head with light-weight ROI and RPN. We call the new head as Lite R-CNN. See the architecture design of the two light-weight heads in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparing with SotA Compact Models</head><p>We compare our searched architectures against both NAS searched and manually designed compact models for Ima-geNet <ref type="bibr" target="#b14">[15]</ref> classification, ADE20K <ref type="bibr" target="#b65">[65]</ref> semantic segmentation, and COCO <ref type="bibr" target="#b31">[32]</ref> object detection. We search topologies for all tasks by training supernet once, sampling one topology for each task, and transfer the searched topology to different versions of FBNetV3 models with different sizes. We use FBNetV3-{A, C, F} and build two smaller models FBNetV3-A R and FBNetV3-A C by mainly shrinking the resolution and channel sizes from FBNetV3-A, respectively. See Appendix D. We name a model using the template FBNetV5-{version}-{task}. For a given task, all models share the same searched topology, as in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>Compared with all the existing compact models including automatically searched and manually designed ones, our FBNetV5 delivers architectures with better accuracy/mIoU/mAP vs. efficiency trade-offs in all the Ima-geNet <ref type="bibr" target="#b14">[15]</ref> classification (e.g., ?1.3% top-1 accuracy under the same FLOPs as compared to FBNetV3-G <ref type="bibr" target="#b57">[57]</ref>), ADE20K <ref type="bibr" target="#b65">[65]</ref> segmentation (e.g., ?1.8% higher mIoU than SegFormer with MiT-B1 as backbone <ref type="bibr" target="#b60">[60]</ref> and 3.6? fewer FLOPs), and COCO <ref type="bibr" target="#b31">[32]</ref> detection tasks (e.g., ?1.1% mAP with 1.2? fewer FLOPs as compared to YOLOX-Nano <ref type="bibr" target="#b40">[41]</ref>). See Tables 2, 3, 4 and <ref type="figure" target="#fig_1">Figure 1</ref> for a detailed comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study on FBNetV5's Search Algorithm</head><p>To verify the effectiveness of the search algorithm proposed in Section 3.3 (i.e., Algorithm 4), we compare the proposed multitask search (Algorithm 4) with single-task search (Algorithm 1) and random search. We sample four architectures from two trained distributions (by Algorithm 4 and Algorithm 1) and a random distribution where each block has a 0.5 probability being sampled. We compare sampled architectures with their best accuracy/mIoU/mAP vs. efficiency trade-off and report the results in <ref type="table" target="#tab_4">Table 5</ref>. First, random architectures achieve strong performance. This demonstrates the effectiveness of the search space design. But compared to the random search, using the same FLOPs, models from multitask search obviously outperforms randomly sampled </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Searched Architectures for Different Tasks</head><p>To better understand the architectures searched by FB-NetV5, we visualize them in <ref type="figure" target="#fig_4">Figure 3</ref>. For the SEG model <ref type="figure" target="#fig_4">(Figure 3-top)</ref>, its blocks between Fusion 1 and Fusion 6 match the U-Net's pattern that gradually increases feature resolutions. See <ref type="figure">Figure 5</ref>-top for a comparison. For the DET model <ref type="figure" target="#fig_4">(Figure 3</ref>-middle), we did not find an obvious pattern to describe it. We leave the interpretation to each reader. Surprisingly, the CLS model contains a lot of blocks from higher resolutions. This contrasts the mainstream models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b63">63]</ref> that only stack layers sequentially. Given  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose FBNetV5, a NAS framework that can search for neural architectures for a variety of CV tasks with reduced human effort and compute cost. FBNetV5 features a simple yet inclusive and transferable search space, a multitask search process disentangled with target tasks' training pipelines, and a novel search algorithm with a constant compute cost agnostic to number of tasks. Our experiments show that in a single run of search, FBNetV5 produces efficient models that significantly outperform the previous SotA models in ImageNet classification, COCO object detection, and ADE20K semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion on Limitations</head><p>There are several limitations of our work. First, we did not explore a more granular search space, e.g., to search for block-wise channel sizes, which can further improve searched models' performance. Second, while our framework can search for multiple tasks in one run, we do not support adding new tasks incrementally, which will further improve the task-scalability. One potential solution is to explore whether we can transfer the searched architectures from one task (e.g., segmentation) to similar tasks (e.g., depth estimation) without re-running the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Block Configurations of FBNetV3-A Supernet</head><p>To explain how to extend an FBNetV3 <ref type="bibr" target="#b12">[13]</ref> to the supernet in FBNetV5, we list the code snippets below. It includes the block configurations of both FBNetV3-A and the supernet extended from FBNetV3-A. It is compatible with with official implementation of FBNetV3 2 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details about the Fusion Module</head><p>Following HRNet <ref type="bibr" target="#b53">[54]</ref>, in the supernet and searched network, we design the Fusion module to fuse feature maps with different resolutions with each other. The original HR-Net's fusion modules are computationally expensive. To reduce cost, we design a parameter-free and (almost) computefree fusion module.</p><p>Each block in the network is fused to all blocks at the next stage. For a block at a given path: 1) if the output feature is at the same path (resolution), the fusion module is essentially a identity connection (blue arrows in <ref type="figure" target="#fig_2">Figure  4</ref>). 2) To fuse the feature to a path with larger channel size and lower resolution, we first down-sample the input feature to the target size, and repeat the original channels by C out /C in times, where C in , C out are input/output channel sizes. If C out is not divisible by C in , we drop the extra channels. This is shown as the red arrows in <ref type="figure" target="#fig_2">Figure 4</ref>. 3) To fuse to a feature with higher resolution and smaller channel sizes, we first up-sample the feature map. Then, we pad the input feature's channels with zero such that the channel size becomes C in = C in /C out ? C out . Finally, we take every C in /C out channels as a group and compute a channel-wise average to produce a new output channel. This is shown as <ref type="figure" target="#fig_2">Figure 4</ref> blue arrows. Features fused to the same block will be summed together as input to the block.</p><p>This fusion module does not require any parameters, and only requires a negligible amount of compute for downsampling, up-sampling, padding, and channel-wise average.</p><p>In the supernet and the searched architectures, if any block (e.g., the ones in (Stage s-1, Path p) or (Stage s, Path p-1)) is skipped, the corresponding connections in the fusion modules from and to the block will also be removed, except the connections from and to other blocks in the same path.  <ref type="figure" target="#fig_2">Figure 4</ref>. Illustration of the fusion module aggregating information from different paths (resolutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Block Fusion 1 Fusion 2 Fusion 3 Fusion 4 Fusion 5 Fusion 6 Fusion 7</head><p>Input Image Skipped Block</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion 1 Fusion 2 Fusion 3 Fusion 4 Fusion 5 Fusion 6 Fusion 7</head><p>Input Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion 1 Fusion 2 Fusion 3 Fusion 4 Fusion 5 Fusion 6 Fusion 7</head><p>Input Image U-Net or FPN PANet BiFPN Fusion 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion 8</head><p>Fusion 8 <ref type="figure">Figure 5</ref>. The search space can represent the topology of U-Net <ref type="bibr" target="#b43">[44]</ref>, PANet <ref type="bibr" target="#b33">[34]</ref>, FPN <ref type="bibr" target="#b29">[30]</ref>, and BiFPN (without the extra edge) <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of the Mainstream Topologies for CV Tasks</head><p>To demonstrate our search space is inclusive, we visualize how can it represent some of the mainstream network topologies for CV tasks in <ref type="figure">Figure 5</ref>. These include 1) the U-Net <ref type="figure">(Figure 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FBNetV3-A C and FBNetV3-A R</head><p>We provide the code snippets below to demonstrate the details about the architectures of FBNetV3-A C and FBNetV3-A R , by mainly shrinking the resolution and channel sizes from FBNetV3-A, respectively.</p><p>Sampling multiple architectures. Algorithm 1 2, 3, 4 show we sample 1 architecture in each forward pass. Although it still gives an unbiased estimation of the task loss, small sample sizes lead to large variations. In practice, we implement the supernet training with distributed data parallel in Pytorch, such that each thread independently samples an architecture from the same distribution. We use 16 threads for supernet training, therefore, sampling 16 architectures per iteration to reduce the estimation variance.</p><p>Self-normalized importance sampling. In Equation <ref type="formula" target="#formula_12">(8)</ref>, we compute the importance weight as r(a) = p ? (a)/q(a). In some extreme cases if q(a) is too small relative to p ? (a), r(a) will become very large that destabilize the supernet training. To prevent this, we actually use the self-normalized importance sampling and re-write Equation <ref type="formula" target="#formula_12">(8)</ref> as</p><formula xml:id="formula_16">E a t ?p ? t { t (a t , w)} ? N i=1 r(a i ) t (a i , w) N i=1 r(a i ) ,<label>(11)</label></formula><p>with a i ? p ? t . This still gives an unbiased estimation <ref type="bibr" target="#b37">[38]</ref>, but will prevent the loss from becoming exceedingly large.</p><p>During supernet training, we implement this through an all-gather operation to collect r(a i ) from all threads and compute the normalized importance weight. Loss normalization. In Equation <ref type="formula">(9)</ref>, we scale the gradient ? ? t log p ? t (a) by the associated loss t (a) to determine whether we should suppress or encourage the sampled architecture a. However, one challenge is that for different tasks, the loss t may have different mean and variance, so the gradients of different tasks can be scaled differently. To address this, instead of using the raw task-loss in Equation <ref type="formula">(9)</ref>, we use a normalized task-loss, computed as? t (a) = ( (a)?? )/? , where ? , ? is the mean and standard deviation of the task loss in the past 200 steps. The mean ? provides a baseline to evaluate how does the sampled architecture a compare with the average. This is similar to the Reinforcement Learning approach of using "advantage" instead of reward for policy gradient. The scaling factor 1/? ensures that all losses are scaled properly without needing to tune the task-specific learning rate.</p><p>Cost regularization. In addition to the original task loss, e.g., cross-entropy for classification, we add a cost regularization term computed as</p><formula xml:id="formula_17">? c max(0, b a b c b b c b ? 0.5),</formula><p>where c b is the cost (e.g., FLOPs) of block-b, and a b denotes whether to select block-b or not. ? c is a loss coefficient. 0.5 is the relative cost target.</p><p>Warmup training. Similar to the observation of <ref type="bibr" target="#b57">[57]</ref>, before training the architecture parameters ?, we need to first sufficiently train the model weights w. This is because at the beginning of the supernet training, the loss will always drop regardless of the choice of architecture a. In our implementation, we use warmup training to first train w sufficiently and then begin to update ? and w jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Details about the Search and Training Process Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Search Process Implementation</head><p>We introduce the implementation details of the search process of FBNetV5. As discussed in Section 3, our search is conducted by training a supernet on a multitask proxy dataset. Details of the dataset creation can be found in Section 3.2, supernet design can be found in Section 3.1. Our search is based on the supernet extended from an FBNetV3-A model as illustrated in Section 3.1. On top of the supernet we use the FBNet-V3 style classification head attached to the end of Path 3. We use a Faster R-CNN head attached to Path 2 for object detection, and a single convolutional layer as the segmentation head attached to Path 2 for semantic segmentation. Note since we only care about topologies, heads used during search can be different from the heads for downstream task. We pretrain the extended supernet on the ImageNet for classification, and then train it on the multitask proxy dataset. We implement the search algorithm in D2go 3 powered by Pytorch <ref type="bibr" target="#b38">[39]</ref> and Detectron2 <ref type="bibr" target="#b58">[58]</ref>. To train the supernet, we use a total batch size of 768. The images are resized such that the short size is 256, and we take a random crop with size 224x224 to feed to the model. We train the supernet for 9375 steps using SGD with a base learning rate of 0.96. We decay the learning rate by 10x at 3125 steps. We set the initial sampling probability of all blocks to 0.5. We do not update the architecture parameters until 6250 steps, and we the architecture parameter's learning rate is 0.01 of the regular learning rate for weights. We use 16 V100 GPUs to train the supernet. It takes about 10 hours to finish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Training Process Implementation</head><p>For training the task-specific architectures searched by FBNetV5, we follow existing SotA training recipes for each task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b58">58]</ref> and use PyTorch <ref type="bibr" target="#b38">[39]</ref> for all the experiments.</p><p>For ImageNet <ref type="bibr" target="#b14">[15]</ref> image classification, we use the FB-NetV3 style MBPool+FC classification head on top of the final feature map from Path 3 in <ref type="figure">Figure 2</ref>. We adopt the distillation based training settings in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref> and use a large pretrained model that has a 85.5% top-1 accuracy on Ima-geNet as the teacher model. We use a batch size of 4096 on 64 V100 GPUs for 1100 epochs, using SGD with momentum 0.9 and weight decay 2?10 ?5 as the optimizer, initializing the learning rate as 4.0 with 11 epochs warm-up from 0.01, and decaying it each epoch with a factor of 0.9875.</p><p>For ADE20K semantic segmentation, we modify the MaskFormer's <ref type="bibr" target="#b9">[10]</ref> segmentation head to a lighter version, i.e., we use a pixel decoder with a 3?3 convolution layer and shrink the transformer decoder to only contain 1 Transformer layer, dubbed as Lite MaskFormer. The pixel decoder is attached to the end of Path 1, and the transformer decoder is attached to Path 3. We use the same training settings for ResNet backbone in <ref type="bibr" target="#b9">[10]</ref> to train all the searched architectures except using 320k iterations for bigger models (FBNetV5-A/C/F-SEG in <ref type="table">Table 3</ref>) following <ref type="bibr" target="#b55">[55]</ref>. We initialize the backbone with the weights of the ImageNetpretrained supernet.</p><p>For COCO <ref type="bibr" target="#b31">[32]</ref> object detection, we use the searched architectures as the backbone feature extractor. We attach a Faster R-CNN <ref type="bibr" target="#b42">[43]</ref> head on Path 1 of the supernet. We redesign the ROI and RPN head to have a lighter architecture, and reduce the number of ROI proposals to 30 and name this version as Lite R-CNN. We follow most of the default training settings in <ref type="bibr" target="#b58">[58]</ref> while using a batch size of 256 to train all the searched architectures for 150k iterations with a base learning rate of 0.16, and decay the learning rate by 10 after 140K steps. We keep an exponential moving average (EMA) of the model weights, and evaluate on the EMA model. The same as the settings in ADE20K above, we initialize the backbone with the weights from the ImageNetpretrained supernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Design of light Detection and Segmentation Head G.1. Architecture of the Lite MaskFormer Head</head><p>MaskFormer <ref type="bibr" target="#b9">[10]</ref> consists of three components, a pixel decoder (PD), a transformer decoder (TD), and a segmentation module (SM). The pixel decoder is used to generate the per-pixel embeddings. The transformer decoder is designed to output the per-segment embeddings which encode the global information of each segment. The segmentation module converts the per-segment embeddings to mask embeddings via a Multi-Layer Perceptron (MLP), and then obtain final predication via a dot product between the per-pixel embeddings from the pixel decoder and the mask embeddings.</p><p>We squeeze both the pixel decoder and the transformer decoder to build the Lite MaskFormer used in our experiments.</p><p>Our pixel decoder takes the output from Path 1 and leverage a 3?3 convolution layer to generate the per-pixel embeddings.</p><p>Our transformer decoder follows the design of Mask-Former's transformer decoder, i.e., the same with DETR <ref type="bibr" target="#b4">[5]</ref>. But we shrink it to only contain 1 Transformer <ref type="bibr" target="#b0">[1]</ref> layer and attach it to the output of Path 3.</p><p>We further demonstrate the distribution of our models' FLOPs in <ref type="table">Table 6</ref>. The total FLOPs is the sum of BB, PD, TD, and SM FLOPs, and it is computed based on the input resolution of (short size ? short size) following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">60]</ref>. <ref type="table">Table 6</ref>. FLOPs of FBNetV5 segmentation models. BB, PD, TD, SM columns reports the million (M) FLOPs of the backbone, pixel decoder, transformer decoder and segmentation module of a model given the input size as (short size ? short size). Column Total is the sum of BB, PD, TD, SM. For object detection, we attach Faster R-CNN <ref type="bibr" target="#b42">[43]</ref> head to our searched backbones. Faster R-CNN detection contains two component, a region proposal network (RPN) and a region-of-interest (ROI) head. We use light-weight RPN and ROI heads to save the overall compute cost.</p><p>Our ROI head contains a inverted resitual block (IRB) <ref type="bibr" target="#b44">[45]</ref> with kernel size 3, expansion ratio 3, output channel size 96. We also use Squeeze-Excitation <ref type="bibr" target="#b24">[25]</ref> and HSigmoid activation following <ref type="bibr" target="#b23">[24]</ref>. The output of RPN is fed to a single convolution layer to generate RPN output.</p><p>Our RPN head contains 4 IRB blocks with the same kernel size of 3; expansion ration of 4, 6, 6, 6; output channel size of 128, 128, 128, 160. The IRB blocks do not use SE or HSigmoid. We use an ROIPool operator to extract feature maps from a region-of-interest, and reshape the spatial size to 6x6. The first IRB block further down-samples the input resolution to 3x3. The output of the IRB blocks are projected by a single conv layer to predict ROI output (bounding box prediction, class prediction, etc.).</p><p>During inference, we select the 30 regions post NMS and feed them to ROI. Under this setting, our models FLOPs distribution is shown in <ref type="table" target="#tab_6">Table 7</ref>. Note that the total FLOPs of our model is computed based on the reference input size. The total FLOPs is the sum of BB, RPN, and ROI FLOPs. The average FLOPs reported in <ref type="table" target="#tab_3">Table 4</ref> is computed based on images in the COCO val set. H. Average FLOPs of R-CNN models.</p><p>In <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table" target="#tab_6">Table 7</ref>, we report the average FLOPs of our model on the COCO validation dataset. This is because our R-CNN based detection model does not fix the input size, while our baselines <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> takes a fixed input size. It is a more fair to use the average FLOPs of R-CNN models to compare models with a fixed input size.</p><p>During inference, our R-CNN model re-size images using the following strategy. We first define two parameters min size (set to 224 or 320) and max size (set to 320 or 640). For an input image, we first resize the image such that its short size becomes min size while keeping the aspect ratio the same. After this, if the longer side the image becomes larger than max size, we re-size the image again to make sure the longer side becomes max size, while not changing the aspect ratio.</p><p>To compute the average FLOPs, we first compute the backbone (BB), RPN, ROI, and total flops of the model based on a reference input (e.g., 213x320 or 320x481), as in <ref type="table" target="#tab_6">Table 7</ref>. Then, we compute the number of pixels in the reference image, and the average number of pixels for all images in the dataset. We compute a ratio ratio between the average and reference pixel number. Finally, we compute the average FLOPs as ratio x (BB + RPN) + ROI, where BB, RPN, ROI denotes the backbone, RPN, ROI FLOPs of the model. We do not scale ROI since the backbone and RPN flops is determined by the input resolution while ROI's FLOPs do not depend on input resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FLOPs</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>for iter = 1, ? ? ? , N do 2: Sample a batch of data x 3: Sample a ? p ? 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 4 A</head><label>4</label><figDesc>Single Run Multitask NAS with Importance Sampling and REINFORCE 1: for iter = 1, ? ? ? , N do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6 : 7 :</head><label>67</label><figDesc>Importance Sampling ? t ? p ? t (a)/q(a)Accumulate loss = + ? t ? t (a, w, y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the searched architectures for semantic segmentation (SEG), object detection (DET), and image classification (CLS) tasks. that our searched CLS model demonstrates stronger performance than sequential architectures, this may open up a new direction for the classification model design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 # 8 [ 9 [ 13 [ 18 [ 19 [ 20 [ 21 [ 25 [ 26 [ 27 [ 8 [ 9 [ 13 [ 14 [ 18 [ 19 [ 20 [ 25 [ 26 [ 27 [ 37 [ 38 [</head><label>18913181920212526278913141819202526273738</label><figDesc>Official FBNetV3-A 2 # input_size: 224 3 # [[Operator, Channels, Stride, Repeats]] 4 # Partition 0 5 [["conv_k3_hs",16,2,1]],<ref type="bibr" target="#b5">6</ref> [["ir_k3_hs",16,1,2,{"expansion": 1}]],<ref type="bibr" target="#b6">7</ref> [ "ir_k5_hs",24,2,1,{"expansion": 4}], "ir_k5_hs",24,1,3,{"expansion": 2}], 10 ], 11 # Partition 1 12 [ "ir_k5_sehsig_hs",40,2,1,{"expansion": 5}],<ref type="bibr" target="#b13">14</ref> ["ir_k5_sehsig_hs",40,1,4,{"expansion": 3}],<ref type="bibr" target="#b14">15</ref> ],<ref type="bibr" target="#b15">16</ref> # Partition 2<ref type="bibr" target="#b16">17</ref> [ "ir_k5_hs",72,2,1,{"expansion": 5}], "ir_k3_hs",72,1,4,{"expansion": 3}], "ir_k3_sehsig_hs",120,1,1,{"expansion": 5}], "ir_k5_sehsig_hs",120,1,5,{"expansion": 3}],<ref type="bibr" target="#b21">22</ref> ], 23 # Partition 3<ref type="bibr" target="#b23">24</ref> [ "ir_k3_sehsig_hs",184,2,1,{"expansion": 6}], "ir_k5_sehsig_hs",184,1,5,{"expansion": 4}], "ir_k5_sehsig_hs",224,1,1,{"expansion": 6}],<ref type="bibr" target="#b27">28</ref> ], Listing 1. Code snippets of FBNetV3-A 1 # Supernet extended from FBNetV3-A 2 # input_size: 224, 3 # [[Operator, Channels, Stride, Repeats]] 4 # Path 0, Stage 0 5 [["conv_k3_hs",16,2,1]], 6 [["ir_k3_hs",16,1,2,{"expansion": 1}]],<ref type="bibr" target="#b6">7</ref> [ "ir_k5_hs",24,2,1,{"expansion": 4}], "ir_k5_hs",24,1,3,{"expansion": 2}], 10 ], 11 # Path 1, Stage 0 12 [ "ir_k5_sehsig_hs",40,2,1,{"expansion": 5}], "ir_k5_sehsig_hs",40,1,4,{"expansion": 3}],<ref type="bibr" target="#b14">15</ref> ],<ref type="bibr" target="#b15">16</ref> # Path 2, Stage 0 17 [ "ir_k5_hs",72,2,1,{"expansion": 5}], "ir_k3_hs",72,1,4,{"expansion": 3}], "ir_k3_sehsig_hs",120,1,1,{"expansion": 5}],<ref type="bibr" target="#b20">21</ref> ["ir_k5_sehsig_hs",120,1,5,{"expansion": 3}],<ref type="bibr" target="#b21">22</ref> ], 23 # Path 3, Stage 0 2 https://github.com/facebookresearch/mobilevision/blob/main/mobile cv/arch/fbnet v2/fbnet modeldef cls fbnetv3.py<ref type="bibr" target="#b23">24</ref> [ "ir_k3_sehsig_hs",184,2,1,{"expansion": 6}], "ir_k5_sehsig_hs",184,1,5,{"expansion": 4}], "ir_k5_sehsig_hs",224,1,1,{"expansion": 6}],<ref type="bibr" target="#b27">28</ref> ],<ref type="bibr" target="#b28">29</ref> # Path 0, Stage 1 to Stage s 30 [["ir_k5_hs",24,1,2,{"expansion": 2}]], 31 # Path1, Stage 1 to Stage s 32 [["ir_k5_sehsig_hs",40,1,2,{"expansion": 3}]], 33 # Path2, Stage 1 to Stage s 34 [["ir_k5_sehsig_hs",120,1,2,{"expansion": 3}]], 35 # Path3, Stage 1 to Stage s 36 [ "ir_k5_sehsig_hs",224,1,1,{"expansion": 4}], "ir_k5_sehsig_hs",224,1,1,{"expansion": 6}], 39 ], 40 # Fusion 41 [["fusion", [24, 40, 120, 224], 1, 1]], Listing 2. Code snippets of the supernet extended from FBNetV3-A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>-top) and PANet (Figure 5-middle) topology for semantic segmentation and 2) the FPN (Figure 5-top) and BiFPN (Figure 5-bottom) topology for object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of the differentiable NAS algorithms. T represents the number of tasks.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Search Cost</cell></row><row><cell cols="2">Search Algorithms #Tasks to Handle</cell><cell cols="2">#Forward #Backprop.</cell></row><row><cell></cell><cell></cell><cell>Per Iter</cell><cell>Per Iter</cell></row><row><cell>Algorithm 1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Algorithm 2</cell><cell>T</cell><cell>T</cell><cell>T</cell></row><row><cell>Algorithm 3</cell><cell>T</cell><cell>1</cell><cell>T</cell></row><row><cell>Algorithm 4</cell><cell>T</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3</head><label>23</label><figDesc>Comparisons with SotA compact models on the Ima-geNet<ref type="bibr" target="#b14">[15]</ref> image classification task.</figDesc><table><row><cell>Model</cell><cell>Input Size</cell><cell>FLOPs</cell><cell cols="2">Accuracy (%, Top-1)</cell></row><row><cell>HR-NAS-A [16]</cell><cell>224 ? 224</cell><cell>267M</cell><cell>76.6</cell><cell></cell></row><row><cell>LeViT-128S [22]</cell><cell>224 ? 224</cell><cell>305M</cell><cell>76.6</cell><cell></cell></row><row><cell>BigNASModel-S [63]</cell><cell>192 ? 192</cell><cell>242M</cell><cell>76.5</cell><cell></cell></row><row><cell cols="2">MobileNetV3-1.25x [24] 224 ? 224</cell><cell>356M</cell><cell>76.6</cell><cell></cell></row><row><cell>FBNetV5-A R -CLS</cell><cell>160 ? 160</cell><cell>215M</cell><cell>77.2</cell><cell></cell></row><row><cell>HR-NAS-B [16]</cell><cell>224 ? 224</cell><cell>325M</cell><cell>77.3</cell><cell></cell></row><row><cell>LeViT-128 [22]</cell><cell>224 ? 224</cell><cell>406M</cell><cell>78.6</cell><cell></cell></row><row><cell>EfficientNet-B0 [49]</cell><cell>224 ? 224</cell><cell>390M</cell><cell>77.3</cell><cell></cell></row><row><cell>FBNetV5-A C -CLS</cell><cell>224 ? 224</cell><cell>280M</cell><cell>78.4</cell><cell></cell></row><row><cell>EfficientNet-B1 [49]</cell><cell>240 ? 240</cell><cell>700M</cell><cell>79.1</cell><cell></cell></row><row><cell>FBNetV3-E [13]</cell><cell>264 ? 264</cell><cell>762M</cell><cell>81.3</cell><cell></cell></row><row><cell>FBNetV5-A-CLS</cell><cell>224 ? 224</cell><cell>685M</cell><cell>81.7</cell><cell></cell></row><row><cell>LeViT-256 [22]</cell><cell>224 ? 224</cell><cell>1.1G</cell><cell>81.6</cell><cell></cell></row><row><cell>EfficientNet-B2 [49]</cell><cell>260 ? 260</cell><cell>1.0G</cell><cell>80.3</cell><cell></cell></row><row><cell cols="2">BigNASModel-XL [63] 288 ? 288</cell><cell>1.0G</cell><cell>80.9</cell><cell></cell></row><row><cell>FBNetV3-F [13]</cell><cell>272 ? 272</cell><cell>1.2G</cell><cell>82.5</cell><cell></cell></row><row><cell>FBNetV5-C-CLS</cell><cell>248 ? 248</cell><cell>1.0G</cell><cell>82.6</cell><cell></cell></row><row><cell>Swin-T [35]</cell><cell>224 ? 224</cell><cell>4.5G</cell><cell>81.3</cell><cell></cell></row><row><cell>LeViT-384 [22]</cell><cell>224 ? 224</cell><cell>2.4G</cell><cell>82.6</cell><cell></cell></row><row><cell>BossNet-T1 [28]</cell><cell>288 ? 288</cell><cell>5.7G</cell><cell>81.6</cell><cell></cell></row><row><cell>EfficientNet-B4 [49]</cell><cell>380 ? 380</cell><cell>4.2G</cell><cell>82.9</cell><cell></cell></row><row><cell>FBNetV3-G [13]</cell><cell>320 ? 320</cell><cell>2.1G</cell><cell>82.8</cell><cell></cell></row><row><cell>FBNetV5-F-CLS</cell><cell>272 ? 272</cell><cell>2.1G</cell><cell>84.1</cell><cell></cell></row><row><cell>Backbone</cell><cell>Head</cell><cell cols="2">Short FLOPs Size</cell><cell>mIoU (%)</cell></row><row><cell>HR-NAS-A [16]</cell><cell>Concatenation [16]</cell><cell>512</cell><cell>1.4G</cell><cell>33.2</cell></row><row><cell>MobileNetV3-Large [29]</cell><cell>Lite MaskFormer</cell><cell>448</cell><cell>1.5G</cell><cell>29.2</cell></row><row><cell>FBNetV5-A C -SEG</cell><cell>Lite MaskFormer</cell><cell>384</cell><cell>1.3G</cell><cell>35.6</cell></row><row><cell>HR-NAS-B [16]</cell><cell>Concatenation [16]</cell><cell>512</cell><cell>2.2G</cell><cell>34.9</cell></row><row><cell>EfficientNet-B0 [49]</cell><cell>Lite MaskFormer</cell><cell>448</cell><cell>2.1G</cell><cell>31.3</cell></row><row><cell>FBNetV5-A R -SEG</cell><cell>Lite MaskFormer</cell><cell>384</cell><cell>1.8G</cell><cell>37.8</cell></row><row><cell>MiT-B0 [60]</cell><cell>SegFormer [60]</cell><cell>512</cell><cell>8.4G</cell><cell>37.4</cell></row><row><cell>FBNetV5-A-SEG</cell><cell>Lite MaskFormer</cell><cell>384</cell><cell>2.9G</cell><cell>41.2</cell></row><row><cell>MiT-B1 [60]</cell><cell>SegFormer [60]</cell><cell>512</cell><cell>15.9G</cell><cell>42.2</cell></row><row><cell>FBNetV5-C-SEG</cell><cell>Lite MaskFormer</cell><cell>448</cell><cell>4.4G</cell><cell>44.0</cell></row><row><cell>Swin-T [35]</cell><cell>UperNet [59]</cell><cell>512</cell><cell>236G</cell><cell>46.1  ?</cell></row><row><cell>Swin-T [35]</cell><cell>MaskFormer [10]</cell><cell>512</cell><cell>55G</cell><cell>46.7</cell></row><row><cell>ResNet-50 [23]</cell><cell>MaskFormer [10]</cell><cell>512</cell><cell>53G</cell><cell>44.5</cell></row><row><cell>PVT-Large [55]</cell><cell>Semantic FPN [27]</cell><cell>512</cell><cell>80G</cell><cell>44.8  ?</cell></row><row><cell>FBNetV5-F-SEG</cell><cell>Lite MaskFormer</cell><cell>512</cell><cell>9.4G</cell><cell>46.5</cell></row></table><note>. Comparisons with SotA compact models on the ADE20K semantic segmentation task. All mIoUs are reported in the single- scale setting (except those marked with ? ) in ADE20K val. and FLOPs is measured with the input resolution of (short size ? short size) following [10, 60]. The implementation details of Lite MaskFormer are illustrated in Appendix G.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with SotA compact models on the COCO object detection task. mAPs are based on COCO val. For<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">61]</ref>, we cite their FLOPs with the given resolution. For R-CNN models, since their input sizes are not fixed, we report the average FLOPs on the COCO val dataset. See Appendix H for details.</figDesc><table><row><cell>Backbone</cell><cell>Head</cell><cell cols="2">Short, Long FLOPs Size</cell><cell>mAP (%)</cell></row><row><cell>ShuffleNetV2 1.0x [36]</cell><cell>NanoDet-m [41]</cell><cell>320, 320</cell><cell>720M</cell><cell>20.6</cell></row><row><cell>EfficientNet-B0 [49]</cell><cell>Lite R-CNN</cell><cell>224, 320</cell><cell>793M</cell><cell>23.1</cell></row><row><cell>FBNetV5-A C -DET</cell><cell>Lite R-CNN</cell><cell>224, 320</cell><cell>713M</cell><cell>25.0</cell></row><row><cell>MobileDets [61]</cell><cell>SSDLite [45]</cell><cell>320, 320</cell><cell>920M</cell><cell>25.6</cell></row><row><cell>ShuffleNetV2 1.0x [36]</cell><cell>NanoDet-m [41]</cell><cell>416, 416</cell><cell>1.2G</cell><cell>23.5</cell></row><row><cell cols="2">Modified CSP v5 [19] YOLOX-Nano [19]</cell><cell>416, 416</cell><cell>1.1G</cell><cell>25.3</cell></row><row><cell>EfficientNet-B2 [49]</cell><cell>Lite R-CNN</cell><cell>224, 320</cell><cell>1.2G</cell><cell>24.9</cell></row><row><cell>FBNetV5-A R -DET</cell><cell>Lite R-CNN</cell><cell>224, 320</cell><cell>908M</cell><cell>26.4</cell></row><row><cell>ShuffleNetV2 1.5x [36]</cell><cell>NanoDet-m [41]</cell><cell>416, 416</cell><cell>2.4G</cell><cell>26.8</cell></row><row><cell>EfficientNet-B3 [49]</cell><cell>Lite R-CNN</cell><cell>224, 320</cell><cell>1.6G</cell><cell>26.2</cell></row><row><cell>FBNetV5-A-DET</cell><cell>Lite R-CNN</cell><cell>224, 320</cell><cell>1.35G</cell><cell>27.2</cell></row><row><cell>FBNetV5-A C -DET</cell><cell>Lite R-CNN</cell><cell>320, 640</cell><cell>1.37G</cell><cell>28.9</cell></row><row><cell>FBNetV5-A R -DET</cell><cell>Lite R-CNN</cell><cell>320, 640</cell><cell>1.80G</cell><cell>30.4</cell></row></table><note>models by achieving ?0.3% higher accuracy on image clas- sification, ?1.6% higher mIoU on semantic segmemtation, and ?0.4% higher mAP on object detection. Compared with single-task search, models searched by multitask search de- liver very similar performance (e.g., 2.8G vs. 2.7G FLOPs under the same mIoU on ADE20K [65]) while reducing the search cost for each task by a factor of T times.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Effectiveness of our search algorithms when benchmarked in ImageNet<ref type="bibr" target="#b14">[15]</ref> image classification (CLS), ADE20K<ref type="bibr" target="#b65">[65]</ref> semantic segmentation (SEG), and COCO<ref type="bibr" target="#b31">[32]</ref> object detection (DET). T represents the number of tasks. All the models of SEG are trained for 160K iterations for fast verification.</figDesc><table><row><cell>Tasks</cell><cell>Search Algorithm</cell><cell>Search Cost (GPU hours)</cell><cell>FLOPs</cell><cell>Top-1 Accuracy/ mIoU / mAP (%)</cell></row><row><cell></cell><cell>Random</cell><cell>-</cell><cell>769M</cell><cell>81.5</cell></row><row><cell>CLS</cell><cell>Single Task (Alg. 1)</cell><cell>4000</cell><cell>688M</cell><cell>81.9 (?0.4)</cell></row><row><cell></cell><cell>FBNetV5 (Alg. 4)</cell><cell>4000 / T</cell><cell>726M</cell><cell>81.8 (?0.3)</cell></row><row><cell></cell><cell>Random</cell><cell>-</cell><cell>2.9G</cell><cell>38.8</cell></row><row><cell>SEG</cell><cell>Single Task (Alg. 1)</cell><cell>4000</cell><cell>2.7G</cell><cell>40.4 (?1.6)</cell></row><row><cell></cell><cell>FBNetV5 (Alg. 4)</cell><cell>4000 / T</cell><cell>2.8G</cell><cell>40.4 (?1.6)</cell></row><row><cell></cell><cell>Random</cell><cell>-</cell><cell>1.34G</cell><cell>26.8</cell></row><row><cell>DET</cell><cell>Single Task (Alg. 1)</cell><cell>4000</cell><cell>1.36G</cell><cell>27.3 (?0.5)</cell></row><row><cell></cell><cell>FBNetV5 (Alg. 4)</cell><cell>4000 / T</cell><cell>1.36G</cell><cell>27.2 (?0.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>FLOPs of FBNetV5 detection models. BB, RPN, ROI columns reports the million (M) FLOPs of the backbone, RPN, and ROI of a model given the reference input size. Column Total is the sum of BB, RPN, ROI. Column Avg. reports the average FLOPs of the model on the COCO val set.</figDesc><table><row><cell>Model</cell><cell cols="2">Ref. Size BB</cell><cell cols="2">RPN ROI Total Avg.</cell></row><row><cell cols="2">FBNetV5-A C -DET 213x320</cell><cell>399</cell><cell>152 182 733</cell><cell>713</cell></row><row><cell cols="2">FBNetV5-A R -DET 213x320</cell><cell>601</cell><cell>152 182 935</cell><cell>908</cell></row><row><cell>FBNetV5-A-DET</cell><cell>213x320</cell><cell cols="2">1054 158 186 1398</cell><cell>1354</cell></row><row><cell cols="2">FBNetV5-A C -DET 320x481</cell><cell>912</cell><cell>347 182 1441</cell><cell>1367</cell></row><row><cell cols="2">FBNetV5-A R -DET 320x481</cell><cell cols="2">1372 347 182 1901</cell><cell>1800</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/d2go</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"># FBNetV3-A_C 2 # input_size: 224 3 # [[Operator, Channels, Stride, Repeats]]</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25">["ir_k3_sehsig_hs",138,2,1,{"expansion": 6}],<ref type="bibr" target="#b25">26</ref> ["ir_k3_sehsig_hs",138,1,5,{"expansion": 4}],<ref type="bibr" target="#b26">27</ref> ["ir_k3_sehsig_hs",168,1,1,{"expansion": 6}],<ref type="bibr" target="#b27">28</ref> ],Listing 4. Code snippets of FBNetV3-AR E. Important Implementation Details of Algorithm 4We provide several impotant implementation details of Algorithm 4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/d2go</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probabilistic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Paolo Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10952</idno>
		<title level="m">Progressive darts: Bridging the optimization gap for NAS in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DetNAS: Backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6642" to="6652" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ScaleNAS: One-shot learning of scale-aware representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Pai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evolving search space for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzheng</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="6659" to="6669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FBNetV3: Joint architecture-recipe search using neural acquisition function. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?ter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11398" to="11407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Searching efficient high-resolution neural architectures with lightweight transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hr-Nas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">NAS-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11592" to="11601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Exceeding YOLO series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task self-training for learning general representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Searching for Mo-bileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Boss-NAS: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12424</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Searching for fast model families on datacenter accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman P Jouppi</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8085" to="8095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Monte Carlo theory, methods and examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Art</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rangilyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nanodet</surname></persName>
		</author>
		<ptr target="https://github.com/RangiLyu/nanodet,2021.7" />
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SqueezeNAS: Fast neural architecture search for faster semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Landola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Sidhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">AlphaNet: Improved training of supernet with alpha-divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07954</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<imprint>
			<publisher>Mingkui</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">MobileDets: Searching for object detection architectures for mobile accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14525</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">FP-NAS: Fast probabilistic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15139" to="15148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">BigNAS: Scaling up neural architecture search with big single-stage models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="702" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">DCNAS: Densely connected neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13956" to="13967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Probabilistic two-stage detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
