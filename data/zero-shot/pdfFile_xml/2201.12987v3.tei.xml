<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaoyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using posthoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20%? in interpretation AUC and 5%? in prediction accuracy. Our code is available at https: //github.com/Graph-COM/GSAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph learning models are widely used in science, such as physics <ref type="bibr" target="#b4">(Bapst et al., 2020)</ref> and biochemistry <ref type="bibr" target="#b19">(Jumper et al., 2021)</ref>. In many such disciplines, building more accurate predictive models is typically not the only goal. It is often more crucial for scientists to discover the patterns from the data that induce certain predictions <ref type="bibr" target="#b10">(Cranmer et al., 2020)</ref>. For example, identifying the functional groups in a molecule that yield its certain properties may provide insights to guide further experiments <ref type="bibr" target="#b44">(Wencel-Delord &amp; Glorius, 2013)</ref>.</p><p>Recently, graph neural networks (GNNs) have become almost the de fato graph learning models due to their great expressive power <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b49">Xu et al., 2019)</ref>. However, their expressivity is often built upon a highly nonlinear entanglement of irregular graph features. So, it is often quite challenging to figure out the patterns in the data that GNNs use to make predictions.</p><p>Many works have been recently proposed to extract critical data patterns for the prediction by interpreting GNNs in post-hoc ways <ref type="bibr" target="#b50">(Ying et al., 2019;</ref><ref type="bibr">Yuan et al., 2020a;</ref><ref type="bibr" target="#b43">Vu &amp; Thai, 2020;</ref><ref type="bibr" target="#b28">Luo et al., 2020;</ref><ref type="bibr" target="#b34">Schlichtkrull et al., 2021;</ref><ref type="bibr">Yuan et al., 2021;</ref><ref type="bibr" target="#b26">Lin et al., 2021;</ref><ref type="bibr">Henderson et al., 2021)</ref>. They work on a pre-trained model and propose different types of combinatorial search methods to detect the subgraphs of the input data that affect the model predictions the most.</p><p>In contrast to the above post-hoc methods, inherently interpretable models have been rarely investigated for graph learning tasks. There are two main concerns regarding such models. First, the prediction accuracy and inherent interpretability of a model often forms a trade-off <ref type="bibr" target="#b12">(Du et al., 2019)</ref>. Practitioners may not allow sacrificing prediction accuracy for better interpretability. Second, the attention mechanism, a widely-used technique to provide inherent interpretability, often cannot provide faithful interpretation <ref type="bibr" target="#b27">(Lipton, 2018)</ref>. The rationale of the attention mechanism is to learn weights for different features during the model training, and the rank of the learned weights can be interpreted as the importance of certain features <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b48">Xu et al., 2015)</ref>. However, recent extensive evaluations in NLP tasks <ref type="bibr" target="#b35">(Serrano &amp; Smith, 2019;</ref><ref type="bibr" target="#b17">Jain &amp; Wallace, 2019;</ref><ref type="bibr" target="#b29">Mohankumar et al., 2020)</ref> have shown that the attention may not weigh the features that dominate the model output more than other features. In particular, for graph learning tasks, the widely-used graph attention models <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b25">Li et al., 2016)</ref> seem unable to provide any reliable interpretation of the data <ref type="bibr" target="#b50">(Ying et al., 2019;</ref><ref type="bibr">Yu et al., 2021)</ref>.</p><p>Along another line of research, invariant learning <ref type="bibr">(Pearl arXiv:2201.12987v3 [cs.</ref>LG] 17 Jun 2022 <ref type="figure">Figure 1</ref>. The architecture of GSAT. g ? encodes the input graph G and learns stochastic attention ? (from Bernoulli distributions) that randomly drop the edges and obtain a perturbed graph GS. f ? encodes GS to make predictions. GSAT does not constrain the size of GS but injects stochasticity to constrain information. The subgraph of GS with learnt reduced-stochasticity (edges with pe ? 1) provides interpretation. GSAT is a unified model by adopting just one GNN for both g ? and f ? . GSAT can be either trained from scratch or start from a pre-trained GNN predictor f ? . <ref type="bibr" target="#b1">Arjovsky et al., 2019;</ref><ref type="bibr" target="#b6">Chang et al., 2020;</ref><ref type="bibr">Krueger et al., 2021)</ref> has been proposed to provide inherent interpretability and better generalizability. They argue that the models na?vely trained over biased data may risk capturing spurious correlations between the input environment features and the labels, and thus suffer from severe generalization issues. So, they propose to train models that align with the causal relations between the signal features and the labels. However, such training approaches to match causal relations typically have high computational complexity.</p><p>In this work, we are to address the above concerns by proposing Graph Stochastic Attention (GSAT), a novel attention mechanism to build inherently interpretable and well generalizable GNNs. The rationale of GSAT roots in the notion of information bottleneck (IB) <ref type="bibr" target="#b40">(Tishby et al., 2000;</ref><ref type="bibr" target="#b39">Tishby &amp; Zaslavsky, 2015)</ref>. We formulate the attention as an IB by injecting stochasticity into the attention to constrain the information flow from the input graph to the prediction <ref type="bibr" target="#b36">(Shannon, 1948)</ref>. Such stochasticity over the label-irrelevant graph components will be kept during the training while that over the label-relevant ones can automatically get reduced. This difference eventually provides model interpretation. By penalizing the amount of information from the input data, GSAT is also expected to be more generalizable.</p><p>Our study achieves the following observations and contributions. First, the IB principle frees GSAT from any potentially biased assumptions adopted in previous methods such as the size or the connectivity constraints on the detected graph patterns. Even when those assumptions are satisfied, GSAT still works the best without using such assumptions, while when those assumptions are not satisfied, GSAT achieves significantly better interpretation. See the sampled interpretation result visualizations in <ref type="figure" target="#fig_0">Fig. 2</ref> and <ref type="figure" target="#fig_1">Fig. 3</ref>. Second, from the perspective of IB, all post-hoc interpretation methods are suboptimal. They essentially optimize a model without any information control and then perform a single-step projection to an information-controlled  <ref type="bibr" target="#b34">(Schlichtkrull et al., 2021)</ref> (third row) on MNIST-75sp. The first row shows the ground-truth. Different digit samples contain interpretable subgraphs of different sizes, while GSAT is not sensitive to such varied sizes.  <ref type="bibr" target="#b34">(Schlichtkrull et al., 2021</ref>) (second row) on a motif example, where graphs with three house motifs and graphs with two house motifs represent two classes. Samples may contain disconnected interpretable subgraphs, while GSAT detects them accurately. More details can be found in Appendix C.4. space, which makes the final interpretation performance sensitive to the pre-trained models. Third, by reducing the information from the input graph, GSAT can provably remove spurious correlations in the training data under certain assumptions and achieve better generalization. Fourth, if a pre-trained model is provided, GSAT may further improve both of its interpretation and prediction accuracy.</p><p>We evaluate GSAT in terms of both interpretability and label-prediction performance. Experiments over 8 datasets show that GSAT outperforms the state-of-the-art (SOTA) methods by up to 20%? in interpretation AUC and 5%? in prediction accuracy. Notably, GSAT achieves the SOTA performance on molhiv on OGB <ref type="bibr" target="#b15">(Hu et al., 2020)</ref> among the models that do not use manually-designed expert features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>As preliminaries, we define a few notations and concepts.</p><p>Graph. An attributed graph can be denoted as G = (A, X) where A is the adjacency matrix and X includes node attributes. Let V and E denote the node set and the edge set, respectively. We focus on graph-level tasks: A training set of graphs with their labels (G (i) , Y (i) ), i = 1, ..., n are given, where each sample (G (i) , Y (i) ) is assumed to be IID sampled from some unknown distribution P Y?G = P Y|G P G .</p><p>Label-relevant Subgraph. A label-relevant subgraph refers to the subgraph G S of the input graph G that mostly indicates the label Y . For example, to determine the solubility of a molecule, the hydroxy group -OH is a positivelabel-relevent subgraph, as if it exists, the molecule is often soluble to the water. Finding label-relevant subgraphs is a common goal of interpretable graph learning.</p><p>Attention Mechanism. Attention mechanism has been widely used in interpretable neural networks for NLP and CV tasks <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b48">Xu et al., 2015;</ref><ref type="bibr" target="#b41">Vaswani et al., 2017)</ref>. However, GNNs with attention <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018)</ref> often generate low-fidelity attention weights. As it learns multiple weights for every edge, it is far from trivial to combine those weights with the irregular graph structure to perform graph label-relevant feature selection.</p><p>There are two types of attention models: One normalizes the attention weights to sum to one <ref type="bibr" target="#b2">(Bahdanau et al., 2015)</ref>, while the other learns weights between [0, 1] without normalization <ref type="bibr" target="#b48">(Xu et al., 2015)</ref>. As the counterparts in GNN models, GAT adopts the normalized one <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018)</ref> while GGNN adopts the unnormalized one <ref type="bibr" target="#b25">(Li et al., 2016)</ref>. Our method belongs to the second category.</p><p>Graph Neural Network. GNNs are neural network models that encode graph-structured data into node representations or graph representations. They initialize each node feature representation with its attributes h (0) v = X v and then gradually update it by aggregating representations from its neighbors, i.e., h</p><formula xml:id="formula_0">(l+1) v ? q(h (l) v , {h (l) u |u : (u, v) ? E})</formula><p>where q(?) denotes a function implemented by NNs <ref type="bibr" target="#b13">(Gilmer et al., 2017)</ref>. Graph representations are often obtained via an aggregation (sum/mean) of node representations.</p><p>Learning to Explain (L2X). L2X <ref type="bibr" target="#b7">(Chen et al., 2018)</ref> studies the feature selection problem in the regular feature space and proposed a mutual information (MI) maximization rule to select a fixed number of features. Specifically, let I(a; b) a,b P(a, b) log P(a,b) P(a)P(b) denote the MI between two random variables a and b. Large MI indicates certain high correlation between two random variables. Hence, with input features X ? R F , L2X is to search a k-sized set of indices S ? {1, 2, ..., F }, where k = |S| &lt; F , such that the features in the subspace indexed by S (denoted by X S ) maximizes the mutual information with the labels Y , i.e., max S?{1,2,...,F }</p><formula xml:id="formula_1">I(X S ; Y ), s.t. |S| ? k.</formula><p>(1)</p><p>Our model is inspired by L2X. However, as graph features and their interpretable counterparts are in an irregular space without a fixed dimension, directly applying L2X may achieve subpar performance in graph learning tasks. We propose to use information constraint instead in Sec. 3.1.</p><p>Later, we will also use the entropy defined as H(a) ? a P(a) log P(a) and the KL-divergence defined as KL(P(a)||Q(a)) a P(a) log P(a) Q(a) <ref type="bibr" target="#b9">(Cover, 1999)</ref>.</p><p>Maximizing initial predictor optimal solution one-step proj. <ref type="figure">Figure 4</ref>. Post-hoc methods just perform one-step projection to the information-constrained space, which is always suboptimal and the interpretation performance is sensitive to the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Learning Interpretation via GIB</head><p>In this section, we will first propose the GIB-based objective for interpretable graph learning and point out the issues of post-hoc GNN interpretation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GIB-based Objective for Interpretation</head><p>Finding label-relevant subgraphs in graph learning tasks has unique challenges. As for the irregularity of graph structures, graph learning models often have to deal with the input graphs of various sizes. The critical subgraph patterns may be also of different sizes and be highly irregular. Consider the example of molecular solubility again, although the functional groups for positive solubility such as -OH, -NH 2 are of similar sizes, those for negative solubility range from small groups (e.g., -Cl) to extremely large ones (e.g. -C 10 H 9 ). And, a molecule may contain multiple functional groups scattered in the graph that determine its properties. Given these observations, it is not proper to just mimic the cardinality constraint used for a regular dimension space (Eq. (1)) and select subgraphs of certain sizes potentially with a connectivity constraint as done in <ref type="bibr" target="#b50">(Ying et al., 2019)</ref>. Inspired by the graph information bottleneck (GIB) principle <ref type="bibr" target="#b45">(Wu et al., 2020;</ref><ref type="bibr">Yu et al., 2021)</ref>, we propose to use information constraint instead to select label-relevant subgraphs, i.e., solving</p><formula xml:id="formula_2">max G S I(G S ; Y ), s.t. I(G S ; G) ? ?, G S ? G sub (G) (2)</formula><p>where G sub (G) denotes the set of the subgraphs of G. Note that GIB does not impose any potentially biased constraints such as the size or the connectivity of the selected subgraphs. Instead, GIB uses the information constraint I(G S ; G) ? ? to select G S that inherits only the most indicative information from G to predict the label Y by maximizing I(G S ; Y ). As thus, G S provides model interpretation. <ref type="bibr">Yu et al. (2021)</ref> also considered using GIB to select subgraphs. However, we adopt a fundamentally different mechanism that we will provide a detailed comparison in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Issues of Post-hoc GNN Interpretation Methods</head><p>Almost all previous GNN interpretation methods are posthoc, such as GNNExplainer <ref type="bibr" target="#b50">(Ying et al., 2019)</ref>, PGEx-  <ref type="figure">Figure 5</ref>. Issues of post-hoc interpretation methods. All methods are trained with 10 random seeds; post-hoc methods are also provided with models pre-trained with different seeds. Interpretation performance and the training losses of Eq. 2 for GSAT and Eq. 4 for others are shown. We guarantee that all the pre-trained models are well-trained in their pre-training stage (Acc. ?100% Ba-2Motif, ?90% Mutag).</p><p>plainer <ref type="bibr" target="#b28">(Luo et al., 2020)</ref> and GraphMask <ref type="bibr" target="#b34">(Schlichtkrull et al., 2021)</ref>. Given a pre-trained predictor f ? (?) : G ? Y, they try to find out the subgraph G S that impacts the model predictions the most, while keeping the pre-trained model unchanged. This procedure essentially first maximizes the MI between f ? (G) and Y and obtains a model parameter</p><formula xml:id="formula_3">? arg max ? I(f ? (G); Y ),<label>(3)</label></formula><p>and then optimizes a subgraph extractor g ? vi?</p><formula xml:id="formula_4">? arg max ? I(f?(G S ); Y ), s.t. G S = g ? (G) ? ?. (4)</formula><p>where ? implies a subset of the subgraphs G sub (G) that satisfy some constraints, e.g., the cardinality constraint adopted by GNNExplainer and PGExplainer. Let us temporarily ignore the difference between different constraints and just focus on the optimization objective. The post-hoc objective Eq. (4) and GIB (Eq. (2)) share some similar spirits. However, the post-hoc methods may not give or even approximate the optimal solution to Eq.</p><p>(2) because f ? ? g ? is not jointly trained. From the optimization perspective, post-hoc methods just perform one-single step projection (see <ref type="figure">Fig. 4</ref>) from the model f? in an unconstrained space to f? ? g? in the information-constrained space ? where the projection rule follows that the induced MI decrease I(f?(G); Y ) ? I(f?(g?(G)); Y ) gets minimized.</p><p>In practice, such a suboptimal behavior will yield two undesired consequences. First, f? may not fully extract the information from G S = g ? (G) to predict Y during the optimization of Eq. (4) because f? is originally trained to make</p><formula xml:id="formula_5">I(f?(G); Y ) approximate I(G, Y ) while (G S , Y ) = (g ? (G), Y ) follows a distribution different from (G, Y )</formula><p>. Therefore, I(f?(G S ); Y ) may not well approximate I(G S ; Y ), and thus may mislead the optimization of g ? and disable g ? to select G S that indeed indicates Y . GN-NExplainer suffers from this issue over Ba-2Motif as shown in <ref type="figure">Fig. 5</ref>: The training loss, ?I(f?(G S ); Y ) keeps high and the interpretation performance is subpar. It is possible to further decrease the training loss via a more aggressive optimization of g ? . However, the models may risk overfitting the data, which yields the second issue.</p><p>An aggressive optimization of g ? may give a large empirical MI? f?(g ? (G)); Y (or a small training loss equivalently) by selecting features that help to distinguish labels for training but are essentially irrelevant to the labels or spuriously correlated with the labels in the population level. Previous works have shown that label-irrelevant features are known to be discriminative enough to even identify each graph in the training dataset let alone the labels <ref type="bibr" target="#b38">(Suresh et al., 2021)</ref>. Empirically, we indeed observe such overfitting problems of all post-hoc methods over Mutag as shown in <ref type="figure">Fig. 5</ref>, especially PGExplainer and GraphMask. In the first 5 to 10 epochs, these two models succeed in selecting good explanations while having a large training loss. Further training successfully decreases the loss (after 10 epochs) but degenerates the interpretation performance substantially. This might also be the reason why in the original literatures of these post-hoc methods, training over only a small number of epochs is suggested. However, in practical tasks, it is hard to have the ground truth interpretation labels to verify the results and decide a trusty stopping criterion.</p><p>Another observation of <ref type="figure">Fig. 5</ref> also matches our expectation: From the optimization perspective, post-hoc methods suffer from an initialization issue. Their interpretability can be highly sensitive to the pre-trained model f?, as empirically demonstrated by the large variances in <ref type="figure">Fig. 5</ref>. Only if the pretrained f? approximates the optimal f ? * , the performance can be roughly guaranteed. So, a joint training of f ? ? g ? according to the GIB principle Eq.</p><p>(2) is typically needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stochastic Attention Mechanism for GIB</head><p>In this section, we will first give a tractable variational bound of the GIB objective (Eq. (2)), and then introduce our model GSAT with the stochastic attention mechanism. We will further discuss how the stochastic attention mechanism improves both model interpretation and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A Tractable Objective for GIB</head><p>GSAT is to learn an extractor g ? with parameter ? to extract G S ? G sub (G). g ? blocks the label-irrelevant informa-tion in the data G via injected stochasticity while allowing the label-relevant information kept in G S to make predictions. In GSAT, g ? (G) essentially gives a distribution over G sub (G). We also denote this distribution as P ? (G S |G). Later, g ? (G) and P ? (G S |G) are used interchangeably.</p><p>Putting the constraint into the objective (Eq.(2)), we obtain the optimization of g ? via GIB, i.e., for some ? &gt; 0, </p><formula xml:id="formula_6">min ? ?I(G S ; Y ) + ?I(G S ; G), s.t. G S ? g ? (G). (5) Next,</formula><formula xml:id="formula_7">(G S ; Y ), we introduce a param- eterized variational approximation P ? (Y |G S ) for P(Y |G S ).</formula><p>We obtain a lower bound:</p><formula xml:id="formula_8">I (G S ; Y ) ? E G S ,Y [log P ? (Y |G S )] + H(Y ). (6) Note that P ? (Y |G S ) essentially works as the predictor f ? : G ? Y with parameter ? in our model. For the term I(G S ; G), we introduce a variational approxi- mation Q(G S ) for the marginal distribution P(G S ) = G P ? (G S |G)P G (G).</formula><p>And, we obtain an upper bound:</p><formula xml:id="formula_9">I (G s ; G) ? E G [KL(P ? (G S |G)||Q(G S ))]<label>(7)</label></formula><p>Plugging in the above two inequalities, we obtain a variational upper bound of Eq. (5) as the objective of GSAT:</p><formula xml:id="formula_10">min ?,? ?E [log P ? (Y |G S )] + ?E [KL(P ? (G S |G)||Q(G S ))] , s.t. G S ? P ? (G S |G).<label>(8)</label></formula><p>Next, we specify P ? (aka f ? ), P ? (aka g ? ) and Q in GSAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GSAT and Stochastic Attention Mechanism</head><p>For clarity, we introduced the predictor f ? and the extractor g ? separately. Actually, GSAT is a unified model as f ? , g ? share the same GNN encoder except their last layers.</p><p>Stochastic Attention via P ? . The extractor g ? first encodes the input graph G via the GNN into a set of node represen-</p><formula xml:id="formula_11">tations {h v |v ? V }. For each edge (u, v) ? E, g ? contains an MLP layer plus sigmoid that maps the concatenation (h u , h v ) into p uv ? [0, 1].</formula><p>Then, for each forward pass of the training, we sample stochastic attention from Bernoulli distributions ? uv ? Bern(p uv ). To make sure the gradient w.r.t. p uv is computable, we apply the gumbel-softmax reparameterization trick <ref type="bibr" target="#b18">(Jang et al., 2017)</ref>. The extracted graph G S will have an attention-selected subgraph as A S = ? A.</p><p>Here ? is the matrix with entries ? uv for (u, v) ? E or zeros for the non-edge entries. A is the adjacency matrix of G and is entry-wise product. The distribution of G S given G through the above procedure characterizes P ? (G S |G), so</p><formula xml:id="formula_12">P ? (G S |G) = u,v?E P(? uv |p uv ),</formula><p>where p uv is a function of G. This essentially makes the attention ? uv to be conditionally independent across different edges given the input graph G.</p><p>Prediction via P ? . The predictor f ? adopts the same GNN to encode the extracted graph G S to a graph representation, and finally passes such representation through an MLP layer plus softmax to model the distribution of Y . This procedure gives the variational distribution P ? (Y |G S ).</p><p>Marginal Distribution Control via Q. The bound Eq. <ref type="formula" target="#formula_9">(7)</ref> is always true for any Q(G S ). We define Q(G S ) as follows.</p><p>For every graph G ? P G and every two directed node pair</p><formula xml:id="formula_13">(u, v) in G, we sample ? uv ? Bern(r) where r ? [0, 1]</formula><p>is a hyperparameter. We remove all edges in G and add all edges</p><formula xml:id="formula_14">(u, v) if ? uv = 1. Suppose the obtained graph is G S . This procedure defines the distribution Q(G S ) = G P(? |G)P G (G).</formula><p>As ? is independent from the graph G given its size n, Q(G S ) = n P(? |n)P G (G = n) = P(n) n u,v=1 P(? uv ). The probability of an n-sized graph P(n) is a constant and thus will not affect the model. Note that our choice of Q(G S ) shares the similar spirit of using standard Gaussian as the latent distribution with variational auto-encoders <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref>.</p><p>Using the above P ? , the first term in Eq.(8) reduces to a standard cross entropy loss. Using P ? and Q, the KL-divergence term becomes, for every G ? P G , n as the size of G,</p><formula xml:id="formula_15">KL(P ? (G S |G)||Q(G S )) = (9) (u,v)?E p uv log p uv r + (1 ? p uv ) log 1 ? p uv 1 ? r + c(n, r).</formula><p>where c(n, r) is a constant without any trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Interpretation Mechanism of GSAT</head><p>The interpretability of GSAT essentially comes from the information control: GSAT decreases the information from the input graphs by injecting stochasticity via attention into G S . In the training, the regularization term Eq.(9) would try to assign large stochasticity for all edges, yet driven by the classification loss min ?I(G S ; Y ) (equivalent to crossentropy loss), GSAT can learn to reduce such stochasticity of the attention on the task-relevant subgraphs. So, it is not the entire G S but the part of G S with the stochasticityreduced attention, aka p uv ? 1, that provide model interpretation. Therefore, when GSAT provides interpretation, in practice, one can rank all edges according to p uv and use those top ranked ones (given a certain budget if needed) as the detected subgraph for interpretation. The contribution of injecting stochasticity to the performance is so significant as shown in experiments <ref type="table" target="#tab_4">(Table 5)</ref>, so is the contribution of our regularization term (Eq. (9)) when we compare it with the sparsity-driven 1 -norm ( <ref type="figure">Fig. 7)</ref>.</p><p>GSAT is substantially different from previous methods, as we do not use any sparsity constraints such as 1 -norm <ref type="bibr" target="#b50">(Ying et al., 2019;</ref><ref type="bibr" target="#b28">Luo et al., 2020)</ref>, 0 -norm <ref type="bibr" target="#b34">(Schlichtkrull et al., 2021)</ref> or 2 -regression to {0, 1} (Yu et al., 2021) to select size-constrained (or connectivity-constrained) subgraphs. We actually observe that setting r away from 0 in the marginal regularization (Eq. (9)), i.e., pushing G S away from being sparse often provides more robust interpretation. This matches our intuition that GIB by definition does not make any assumptions on the selected subgraphs but just constrains the information from the original graphs. Our experiments show that GSAT outperform baselines significantly without leveraging those assumptions in the optimization even if the label-relevant subgraphs satisfy these assumptions. If the label-relevant subgraphs are indeed disconnected or vary in sizes, the improvement of GSAT is expected to be even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Comparison on Interpretation Mechanism</head><p>PGExplainer and GraphMask also have stochasticity in their models <ref type="bibr" target="#b28">(Luo et al., 2020;</ref><ref type="bibr" target="#b34">Schlichtkrull et al., 2021)</ref>. However, their main goal is to enable a gradient-based search over a discrete subgraph-selection space rather than control the information as GSAT does. Hence, they did not in principle derive the information regularization as ours (Eq. <ref type="formula">(9)</ref>) but adopt sparsity constraints to extract a small subgraph G S directly used for interpretation.</p><p>IB-subgraph (Yu et al., 2021) considers using GIB as the objective but does not inject any stochasticity to generate G S , so its selected subgraph G S is a deterministic function of G. Specifically, IB-subgraph samples batches of graphs G to estimate I(G S ; G) and optimize a deterministic function G S = g ? (G) to minimize such MI estimation. In this case I(G S ; G)(= H(G S ) ? H(G S |G)) reduces to the entropy H(G S ), which tends to give a small-sized G S , because the space of small graphs is small and has a lower upper bound of the entropy. By contrast, G S ? g ? (G) is random in GSAT, and GSAT implements GIB mainly by increasing H(G S |G) via injecting stochasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Guaranteed Spurious Correlation Removal</head><p>GSAT can remove spurious correlations in the training data and has guaranteed interpretability. We may prove that if there exists a correspondence between a subgraph pattern G * S and the label Y , the pattern G * S is the optimal solution of the GIB objective (Eq. (2)). Proof. Consider the following derivation:</p><formula xml:id="formula_16">Theorem 4.1. Suppose each G contains a subgraph G * S such that Y is determined by G * S in the sense that Y = f (G * S ) + for some deterministic invertible function f with randomness that is independent from G. Then, for any ? ? [0, 1], G S = G * S maximizes the GIB I (G S ; Y ) ? ?I (G S ; G), where G S ? G sub (G).</formula><formula xml:id="formula_17">I(G S ; Y ) ? ?I(G S ; G) =I(Y ; G, G S ) ? I(G; Y |G S ) ? ?I(G S ; G) =I(Y ; G, G S ) ? (1 ? ?)I(G; Y |G S ) ? ?I(G; G S , Y ) =I(Y ; G) ? (1 ? ?)I(G; Y |G S ) ? ?I(G; G S , Y ) =(1 ? ?)I(Y ; G) ? (1 ? ?)I(G; Y |G S ) ? ?I(G; G S |Y ),</formula><p>where the third equality is because G S ? G sub (G), then (G S , G) holds no more information than G.</p><formula xml:id="formula_18">If ? ? [0, 1], G S that maximizes I(G S , Y ) ? ?I(G S ; G) can also minimize (1 ? ?)I(G; Y |G S ) + ?I(G; G S |Y ). As I(G; Y |G S ) ? 0, I(G; G S |Y ) ? 0, the lower bound of (1 ? ?)I(G; Y |G S ) + ?I(G; G S |Y ) is 0. G * S is the subgraph that makes (1 ? ?)I(G; Y |G * S ) + ?I(G; G * S |Y ) = 0. This is because (a) Y = f (G * S ) + where is independent of G so I(G; Y |G * S ) = 0 and (b) G * S = f ?1 (Y ? ) where is independent of G so I(G; G * S |Y ) = 0. Therefore, G S = G * S maximizes GIB I (G S ; Y ) ? ?I (G S ; G), where G S ? G sub (G).</formula><p>Although G * S determines Y , in the training dataset the data G and Y may have some spurious correlation caused by the environment <ref type="bibr" target="#b30">(Pearl et al., 2016;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2019;</ref><ref type="bibr" target="#b6">Chang et al., 2020;</ref><ref type="bibr">Krueger et al., 2021)</ref>. That is, G\G * S may have some correlation with the label, but this correlation is spurious and is not the true reason that determines its label (illustrated in <ref type="figure">Fig. 6</ref>). A model trained over G to predict Y via just MI maximization may capture such spurious correlation. If such correlation is changed during the test phase, the model suffers from performance decay.</p><p>However, Theorem 4.1 indicates that GSAT by optimizing the GIB objective has the capability to address the above issue by only extracting G * S , which removes the spurious correlation and also provides guaranteed interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Fine-tuning and Interpreting a Pre-trained Model</head><p>GSAT can also fine-tune and interpret a pre-trained GNN. Given a GNN f? pre-trained by max ? I(f ? (G); Y ), GSAT can fine-tune it via max ?,? I(f ? (G S ); Y ) ? ?I(G S ; G), G S ? g ? (G) by initializing the GNN used in g ? and f ? as the one in the pre-trained model f?.</p><p>We observe that this framework almost never hurts the original prediction performance (and sometimes even boosts it).</p><p>Moreover, this framework often achieves better interpretation results compared with training the GNN from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Other Related Works</head><p>Besides the models <ref type="bibr" target="#b50">(Ying et al., 2019;</ref><ref type="bibr" target="#b28">Luo et al., 2020;</ref><ref type="bibr" target="#b34">Schlichtkrull et al., 2021;</ref><ref type="bibr">Yu et al., 2021)</ref> that we have compared with in detail in Sec. 3.2 and Sec. 4.4, we review some other interpretation methods here.</p><p>Most previous works on GNN interpretation are posthoc <ref type="bibr" target="#b33">(Ribeiro et al., 2016)</ref>. Some works strongly rely on the connectivity assumption and only search over the space of connected subgraphs for interpretation. They adopt either reinforcement learning <ref type="bibr">(Yuan et al., 2020a)</ref> or Monte Carlo tree search <ref type="bibr">(Yuan et al., 2021)</ref>. Other methods including PGM-Explainer <ref type="bibr" target="#b43">(Vu &amp; Thai, 2020)</ref> leveraging graphical models, Gem <ref type="bibr" target="#b26">(Lin et al., 2021)</ref> checking Granger causality and Graphlime <ref type="bibr" target="#b16">(Huang et al., 2020)</ref> using HSIC Lasso are only applied to node-level task interpretation. Some works check the gradients w.r.t. the input features to find important features <ref type="bibr" target="#b32">(Pope et al., 2019;</ref><ref type="bibr" target="#b3">Baldassarre &amp; Azizpour, 2019)</ref>.</p><p>Much fewer works have considered intrinsic interpretation. Recently, <ref type="bibr" target="#b46">Wu et al. (2022)</ref> has proposed DIR to make the model avoid overfitting spurious correlations and only capture invariant rationales to provide interpretability. However, DIR needs to iteratively break graphs into subgraphs and assemble subgraphs into graphs during the model training, which is far more complicated than GSAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate our method for both interpretability and prediction performance. We will compare our method with both state-of-the-art (SOTA) post-hoc interpretation methods and inherently interpretable models. We will also compare with several invariant learning methods to demonstrate the ability of GSAT to remove spurious correlations. We briefly introduce datasets, baselines and experiment settings here, and more details can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>Mutag <ref type="bibr" target="#b11">(Debnath et al., 1991)</ref> is a molecular property prediction dataset. Following <ref type="bibr" target="#b28">(Luo et al., 2020)</ref>, -NO 2 and -NH 2 in mutagen graphs are labeled as ground-truth explanations.</p><p>BA-2Motifs <ref type="bibr" target="#b28">(Luo et al., 2020)</ref> is a synthetic dataset with binary graph labels. House motifs and cycle motifs give class labels and thus are regarded as ground-truth explanations for the two classes respectively. <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> is a synthetic dataset with three graph classes. Each class contains a particular motif that can be regarded as the ground-truth explanation. Some spurious correlation between the rest graph components (other than the motifs) and the labels also exists in the training data. The degree of such correlation is controlled by b, and we include datasets with b = 0.5, 0.7 and 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spurious-Motif</head><p>MNIST-75sp <ref type="bibr" target="#b22">(Knyazev et al., 2019)</ref> is an image classification dataset, where each image in MNIST is converted to a superpixel graph. Nodes with nonzero pixel values provide ground-truth explanations. Note that the subgraphs that provide explanations are of different sizes in this dataset.</p><p>Graph-SST2 <ref type="bibr" target="#b37">(Socher et al., 2013;</ref><ref type="bibr">Yuan et al., 2020b</ref>) is a sentiment analysis dataset, where each text sequence in SST2 is converted to a graph. Following the splits in <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>, this dataset contains degree shifts and no ground-truth explanation labels. So, we only evaluate prediction performance and provide interpretation visualizations. <ref type="bibr" target="#b47">(Wu et al., 2018;</ref><ref type="bibr" target="#b15">Hu et al., 2020</ref>) is a molecular property prediction datasets. We also evaluate GSAT on molbace, molbbbp, molclintox, moltox21 and molsider datasets from OGBG. As there are no ground truth explanation labels for these datasets, we only evaluate the prediction performance of GSAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OGBG-Molhiv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Baselines and Setup</head><p>Interpretability Baselines. We compare interpretability with post-hoc methods GNNExplainer <ref type="bibr" target="#b50">(Ying et al., 2019)</ref>, PGExplainer <ref type="bibr" target="#b28">(Luo et al., 2020)</ref>, GraphMask <ref type="bibr" target="#b34">(Schlichtkrull et al., 2021)</ref>, and inherently interpretable models DIR <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> and <ref type="bibr">IB-subgraph (Yu et al., 2021)</ref>.</p><p>Prediction Baselines. We compare prediction performance with the backbone models GIN <ref type="bibr" target="#b49">(Xu et al., 2019)</ref> and PNA <ref type="bibr" target="#b8">(Corso et al., 2020)</ref>, and inherently interpretable models DIR <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> and <ref type="bibr">IB-subgraph (Yu et al., 2021)</ref>.</p><p>Invariant Learning Baselines. We compare the ability to remove spurious correlations with invariant learning methods IRM <ref type="bibr" target="#b1">(Arjovsky et al., 2019)</ref>, V-REx <ref type="bibr">(Krueger et al., 2021)</ref> and DIR <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>. Baseline results yielded by empirical risk minimization (ERM) are also included.</p><p>Metrics. For interpretation evaluation, we report explanation ROC AUC following <ref type="bibr" target="#b50">(Ying et al., 2019;</ref><ref type="bibr" target="#b28">Luo et al., 2020)</ref>. For prediction performance, we report classification ROC AUC for all OGBG datasets and report accuracy for all other datasets. All the results are averaged over 10 times tests with different random seeds. For the post-hoc methods, we do not cherry pick a pre-trained model. Instead, in each test, we interpret a model pre-trained independently that achieves the best validation performance.</p><p>Setup. Since we focus on graph classification tasks, GIN <ref type="bibr" target="#b49">(Xu et al., 2019)</ref> is used as the backbone model for both baselines and GSAT. We also apply PNA <ref type="bibr" target="#b8">(Corso et al., 2020)</ref> to further test the wide applicability of GSAT, for which we adopt the no-scalars version since the scalars used in PNA <ref type="table">Table 1</ref>. Interpretation Performance (AUC). The underlined results highlight the best baselines. The bold font and bold ? font highlight when GSAT outperform the means of the best baselines based on the mean of GSAT and the mean-2*std of GSAT, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BA-2MOTIFS</head><p>MUTAG are essentially a type of attention, which may conflict with our method. GIN+GSAT denotes using GIN as the base GNN encoder of GSAT, and PNA+GSAT means replacing the GNN encoder with PNA. In addition, we apply GSAT to fine-tune and interpret pre-trained models as described in Sec. 4.6, which is highlighted as GSAT * . In all the experiments, we use r = 0.7 in Eq. (9) by default or otherwise specified. Our studies have shown that GSAT is generally robust when r ? [0.5, 0.9] (see <ref type="figure">Fig. 7</ref> later).</p><formula xml:id="formula_19">MNIST-75SP SPURIOUS-MOTIF b = 0.5 b = 0.7 b = 0.9<label>GNNEXPLAINER</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Result Comparison and Analysis</head><p>Interpretability Results. As shown in <ref type="table">Table 1</ref>, our methods significantly outperform the baselines by 9%? on average and up to 20%?. If we just compare among inherently interpretable models, the boost is even more significant. Moreover, GSAT also provides much stabler interpretation than the baselines as for the much smaller variance. GSAT * via fine-tuning a pre-trained model can often further boost the interpretation performance. Also, when the more expressive model PNA is used as the backbone, we find the posthoc methods are likely to suffer from the overfitting issue as explained in Sec. 3.2. However, GSAT does not suffer from that and can yield even better interpretation results. Over Ba-2Motifs and Mutag, GNNExplainer and PGExplainer work worse than what reported in <ref type="bibr" target="#b28">(Luo et al., 2020)</ref> as we do not cherry pick the pre-trained model. However, GSAT still significantly outperforms their reported performance in the Appendix C.4. We also provide visualizations of the subgraphs discovered by GSAT in Appendix D.</p><p>Prediction Results. As explained in Sec. 4.5, being trained via the GIB principle, GSAT is more generalizable and thus may achieve even better prediction performance. As shown in <ref type="table" target="#tab_1">Table 2</ref>, GIN+GSAT significantly outperforms the backbone GIN over the Spurious-Motif datasets, where spurious correlation exists in the training data. For other datasets, GIN+GSAT can achieve comparable results, which matches our claim that GSAT provides interpretation without hurting the prediction. IB-subgraph, trained via the GIB principle, also achieves good prediction performance though its interpretability is poor <ref type="table">(Table 1)</ref>. When PNA is used, GSAT improves it by about 1 ? 5% on the datasets in the first three columns. Notably, GSAT * achieves the SOTA performance on molhiv among all models that do not incorporate expert knowledge according to the leaderboard. Unexpectedly, PNA achieves very good performance on Spurious-Motif and GSAT * just slightly improves it. Our results on the other 5 molecular datasets from OGBG are showed in <ref type="table" target="#tab_2">Table  3</ref>, where GSAT and GSAT * mostly outperform PNA.</p><p>Invariant Learning Results. We note that DIR achieves a bit lower prediction performance in <ref type="table" target="#tab_1">Table 2</ref> than what reported in <ref type="bibr" target="#b46">(Wu et al., 2022</ref>) even after we extensively tune its  <ref type="table">Table 4</ref>. Direct comparison (Acc.) with invariant learning methods on the ability to remove spurious correlations, by applying the backbone model used in <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>. parameters, which is probably due to the different backbone models used. Hence, we also compare with DIR by using their backbone model. And we include several invariant learning baselines reported in DIR to further demonstrate the ability of GSAT to remove spurious correlations. Results are shown in <ref type="table">Table 4</ref>. GSAT significantly outperforms all invariant learning methods on spurious correlation removal, even without utilizing causality analysis, which further validates our claims in Sec. 4.5. A comparison of interpretability of these models is shown in <ref type="table">Table 7</ref> in the appendix.</p><formula xml:id="formula_20">SPURIOUS-MOTIF b = 0.5 b = 0.7 b = 0.9<label>ERM</label></formula><p>Ablation Studies. We conduct ablation studies from three aspects: First, the importance of stochasticity in GSAT, where we replace the Bernoulli sampling procedure with setting attention ? uv = p uv without stochasticity; Second, the importance of the information regularization term (Eq. (9)), where we set its coefficient ? = 0 in Eq. (8); Third, the superiority of the information regularization term over the sparsity-driven term 1 -norm.</p><p>As shown in <ref type="table" target="#tab_4">Table 5</ref>, the performance drops significantly when there is either no stochasticity or ? = 0. Specifically, GSAT-NoStoch means applying deterministic attention ? [0, 1], which causes the most performance drop. GSAT-NoStoch-? = 0 corresponds to using deterministic attention without the regularization term in Eq. (9), which causes the second most performance drop. GSAT-? = 0 denotes applying stochastic attention with no regularization, which performs better than baselines but worse than original GSAT and suffers from large variance. Overall, no stochasticity yields the biggest drop, which well matches our theory. This also implies that directly using the deterministic attention mechanisms such as GAT <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018)</ref> or GGNN <ref type="bibr" target="#b25">(Li et al., 2016)</ref> may not yield good interpretability. <ref type="figure">Fig. 7</ref> shows that our information regularization term can achieve consistently better performance than the sparsitydriven 1 -norm regularization even when the grid search is used to tune hyperparameters. We also observe that when r is close to 0, the results often get decreased or have higher   <ref type="figure">Figure 7</ref>. Comparison between (a) using the information constraint in Eq. (9) and (b) replacing it with 1-norm. Results are shown for Spurious-Motif b = 0.5, where r is tuned from 0.9 to 0.1 and the coefficient of the 1-norm ?1 is tuned from 1e-5 to 1.</p><p>variance. The best performance is often achieved when r ? [0.5, 0.9], which matches our theory. More results on other datasets can be found in <ref type="figure" target="#fig_6">Fig. 8</ref> in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Graph Stochastic Attention (GSAT) is a novel attention mechanism to build interpretable graph learning models. GSAT injects stochasticity to block label-irrelevant information and leverages the reduction of stochasticity to select label-relevant subgraphs. Such rationale is grounded by the information bottleneck principle. GSAT has many transformative characteristics. For example, it removes the sparsity, continuity or other potentially biased assumptions in graph learning interpretation without performance decay. It can also remove spurious correlation to better the model generalization. As a by-product, we also reveal a potentially severe issue behind post-hoc interpretation methods from the optimization perspective of information bottleneck.</p><p>Yu, J., Xu, T., Rong, Y., Bian, Y., Huang, J., and He, R. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Notations for Information Theory and Graph Neural Networks</head><p>Entropy. Given a discrete random variable a, its entropy is defined as H(a) ? a P(a) log P(a). If a is a continuous random variable, its differential entropy is defined as H(a) ? a P(a) log P(a)da.</p><p>KL-Divergence. Given two distributions P(x) and Q(x), KL-Divergence is used to measure the difference between P and Q, and it is defined as KL(P(x)||Q(x)) x P(x) log P(x) Q(x) . Mutual Information. Given two random variables a and b, the mutual information (MI) I(a; b) is a measure of the mutual dependence between them. MI quantifies the amount of information regarding one random variable if another random variable is known. <ref type="figure">Formally, I(a; b)</ref> a,b P(a, b) log P(a,b) P(a)P(b) , <ref type="figure">where P(a, b)</ref> is the joint distribution and P(a), P(b) are the marginal distributions. By definition, I(a, b) = KL <ref type="figure">(P(a, b)</ref></p><formula xml:id="formula_21">||P(a)P(b)) = a,b P(a, b) log P(a|b)? b P(b) log P(b) = ?H(a|b) + H(b).</formula><p>Graph Neural Networks (GNNs). Given an L-layer GNN, let h (l) v denote the node representation for node v in the i th layer and N (v) denote a set of nodes adjacent to node v. Let h (0) v be the node feature X v . Most GNNs follow a message passing scheme, where there are two main steps in each layer: (1) neighbourhood aggregation, m</p><formula xml:id="formula_22">(l) v = AGG({h (l?1) u |u ? N (v)}); (2) node representation update, h (l) v = UPDATE(m (l) v , h (l?1) v</formula><p>). For graph classification tasks, after obtaining h (L) v for each node, the graph representation is given by h G = POOL({h (L) v |v ? V }) and h G will be used to make predictions. The above AGG, UPDATE, POOL are three functions. AGG and POOL are typically implemented via SUM, MEAN and MAX while UPDATE is a fully connected (typically shallow) neural network. In some cases, edge representations may be in need, and they are often given by h</p><formula xml:id="formula_23">(l) u,v = CONCAT(h (l) u , h (l) v ).</formula><p>B. Variational Bounds for the GIB Objective -Eq. (6) and Eq. <ref type="formula" target="#formula_9">(7)</ref> From Eq. (5), the IB objective is:</p><formula xml:id="formula_24">min ? ?I(G S ; Y ) + ?I(G S ; G), s.t. G S ? g ? (G).<label>(10)</label></formula><p>To optimize it, we introduce two variational bounds on the two terms, respectively.</p><p>For the first term I (G S ; Y ), by definition:</p><formula xml:id="formula_25">I (G S ; Y ) = E G S ,Y log P(Y |G S ) P(Y ) .<label>(11)</label></formula><p>Since P(Y |G S ) is intractable, we introduce a variational approximation P ? (Y |G S ) for it. Then, we obtain a lower bound for Eq. <ref type="formula">(6)</ref>:</p><formula xml:id="formula_26">I (G S ; Y ) = E G S ,Y log P ? (Y |G S ) P(Y ) + E G S [KL(P(Y |G S )||P ? (Y |G S ))] ? E G S ,Y log P ? (Y |G S ) P(Y ) = E G S ,Y [log P ? (Y |G S )] + H(Y ).<label>(12)</label></formula><p>For the second term I (G; G S ), by definition:</p><formula xml:id="formula_27">I (G; G S ) = E G S ,G log P(G S |G) P(G S ) .<label>(13)</label></formula><p>Since P(G S ) is intractable, we introduce a variational approximation Q(G S ) for the marginal distribution P(G S ) = G P ? (G S |G)P G (G). Then, we obtain an upper bound for Eq. (7):  <ref type="bibr" target="#b28">(Luo et al., 2020)</ref>, which are given a selected pre-trained model.  <ref type="table">Table 7</ref>. Direct comparison with the interpretation precision@5 of DIR reported in <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> based on the backbone model in <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>. Mutag <ref type="bibr" target="#b11">(Debnath et al., 1991)</ref> is a molecular property prediction dataset, where nodes are atoms and edges are chemical bonds. Each graph is associated with a binary label based on its mutagenic effect. Following <ref type="bibr" target="#b28">(Luo et al., 2020)</ref>, -NO 2 and -NH 2 in mutagen graphs are labeled as ground-truth explanations.</p><formula xml:id="formula_28">I (G; G S ) = E G S ,G log P ? (G S |G) Q(G S ) ? KL (P(G S )||Q(G S )) ? E G [KL (P ? (G S |G)||Q(G S ))] .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPURIOUS-MOTIF</head><formula xml:id="formula_29">b = 0.5 b = 0.7 b = 0.9<label>GNNEXPLAINER</label></formula><p>BA-2Motifs <ref type="bibr" target="#b28">(Luo et al., 2020)</ref> is a synthetic dataset, where the base graph is generated by Barab?si-Albert (BA) model. Each base graph is attached with a house-like motif or a five-node cycle motif. House motifs and cycle motifs give class labels and thus are regarded as ground-truth explanations for the two classes respectively. <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> is a synthetic dataset with three graph classes. Following the notations in <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>, each graph consists of a base graph (tree/ladder/wheel denoted by? S = 0, 1, 2 respectively, with some abuse of notations) and a motif (cycle/house/crane denoted by G S = 0, 1, 2, respectively, with some abuse of notations). The label is determined only by G S , while there also exists spurious correlation between the label and? S . Specifically, to construct a graph in the training set, G S will be sampled uniformly, while? S will be sampled with probability P(? S ), where P(? S ) = b if G S = G S ; otherwise P(? S ) = (1 ? b)/2. So, b is a parameter used to control the degree of such spurious correlation. When b = 1/3, there is no spurious correlation. We include datasets with b = 0.5, b = 0.7 and b = 0.9. Note that for testing data, the motifs and bases are randomly attached to each other, which can test if the model overfits the spurious correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spurious-Motif</head><p>MNIST-75sp <ref type="bibr" target="#b22">(Knyazev et al., 2019)</ref> is a image classification dataset, where each image in MNIST is converted to a superpixel graph. Each node in the graph represents a superpixel and edges are formed based on spatial distance between superpixel centers. Node features are the coordinates of their centers of masses. Nodes with nonzero pixel values provide ground-truth explanations. Note that the subgraphs that provide explanations are of different sizes in this dataset.</p><p>Graph-SST2 <ref type="bibr" target="#b37">(Socher et al., 2013;</ref><ref type="bibr">Yuan et al., 2020b</ref>) is a sentiment analysis dataset, where each text sequence in SST2 is converted to a graph. Each node in the graph represents a word and edges are formed based on relationships between different words. We follow the dataset splits in <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> to create degree shifts in the training set, which can better test generalizability of models. Specifically, graphs with higher average node degree will be used to train and validate models, while graphs with fewer nodes will be used to test models. And this dataset contains no ground-truth explanation labels, so we only evaluate prediction performance here and provide interpretation visualizations in Appendix D.</p><p>OGBG-Molhiv <ref type="bibr" target="#b47">(Wu et al., 2018;</ref><ref type="bibr" target="#b15">Hu et al., 2020)</ref> is a molecular property prediction datasets, where nodes are atoms and edges are chemical bonds. A binary label is assigned to each graph according to whether a molecule inhibits HIV virus replication or not. We also evaluate GSAT on molbace, molbbbp, molclintox, moltox21 and molsider datasets from OGBG. As there are no ground truth explanation labels for these datasets, we only evaluate the prediction performance of GSAT.   <ref type="formula">(9)</ref> and (b) replacing it with 1-norm, where r is tuned from 0.9 to 0.1 and the coefficient of the 1-norm ?1 is tuned from 1e-5 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Details on Hyperparameter Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1. BACKBONE MODELS</head><p>Backbone Architecture. We use a two-layer GIN <ref type="bibr" target="#b49">(Xu et al., 2019)</ref> with 64 hidden dimensions and 0.3 dropout ratio. We use the setting from <ref type="bibr" target="#b8">(Corso et al., 2020)</ref> for PNA, which has 4 layers with 80 hidden dimensions, 0.3 dropout ratio, and no scalars are used. For OGBG-Mol datasets, we directly follow <ref type="bibr" target="#b8">(Corso et al., 2020)</ref> using (mean, min, max, std) aggregators for PNA; yet we find PNA has convergence issues on other datasets when sum aggregator is not used. Hence, PNA uses (mean, min, max, std, sum) aggregators for all other datasets.</p><p>Dataset Splits. For Ba-2Motifs, we split it randomly into three sets (80%/10%/10%). For Mutag, we split it randomly into 80%/20% to train and validate models, and following <ref type="bibr" target="#b28">(Luo et al., 2020)</ref> we use mutagen molecules with -NO 2 or -NH 2 as test data (because only these samples have explanation labels). For MNIST-75sp, we use the default splits given by <ref type="bibr" target="#b22">(Knyazev et al., 2019)</ref>; due to its large size in the graph setting, we also reduce the number of training samples following <ref type="bibr" target="#b46">(Wu et al., 2022)</ref> to speed up training. For Graph-SST2, Spurious-Motifs and OGBG-Mol, we use the default splits given by <ref type="bibr">(Yuan et al., 2020b)</ref> and <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>. Following <ref type="bibr" target="#b8">(Corso et al., 2020)</ref>, edge features are not used for all OGBG-Mol datasets.</p><p>Epoch. We tune the number of epochs to make sure the convergence of all models. When GIN is used as the backbone model, MNIST-75sp and OGBG-Molhiv are trained for 200 epochs, and all other datasets are trained for 100 epochs. When PNA is used, Mutag and Ba-2Motifs are trained for 50 epochs and all other datasets are trained for 200 epochs. We report the performance of the epoch that achieves the best validation prediction performance and use the models that achieve such best validation performance as the pre-trained models. When multiple epochs achieve the same best performance, we report the one with the lowest validation prediction loss.</p><p>Batch Size. All datasets use a batch size of 128; except for MNIST-75sp we use a batch size of 256 to speed up training due to its large size in the graph setting.</p><p>Learning Rate. GIN uses 0.003 learning rate for Spurious-Motifs and 0.001 for all other datasets. PNA uses 0.01 learning rate with scheduler following <ref type="bibr" target="#b8">(Corso et al., 2020)</ref>, 0.003 learning rate for Graph-SST2 and Spurious-Motifs, and 0.001 learning rate for all other datasets.</p><p>DIR. Causal ratio is tuned for Ba-2Motif and Mutag. Since the other datasets we use are the same, we use the recommended settings from <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>. However, even though datasets are the same, we find the same ? specified in their source code do not work well in our setting. Hence, we tune ? from (10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IB-subgraph.</head><p>Due to the extreme inefficiency of IB-subgraph, we are only able to tune its mi-weight around the recommended value from (2, 0.2, 0.02). And we use the default inner loop iterations and con-weight as specified in their source code. IB-subgraph needs ?40 hours to train 100 epochs for 1 seed on Spurious-Motif and ?150 hours for OGBG-Molhiv on a Quadro RTX 6000. By contrast, GSAT only needs ?15 minutes to train 100 epochs on OGBG-Molhiv.</p><p>Random Seed. All methods are trained with 10 different random seeds; except for IB-subgraph we train it for 5 different random seeds due to its inefficiency. For post-hoc methods, the pre-trained models are also trained with 10 different random seeds instead of a fixed pre-trained model in <ref type="bibr" target="#b28">(Luo et al., 2020)</ref>. For inherently interpretable models, GSAT, IB-subgraph and DIR, we average the best epoch's performance according to their validation prediction performance. For post-hoc baselines, we average their last epoch's performance. For IB-subgraph, we stop training when there is no improvement over 20 epochs to make the training possible on large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Node/Edge Attention</head><p>We also explore node-level attention, and we find it is especially useful for molecular datasets and datasets with large graph sizes. Hence, we use node-level attention for on Mutag, MNIST-75sp and OGBG-Mol datasets, and for all other datasets we use edge attention. Specifically, when node attention is used, the MLP layers in P ? will take as input the node embeddings and output p v for each v ? V . Then, the stochastic node attention is sampled for each node ? v ? Bern(p v ). After that, ? uv is obtained by ? uv = ? u ? v . <ref type="figure" target="#fig_1">Fig. 3</ref> shows an experiment with disconnected critical subgraphs, where the dataset is generated in a similar way used to generate Ba-2Motifs. Specifically, each base graph is generated using the BA model and will be attached with two house motifs or three house motifs randomly. The number of house motifs represents the graph class. Both GSAT and GraphMask  <ref type="figure">Figure 10</ref>. Visualizing label-relevant subgraphs discovered by GSAT for Mutag. -NO2 and -NH2 are ground-truth explanations. We only present mutagen graphs as only these graphs are with ground-truth explanation labels. are trained with the same settings used on Ba-2Motifs. <ref type="table" target="#tab_6">Table 6</ref> shows a direct comparison with PGExplainer and GNNExplainer between the interpretation ROC AUC reported in <ref type="bibr" target="#b28">(Luo et al., 2020)</ref> and the performance of GSAT. And GSAT still outperforms their methods significantly. <ref type="table">Table 4 and Table 7</ref> show direct comparisons with DIR, where we apply GSAT with the backbone model used in DIR. And GSAT still greatly outperforms their method. <ref type="table" target="#tab_9">Table 8</ref> shows the ablation study on ? and stochasticity in GSAT, where PNA is the backbone model. <ref type="figure" target="#fig_6">Figure 8</ref> shows the ablation study of the information constraint introduced in Eq. (9) on Spurious-Motif b = 0.7 and b = 0.9. We observe the same trends from these ablation studies as discussed in Sec. 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Further Supplementary Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interpretation Visualization</head><p>We provide visualizations of the label-relevant subgraphs discovered by GSAT on eight datasets, as shown from <ref type="figure" target="#fig_7">Fig. 9</ref> to <ref type="figure" target="#fig_11">Fig. 16</ref>. The transparency of the edges shown in the figures represents the normalized attention weights learned by GSAT. The normalized attention weights are to rescale the learnt weights {p uv |(u, v) ? E} to [0, 1]: For each graph, denote p min = min{p uv |(u, v) ? E} and p max = max{p uv |(u, v) ? E}. We rescale the weights according t? p uv = p uv ? p min p max ? p min (15) <ref type="figure">Figure 11</ref>. Visualizing label-relevant subgraphs discovered by GSAT for Spurious-Motif b = 0.5. Nodes colored pink are ground-truth explanations, and each row represents a graph class.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visualizing attention (normalized to [0, 1]) of GSAT (second row) v.s. masks of GraphMask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualizing attention (normalized to [0, 1]) of GSAT (first row) and masks of GraphMask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>we follow Alemi et al. (2016); Poole et al. (2019); Wu et al. (2020) to derive a tractable variational upper bound of the two terms in Eq. (5). Detailed derivation is given in Appendix B. For the term I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>G * S determines Y . However, the environment features in G\G * S may contain spurious (backdoor) correlation with Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0.9/1e-5 0.8/1e-4 0.7/5e-4 0.6/1e-3 0.5/5e-3 0.4/1e-2 0.3/5e-2 0.2/1e-1 0.1/1 1e-5 0.8/1e-4 0.7/5e-4 0.6/1e-3 0.5/5e-3 0.4/1e-2 0.3/5e-2 0.2/1e-1 0.1/1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>0.9/1e-5 0.8/1e-4 0.7/5e-4 0.6/1e-3 0.5/5e-3 0.4/1e-2 0.3/5e-2 0.2/1e-1 0.1/1 1e-5 0.8/1e-4 0.7/5e-4 0.6/1e-3 0.5/5e-3 0.4/1e-2 0.3/5e-2 0.2/1e-1 0.1/1 Spurious-Motif, b = 0.7 0.9/1e-5 0.8/1e-4 0.7/5e-4 0.6/1e-3 0.5/5e-3 0.4/1e-2 0.3/5e-2 0.2/1e-1 0.1/1 1e-5 0.8/1e-4 0.7/5e-4 0.6/1e-3 0.5/5e-3 0.4/1e-2 0.3/5e-2 0.2/1e-1 0.1/1 Ablation study on (a) using the info. constraint in Eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Visualizing label-relevant subgraphs discovered by GSAT for Ba-2Motifs. Nodes colored pink are ground-truth explanations, and each row represents a graph class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Visualizing label-relevant subgraphs discovered by GSAT for Spurious-Motif b = 0.7. Nodes colored pink are ground-truth explanations, and each row represents a graph class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Visualizing label-relevant subgraphs discovered by GSAT for Spurious-Motif b = 0.9. Nodes colored pink are ground-truth explanations, and each row represents a graph class. Visualizing label-relevant subgraphs discovered by GSAT for OGBG-Molhiv. Each row represents a graph class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Visualizing label-relevant subgraphs discovered by GSAT for Graph-SST2. The top two rows show sentences with negative sentiment, and the bottom two rows show sentences with positive sentiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .</head><label>16</label><figDesc>Visualizing label-relevant subgraphs discovered by GSAT for MNIST-75sp. The first row shows the raw images and the second row shows the normalized attention weights learned by GSAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Computer Science, Purdue University, West Lafayette, USA 2 Department of Physics and Astronomy, Purdue University, West Lafayette, USA. Correspondence to: Siqi Miao &lt;miao61@purdue.edu&gt;, Pan Li &lt;panli@purdue.edu&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>97.43 ? ? 1.77 97.75 ? ? 0.92 83.70 ? ? 1.46 85.55 ? ? 2.57 85.56 ? ? 1.93 83.59 ? ? 2.56 PNA+GSAT 93.77 ? 3.90 99.07 ? ? 0.50 84.68 ? ? 1.06 83.34 ? ? 2.17 86.94 ? ? 4.05 88.66 ? ? 2.44 PNA+GSAT * 89.04 ? 4.92 96.22 ? ? 2.08 88.54 ? ? 0.72 90.55 ? ? 1.48 89.79 ? ? 1.91 89.54 ? ? 1.78 Prediction Performance (Acc.). The bold font highlights the inherently interpretable methods that significantly outperform the corresponding backbone model, GIN or PNA, when the mean-1*std of a method &gt; the mean of its corresponding backbone model.</figDesc><table><row><cell></cell><cell>67.35 ? 3.29</cell><cell>61.98 ? 5.45</cell><cell>59.01 ? 2.04</cell><cell>62.62 ? 1.35</cell><cell>62.25 ? 3.61</cell><cell>58.86 ? 1.93</cell></row><row><cell>PGEXPLAINER</cell><cell>84.59 ? 9.09</cell><cell>60.91 ? 17.10</cell><cell>69.34 ? 4.32</cell><cell>69.54 ? 5.64</cell><cell>72.33 ? 9.18</cell><cell>72.34 ? 2.91</cell></row><row><cell>GRAPHMASK</cell><cell>92.54 ? 8.07</cell><cell>62.23 ? 9.01</cell><cell>73.10 ? 6.41</cell><cell>72.06 ? 5.58</cell><cell>73.06 ? 4.91</cell><cell>66.68 ? 6.96</cell></row><row><cell>IB-SUBGRAPH</cell><cell>86.06 ? 28.37</cell><cell>91.04 ? 6.59</cell><cell>51.20 ? 5.12</cell><cell>57.29 ? 14.35</cell><cell>62.89 ? 15.59</cell><cell>47.29 ? 13.39</cell></row><row><cell>DIR</cell><cell>82.78 ? 10.97</cell><cell>64.44 ? 28.81</cell><cell>32.35 ? 9.39</cell><cell>78.15 ? 1.32</cell><cell>77.68 ? 1.22</cell><cell>49.08 ? 3.66</cell></row><row><cell>GIN+GSAT</cell><cell cols="3">98.74  ? ? 0.55 99.60  ? ? 0.51 83.36  ? ? 1.02</cell><cell>78.45 ? 3.12</cell><cell>74.07 ? 5.28</cell><cell>71.97 ? 4.41</cell></row><row><cell cols="4">GIN+GSAT  MOLHIV (AUC) GRAPH-SST2 MNIST-75SP</cell><cell>b = 0.5</cell><cell>SPURIOUS-MOTIF b = 0.7</cell><cell>b = 0.9</cell></row><row><cell>GIN</cell><cell>76.69 ? 1.25</cell><cell>82.73 ? 0.77</cell><cell>95.74 ? 0.36</cell><cell>39.87 ? 1.30</cell><cell>39.04 ? 1.62</cell><cell>38.57 ? 2.31</cell></row><row><cell>IB-SUBGRAPH</cell><cell>76.43 ? 2.65</cell><cell>82.99 ? 0.67</cell><cell cols="4">93.10 ? 1.32 54.36 ? 7.09 48.51 ? 5.76 46.19 ? 5.63</cell></row><row><cell>DIR</cell><cell>76.34 ? 1.01</cell><cell>82.32 ? 0.85</cell><cell cols="3">88.51 ? 2.57 45.49 ? 3.81 41.13 ? 2.62</cell><cell>37.61 ? 2.02</cell></row><row><cell>GIN+GSAT</cell><cell>76.47 ? 1.53</cell><cell>82.95 ? 0.58</cell><cell cols="4">96.24 ? 0.17 52.74 ? 4.08 49.12 ? 3.29 44.22 ? 5.57</cell></row><row><cell>GIN+GSAT  *</cell><cell>76.16 ? 1.39</cell><cell>82.57 ? 0.71</cell><cell cols="3">96.21 ? 0.14 46.62 ? 2.95 41.26 ? 3.01</cell><cell>39.74 ? 2.20</cell></row><row><cell>PNA (NO SCALARS)</cell><cell>78.91 ? 1.04</cell><cell>79.87 ? 1.02</cell><cell>87.20 ? 5.61</cell><cell>68.15 ? 2.39</cell><cell>66.35 ? 3.34</cell><cell>61.40 ? 3.56</cell></row><row><cell>PNA+GSAT</cell><cell>80.24 ? 0.73</cell><cell cols="3">80.92 ? 0.66 93.96 ? 0.92 68.74 ? 2.24</cell><cell>64.38 ? 3.20</cell><cell>57.01 ? 2.95</cell></row><row><cell>PNA+GSAT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>** 80.67 ? 0.95 82.81 ? 0.56 92.38 ? 1.44 69.72 ? 1.93 67.31 ? 1.86 61.49 ? 3.46</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Generalization ROC AUC on other OGBG-Mol datasets. The bold font highlights when GSAT outperforms PNA. ? 3.02 67.21 ? 1.34 86.72 ? 2.33 75.08 ? 0.64 56.51 ? 1.90 GSAT 77.41 ? 2.42 69.17 ? 1.12 87.80 ? 2.36 74.96 ? 0.66 57.58 ? 1.23 GSAT * 73.61 ? 1.59 66.30 ? 0.79 89.26 ? 1.66 75.71 ? 0.48 59.19 ? 1.03</figDesc><table><row><cell></cell><cell>MOLBACE</cell><cell>MOLBBBP</cell><cell>MOLCLINTOX</cell><cell>MOLTOX21</cell><cell>MOLSIDER</cell></row><row><cell>PNA</cell><cell>73.52</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on ? and stochasticity in GSAT (GIN as the backbone model) on Spurious-Motif. We report both interpretation ROC AUC (top) and prediction accuracy (bottom).</figDesc><table><row><cell>SPURIOUS-MOTIF</cell><cell>b = 0.5</cell><cell>b = 0.7</cell><cell>b = 0.9</cell></row><row><cell>GSAT</cell><cell>79.81 ? 3.98</cell><cell>74.07 ? 5.28</cell><cell>71.97 ? 4.41</cell></row><row><cell>GSAT-? = 0</cell><cell>66.00 ? 11.04</cell><cell>65.92 ? 3.28</cell><cell>66.31 ? 6.82</cell></row><row><cell>GSAT-NOSTOCH</cell><cell>59.64 ? 5.33</cell><cell>55.78 ? 2.84</cell><cell>55.27 ? 7.49</cell></row><row><cell>GSAT-NOSTOCH-? = 0</cell><cell>63.37 ? 12.33</cell><cell>60.61 ? 10.08</cell><cell>66.19 ? 7.76</cell></row><row><cell>GIN</cell><cell>39.87 ? 1.30</cell><cell>39.04 ? 1.62</cell><cell>38.57 ? 2.31</cell></row><row><cell>GSAT</cell><cell>51.86 ? 5.51</cell><cell>49.12 ? 3.29</cell><cell>44.22 ? 5.57</cell></row><row><cell>GSAT-? = 0</cell><cell>45.97 ? 8.37</cell><cell>49.67 ? 7.01</cell><cell>49.84 ? 5.45</cell></row><row><cell>GSAT-NOSTOCH</cell><cell>40.34 ? 2.77</cell><cell>41.90 ? 3.70</cell><cell>37.98 ? 2.64</cell></row><row><cell>GSAT-NOSTOCH-? = 0</cell><cell>43.41 ? 8.05</cell><cell>45.88 ? 9.54</cell><cell>42.25 ? 9.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Direct comparison with the interpretation ROC AUC of GNNExplainer and PGExplainer reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Ablation study on ? and stochasticity in GSAT (PNA as the backbone model) on Spurious-Motif. We report both interpretation ROC AUC (top) and prediction accuracy (bottom).</figDesc><table><row><cell>SPURIOUS-MOTIF</cell><cell>b = 0.5</cell><cell>b = 0.7</cell><cell>b = 0.9</cell></row><row><cell>PNA+GSAT</cell><cell>83.34 ? 2.17</cell><cell>86.94 ? 4.05</cell><cell>88.66 ? 2.44</cell></row><row><cell>PNA+GSAT-? = 0</cell><cell>82.01 ? 6.43</cell><cell>78.88 ? 6.74</cell><cell>80.53 ? 5.03</cell></row><row><cell>PNA+GSAT-NOSTOCH</cell><cell>79.72 ? 3.86</cell><cell>76.36 ? 2.57</cell><cell>80.21 ? 3.76</cell></row><row><cell cols="4">PNA+GSAT-NOSTOCH-? = 0 78.69 ? 10.77 78.97 ? 13.95 79.91 ? 13.11</cell></row><row><cell>PNA</cell><cell>68.15 ? 2.39</cell><cell>66.35 ? 3.34</cell><cell>61.40 ? 3.56</cell></row><row><cell>PNA+GSAT</cell><cell>68.74 ? 2.24</cell><cell>64.38 ? 3.20</cell><cell>57.01 ? 2.95</cell></row><row><cell>PNA+GSAT-? = 0</cell><cell cols="3">59.68 ? 7.28 58.03 ? 11.84 53.94 ? 8.11</cell></row><row><cell>PNA+GSAT-NOSTOCH.</cell><cell cols="2">51.92 ? 11.17 41.22 ? 7.72</cell><cell>39.56 ? 2.74</cell></row><row><cell cols="4">PNA+GSAT-NOSTOCH.-? = 0 56.54 ? 6.88 48.93 ? 10.33 45.82 ? 9.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism</figDesc><table><row><cell>mocking</cell><cell>them</cell><cell>now</cell><cell>is</cell><cell>an</cell><cell>exercise</cell><cell cols="2">in pointlessness</cell><cell>.</cell><cell>characters</cell><cell>who</cell><cell>are</cell><cell>nearly</cell><cell></cell><cell>impossible</cell><cell>to</cell><cell>care</cell><cell>about</cell><cell>comes</cell><cell>across</cell><cell>as</cell><cell>lame</cell><cell></cell><cell>and</cell><cell>sophomoric</cell></row><row><cell>the</cell><cell cols="2">acting</cell><cell>is</cell><cell></cell><cell>robotically</cell><cell>italicized</cell><cell></cell><cell>,</cell><cell>will</cell><cell>leave</cell><cell>you</cell><cell>wanting</cell><cell>to</cell><cell>abandon</cell><cell>the</cell><cell>theater</cell><cell>.</cell><cell>the</cell><cell>director</cell><cell>'s</cell><cell>many</cell><cell>dodges</cell><cell>and</cell><cell>turns</cell></row><row><cell>adds</cell><cell>an</cell><cell></cell><cell>almost</cell><cell>constant</cell><cell>mindset</cell><cell>of</cell><cell></cell><cell>suspense</cell><cell>the</cell><cell cols="2">personal</cell><cell>touch</cell><cell></cell><cell>of</cell><cell cols="2">manual</cell><cell>animation</cell><cell>smarter</cell><cell>than</cell><cell>any</cell><cell>50</cell><cell>other</cell><cell>filmmakers</cell><cell>still</cell></row><row><cell>funny</cell><cell>in</cell><cell>a</cell><cell>sick</cell><cell>,</cell><cell>twisted</cell><cell>sort</cell><cell>of</cell><cell>way</cell><cell>a</cell><cell cols="2">delightful coming</cell><cell>-</cell><cell>of</cell><cell>-</cell><cell>age</cell><cell>story</cell><cell>.</cell><cell>grace</cell><cell>this</cell><cell>deeply</cell><cell cols="2">touching</cell><cell>melodrama</cell><cell>.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We greatly thank the actionable suggestions given by reviewers. S. Miao and M. Liu are supported by the National Science Foundation (NSF) award HDR-2117997. P. Li is supported by the JPMorgan Faculty Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">D. Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explainability techniques for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshops, 2019 Workshop on Learning and Reasoning with Graph-Structured Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unveiling the predictive power of static structure in glassy systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwi?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Obika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="448" to="454" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to explain: An information-theoretic perspective on model interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="883" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering symbolic models from deep learning with inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17429" to="17442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Techniques for interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving molecular graph neural network explainability with orthonormalization and induced sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4203" to="4213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graphlime</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06216</idno>
		<title level="m">Local interpretable model explanations for graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4204" to="4214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Outof-distribution generalization via risk extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Caballero</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Binas</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Priol</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
			<affiliation>
				<orgName type="collaboration">rex</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6666" to="6679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards transparent and explainable attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mohankumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4206" to="4216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Causal Inference in Statistics: A Primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explainability methods for graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10772" to="10781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model-agnostic interpretability of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Workshop on Human Interpretability in Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interpreting graph neural networks for {nlp} with differentiable edge masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Is attention interpretable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial graph augmentation to improve graph contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15920" to="15933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Theory Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The information bottleneck method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>physics/0004057</idno>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pgm-explainer: Probabilistic graphical model explanations for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Thai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12225" to="12235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">C-h bond activation enables the rapid construction and late-stage diversification of functional molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wencel-Delord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Glorius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature chemistry</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="20437" to="20448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generating explanations for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gnnexplainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9240" to="9251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">If not specified, GSAT uses the same settings mentioned for the backbone models. All Spurious-Motif datasets share the same hyperparameters</title>
		<imprint/>
	</monogr>
	<note>Basic Setting. which are tuned based on b = 0.5</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">When PNA is used, GSAT uses 0.001 learning rate for all OGBG-Mol datasets; otherwise it uses the same learning rate as mentioned above</title>
		<imprint/>
	</monogr>
	<note>Learning Rate</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Ba-2Motif and Mutag use r = 0.5, and all other datasets use r = 0.7. We find r = 0.7 can generally provide great performance for all datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>r in Equation. Inspired by curriculum learning</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">2017) is not tuned, and we use 1 for all datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Temperature</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Temperature used in the Gumbel-softmax trick</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">If not specified, baselines use the same settings mentioned for the backbone models. All Spurious-Motif datasets share the same hyperparameters</title>
		<imprint/>
	</monogr>
	<note>Basic Setting. which are tuned based on b = 0.5</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">001), based on validation interpretation ROC AUC. The coefficient of the entropy regularization term is set to the recommended value 1. Again, in a real-world setting</title>
		<imprint/>
	</monogr>
	<note>GNNExplainer. We tune the learning rate from (1, 0.1, 0.01, 0.001) and the coefficient of the 1 -norm from (0.1, 0.01, 0.. post-hoc methods have no clear metric to tune hyper-parameters</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">2020), including the temperature, the coefficient of 1 -norm regularization and the coefficient of entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pgexplainer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>We use the tuned recommended settings from</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">2021), including the temperature, gamma, zeta and the coefficient of 0 -norm regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Graphmask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlichtkrull</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>We use the recommended settings from</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
