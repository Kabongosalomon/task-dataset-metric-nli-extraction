<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoWeird: Weird Translational Scoring Function Identified by Random Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology Clear Water Bay</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
							<email>zhangyongqi@4paradigm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AutoWeird: Weird Translational Scoring Function Identified by Random Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scoring function (SF) measures the plausibility of triplets in knowledge graphs. Different scoring functions can lead to huge differences in link prediction performances on different knowledge graphs. In this report, we describe a weird scoring function found by random search on the open graph benchmark (OGB). This scoring function, called AutoWeird, only uses tail entity and relation in a triplet to compute its plausibility score. Experimental results show that AutoWeird achieves top-1 performance on ogbl-wikikg2 data set, but has much worse performance than other methods on ogbl-biokg data set. By analyzing the tail entity distribution and evaluation protocol of these two data sets, we attribute the unexpected success of AutoWeird on ogbl-wikikg2 to inappropriate evaluation and concentrated tail entity distribution. Such results may motivate further research on how to accurately evaluate the performance of different link prediction methods for knowledge graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scoring function (SF), which measures the plausibility of triplets in knowledge graphs (KGs), plays an important role to learn from KGs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref>. To fairly evaluate different SFs, open graph benchmark (OGB) <ref type="bibr" target="#b4">[5]</ref> sets up leaderboards to evaluate the different models on large-scale graphs. Among the two leaderboards for link prediction task in KG, we observe that the leading models on ogbl-wikikg2 have similar forms by modeling embeddings as translations in vector spaces. To analyze the success of these SFs and try to design a better SF, we are motivated to use automated machine learning (AutoML) techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> to search SFs in this type.</p><p>Based on the translational models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16]</ref>, we first set up a search space that can cover all these translational models as special cases. Then, we conduct random search in this space to explore new designs of scoring functions. After evaluating 10 SFs, we identified a weird SF which does not contain head entity embedding but outperforms the existing models. This is abnormal as triplets in KGs are formed by head entity, relation and tail entity. By analyzing the distribution of tail entities, we guess the success of such a weird SF may attribute to the concentrated distribution and the evaluation protocol with random negative sampling.</p><p>This tech-report is organized as follows. First, we introduce the search space, search algorithm and the training objective in Section 2. Then in Section 3, we show the the top-1 performance achieved by the weird translation-based SF on ogbl-wikikg2 and our analysis on the searched results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The scoring function used here is found by automated machine learning techniques (AutoML) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. We first describe the search space where we find the scoring function (SF) in Section 2.1. And our search algorithm is introduced in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search space</head><p>Our search space is motivated by translation-based models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16]</ref> on the ogbl-wikikg2 leaderboard <ref type="bibr" target="#b0">1</ref> . Our search space is designed as follows:</p><p>? We set up the entity embeddings with two parts e 0 , e 1 , relation embeddings with three parts r 0 , r 1 , r 2 . All these vectors have same dimension d, i.e. e 0 , e 1 , r 0 , r 1 , r 2 ? R d . ? The scoring function is given by combining the the different components, i.e., s(h, r, t) = ? f (e h 0 , e h 1 , r 0 , r 1 , r 2 , e t 0 , e t 1 ) 1 , where e h 0 , e h 1 denote the embeddings for head entity h, e t 0 , e t 1 denote the embeddings for tail entity t, r 0 , r 1 , r 2 denote the embeddings for relation r, and ? 1 is the 1 norm.</p><p>? The function f (?) outputs a vector in R d , and is computed by adding/subtracting first-order terms (e h 0 , e h 1 , r 0 , r 1 , r 2 , e t 0 , e t 1 ) and/or second-order terms (element-wise product between any of these two vectors), e.g., e h 0 ? e t 1 and e h 0 ? r 1 , for entity and relation embeddings. We can see that many translation-based SFs that are already on the OGB leaderboard <ref type="bibr" target="#b4">[5]</ref> can all be covered in this search space. Some examples are listed in <ref type="table">Table 1</ref>. <ref type="table">Table 1</ref>: Example knowledge graph embedding models that can be covered by our search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Scoring Function</p><formula xml:id="formula_0">TransE [1] ? e h 0 ? e t 0 + r0 InterHT [12] ? e h 0 ? e t 1 ? e h 1 ? e t 0 + r0 TripleRE [15] ? e h 0 ? r1 ? e t 0 ? r2 + r0 PairRE [2] ? e h 0 ? r1 ? e t 0 ? r2 TranS [16] ? e h 0 ? e t 1 ? e h 1 ? e t 0 + r0 + e h 0 ? r1 + e t 0 ? r2 AutoWeird ? ? e t 1 ? r2 + e t 0 ? r0 + e t 0 ? r2 ? r0</formula><p>This search space covers a large number of different scoring functions. Note that with 7 different vectors e h 0 , e h 1 , r 0 , r 1 , r 2 , e t 0 , e t 1 , the total number of first-order and second-order terms is 7 + 7 ? 7 = 56. And for each of these 56 terms, we have three possible choices for its coefficient: 1, -1 and 0, where 0 coefficient means that it does not appear in f (?). Therefore, the total number of possible scoring functions can be as large as 3 56 ? 5.23 ? 10 26 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search algorithm</head><p>To find new scoring functions from this search space, we consider the following bi-level optimization problem: min</p><formula xml:id="formula_1">? M val ({e(?), r(?)}) s.t. {e(?), r(?)} = min {e,r} L tr ({e, r}; s ? ),</formula><p>where ? denotes the hyper-parameters that specify the SF s ? , M val refers to the validation performance (e.g., MRR on the validation set), and L tr refers to the training loss. We use the self-adversarial negative sampling loss <ref type="bibr" target="#b8">[9]</ref>, which is defined as follows:</p><formula xml:id="formula_2">L({e, r}; s ? ) = ? log ?(? ? s ? (h, r, t)) ? n i=1 1 k log ?(s ? (h i , r, t i ) ? ?),<label>(1)</label></formula><p>where ? is a fixed margin, ? is the sigmoid function, and (h i , r, t i ) is the i-th negative triplet.</p><p>For simplicity, we use random search to solve this problem. We first restrict the f (?) function to be computed by 5 first-order or second-order terms, and randomly choose 4 terms among all 56 terms. Then for each term, we randomly assign a coefficient 1 or -1 for it. The final function f (?) is computed by summing over these terms multiplied by their coefficients. We repeat the random search for 10 times. The searched SF, referred as AutoWeird, is also shown in <ref type="table">Table 1</ref>. Different with all previous scoring functions, it only uses information from the relation (r 0 , r 2 ) and tail entity (e t 0 and e t 1 ) to compute the score. Intuitively, it should have bad performances as it does not consider the head entity. However, as will be demonstrated in experimental parts, it can achieve state-of-the-art performances on some KG data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we will first introduce the dataset. Then we will introduce implementation details, experimental results, and the post-analysis on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and metric</head><p>The ogbl-wikikg2 <ref type="bibr" target="#b4">[5]</ref> is a large KG dataset extracted from Wikidata <ref type="bibr" target="#b10">[11]</ref>. It contains a set of triplet edges, capturing the different types of relations between entities in the world. There are 2, 500, 604 entities, 535 relation types and 17, 137, 181 triplets in the whole data set.</p><p>We use original dataset splitting configuration. It splits the triplets according to time, simulating a realistic KG completion scenario that aims to fill in missing triplets that are not present at a certain timestamp. The training set contains 16,109,182 triplets, the validation set contains 429,456 triplets, and the test set contains 598,543 triplets.</p><p>Following the official guideline, we evaluate the KG embedding performance by predicting new triplet edges according to the training edges. The evaluation metric follows the standard filtered metric <ref type="bibr" target="#b0">[1]</ref> which is widely used in KG. Specifically, each test triplet edges are corrupted by replacing its head or tail with randomly-sampled 500 negative entities, while ensuring the resulting triplets do not appear in KG. The goal is to rank the true head (or tail) entities higher than the negative entities, which is measured by Mean Reciprocal Rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>In our experiments, the Adam <ref type="bibr" target="#b7">[8]</ref> optimizer is used with a learning rate of 0.0005. The batch size of the model is set to 512. To prevent overfitting, we use dropout technique, and set it to 0.1. The negative sampling size is set to 128. And the dimension d of each embedding vector e 0 , e 1 , r 0 , r 1 , r 2 ? R d is set to 200. The maximum number of training steps is 300,000. We validate the model every 20 thousand steps. The number of anchors for NodePiece are 20,000. And ? in the loss function is set to 6. These hyper-parameters are selected according to the performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results on ogbl-wikikg2 are shown in <ref type="table" target="#tab_0">Table 2</ref>. By only using information from relation and tail entity, the AutoWeird model still achieves an MRR of 0.7368 on validation set and 0.7368 on test set, which outperforms all previous models on the ogbl-wikikg2 dataset. The revised version of AutoWeird model also has a performance better than all the previous methods. These counter-intuitive results demonstrate some hidden problems on the evaluation of KG embedding models. Hence, we conduct a post-analysis in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Post-analysis: why AutoWeird is effective?</head><p>To analyze why AutoWeird is effective, we first count the occurrence of tail entities after adding inverse relations to the data set. Specifically, for each entity, we count how many times it appears as  <ref type="bibr" target="#b8">[9]</ref> 1250M 250 0.4332 ? 0.0025 0.4353 ? 0.0028 PairRE <ref type="bibr" target="#b1">[2]</ref> 500M 200 0.5208 ? 0.0027 0.5423 ? 0.0020 AutoSF <ref type="bibr" target="#b16">[17]</ref> 500M -0.5458 ? 0.0052 0.5510 ? 0.0063 ComplEx <ref type="bibr" target="#b9">[10]</ref> 1251M 250 0.5027 ? 0.0027 0.3759 ? 0.0016 TripleRE <ref type="bibr" target="#b14">[15]</ref> 501M 200 0.5794 ? 0.0020 0.6045 ? 0.0024</p><p>ComplEx-RP <ref type="bibr" target="#b2">[3]</ref> 250M 50 0.6392 ? 0.0045 0.6561 ? 0.0070 AutoSF + NodePiece <ref type="bibr" target="#b3">[4]</ref> 6.9M -0.5703 ? 0.0035 0.5806 ? 0.0047 TripleREv2 + NodePiece <ref type="bibr" target="#b14">[15]</ref> 7.3M 200 0.6582 ? 0.0020 0.6616 ? 0.0018 InterHT + NodePiece <ref type="bibr" target="#b11">[12]</ref> 19.2M 200 0.6779 ? 0.0018 0.6893 ? 0.0015 TripleREv3 + NodePiece <ref type="bibr" target="#b14">[15]</ref> 36.4M 200 0.6866 ? 0.0014 0.6955 ? 0.0008 TranS + NodePiece <ref type="bibr" target="#b15">[16]</ref> 19  188M 250 0.7989 ? 0.0004 0.7997 ? 0.0002 ComplEx <ref type="bibr" target="#b9">[10]</ref> 188M 250 0.8095 ? 0.0007 0.8105 ? 0.0001 PairRE <ref type="bibr" target="#b1">[2]</ref> 188M 200 0.8164 ? 0.0005 0.8172 ? 0.0005 AutoSF <ref type="bibr" target="#b16">[17]</ref> 93.8M -0.8309 ? 0.0008 0.8317 ? 0.0007 TripleRE <ref type="bibr" target="#b14">[15]</ref> 470M 200 0.8348 ? 0.0007 0.8360 ? 0.0006 ComplEx-RP <ref type="bibr" target="#b2">[3]</ref> 188M 1000 0.8492 ? 0.0002 0.8497 ? 0.0002 AutoBLM+KGBench <ref type="bibr" target="#b17">[18]</ref> 192M -0.8536 ? 0.0003 0.8548 ? 0.0002</p><p>AutoWeird 376M 500 0.6755 ? 0.0004 0.6752 ? 0.0003</p><p>EntOccur --0.3387 0.3387 the tail entity 2 in the training set. As is shown in <ref type="figure" target="#fig_4">Figure 1(a)</ref>, a few number of entities has more than one million occurrences in the training set. Such a concentrated distribution also exists in the test triplets <ref type="figure" target="#fig_4">(Figure 1(b)</ref>), leading to a strong bias to the high occurrence entities.</p><p>Note that the evaluation protocol in ogbl-wikikg2 is to rank the positive tail entities over a set of 500 randomly sampled negative entities. Hence, the bias will become more severe during evaluation. As in <ref type="figure" target="#fig_4">Figure 1</ref>(a) and 1(b), if we only randomly sample 500 entities from 10 6 entities, the majority of them will has low occurrence.</p><p>Based on these observations, we design a simple method, named as EntOccur, by using the occurrence as scores. For each triplet (h, r, t) in the training set, we count the occurrence of entities t for each relation r. Then the score of (h, r, t) in inference is directly set as the occurrence of t with respect to the specific relation r. As in <ref type="table" target="#tab_0">Table 2</ref>, such a simple model EntOccur outperforms some embedding models like TransE and RotatE.</p><p>We further evaluate the searched SF AutoWeird and the simple solution EntOccur on the other KG link prediction dataset ogbl-biokg <ref type="bibr" target="#b4">[5]</ref>. By comparing the performance of AutoWeird and EntOccur with the results on the leaderboard 3 in <ref type="table" target="#tab_2">Table 3</ref>, AutoWeird and EntOccur are much worse. By comparaing ogbl-biokg with ogbl-wikikg2, we find that     ? the number of entities in ogbl-biokg is 93, 773, much smaller than ogbl-wikikg2 with 2, 500, 604;</p><p>? the negative samples for evaluation in ogbl-biokg have the same type with the positive tail entity.</p><p>Hence, the negative entities in ogbl-biokg have higher correlation with the positive tail entity than ogbl-wikikg2, alleviating the bias towards entities with high occurrence. As such, we attribute the success of AutoWeird in ogbl-wikikg2 to the inappropriate evaluation protocol.</p><p>In conclusion, these results show that there is a relatively strong correlation between relations and a few high occurrence entities in ogbl-wikikg2. And the correlation may lead to the weird scoring function, AutoWeird in <ref type="table">Table 1</ref>. A better way to compare the different models may be (1) using the full set of negative entities, or (2) using some highly correlated negative entities, or (3) using the larger number of negative entities rather than 500 randomly sampled ones. However, due to the time complexity in evaluating millions of negative entities for each triplet in ogbl-wikikg2, we leave it as a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Training set of ogbl-wikikg2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Testing set of ogbl-wikikg2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Training set of ogbl-biokg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Testing set of ogbl-biokg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Tail entity occurrence distribution for different KG data sets.? the distributions of tail entity occurrence in ogbl-biokg inFigure 1(c) and 1(d) are not as concentrated as those of ogbl-wikikg2 in Figure 1(a) and 1(b);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Results on the ogbl-wikikg2 dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params #Dims</cell><cell>Test MRR</cell><cell>Valid MRR</cell></row><row><cell>TransE [1]</cell><cell>1251M</cell><cell>500</cell><cell cols="2">0.4256 ? 0.0030 0.4272 ? 0.0030</cell></row><row><cell>RotatE</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the ogbl-biokg dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params #Dims</cell><cell>Test MRR</cell><cell>Valid MRR</cell></row><row><cell>TransE [1]</cell><cell>188M</cell><cell>500</cell><cell cols="2">0.7452 ? 0.0004 0.7456 ? 0.0003</cell></row><row><cell>RotatE [9]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://ogb.stanford.edu/docs/leader_linkprop/#ogbl-wikikg2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For simplicity, we regard the head prediction as tail prediction by adding inverse relations, i.e., (t, rinv, h) for (h, r, t) in the datasets.<ref type="bibr" target="#b2">3</ref> https://ogb.stanford.edu/docs/leader_linkprop/#ogbl-biokg</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel search space for the scoring function in knowledge graph embedding model. We find a novel scoring function from this search space by random search, which only depends on tail entity and relation. Empirical results on the ogbl-wikikg2 dataset demonstrate that the searched scoring function is able to achieve state-of-the-art results among all previous models. These strange results should motivate us to consider potential problems on the evaluation of KG embedding models.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03798</idno>
		<title level="m">Pairre: Knowledge graph embeddings via paired relation vectors</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relation prediction as an auxiliary training objective for improving multi-relational graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02834</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12144</idno>
		<title level="m">Nodepiece: Compositional and parameterefficient representations of large knowledge graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automated machine learning: methods, systems, challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer Nature</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">130</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interht: Knowledge graph embeddings by interaction between head and tail entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04897</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.0095</idno>
		<title level="m">Triplere: Knowledge graph embeddings via triple relation vectors</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Trans: Transition-based knowledge graph embedding with synthetic relation representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08401</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilinear scoring function search for knowledge graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
