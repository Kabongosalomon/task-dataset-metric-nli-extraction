<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection Human-Object Interaction Detection, Graph Neural Networks, Zero- Shot Learning ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
							<email>ye-liu@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Resource and Environmental Sciences</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
							<email>chencw@buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
						</author>
						<title level="a" type="main">ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection Human-Object Interaction Detection, Graph Neural Networks, Zero- Shot Learning ACM Reference Format</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA MM &apos;20; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413600</idno>
					<note>KEYWORDS 2020. ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection. In. ACM, New York, NY, USA, 9 pages. ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS ? Computing methodologies ? Activity recognition and un- derstanding; Scene understanding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of Human-Object Interaction (HOI) Detection, which aims to locate and recognize HOI instances in the form of ?? , , ? in images. Most existing works treat HOIs as individual interaction categories, thus can not handle the problem of long-tail distribution and polysemy of action labels. We argue that multi-level consistencies among objects, actions and interactions are strong cues for generating semantic representations of rare or previously unseen HOIs. Leveraging the compositional and relational peculiarities of HOI labels, we propose ConsNet, a knowledge-aware framework that explicitly encodes the relations among objects, actions and interactions into an undirected graph called consistency graph, and exploits Graph Attention Networks (GATs) to propagate knowledge among HOI categories as well as their constituents. Our model takes visual features of candidate human-object pairs and word embeddings of HOI labels as inputs, maps them into visual-semantic joint embedding space and obtains detection results by measuring their similarities. We extensively evaluate our model on the challenging V-COCO and HICO-DET datasets, and results validate that our approach outperforms stateof-the-arts under both fully-supervised and zero-shot settings. Code is available at https://github.com/yeliudev/ConsNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Illustration of knowledge-aware human-object interaction detection. Red, blue and black lines represent functionally similar objects, behaviorally similar actions, and holistically similar interactions. We argue that successful detection of an HOI should benefit from the knowledge obtained from similar objects, actions, and interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Beyond detecting individual human or object instances in images, it is crucial for machines to also recognize how they interact with each other, which can be essential cues to understand the humancentric visual world. The task of Human-Object Interaction (HOI) Detection aims to locate and recognize HOI instances in images. For example, detecting ?? , , ? refers to locating "human" and "cat", as well as predicting the action "feed" for this humanobject pair. Instead of inferring ambiguous spatial relations among objects, e.g. "cat is on the bed", HOI detection plays a pivotal role to understand what is happening in the scene. Studying HOIs can benefit many down-stream visual understanding tasks including image captioning <ref type="bibr" target="#b24">[25]</ref>, image retrieval <ref type="bibr" target="#b44">[45]</ref>, and visual question answering <ref type="bibr" target="#b12">[13]</ref>.</p><p>Most existing works on HOI detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> treat HOIs as individual interaction categories and focus on mining visual representations of human-object pairs to improve classification performances. Despite previous successes, these conventional approaches still face two challenges. First, compared with other ride horse ride snowboard ride bus ride boat ride skis ride motorcycle <ref type="figure">Figure 2</ref>: Polysemy of action labels. All the HOIs above share the same action label "ride", but the actual implications of these actions are inconsistent, as can be seen from the inferred human poses.</p><p>action-based recognition tasks, what makes HOI detection challenging is that labels of HOIs are fine-grained and are related to the specific object category. The quadratic number of combinations of actions and objects brings prohibitive annotation cost. Hence, noncompositional methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> are largely restricted by the coverage and long-tail distribution of exhaustive HOI annotations. Second, the compositional peculiarity of HOI labels also leads to the polysemy of action labels. As an example shown in <ref type="figure">Figure 2</ref>, collocated with different objects, the actual implications of action "ride" are sometimes inconsistent. Such phenomenon brings ambiguities and extra challenges to compositional methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In this work, we address the above two challenges by proposing a knowledge-aware approach (as shown in <ref type="figure">Figure 1</ref>) for HOI detection. For the first challenge, we claim that the key to dealing with the imbalance and scarcity of HOI training samples is to distill knowledge obtained from non-rare categories, and transfer it to rare or unseen ones. Considering that humans have the ability to perceive unseen interactions, e.g. ?? , , ? ?, because they can make use of their common sense to imagine what it would be like based on similar HOIs such as ?? , , ? and ?? , , ? ?, as well as similar actions or objects such as "sit on" or "horse". To jointly capture the compositional peculiarities and multi-level similarities among HOIs, we define three types of consistencies at different granularities. At unigram level, we introduce functional consistency which depicts the functional similarities among objects, and behavioral consistency that represents the similarities of human behaviors when performing different actions. At trigram level, we present interactional consistency, which denotes the holistic similarities among HOIs. We further construct an undirected graph, namely consistency graph, to explicitly encode these relations. Each node in the consistency graph represents an HOI label or one of its entities. The three types of consistencies are encoded as edges among the nodes. That is, two object, action or interaction nodes are linked if they have whichever the consistencies above. We then use word embeddings of HOI labels as input features of nodes, and exploit recently introduced Graph Attention Networks (GATs) <ref type="bibr" target="#b37">[38]</ref> to perform message passing on the consistency graph, enabling the model to learn semantic representations of HOIs in a transductive manner.</p><p>When it comes to the second challenge, we argue that an appropriate perception of HOI should benefit from both unigram and trigram representations. Take the HOI ?? , , ? for instance. At unigram level, we ought to make sure that the subject is a human, the object is a bicycle, and the subject is performing the action "ride". At trigram level, we should also deem that the human-object pair is performing the right interaction holistically. In our model, HOI detection scores are estimated based on the similarities between visual and semantic embeddings of human, object, action, and interaction. Such a decomposition strategy helps capture implications of HOIs at multiple granularities, thus can better handle the polysemy of action labels. Moreover, our model has the ability to transfer knowledge from familiar HOIs to HOIs with unseen actions, objects, or action-object combinations. Note that detecting HOIs with unseen actions may not be performed by previous methods.</p><p>The main contributions of our work are as follows:</p><p>? We propose a knowledge-aware approach to model relations among HOIs at both unigram and trigram level, and exploit Graph Attention Networks to predict semantic representations of HOIs based on their word embeddings. ? We introduce a data-driven method to estimate consistencies and construct the consistency graph using visual-semantic representations of HOI labels, which can jointly capture visual and semantic features of HOIs. ? Our approach outperforms state-of-the-arts under both fullysupervised and zero-shot settings on the challenging V-COCO and HICO-DET datasets. Further experiments also show that our model has the ability to detect HOIs with unseen actions, which may not be performed by previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Human-Object Interaction Detection. Human-Object Interaction Detection plays a crucial role in human-centric scene understanding since the problem was first introduced by Gupta and Malik <ref type="bibr" target="#b13">[14]</ref>. Most previous works can be divided into compositional methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref> and non-compositional methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. Compositional methods learn separate detectors for objects and actions, then fuse the confidences to generate HOI detection results. However, these approaches suffer from the polysemy of action labels. Non-compositional methods avoid this problem by predicting fine-grained HOI labels directly, but they are restricted by the long-tail distribution of HOI categories. Recently introduced hybrid model <ref type="bibr" target="#b32">[33]</ref> has shown that using multi-granularity representations of HOIs may solve the above contradiction. Nonetheless, all these  <ref type="figure">Figure 3</ref>: Overall architecture of our framework. The input image is fed into a pre-trained object detector to obtain bounding boxes ? , with detection confidences ? , of humans and objects. The bounding boxes are then used to crop visual features ? , from FPN and compute spatial configuration . Subsequently, visual embedding network maps these features into multilevel visual embeddings ? , , , . On the other side, semantic embedding network encodes HOI labels into vectors using a pre-trained language model. The word embeddings serve as input features of nodes in the consistency graph. By performing GATs, these features are propagated among neighboring nodes and be transformed into semantic embeddings ? , , , . The HOI detection results are then generated by measuring the similarities among visual embeddings and semantic embeddings. methods ignore the implicit relations among HOIs, thus we extend the hybrid model by incorporating common sense knowledge for generating semantic embeddings.</p><p>Graph Neural Networks. The past few years have witnessed the rapid development of representation learning on graphs <ref type="bibr" target="#b45">[46]</ref>. The majority of these methods are under the Message Passing Neural Networks (MPNN) framework <ref type="bibr" target="#b9">[10]</ref> which decomposes the pipeline into message functions, vertex update functions, and readout functions. Kipf et al. <ref type="bibr" target="#b20">[21]</ref> extend the convolution operation <ref type="bibr" target="#b22">[23]</ref> from euclidean data to non-euclidean data and proposed Graph Convolutional Networks (GCNs). Wu et al. <ref type="bibr" target="#b43">[44]</ref> introduced SGCs to simplify GCNs by removing the non-linearities and merging the weights. Hamilton et al. <ref type="bibr" target="#b15">[16]</ref> proposed GraphSAGE to realize inductive learning on graphs. In this work, we exploit Graph Attention Networks (GATs) <ref type="bibr" target="#b37">[38]</ref> that incorporate multi-head attention mechanism to model the relations of neighboring nodes. The learned attention coefficients in GATs serve as the weights of consistencies.</p><p>Zero-Shot Learning. Most recent zero-shot learning methods can be divided into two protocols <ref type="bibr" target="#b42">[43]</ref>. One is to learn semantic representations of categories that can be mapped to visual classifiers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The other is to make use of knowledge graphs to distill the knowledge <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In this work, with the help of GNNs and language models, we learn the explicit and implicit knowledge of HOIs from consistency graph and word embeddings respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we introduce our approach on knowledge-aware HOI detection. As illustrated in <ref type="figure">Figure 3</ref>, the entire framework can be divided into two sub-modules, namely visual embedding network and semantic embedding network. These sub-modules map visual representations of human-object pairs and word embeddings of HOI labels into visual-semantic joint embedding space. HOI detection results are then generated by measuring similarities between visual and semantic embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given an image and a set of HOI categories of interest H = {1, ..., }, the task of human-object interaction detection is to detect all the human-object pairs in , where the humans and objects are participating one or multiple pre-defined interactions. The outputs of HOI detection would be a set of tuples T = {? ? , , ?, ?}, where ? , ? R 4 denotes bounding boxes of the human and the object, and ?, represents a vector where ?, ? {0, 1} indicates whether the HOI class is assigned to this human-object pair. Note that a person may have several interactions with multiple objects simultaneously, thus different HOIs may share the same human, action or object.</p><p>We adopt a three-stage HOI detection pipeline by generating a set of human-object pairs as candidates, filtering out non-interactive ones and classifying the remainders into multiple interaction categories. In the first stage, a pre-trained object detector is used to collect bounding boxes of humans ? and objects , along with their corresponding detection confidences ? , . We only keep top detections with confidences higher than a threshold , where ? {?, } denotes human or object. The candidates are then obtained by pairing up all the humans and objects extensively.</p><p>Recent works have shown that in most cases, the majority of humans and objects in an image are not interacting with each other. Such a severe imbalance between positive and negative candidates makes HOI classification challenging. To address this problem, Li et al. <ref type="bibr" target="#b25">[26]</ref> proposed the strategy of non-interactive suppression (NIS) to filter out and suppress potential non-interactive candidates. In the second stage, we predict the class-irrelevant interactiveness ?, for each candidate by</p><formula xml:id="formula_0">?, = ( ?? ? {?, , , } )<label>(1)</label></formula><p>where (?) denotes the Sigmoid function and , ? {?, , , } indicates the interactiveness score at human, object, action or interaction level. Candidates with interactiveness ?, lower than a threshold ?, would be discarded. The remaining ones are then fed into HOI classifier for further interaction classification.</p><p>In the third stage, we classify the candidates into HOI categories in a knowledge-aware manner. For each candidate, the confidence of assigning HOI class to it can be given by</p><formula xml:id="formula_1">( = 1| , ? , , ? , ) = ?, ? ?, ? ? ?<label>(2)</label></formula><p>where ?, is the HOI classification score given by the HOI classifier. Interactiveness ?, , human detection confidence ? ? ? and object detection confidence ? serve as suppression terms on potential non-interactive or non-existent candidates. The HOI classification score ?, can be given by</p><formula xml:id="formula_2">?, = ( ?? ? {?, , , } ? ? ? 2 ? ? ? 2 ? )<label>(3)</label></formula><p>where denotes visual embeddings of the candidate, including human ? , object , action , and interaction . represents semantic embeddings of these entities for HOI class . We treat as templates of HOIs and measure the distance among visual and semantic embeddings by computing cosine similarities. Note that we also add a scale factor to control the range of outputs.</p><p>The visual embeddings , interactiveness ?, and semantic embeddings are generated by visual embedding network and semantic embedding network. Details of the embedding networks are explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Embedding Network</head><p>Visual embedding network takes image as well as bounding boxes of human and object ? , as inputs, and generates visual embeddings of human ? , object , action , and interaction . These visual embeddings are constructed based on visual features of human ? , object , and their spatial configuration . We adopt ResNet-50-FPN <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, which can be shared with the object detector, as the feature extractor. We obtain the visual features of human and object by cropping the appropriate level of feature map from FPN using RoIAlign <ref type="bibr" target="#b16">[17]</ref> according to their bounding boxes. Spatial configuration of a candidate is computed by</p><formula xml:id="formula_3">= ? ? {?, } ( 1 ? ? 2 ? ? 1 ? ? 2 ? )<label>(4)</label></formula><p>where ? denotes concatenation operation, 1 , 2 , 1 , 2 , ? {?, } are coordinates of the human or object bounding box, ( , ) and represent the origin and area of the union box respectively. The computed spatial configuration would be an 1 ? 8 vector. We hypothesize that visual embeddings of human and object can be predicted by their own visual features , ? {?, }, while visual embeddings of action and interaction are jointly affected by visual features of human and object , ? {?, } as well as their spatial configuration .</p><formula xml:id="formula_4">( , | , ? , ) = ( , | ), ? {?, }<label>(5)</label></formula><formula xml:id="formula_5">( , | , ? , ) = ( , | ? , , ), ? { , }<label>(6)</label></formula><p>Based on the hypotheses above, we introduce two types of embedding blocks, i.e. mapper block and fusion block, to predict interactiveness ?, and generate visual embeddings , ? {?, , , } for candidates. Details of the embedding blocks are described in section 3.2.1 and 3.2.2. <ref type="figure" target="#fig_1">Figure 4</ref> (a), mapper block only takes visual features of the human or object as inputs. These visual features are first transformed into hidden states by a multi-layer perceptron (MLP). After that, two MLPs are used to map the dimensions of hidden states to 1 ? 1 and 1 ? 1024 respectively. The two outputs are interactiveness and visual embeddings . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Mapper Block. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic Embedding Network</head><p>To jointly capture multi-level consistencies among HOIs, we incorporate a knowledge graph, namely consistency graph, into the semantic embedding network to help generate semantic embeddings of HOI categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>Constructing the Graph. Instead of using a large-scale knowledge graph, we distill the knowledge and construct a much smaller one, which only contains consistencies and compositional relations among HOIs and their entities. As illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>, each HOI category refers to three entity nodes and one interaction node in the consistency graph. HOIs with shared entities would share the entity nodes as well. For instance, ?? , , ? and ?? , , ? ? are represented by four entity nodes "human", "ride", "bicycle", and "horse", as well as two interaction nodes "human ride bicycle" and "human ride horse".</p><p>We first add edges among interaction nodes and their corresponding entity nodes, which serve as bridges among different levels of consistencies. The other edges are defined based on the consistencies among objects, actions, and interactions. That is, if two ResNet-50 45.3 TIN-RP T2 C D <ref type="bibr" target="#b25">[26]</ref> ResNet-50 48.7 BAR-CNN <ref type="bibr" target="#b21">[22]</ref> Inception-ResNet 43.6 Wang et al. <ref type="bibr" target="#b40">[41]</ref> ResNet-50 47.3 PMFNet <ref type="bibr" target="#b38">[39]</ref> ResNet-50 52.0 IP-Net <ref type="bibr" target="#b41">[42]</ref> Hourglass-104 51.0 VSGNet <ref type="bibr" target="#b36">[37]</ref> ResNet-152 51.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConsNet (ours)</head><p>ResNet-50-FPN 53.2 nodes are semantically consistent with each other, an edge would be added to enable message passing between them. We estimate the multi-level consistencies using cosine similarity by</p><formula xml:id="formula_6">? ( , ) = ? ? ? 2 ? ? ? 2 , ? { , , }<label>(7)</label></formula><p>where ? { , , } indicates the type of the node, ? ( , ) denotes the consistency between node and , and represent visualsemantic joint features of the two nodes respectively. For each node, we link itself with only top consistent nodes. We propose a data-driven approach to generate the joint features of nodes. First, we collect all the visual features of humans and objects in the dataset using a pre-trained object detector. These features are regarded as visual representations of actions and objects respectively. We then compute the average of all the visual representations with the same label to obtain the universal visual representations of these categories. Second, we adopt a pre-trained language model to generate word embeddings of node labels. Note that a label may contain multiple words, we fuse the word embeddings by computing their weighted sum. After collecting universal visual representations and word embeddings of node labels, we obtain the joint features of nodes by</p><formula xml:id="formula_7">= ( ? ? ? 2 ) ? ( ? ? ? 2 ), ? { , , }<label>(8)</label></formula><p>where and are visual and semantic representations of node labels, and are the weights of the representations. The L-2 normalized, re-weighted and concatenated visual-semantic representations are then used to estimate multi-level consistencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Learning to Aggregate Semantic Representations.</head><p>Graph Attention Networks (GATs) <ref type="bibr" target="#b37">[38]</ref> are first introduced for the task of semi-supervised node classification. Instead of simply averaging the features of neighboring nodes like GCNs <ref type="bibr" target="#b20">[21]</ref> or SGCs <ref type="bibr" target="#b43">[44]</ref>, GATs aggregate node features using a self-attention strategy. A single-level GAT layer can be represented as</p><formula xml:id="formula_8">? = ? =1 ( ?? ? , ? W ? ? )<label>(9)</label></formula><p>where ? and ? denote the hidden states of node and , indicates the number of attention heads, is the ReLU nonlinearity, represents the collection of node and its neighbours, , is the attention coefficient learned by the model and W refers to the weights of this layer. In order to fix the output dimensions of the last GAT layer, we replace its concatenation with average operation. The attention coefficient , can be predicted by</p><formula xml:id="formula_9">, = (?(W ? ? ? W ? ? )) ? (?(W ? ? ? W ? ? ))<label>(10)</label></formula><p>where W denotes the weights for estimating attention coefficient, ? is a single layer feed-forward network. The model uses masked softmax to obtain the normalized attention coefficients , . In this work, we adopt a three-layer GAT to propagate node features on the consistency graph. The input is a node feature matrix Z ? R ? given by a pre-trained ELMo <ref type="bibr" target="#b31">[32]</ref>. After three layers of GATs, the node features are mapped to dimensions, which are the same with visual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Learning</head><p>During training, visual embedding network learns to map visual features of human-object pairs into visual-semantic joint embedding space, while semantic embedding network learns to generate semantic embeddings of HOI categories. When testing, the semantic embeddings can be pre-computed and be used as templates of HOI categories. Since all the proposed components are differentiable, the whole model can be trained in an end-to-end manner. The overall objective of training is to minimize the distance among visual embeddings and semantic embeddings. We learn the parameters of the whole model by supervising ?, and ?, with the following binary cross-entropy losses:</p><formula xml:id="formula_10">L = ?( ? ( ?, ) + (1 ? ) ? (1 ? ?, ))<label>(11)</label></formula><formula xml:id="formula_11">L = ? 1 ?? =1 ( ? ( ?, ) + (1 ? ) ? (1 ? ?, ))<label>(12)</label></formula><p>where denotes interactiveness label and indicates HOI label. The interactiveness loss L and classification loss L are jointly optimized using their weighted sum by</p><formula xml:id="formula_12">L = L + ? L<label>(13)</label></formula><p>where is a scale factor balancing the loss weights. Note that we optimize the classification loss only with positive samples and the interactiveness loss with both positive and negative samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed method on the challenging V-COCO <ref type="bibr" target="#b13">[14]</ref> and HICO-DET <ref type="bibr" target="#b3">[4]</ref> datasets. We first evaluate our method under the fully-supervised settings on both of the datasets, following by zero-shot settings on HICO-DET dataset. The zero-shot settings includes three scenarios, i.e. unseen actionobject combination, unseen object, and unseen action. An extensive ablation study is also reported after the evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>V-COCO is a subset of MS-COCO 2014 dataset <ref type="bibr" target="#b28">[29]</ref>, it has 2,533 images for training, 2,867 images for validation and 4,946 images for testing. Each person is annotated with binary labels of 26 action categories. HICO-DET is another large-scale HOI detection dataset that extends annotations of HICO <ref type="bibr" target="#b4">[5]</ref> from image-level to instancelevel. The trainval split has 38,118 images while the test split has 9,658 images. It contains 117 action classes for 80 object classes, resulting in 600 HOI categories. We follow the standard evaluation metric introduced by Chao et al. <ref type="bibr" target="#b3">[4]</ref> that uses mean average precision (mAP) to measure the detection performance. An HOI detection is considered as a true positive when both the bounding boxes of the human and object have intersection over union (IoU) with a ground truth greater than 0.5, and the predicted HOI label is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We adopt Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> with ResNet-50-FPN as the object detector. The same backbone and neck are also used for feature extraction. We train the object detector on MS-COCO 2017 dataset using MMDetection <ref type="bibr" target="#b5">[6]</ref> and then freeze its weights. When training the HOI classifier, we use all the detections with confidence greater than 0.1 and make use of both ground truths and the detected candidate pairs. When testing, we only consider up to 10 humans with confidence greater than 0.5 and up to 20 objects with confidence greater than 0.1 per image to reduce computational cost.</p><p>We add batch normalization <ref type="bibr" target="#b18">[19]</ref> and ReLU nonlinearity <ref type="bibr" target="#b11">[12]</ref> after all hidden layers. The classification losses of different samples are weighted to prevent overfitting. Each training mini-batch contains 64 samples with the ratio of positive and negative samples 1 : 3. For all experiments, we use Stochastic Gradient Descent (SGD) optimizer with initial learning rate 0.01, momentum 0.9, and weight decay 0.0001. The linear warm-up policy starting from 0.001 learning rate for 500 iterations is adopted. All the models are trained for 5 epochs using cosine annealing learning rate schedule. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fully-Supervised HOI Detection</head><p>We first evaluate our model under fully-supervised settings. For both datasets, we train the model on trainval split and evaluate it on test split. The comparisons on V-COCO and HICO-DET datasets are shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>. Our method outperforms the previous best models on each subset. Note that for HICO-DET dataset, the object detectors in Bansal et al. <ref type="bibr" target="#b0">[1]</ref> and PPDM <ref type="bibr" target="#b26">[27]</ref> are trained on MS-COCO and finetuned on HICO-DET, which may provide more potential true positives and largely reduce false positives.</p><p>To be directly comparable, we also report the performance of our model with a finetuned detector called ConsNet-F, indicating that our method still achieves higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Zero-Shot HOI Detection</head><p>Shen et al. <ref type="bibr" target="#b35">[36]</ref> first introduced the concept of zero-shot HOI detection that detects HOIs with unseen action-object combinations, where the actions and objects are seen in other HOIs. Bansal et al. <ref type="bibr" target="#b0">[1]</ref> proposed to detecting HOIs with unseen objects. We now extend the task further and introduce the scenario of detecting HOIs with unseen actions, which means the model should have the ability to analogize semantic representations of new actions based on similar actions or interactions, which is much more challenging than the two scenarios above. Below we report the performance comparisons under these scenarios on HICO-DET dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Unseen Combination</head><p>Scenario. The first three rows in <ref type="table" target="#tab_2">Table 3</ref> shows the comparison of our method with others under unseen combination scenario. We use the same 5 sets of 120 unseen classes as Bansal et al. and report the means of the results. The comparison shows that our approach does much better on detecting unseen HOIs with seen actions and objects. <ref type="table" target="#tab_2">Table 3</ref> presents the performance comparison under unseen object scenario. Our model marginally outperforms the previous best method on unseen classes while having similar performance on seen classes, indicating that our method can better generalize to unseen objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Unseen Object Scenario. Line 4~5 in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Unseen Action Scenario.</head><p>In this scenario, we randomly select 22 actions, define them as unseen and remove all the training samples containing these actions. The full list of unseen actions will be publicly available. We then train the model on the remaining samples and evaluate on the full test split. The last row in <ref type="table" target="#tab_2">Table 3</ref> reports the performance of our approach on detecting HOIs with unseen actions. The results show that our model has the ability to detect HOIs even if the action is previously unseen, which is challenging because transferring the knowledge of actions is much harder than objects. Moreover, our approach can even do slightly better than some early methods under fully-supervised settings. <ref type="figure" target="#fig_3">Figure 6</ref> shows qualitative results of both fully-supervised and zero-shot HOI detection using our method. Even if our model has never seen the objects or actions before, the semantic embedding network can still benefit from seen HOIs and generate semantic representations of unseen HOIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>In order to analyze the significance of the proposed knowledgeaware strategy for generating semantic representations, we evaluate the models with different styles of semantic embedding networks and types of language models. All the experiments are performed on HICO-DET dataset under fully-supervised settings and the results are shown in <ref type="table" target="#tab_3">Table 4</ref>. Compared with not using semantic embedding network and simply using an MLP, HOI detection results on rare classes are largely improved with the use of GNNs. This is because the aggregation functions of GNNs can help transfer knowledge from non-rare classes to rare ones. The comparison also shows that with learnable attention coefficients, GATs are more flexible than other GNNs for generating semantic embeddings. Besides, the number of GAT layers matters. Deeper GATs can bring more learnable parameters, while it may cause the over-smoothing problem <ref type="bibr" target="#b23">[24]</ref>, leading to a performance drop. Performances are also considerably improved by changing word embeddings from Word2Vec <ref type="bibr" target="#b29">[30]</ref>, GloVe <ref type="bibr" target="#b30">[31]</ref>, or FastText <ref type="bibr" target="#b19">[20]</ref> to ELMo <ref type="bibr" target="#b31">[32]</ref>. The probable reason is that ELMo can better capture information at trigram level since the triplet is considered jointly as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we propose an end-to-end trainable framework for knowledge-aware human-object interaction detection by incorporating a consistency graph and exploiting GATs to propagate knowledge among nodes. Leveraging such a graph structure and message passing strategy, the model can capture and transfer knowledge about HOIs at different granularities and better generate semantic representations for rare or previously unseen HOIs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Detailed architecture of visual embedding network. a) Mapper block takes visual features of human or object , ? {?, } as inputs, and predicts visual embeddings , ? {?, } as well as interactiveness , ? {?, }. b) Fusion block takes human features ? , object features and their spatial configuration as inputs, and estimates visual embeddings of action or interaction , ? { , }, together with interactiveness .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Pipeline of constructing the consistency graph. a) Consistency graph contains human, object, action, and interaction nodes. b) Each interaction node is linked with its entity nodes. c) Functional consistencies are represented by object-object connections. d) Behavioral consistencies are represented by action-action connections. e) Interactional consistencies are represented by interaction-interaction connections. f) Generalize the rules above and build consistency graph.3.2.2 Fusion Block. As described inFigure 4(b), fusion block receives visual features of the human ? , object and their spatial configuration as inputs, and does the same job as mapper blocks. The only difference is that dimensions of ? , and are mapped to 1 ? 512, 1 ? 512 and 1 ? 256 respectively using MLPs in advance. The concatenation of the mapped features serves as joint features of the human-object pair and be used to estimate and .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on HICO-DET dataset. Our model has the ability to detect seen HOIs, HOIs with unseen objects and HOIs with unseen actions. Note that none of the previous models can detect HOIs with unseen actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Role Detection results on V-COCO dataset under fully-supervised settings.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP role</cell></row><row><cell>Gupta et al. [14]</cell><cell>ResNet-50-FPN</cell><cell>31.8</cell></row><row><cell>InteractNet [11]</cell><cell>ResNet-50-FPN</cell><cell>40.0</cell></row><row><cell>GPNN [34]</cell><cell>DCN</cell><cell>44.0</cell></row><row><cell>iCAN [9]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>HOI Detection results on HICO-DET dataset under fully-supervised settings. R and H represent ResNet and Hourglass respectively.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Full</cell><cell cols="2">Rare Non-Rare</cell></row><row><cell>Shen et al. [36]</cell><cell>VGG-19</cell><cell>6.46</cell><cell>4.24</cell><cell>7.12</cell></row><row><cell>HO-RCNN [4]</cell><cell>CaffeNet</cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell></row><row><cell>InteractNet [11]</cell><cell>R-50-FPN</cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell></row><row><cell>GPNN [34]</cell><cell>DCN</cell><cell>13.11</cell><cell>9.34</cell><cell>14.23</cell></row><row><cell>iCAN [9]</cell><cell>R-50</cell><cell>14.84</cell><cell>10.45</cell><cell>16.15</cell></row><row><cell cols="2">TIN-RP T2 C D [26] R-50</cell><cell>17.22</cell><cell>13.51</cell><cell>18.32</cell></row><row><cell>HOID [40]</cell><cell>R-50-FPN</cell><cell>17.85</cell><cell>12.85</cell><cell>19.34</cell></row><row><cell>Wang et al. [41]</cell><cell>R-50-FPN</cell><cell>16.24</cell><cell>11.16</cell><cell>17.75</cell></row><row><cell>Gupta et al. [15]</cell><cell>R-152</cell><cell>17.18</cell><cell>12.17</cell><cell>18.68</cell></row><row><cell>PMFNet [39]</cell><cell>R-50-FPN</cell><cell>17.46</cell><cell>15.65</cell><cell>18.00</cell></row><row><cell>Peyre et al. [33]</cell><cell>R-50-FPN</cell><cell>19.40</cell><cell>15.40</cell><cell>20.75</cell></row><row><cell>IP-Net [42]</cell><cell>H-104</cell><cell>19.56</cell><cell>12.79</cell><cell>21.58</cell></row><row><cell>VSGNet [37]</cell><cell>R-152</cell><cell>19.80</cell><cell>16.05</cell><cell>20.91</cell></row><row><cell>ConsNet (ours)</cell><cell>R-50-FPN</cell><cell>22.15</cell><cell>17.55</cell><cell>23.52</cell></row><row><cell>Bansal et al. [1]</cell><cell>R-101</cell><cell>21.96</cell><cell>16.43</cell><cell>23.62</cell></row><row><cell>PPDM [27]</cell><cell>H-104</cell><cell>21.73</cell><cell>13.78</cell><cell>24.10</cell></row><row><cell cols="2">ConsNet-F (ours) R-50-FPN</cell><cell>25.94</cell><cell>19.35</cell><cell>27.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>HOI Detection results on HICO-DET dataset under zero-shot settings. UC, UO and UA denote unseen actionobject combination, unseen object and unseen action scenarios respectively.</figDesc><table><row><cell>Method</cell><cell>Type</cell><cell>Full</cell><cell>Seen</cell><cell>Unseen</cell></row><row><cell>Shen et al. [36]</cell><cell></cell><cell>6.26</cell><cell>-</cell><cell>5.62</cell></row><row><cell>Bansal et al. [1]</cell><cell>UC</cell><cell cols="3">12.45?0.16 12.74?0.34 11.31?1.03</cell></row><row><cell>ConsNet (ours)</cell><cell></cell><cell cols="3">19.81?0.32 20.51?0.62 16.99?1.67</cell></row><row><cell>Bansal et al. [1] ConsNet (ours)</cell><cell>UO</cell><cell>13.84 20.71</cell><cell>14.36 20.99</cell><cell>11.22 19.27</cell></row><row><cell>ConsNet (ours)</cell><cell>UA</cell><cell>19.04</cell><cell>20.02</cell><cell>14.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results on HICO-DET dataset under fully-supervised settings. ? means predicting HOI labels using visual embedding network directly.</figDesc><table><row><cell cols="3">Type Embedder Depth</cell><cell>Full</cell><cell cols="2">Rare Non-Rare</cell></row><row><cell>?</cell><cell>-</cell><cell>-</cell><cell>18.90</cell><cell>10.57</cell><cell>21.40</cell></row><row><cell>MLP</cell><cell>ELMo</cell><cell>3</cell><cell>19.01</cell><cell>11.82</cell><cell>21.15</cell></row><row><cell>SGC</cell><cell>ELMo</cell><cell>3</cell><cell>19.63</cell><cell>14.85</cell><cell>21.05</cell></row><row><cell>GCN</cell><cell>ELMo</cell><cell>3</cell><cell>20.15</cell><cell>15.12</cell><cell>21.66</cell></row><row><cell>SAGE</cell><cell>ELMo</cell><cell>3</cell><cell>20.07</cell><cell>15.05</cell><cell>21.58</cell></row><row><cell>GAT</cell><cell>ELMo</cell><cell>2</cell><cell>21.16</cell><cell>16.82</cell><cell>22.46</cell></row><row><cell>GAT</cell><cell>ELMo</cell><cell>3</cell><cell>22.15</cell><cell>17.55</cell><cell>23.52</cell></row><row><cell>GAT</cell><cell>ELMo</cell><cell>4</cell><cell>21.12</cell><cell>16.35</cell><cell>22.54</cell></row><row><cell>GAT</cell><cell>Word2Vec</cell><cell>3</cell><cell>20.59</cell><cell>15.94</cell><cell>21.98</cell></row><row><cell>GAT</cell><cell>GloVe</cell><cell>3</cell><cell>20.63</cell><cell>15.66</cell><cell>22.12</cell></row><row><cell>GAT</cell><cell>FastText</cell><cell>3</cell><cell>20.58</cell><cell>15.68</cell><cell>22.04</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting Human-Object Interactions via Functional Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesized Classifiers for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3476" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to Detect Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HICO: A Benchmark for Recognizing Human-Object Interactions in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MMDetection: Open MMLab Detection Toolbox and Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<editor>Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NEIL: Extracting Visual Knowledge from Web Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-Scale Object Classification Using Label Relation Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for Quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual Semantic Role Labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">No-Frills Human-Object Interaction Detection: Factorization, Layout Encodings, and Training Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9677" to="9685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting Visual Relationships Using Box Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepGCNs: Can GCNs Go As Deep As CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene Graph Generation From Objects, Phrases and Region Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1261" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PPDM: Parallel Point Detection and Matching for Real-Time Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting Unseen Visual Relations Using Analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1981" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Human-Object Interactions by Graph Parsing Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scaling Human-Object Interaction Recognition Through Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S M</forename><surname>Oytun Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13617" to="13626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pose-Aware Multi-Level Feature Network for Human Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discovering Human Interactions With Novel Objects via Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11652" to="11661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Contextual Attention for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5694" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Human-Object Interaction Detection Using Interaction Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey of Zero-Shot Learning: Settings, Methods, and Applications. ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
