<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneous Semantic Segmentation and Outlier Detection in Presence of Domain Shift</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kre?o</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simultaneous Semantic Segmentation and Outlier Detection in Presence of Domain Shift</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent success on realistic road driving datasets has increased interest in exploring robust performance in real-world applications. One of the major unsolved problems is to identify image content which can not be reliably recognized with a given inference engine. We therefore study approaches to recover a dense outlier map alongside the primary task with a single forward pass, by relying on shared convolutional features. We consider semantic segmentation as the primary task and perform extensive validation on WildDash val (inliers), LSUN val (outliers), and pasted objects from Pascal VOC 2007 (outliers). We achieve the best validation performance by training to discriminate inliers from pasted ImageNet-1k content, even though ImageNet-1k contains many roaddriving pixels, and, at least nominally, fails to account for the full diversity of the visual world. The proposed two-head model performs comparably to the C-way multi-class model trained to predict uniform distribution in outliers, while outperforming several other validated approaches. We evaluate our best two models on the WildDash test dataset and set a new state of the art on the WildDash benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Early computer vision approaches focused on producing decent performance on small datasets. This often posed overwhelming difficulties, so researchers seldom quantified the prediction confidence. An important milestone was reached when generalization was achieved on realistic datasets such as Pascal VOC <ref type="bibr" target="#b12">[13]</ref>, CamVid <ref type="bibr" target="#b4">[5]</ref>, KITTI <ref type="bibr" target="#b13">[14]</ref>, and Cityscapes <ref type="bibr" target="#b8">[9]</ref>. These datasets assume closed-world evaluation <ref type="bibr" target="#b38">[39]</ref> in which the training and test subsets are sampled from the same distribution. Such setup has been able to provide a fast feedback on novel approaches due to good alignment with the machine learning paradigm. This further accelerated development and led us to the current state of research where all these datasets are mostly solved, at least in the strongly supervised setup.</p><p>Recent datasets further raise the bar by increasing the number of classes and image diversity. However, despite this increased complexity, the Vistas <ref type="bibr" target="#b35">[36]</ref> dataset is still an insufficient proxy for real-life operation even in a very restricted scenario such as road driving. New classes like bike racks and ground animals were added, however many important classes from non-typical or worst-case images are still absent. These classes include persons in non-standard poses, crashed This work has been partially supported by Croatian Science Foundation. arXiv:1908.01098v1 [cs.CV] 3 Aug 2019 vehicles, rubble, fallen trees etc. Additionally, real-life images may be affected by particular image acquisition faults including hardware defects, covered lens etc. This suggests that foreseeing every possible situation may be an elusive goal, and that our algorithms should be designed to recognize image regions which are foreign to the training distribution.</p><p>The described deficiencies emphasize the need for a more robust approach to dataset design. First, an ideal dataset should identify and target a set of explicit hazards for the particular domain <ref type="bibr" target="#b46">[47]</ref>. Second (and more important), an ideal dataset should endorse open-set recognition paradigm <ref type="bibr" target="#b38">[39]</ref> in order to promote detection of unforeseen hazards. Consequently, the validation (val) and test subsets should contain various degrees of domain shift with respect to the training distribution. This should include moderate domain shift factors (e.g. adverse weather, exotic locations), exceptional situations (e.g. accidents, poor visibility, defects) and outright outliers (objects and entire images from other domains). We argue that the WildDash dataset <ref type="bibr" target="#b46">[47]</ref> represents a step in the right direction, although further development would be welcome, especially in the direction of enlarging the negative part of the test dataset <ref type="bibr" target="#b2">[3]</ref>. Models trained for open-set evaluation can not be required to predict an exact visual class in outlier pixels. Instead, it should suffice that the outliers are recognized, as illustrated in <ref type="figure">Fig. 1</ref> on an image from the WildDash dataset. <ref type="figure">Fig. 1</ref>. The proposed approach for simultaneous semantic segmentation and outlier detection. Our multi-task model predicts i) a dense outlier map, and ii) a semantic map with respect to the 19 Cityscapes classes. The two maps are merged to obtain the final outlier-aware semantic predictions. Our model recognizes outlier pixels (white) on two objects which are foreign to Cityscapes: the ego-vehicle and the yellow forklift. This paper addresses simultaneous semantic segmentation and open-set outlier detection. We train our models on inlier images from two road-driving datasets: Cityscapes <ref type="bibr" target="#b8">[9]</ref> and Vistas <ref type="bibr" target="#b35">[36]</ref>. We consider several outlier detection approaches from the literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21]</ref> and validate their performance on Wild-Dash val (inliers), LSUN val <ref type="bibr" target="#b44">[45]</ref> (outliers), and pasted objects from Pascal VOC 2007 (outliers). Our main hypotheses are i) that training with noisy negatives from a very large and diverse dataset such as ImageNet-1k <ref type="bibr" target="#b9">[10]</ref> can improve outlier detection, and ii) that discriminative outlier detection and semantic segmentation can share features without significant deterioration of either task. We confirm both hypotheses by re-training our best models on WildDash val, Vistas and ImageNet-1k, and evaluating performance on the WildDash benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous approaches to outlier detection in image data are very diverse. These approaches are based on analyzing prediction uncertainty, evaluating generative models, or exploiting a broad secondary dataset which contains both outliers and inliers. Our approach is also related to multi-task models and previous work which explores the dataset quality and dataset bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Estimating Uncertainty (or Confidence) of the Predictions</head><p>Prediction confidence can be expressed as the probability of the winning class or max-softmax for short <ref type="bibr" target="#b19">[20]</ref>. This is useful in image-wide prediction of outliers, although max-softmax must be calibrated <ref type="bibr" target="#b15">[16]</ref> before being interpreted as P (inlier|x). The ODIN approach <ref type="bibr" target="#b31">[32]</ref> improves on <ref type="bibr" target="#b19">[20]</ref> by pre-processing input images with a well-tempered perturbation aimed at increasing the max-softmax activation. These approaches are handy since they require no additional training.</p><p>Some approaches model the uncertainty with a separate head which learns either prediction uncertainty <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref> or confidence <ref type="bibr" target="#b10">[11]</ref>. Such training is able to recognize examples which are hard to classify due to insufficient or inconsistent labels, but is unable to deal with real outliers.</p><p>A principled information-theoretic approach expresses the prediction uncertainty as mutual information between the posterior parameter distribution and the particular prediction <ref type="bibr" target="#b40">[41]</ref>. In practice, the required expectations are estimated with Monte Carlo (MC) dropout <ref type="bibr" target="#b23">[24]</ref>. Better results have been achieved with explicit ensembles of independently trained models <ref type="bibr" target="#b28">[29]</ref>. However, both approaches require many forward passes and thus preclude real-time operation.</p><p>Prediction uncertainty can also be expressed by evaluating per-class generative models of latent features <ref type="bibr" target="#b30">[31]</ref>. However, this idea is not easily adaptable for dense prediction in which latent features typically correspond to many classes due to subsampling and dense labelling. Another approach would be to fit a generative model to the training dataset and to evaluate the likelihood of a given sample. Unfortunately, this is very hard to achieve with image data <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training with Negative Data</head><p>Our approach is most related to three recent approaches which train outlier detection by exploiting a diverse negative dataset <ref type="bibr" target="#b41">[42]</ref>. The approach called outlier exposure (OE) <ref type="bibr" target="#b20">[21]</ref> processes the negative data by optimizing cross entropy between the predictions and the uniform distribution. Outlier detection has also been formulated as binary classification <ref type="bibr" target="#b1">[2]</ref> trained to differentiate inliers from the negative dataset. A related approach <ref type="bibr" target="#b42">[43]</ref> partitions the training data into K folds and trains an ensemble of K leave-one-fold-out classifiers. However, this requires K forward passes. while data partitioning may not be straight-forward.</p><p>Negative training samples can also be produced by a GAN generator <ref type="bibr" target="#b14">[15]</ref>. Unfortunately, existing works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref> have been designed for image-wide prediction in small images. Their adaptation to dense prediction on Cityscapes resolution would not be straight-forward <ref type="bibr" target="#b3">[4]</ref>.</p><p>Soundness of training with negative data has been challenged by <ref type="bibr" target="#b39">[40]</ref> who report under-average results for this approach. However, their experiments average results over all negative datasets (including MNIST), while we advocate for a very diverse negative dataset such as ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-task Training and Dataset Design</head><p>Multi-task models attach several prediction heads to shared features <ref type="bibr" target="#b6">[7]</ref>. Each prediction head has a distinct loss. The total loss is usually expressed as a weighted sum <ref type="bibr" target="#b36">[37]</ref> and optimized in an end-to-end fashion. Feature sharing brings important advantages such as cross-task enrichment of training data <ref type="bibr" target="#b0">[1]</ref> and faster evaluation. Examples of successful multi-task models include combining depth, surface normals and semantic segmentation <ref type="bibr" target="#b11">[12]</ref>, as well as combining classification, bounding box prediction and per-class instance-level segmentation <ref type="bibr" target="#b16">[17]</ref>. A map of task compatibility with respect to knowledge transfer <ref type="bibr" target="#b45">[46]</ref> suggests that many tasks are suitable for multi-task training.</p><p>Dataset quality is as a very important issue in computer vision research. Diverse negative datasets have been used to reduce false positives in several computer vision tasks for a very long time <ref type="bibr" target="#b41">[42]</ref>. A methodology for analyzing the quality of road-driving datasets has been proposed in <ref type="bibr">[48]</ref>. The WildDash dataset <ref type="bibr" target="#b46">[47]</ref> proposes a very diverse validation dataset and the first semantic segmentation benchmark with open-set evaluation <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simultaneous Segmentation and Outlier Detection</head><p>Our method combines two distinct tasks: outlier detection and semantic segmentation, as shown in <ref type="figure">Fig. 1</ref>. We prefer to rely on shared features in order to promote fast inference and synergy between tasks <ref type="bibr" target="#b0">[1]</ref>. We assume that a large, diverse and noisy negative dataset is available for training purposes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense Feature Extractor</head><p>Our models are based on a dense feature extractor with lateral connections <ref type="bibr" target="#b25">[26]</ref>. The processing starts with a DenseNet <ref type="bibr" target="#b21">[22]</ref> or ResNet <ref type="bibr" target="#b18">[19]</ref> backbone, proceeds with spatial pyramid pooling (SPP) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">49]</ref> and concludes with ladder-style upsampling <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>. The upsampling path consists of three upsampling blocks (U1-U3) which blend low resolution features from the previous upsampling stage with high resolution features from the backbone. We speed-up and regularize the learning with three auxiliary classification losses (cf. <ref type="figure">Fig. 2</ref>). These losses have soft targets corresponding to ground truth distribution across the corresponding window at full resolution <ref type="bibr" target="#b26">[27]</ref>. <ref type="figure">Fig. 2</ref>. The proposed two-head model: the classification head recovers semantic segmentation while the outlier detection head identifies pixels where semantic segmentation may be wrong. The output is produced by combining these two dense prediction maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dense Outlier Detection</head><p>There are four distinct approaches to formulate simultaneous semantic segmentation and dense outlier detection over shared features. The C-way multi-class approach attaches the standard classification head to the dense feature extractor (C denotes the number of inlier classes). The inlier probability is formulated as max-softmax. If a negative set is available, this approach can be trained to emit low max-softmax in outliers by supplying a modulated cross entropy loss term towards uniform distribution <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref>. The modulation factor ? KL is a hyperparameter. Unfortunately, training on outliers may compromise classification accuracy and generate false positive outliers at semantic borders.</p><p>The C-way multi-label approach has C sigmoid heads. The final prediction is the class with maximum probability max-?, whereas the inlier probability is formulated as max-?. Unfortunately, this formulation fails to address the competition between classes, which again compromises classification accuracy.</p><p>The C+1-way multi-class approach includes outliers as the C+1-th class, whereas the inlier probability is a 2-way softmax between the max-logit over inlier classes and the outlier logit. To account for class disbalance we modulate the loss due to outliers with ? C+1 . Nevertheless, this loss affects inlier classification weights, which may be harmful when the negatives are noisy (as in our case).</p><p>Finally, the two-head approach complements the C-way classification head with a head which directly predicts the outlier probability as illustrated in <ref type="figure">Fig. 2</ref>. The classification head is trained on inliers while the outlier detection head is trained both on inliers and outliers. The outlier detection head uses the standard cross entropy loss modulated with hyper-parameter ? TH . We combine the resulting prediction maps to obtain semantic segmentation into C+1 classes. The outlier detection head overrides the classification head whenever the outlier probability is greater than a threshold. Thus, the classification head is unaffected by the negative data, which provides hope to preserve the baseline semantic segmentation accuracy even when training on extremely large negative datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Resistance to noisy outlier labels and sensitivity to negative objects in positive context</head><p>Training outlier detection on a diverse negative dataset has to confront noise in negative training data. For example, our negative dataset, ImageNet-1k, contains several classes (e.g. cab, streetcar) which are part of the Cityscapes ontology. Additionally, several stuff classes from Cityscapes (e.g. building, terrain) occur in ImageNet-1k backgrounds. Nevertheless, these pixels are vastly outnumbered by true inliers. This is especially the case when the training only considers the bounding box of the object which defines the ImageNet-1k class. We address this issue by training our models on mixed batches with approximately equal share of inlier and negative images. Thus, we perform many inlier epochs during one negative epoch, since our negative training dataset is much larger than the inlier ones. The proposed batch formation procedure prevents occasional inliers from negative images to significantly affect the training. Additionally, it also favours stable development of batchnorm statistics. Hence, the proposed training approach stands a fair chance to succeed.</p><p>We promote detection of outlier objects in inlier context by pasting negative content into inlier training images. We first resize the negative image to 5% of the inlier image, and then paste it at random. We perform this before the cropping, so some crops may contain only inlier pixels. Unlike <ref type="bibr" target="#b2">[3]</ref>, we do not use the Cityscapes ignore class since it contains many inliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We train most models on Vistas inliers <ref type="bibr" target="#b35">[36]</ref> by mapping original labels to Cityscapes <ref type="bibr" target="#b8">[9]</ref> classes. In some experiments we also use Cityscapes inliers to improve results and explore influence of the domain shift. We train all applicable models on outliers from two variants of ImageNet-1k: the full dataset (ImageNet-1k-full) and the subset in which bounding box annotations are available (ImageNet-1kbb). In the latter case we use only the bounding box for training on negative images (the remaining pixels are ignored) and pasting into positive images.</p><p>We validate semantic segmentation by measuring mIoU on WildDash val separately from outlier detection. We validate outlier detection by measuring pixel level average precision (AP) in two different setups: i) entire images are either negative or positive, and ii) appearance of negative objects in positive context. The former setup consists of many assays across WildDash val and random subsets of LSUN images. The LSUN subsets are dimensioned so that the numbers of pixels in LSUN and WildDash val are approximately equal. Our experiments report mean and standard deviation of the detection AP across 50 assays. The latter setup involves WildDash val images with pasted Pascal animals. We select animals which take up at least 1% of the WildDash resolution, and paste them at random in each WildDash image.</p><p>We normalize all images with ImageNet mean and variance, and resize them so that the shorter side is 512 pixels. We form training batches with random 512?512 crops which we jitter with horizontal flipping. We set the auxiliary loss weight to 0.4 and the classifier loss weight to 0.6. We set ? KL =0.2, ? C+1 =0.05, and ? TH =0.2. We use the standard Adam optimizer and divide the learning rate of pretrained parameters by 4. All our models are trained throughout 75 Vistas epochs, which corresponds to 2 epochs of ImageNet-1k-full, or 5 epochs of ImageNet-1k-bb. We detect outliers by thresholding inlier probability at p IP = 0.5. Our models produce a dense index map for C Cityscapes classes and 1 void class. We obtain predictions at the benchmark resolution by bilinear upsampling.</p><p>We perform ODIN inference as follows. First, we perform the forward pass and the backward pass with respect to the max-softmax activation (we use temperature T=10). Then we determine the max-softmax gradient with respect to pixels. We determine the perturbation by multiplying the sign of the gradient with ?=0.001. Finally, we perturb the normalized input image, perform another forward pass and detect outliers according to the max-softmax criterion. <ref type="table" target="#tab_0">Table 1</ref> presents our results on the WildDash semantic segmentation benchmark, and compares them to other submissions with accompanying publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on the WildDash Benchmark</head><p>Our two models use the same backbone (DenseNet-169 <ref type="bibr" target="#b21">[22]</ref>) and different outlier detectors. The LDN OE model has a single C-way multi-class head. The LDN BIN model has two heads as shown in <ref type="figure">Figure 2</ref>. Both models have been trained on Vistas train, Cityscapes train, and WildDash val (inliers), as well as on ImageNet-1k-bb with pasting (outliers). Our models significantly outperform all previous submissions on negative images, while also achieving the highest meta average mIoU (the principal benchmark metric) and the highest mIoU for classic images. We achieve the second-best iIoU score for classic images, which indicates underperformance on small objects. This is likely due to the fact that we train and evaluate our models on half resolution images. <ref type="table" target="#tab_9">Table 9</ref> compares various approaches for dense outlier detection. All models are based on DenseNet-169, and trained on Vistas (inliers). The first section shows the results of a C-way multi-class model trained without outliers, where outliers are detected with max-softmax <ref type="bibr" target="#b19">[20]</ref> and ODIN + max-softmax <ref type="bibr" target="#b31">[32]</ref>. We note that ODIN slightly improves the results across all experiments. The second section of the table shows the four dense outlier detection approaches (cf. Section 3) which we also train on ImageNet-1k-bb with pasting (outliers). Columns 3 and 4 clearly show that training with noisy and diverse negatives significantly improves outlier detection. However, we also note a reduction of the segmentation score as shown in the column 5. This reduction is lowest for the C-way multi-class model and the two-head model, which we analyze next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Validation of Dense Outlier Detection Approaches</head><p>The two-head model is slightly worse in discriminating WildDash val from LSUN, which indicates that it is more sensitive to domain shift between Vistas train and WildDash val. On the other hand, the two-head model achieves better inlier segmentation (0.7 pp, column 5), and much better outlier detection on Pascal animals (5 pp, column 4). A closer inspection shows that these advantages occur since the single-head C-way approach generates many false positive outlier detections at semantic borders due to lower max-softmax.</p><p>The C+1-way multi-class model performs the worst out of all models trained with noisy outliers. The sigmoid model performs well on outlier detection but underperforms on inlier segmentation. <ref type="table" target="#tab_2">Table 3</ref> explores influence of different backbones to the performance of our twohead model. We experiment with ResNets and DenseNets of varying depths. The upsampling blocks are connected with the first three DenseNet blocks, as shown in <ref type="figure">Fig. 2</ref>. In the ResNet case, the upsampling blocks are connected with the last addition at the corresponding subsampling level. We train on Vistas (inliers) and ImageNet-1k-bb with pasting (outliers). All models achieve very good outlier detection in negative images. There appears to be a trade-off between detection of outliers at negative objects and semantic segmentation accuracy. We opt for better semantic segmentation results since WildDash test does not have negative objects in positive context. We therefore use the DenseNet-169 backbone in most other experiments due to a very good overall performance. <ref type="table" target="#tab_3">Table 4</ref> explores the influence of inlier training data to the model performance. All experiments involve the two-head model based on DenseNet-169, which was trained on outliers from ImageNet-1k-bb with pasting. The results suggest that there is a very large domain shift between Cityscapes and WildDash val. Training on inliers from Cityscapes leads to very low AP scores, which indicates that many WildDash val pixels are predicted as outliers with respect to Cityscapes. This suggests that Cityscapes is not an appropriate training dataset for real-world applications. Training on inliers from Vistas leads to much better results which is likely due to greater variety with respect to camera, time of day, weather, resolution etc. The best results across the board have been achieved when both inlier datasets are used for training. <ref type="table" target="#tab_4">Table 5</ref> explores the impact of negative training data. All experiments feature the two-head model with DenseNet-169 trained on inliers from Vistas. The table shows that training with pasted negatives greatly improves outlier detection on negative objects. It is intuitively clear that a model which never sees a border between inliers and outliers during training does not stand a chance to accurately locate such borders during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Validation of Dense Feature Extractor Backbones</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of the Training Data</head><p>The table also shows that ImageNet-1k-bb significantly boosts inlier segmentation, while also improving outlier detection on negative objects. We believe that this occurs because ImageNet-1k-bb has a smaller overlap with respect to the inlier training data, due to high incidence of Cityscapes classes (e.g. vegetation, sky, road) in ImageNet backgrounds. This simplifies outlier detection due to decreased noise in the training set, and allows more capacity of the shared feature extractor to be used for the segmentation task. The table omits outlier detection in negative images, since all models achieve over 99 % AP on that task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparing the Two-Head and C-way Multi-class Models</head><p>We now compare our two models from <ref type="table" target="#tab_0">Table 1</ref> in more detail. We remind that the two models have the same feature extractor and are trained on the same data. The two-head model performs better in most classic evaluation categories as well as in the negative category, however it has a lower meta average score. <ref type="table" target="#tab_5">Table 6</ref> explores influence of WildDash hazards <ref type="bibr" target="#b46">[47]</ref> on the performance of the two models. The C-way multi-class model has a lower performance drop in most hazard categories. The difference is especially large in images with distortion and overexposure. Qualitative experiments show that this occurs since the two-head model tends to recognize pixels in images with hazards as outliers (cf. <ref type="figure" target="#fig_0">Fig. 3</ref>). <ref type="figure" target="#fig_0">Fig. 3</ref> presents a qualitative comparison of our two submissions to the Wild-Dash benchmark. Experiments in rows 1 and 2 show that the two-head model </p><formula xml:id="formula_0">ldn bin -14% -14% -22% -14% -3% -35% -3% -9% -25% -8% ldn oe -11% -13% -7% -10% -5% -24% 0% -6% -30% -7%</formula><p>performs better in classic images due to better performance on semantic borders. Furthermore, the two-head model is also better in detecting negative objects in positive context (ego-vehicle, the forklift, and the horse). Experiments in row 3 show that the two-head model tends to recognize all pixels in images with overexposure and distortion hazards as outliers. Experiments in rows 4 and 5 show that the two-head model recognizes entire negative images as outliers, while the C-way single-head model is able to recognize positive objects (the four persons) in negative images. These experiments suggest that both models are able to detect outliers at visual classes which are (at least nominally) not present in ImageNet-1k: ego-vehicle, toy-brick construction, digital noise, and text. <ref type="figure">Fig. 4</ref> illustrates space for further improvement by presenting a few failure cases on WildDash test. Each triplet contains a test image (left), the output of the two-head model (center), and the output of the model trained to predict uniform distribution in outliers (right). Rows represent inlier images (1), outlier objects in inlier context (2), inlier images with hazards (3), out-of-scope negatives (4), and abstract negatives <ref type="bibr" target="#b4">(5)</ref>. The two-head model produces more outlier detections while performing better in classic images (cf. <ref type="table" target="#tab_0">Table 1</ref>). <ref type="figure">Fig. 4</ref>. Failure cases on WD test arranged as in <ref type="figure" target="#fig_0">Fig. 3</ref>. Top: the two-head model predicts outliers at trucks. Bottom left: both models fail to accurately detect birds on the road. Bottom right: both models exchange road and sky due to position bias (late experiments show this can be improved with training on smaller crops and scale jittering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>Validation on LSUN and WildDash val suggests that detecting entire outlier images is an easy problem. A useful practical application of this result would be a module to detect whether the camera is out of order due to being covered, dirty or faulty. Validation on pasted Pascal animals suggests that detecting outliers in inlier context is somewhat harder but still within reach of modern techniques. Evaluation on WildDash test shows that our models outperform previous published approaches. We note significant improvement with respect to state of the art in classic and negative images, as well as in average mIoU score. Our models successfully detect all abstract and out-of-scope negatives <ref type="bibr" target="#b46">[47]</ref>, even though much of this content is not represented by ImageNet-1k classes.</p><p>Future work should address further development of open-set evaluation datasets such as <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b2">3]</ref>. In particular, the community would benefit from substantially larger negative test sets which should include diverse non-ImageNet-1k content, as well as outlier objects in inlier context and inlier objects in outlier context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an approach to combine semantic segmentation and dense outlier detection without significantly deteriorating either of the two tasks. We cast outlier detection as binary classification on top of a shared convolutional representation. This allows for solving both tasks with a single forward pass through a convolutional backbone. We train on inliers from standard road driving datasets (Vista, Cityscapes), and noisy outliers from a very diverse negative dataset (ImageNet-1k). The proposed training procedure tolerates inliers in negative training images and generalizes to images with mixed content (inlier background, outlier objects). We perform extensive open-set validation on WildDash val (inliers), LSUN val (outliers), and pasted Pascal objects (outliers). The results confirm suitability of the proposed training procedure. The proposed multi-head model outperforms the C-way multi-label model and the C+1-way multi-class model, while performing comparably to the C-way multi-class model trained to predict uniform distribution in outliers. We apply our two best models to WildDash test and set a new state of the art on the WildDash benchmark.  <ref type="figure" target="#fig_0">125(1-3)</ref>, 95-109 (2017) 49. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: CVPR (2017)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Supplementary material</head><p>We use this supplement to further discuss the experiments from Section 4 of the main paper. We clarify the losses used for training the models and expand on the quantitative results described in the main paper by offering qualitative analysis of model outputs. Furthermore, we offer experiments with two additional outlier detection approaches applied to dense prediction: MC-dropout and trainable confidence.</p><p>In the first section we provide a detailed overview of the training losses. The results of the additional experiments can be seen at the beginning of the second section. The rest of the second and all of the third section are dedicated to illustrating validation experiments on WD-Pascal dataset. WD-Pascal is a set of images which we create by pasting instances of animals from PASCAL VOC 2007 dataset into WildDash validation images. We show that low AP scores on WD-Pascal are strongly affected by false positive detections in WildDash images which occur due to sensitivity to domain shift.</p><p>In the fourth section we give more examples of the performance of our submissions to the WildDash benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Review of loss functions</head><p>Segmentation can be viewed as multi-class classification. To train it, we use the negative log-likelihood loss for each pixel of the output. We use the softmax activation function on the output of the network. Additionally, as a class balancing techinque, the loss for each pixel can be weighted depending on the ground truth label. In our experiments we only used class balancing during the C+1 way multi-class training by setting the ? C+1 to 0.05 to account for the fact that outlier pixels outnumber inlier pixels for each individual class. In the case of the model with trainable confidence, the prediction is adjusted by interpolating between the original predictions and the target probability:</p><formula xml:id="formula_1">P (Y ij = y ij |x) = c ij P (Y ij = y ij |x) + (1 ? c ij )y ij<label>(1)</label></formula><p>When considering segmentation as multi-label classification, we perform C binary classifications. We do this by using sigmoid activation function instead of the softmax activation function at the output of the network. In this setup, each sample is simultaneously a positive for its own class and a negative for the rest of the classes. This setup also allows us to use outliers during training. The ouliers serve as negatives for all of the C binary classifiers.</p><p>To improve the segmentation, we use auxiliary losses at 4, 8, 16 and 32 times lower resolutions. The expected output at each resolution is a distribution over all segmentation classes. We calculate this distribution across the corresponding window from the ground truth labels at full resolution. We only take into account pixels that have a valid label. Since we use soft targets at lower distributions, we use cross entropy loss between the expected distribution and the output of the network at a given resolution.</p><p>The model with trainable confidence is trained by using the confidence loss alongside the multi-class classifier loss. The confidence loss can be interpreted as the negative log-likelihood loss where the expected output is always 1, that is to say, the network is always expected to be confident. <ref type="bibr" target="#b10">[11]</ref> show that the confidence tends to converge into unity and that confidence training tends to be a strong regularizer. To prevent the former they suggest adjusting ? C so that L C tends toward a hyperparameter ? throughout training. To minimize the latter, they also suggest applying Equation 1 only on half of the batch at each iteration. We set ? to 0.15, and perform Equation 1 on every second batch.</p><p>The model with the separate outlier detection head is trained with the additional negative log-likelihood loss. This loss is calculated between the output of the second head and the ground truth labels indicating whether a pixel is an inlier or an outlier.</p><p>The C-way multi-class approach can be trained to emit uniform distribution at outlier samples. This is done by minimizing the Kullback-Leibler (KL) divergence between the uniform distribution and the output of the network. <ref type="table" target="#tab_8">Tables 7 and 8</ref> present an overview of losses used in our experiments. We use the following assumptions in our notation: x is an input image, y contains ground truth segmentation into N C classes and z contains ground truth indicating whether a pixel is an inlier (label 1) or an outlier (label 0). Some pixels can be ignored during training (e.g.the ego-vehicle). To indicate that these pixels should be ignored, they are given a label greater than the number of classes N C in the ground truth segmentation image y and label 2 in z. When training with ouliers, y can be modified in two different ways: i) in the C+1-way multi-class setup, N C is equal to C+1, the label of the outlier pixels is set to C+1, while the label of the ignored pixels needs to be greater than C+1; or ii) the label of the outlier pixel is set to any number greater N C . <ref type="table">Table 7</ref>. Losses used during training, with the following assumptions: x is an input image, y contains ground truth segmentation into NC classes and z contains ground truth indicating whether a pixel is an inlier or an outlier. Dimensions of x, y and z are H ? W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Expression multi-class classifier loss   <ref type="table" target="#tab_9">Table 9</ref> is an extension of <ref type="table" target="#tab_1">Table 2</ref> from the main paper. It provides results of outlier detection and semantic segmentation for a model with a separate confidence head <ref type="bibr" target="#b10">[11]</ref> and a semantic segmentation model trained using Monte Carlo (MC) dropout after each of the dense and upsampling layers, with epistemic uncertainty used as a criterion for outlier detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>. The MC-dropout is also used during the inference phase, meaning that the final output of the model is the mean value of 50 forward passes. Both of the models trained with dropout perform outlier detection better than the baseline model. They, however, perform worse than any of the models trained with noisy negatives. Out of all of the tested models, the model with the confidence head provides the worst outlier detection accuracy. We observe a drop in semantic segmentation accuracy on WildDash validation dataset when compared to the baseline model for all of the additional models.  <ref type="figure" target="#fig_1">Figure 5</ref> gives a qualitative insight into the results from <ref type="table" target="#tab_1">Table 2</ref> of the main paper. It explores validation performance of different outlier detector variants described in Section 3.2 of the main paper.</p><formula xml:id="formula_2">LMC = ? i,j?Gx ?y ij [[zij = 1 ? yij ? NC]] log P (Yij = yij|x), P (Yij = yij|x) = exp s ij y ij (x) c exp s ij c (x) multi-label classifier loss LML = ? i,j?Gx Nc c=1 [[yij ? NC]] [[yij = c ? zij = 0]] log 1 1 + exp s ij y ij (x) +[[yij = c ? zij = 1]] log exp s ij y ij (x) 1 + exp s ij y ij (x) auxiliary loss LAUX = ? r?R i,j?G r x [[N r ij &gt; r 2 2 ]]E y r ij [log P (Y r ij |x)],</formula><p>Row 1 contains images from WD-Pascal. Each subsequent row shows the output of a different outlier detector. Row 2 corresponds to the C-way multiclass model trained without negative samples (baseline). This model assigns high outlier probabilty at semantic borders. Consequently it detects borders of outlier patches. It is however unable to detect entire outlier objects. Since the baseline model with ODIN, the model with the confidence head and the models trained with dropout perform qualitatively similarly, we omit those results for brevity. Row 3 shows the output of the C+1-way multi-class model. This model had the lowest AP score. Qualitative analysis suggests that this model makes more false outlier predictions which in turn lowers the AP score.</p><p>Row 4 shows the C-way multi-label model and row 5 shows the C-way multiclass model. They perform similarly, though the C-way multi-label model seems to be more robust to domain shift (cf. column 4). These models are more successful at outlier detection than the baseline model, though they also assign a high outlier probability to border pixels.</p><p>Row 6 shows the output of the two-head model. It is successful at detecting outliers without falsely detecting borders as outliers. Furthermore, its detections tend to be coarser than in other models.</p><p>All models trained with ImageNet-1k-bb have a problem with large domain shift as exemplified in column 4. This indicates that it would be very hard to achieve high AP scores on WD-Pascal since images with large domain shift are perceived as equally foreign as the pasted Pascal objects. Column 3 shows that models trained using negative data classify the windshield wiper as outlier. This is interesting when compared to the model which has seen WildDash validation during training (row 3 of <ref type="figure" target="#fig_4">Figure 10</ref>). This is further discussed in Section 3. <ref type="figure">Figure 6</ref> illustrates the validation performance of the two-head model depending on the inlier training dataset, which was shown in <ref type="table" target="#tab_3">Table 4</ref>  Interestingly, the model trained on Vistas shows more resilience to unusual conditions (dark image in row 4, unusual vehicle in row 3), while the model trained on combined datasets shows more precision in normal situations (grass in row 1, road in row 2). <ref type="figure">Figure 7</ref> shows the results of the two-head model depending on the negative training dataset (cf. <ref type="table" target="#tab_4">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Influence of the Training Data</head><p>The first column shows the validation images. The second column shows the model trained using ImageNet-1k-full without pasting. It tends to make coarse predictions. Consequently it is unable to detect small outlier patches.</p><p>The third column illustrates the model trained using ImageNet-1k-full with pasting. It is better at detecting outlier samples. It is however sensitive to domain shift. This is because of the increased overlap between positive and negative  <ref type="table" target="#tab_1">Table 2</ref>. Row 1 shows Wilddash val images with pasted PASCAL VOC 2007 animals. Row 2 shows outlier probabilities obtained with the C-way multi-class model trained without negatives. Red colour indicates a high probability that a pixel is an outlier. Subsequent rows correspond to models trained with noisy negatives: the C+1-way multi-class model (row 3), the C-way multi-label model (row 4), the C-way multi-class model (row 5) and the two-head model (row 6).</p><p>images (in classes such as sky, vegetation or road which appear often in the backgrounds of ImageNet images).</p><p>The last column shows the model trained using ImageNet-1k-bb with pasting which performs the best both qualitatively and quantitatively.</p><p>A.4 Comparing the Two-Head and C-way Multi-class Models <ref type="figure" target="#fig_2">Figure 8</ref> accompanies <ref type="table" target="#tab_0">Table 1</ref> of the main paper. It presents the combined output of the two-head model (column 2) and the C-way model trained with ImageNet-1k images (column 3) on images from WildDash test set (column 1).</p><p>The first four rows show images taken in normal conditions, while the last two rows show oulier images.</p><p>The C-way model tends to classify small objects (cf. poles in image in row 3) as well as distant objects (cf. trucks in the distance in the image in the third <ref type="figure">Fig. 6</ref>. Outlier detection with two-head models trained on different inlier datasets. All models have been trained with pasted noisy negatives from ImageNet-1k-bb as presented in from <ref type="table" target="#tab_3">Table 4</ref>. Column 1 contains Wilddash images with pasted PASCAL VOC 2007 animals. Columns 2-4 show predictions of models trained on Cityscapes, Vistas, and Cityscapes and Vistas, respectively. Red colour indicates a high probability that a pixel is an outlier. row) as outliers. This model makes more false outlier detections in typical traffic scenes.</p><p>Furthermore, the two-head model performs better in negative images. Output of the C-way model on negative images contains small patches not classified as outliers. <ref type="figure" target="#fig_3">Figure 9</ref> further clarifies <ref type="table" target="#tab_5">Table 6</ref> from the main paper. It illustrates impact of WildDash hazards on the output of the submitted models.</p><p>The results show that the two-head model is more sensitive to overexposure and distortion hazards, though it succeeds when the hazard is not severe (rows 1-4).</p><p>Row 5 contains an example of occlusion. Both of the models are able to successfully segment the torso or the person behind the pole but they struggle with the lower part. Row 6 contains an example of the particles hazard (rain on the windshiled), while row 7 contains an example of the variation hazard (with tank truck being an atypical example of a truck). <ref type="figure" target="#fig_4">Figure 10</ref> expands on the failure cases shown in <ref type="figure">Figure 4</ref> of the main paper which are worth exploring in future work.</p><p>The first two rows show images with animals on roads. Animals are usually classified as pedestrians. The two-head model classifies most of the pixels of the image in row 1 as outliers. Both of the models fail to classify the animals as outliers accurately. <ref type="figure">Fig. 7</ref>. Outlier detection with two-head models trained on different negative datasets. All models have been trained by pasting negatives into inliers from Vistas as presented in <ref type="table" target="#tab_4">Table 5</ref>. Column 1 shows original Wilddash images with pasted PASCAL VOC 2007 animals. Columns 2-4 show predictions of models trained with noisy negatives from Imagnet-1k-full (without pasting), ImageNet-1k-full (with pasting), and ImageNet-1kbb (with pasting), respectively. Red colour indicates a high probability that a pixel is an outlier.</p><p>The windshield wiper in the row 3 is classified as an inlier. This is because the submitted models were trained on Wilddash val which contains examples of images with windshields wipers. Those pixels are ignored during training but they still influence the features extracted by the dense feature extractor. This view is supported by <ref type="figure" target="#fig_1">Figure 5</ref> where images in column 3 demonstrate that a model trained only on Vistas classifies windshield wipers as outliers. Handling ignored training pixels is a suitable direction for future work.   Each triplet contains a test image (left), the output of the two-head model (center), and the output of the model trained to predict uniform distribution in outliers (right). Rows 1-2 show that our current models are unable to correctly detect small outlier objects. Row 3 shows that the windshield wiper is recognized as inlier, which occurs due to its presence in WildDash val (cf. <ref type="figure" target="#fig_1">Figure 5</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative performance of our two submissions to the WildDash benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Dense outlier detection with models presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative performance on the WildDash benchmark. Each triplet contains a test image (left), the output of the two-head model (center), and the output of the C-way multi-class model trained to predict uniform distribution in outliers (right). The two-head model does not produce false positives at semantic borders (rows 1-4). Both models correctly recognize outliers in negative images (rows 5-6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative performance on WildDash test images with overexposure and distortion hazards. Each triplet contains a test image (left), the output of the two-head model (center), and the output of the C-way multi-class model trained to predict uniform distribution in outliers (right). The two-head model rejects images with severe hazards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative performance of our two submissions to the WildDash benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of the semantic segmentation models on WildDash bench</figDesc><table><row><cell></cell><cell>Meta Avg</cell><cell></cell><cell cols="2">Classic</cell><cell></cell><cell>Negative</cell></row><row><cell>Model</cell><cell>mIoU</cell><cell cols="4">mIoU iIoU mIoU iIoU</cell><cell>mIoU</cell></row><row><cell></cell><cell>cla</cell><cell>cla</cell><cell>cla</cell><cell>cat</cell><cell>cat</cell><cell>cla</cell></row><row><cell>APMoE seg ROB [25]</cell><cell>22.2</cell><cell cols="4">22.5 12.6 48.1 35.2</cell><cell>22.8</cell></row><row><cell>DRN MPC [44]</cell><cell>28.3</cell><cell cols="4">29.1 13.9 49.2 29.2</cell><cell>15.9</cell></row><row><cell>DeepLabv3+ CS [8]</cell><cell>30.6</cell><cell cols="4">34.2 24.6 49.0 38.6</cell><cell>15.7</cell></row><row><cell>LDN2 ROB [28]</cell><cell>32.1</cell><cell cols="4">34.4 30.7 56.6 47.6</cell><cell>29.9</cell></row><row><cell>MapillaryAI ROB [6]</cell><cell>38.9</cell><cell cols="4">41.3 38.0 60.5 57.6</cell><cell>25.0</cell></row><row><cell>AHiSS ROB [34]</cell><cell>39.0</cell><cell cols="4">41.0 32.2 53.9 39.3</cell><cell>43.6</cell></row><row><cell>LDN BIN (ours, two-head)</cell><cell>41.8</cell><cell cols="4">43.8 37.3 58.6 53.3</cell><cell>54.3</cell></row><row><cell>LDN OE (ours, C? multi-class)</cell><cell>42.7</cell><cell cols="4">43.3 31.9 60.7 50.3</cell><cell>52.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Validation of dense outlier detection approaches. WD denotes WildDash val.</figDesc><table><row><cell>Model</cell><cell cols="3">ImageNet AP WD-LSUN AP WD-Pascal mIoU WD</cell></row><row><cell>C? multi-class</cell><cell>55.65 ? 0.80</cell><cell>6.01</cell><cell>49.07</cell></row><row><cell>C? multi-class, ODIN</cell><cell>55.98 ? 0.77</cell><cell>6.92</cell><cell>49.77</cell></row><row><cell>C+1? multi-class</cell><cell>98.92 ? 0.06</cell><cell>33.59</cell><cell>45.60</cell></row><row><cell>C? multi-label</cell><cell>98.75 ? 0.07</cell><cell>57.31</cell><cell>42.72</cell></row><row><cell>C? multi-class</cell><cell>99.49 ? 0.04</cell><cell>41.72</cell><cell>46.69</cell></row><row><cell>two heads</cell><cell>99.25 ? 0.04</cell><cell>46.83</cell><cell>47.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Validation of backbones for the two-head model. WD denotes WildDash val.</figDesc><table><row><cell>Backbone</cell><cell cols="3">AP WD-LSUN AP WD-Pascal mIoU WD</cell></row><row><cell>DenseNet-121</cell><cell>99.05 ? 0.03</cell><cell>55.84</cell><cell>44.75</cell></row><row><cell>DenseNet-169</cell><cell>99.25 ? 0.04</cell><cell>46.83</cell><cell>47.37</cell></row><row><cell>DenseNet-201</cell><cell>98.34 ? 0.07</cell><cell>36.88</cell><cell>47.59</cell></row><row><cell>ResNet-34</cell><cell>97.19 ? 0.07</cell><cell>47.24</cell><cell>45.17</cell></row><row><cell>ResNet-50</cell><cell>99.10 ? 0.04</cell><cell>56.18</cell><cell>41.65</cell></row><row><cell>ResNet-101</cell><cell>98.96 ? 0.06</cell><cell>52.02</cell><cell>43.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Influence of the inlier training dataset to the performance of the two-head model with the DenseNet-169 backbone. WD denotes WildDash val.</figDesc><table><row><cell cols="4">Inlier training dataset AP WD-LSUN AP WD-Pascal mIoU WD</cell></row><row><cell>Cityscapes</cell><cell>66.57 ? 0.86</cell><cell>13.85</cell><cell>11.12</cell></row><row><cell>Vistas</cell><cell>99.25 ? 0.04</cell><cell>46.83</cell><cell>47.17</cell></row><row><cell>Cityscapes, Vistas</cell><cell>99.29 ? 0.03</cell><cell>53.68</cell><cell>47.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Influence of the outlier training dataset to the performance of our two-head model with the DenseNet-169 backbone. WD denotes WildDash val.</figDesc><table><row><cell cols="4">Outlier training dataset outlier pasting AP WD-Pascal mIoU WD</cell></row><row><cell>ImageNet-1k-full</cell><cell>no</cell><cell>2.94</cell><cell>43.13</cell></row><row><cell>ImageNet-1k-full</cell><cell>yes</cell><cell>45.96</cell><cell>43.68</cell></row><row><cell>ImageNet-1k-bb</cell><cell>yes</cell><cell>46.83</cell><cell>47.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Impact of hazards to performance of our WildDash submissions. The hazards are image blur, uncommon road coverage, lens distortion, large ego-hood, occlusion, overexposure, particles, dirty windscreen, underexposure, and uncommon variations. LDN BIN denotes the two-head model. LDN OE denotes the C-way multi-class model. Model Class mIoU drop across WildDash hazards [47] blur cov. dist. hood occ. over. part. screen under. var.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>48. Zendel, O., Murschitz, M., Humenberger, M., Herzner, W.: How good is my test data? introducing safety analysis for computer vision. International Journal of Computer Vision</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Total training losses</figDesc><table><row><cell>Model</cell><cell>Total loss</cell></row><row><cell>C? multi-class, C+1? multi-class</cell><cell>?MCLMC + ?AUXLAUX</cell></row><row><cell>C? multi-class with confidence head</cell><cell>?MCLMC + ?CLC + ?AUXLAUX</cell></row><row><cell>C? multi-label</cell><cell>?MLLML + ?AUXLAUX</cell></row><row><cell>C?multi-class with outliers</cell><cell>?MCLMC + ?KLLKL + ?AUXLAUX</cell></row><row><cell>two heads</cell><cell>?MCLMC + ?THLTH + ?AUXLAUX</cell></row><row><cell cols="2">A.2 Validation of Dense Oultier Detection Approaches</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Validation of dense outlier detection approaches. WD denotes WildDash val.</figDesc><table><row><cell>Model</cell><cell cols="3">ImageNet AP WD-LSUN AP WD-Pascal mIoU WD</cell></row><row><cell>C? multi-class, dropout 0.2</cell><cell>64.09 ? 0.97</cell><cell>11.87</cell><cell>49.13</cell></row><row><cell>C? multi-class, dropout 0.5</cell><cell>63.37 ? 1.13</cell><cell>13.18</cell><cell>47.25</cell></row><row><cell>confidence head</cell><cell>54.40 ? 0.80</cell><cell>4.41</cell><cell>45.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>of the paper. Column 1 shows four validation images. Column 2 presents the corresponding results of the two-head model trained on Cityscapes. This model classifies all of the WildDash pixels as outliers. This indicates that models trained on Cityscapes are very sensitive to domain shift. Column 3 shows that the two-head model trained on Vistas dataset is significantly better at outlier detection. This improvement indicates that models trained on Vistas show more resilience to domain shift. Column 4 depicts a model trained on both Vistas and Cityscapes. It performs similarly to the model trained on Vistas.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discriminative out-of-distribution detection for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno>abs/1808.07703</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<idno>abs/1904.03215</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV. pp</title>
		<imprint>
			<biblScope unit="page" from="44" to="57" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memoryoptimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<idno>abs/1712.02616</idno>
		<imprint>
			<date type="published" when="2017-12-05" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007379606734</idno>
		<ptr target="https://doi.org/10.1023/A:1007379606734" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharw?chter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">The cityscapes dataset</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>CoRR abs/1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision? In: NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pixel-wise attentional gating for parsimonious pixel labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno>arxiv 1805.01556</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV CVRSUAD 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient ladder-style densenets for semantic segmentation of large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno>abs/1905.05661</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Robust semantic segmentation with ladder-densenet models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno>abs/1806.03465</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="6402" to="6413" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A simple unified framework for detecting outof-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training of convolutional networks on multiple heterogeneous datasets for street scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<title level="m">Do deep generative models know what they don&apos;t know? In: ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Does your model know the digit 6 is not a cat? A less biased evaluation of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>outlier&quot; detectors</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Understanding measures of uncertainty for adversarial example detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno>abs/1803.08533</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995347</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2011.5995347" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Out-ofdistribution detection using an ensemble of self supervised leave-out classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">LSUN: construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wilddash -creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
