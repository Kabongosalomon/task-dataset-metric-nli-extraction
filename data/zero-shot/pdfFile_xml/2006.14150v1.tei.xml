<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
							<email>xchang14@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Guo</surname></persName>
							<email>pcguo@nwpu-aslp.org</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">ASLP@NPU</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<email>shinjiw@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Fujita</surname></persName>
							<email>yusuke.fujita.su@hitachi.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Hitachi, Ltd. Research &amp; Development Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">ASLP@NPU</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural sequence-to-sequence models are well established for applications which can be cast as mapping a single input sequence into a single output sequence. In this work, we focus on one-to-many sequence transduction problems, such as extracting multiple sequential sources from a mixture sequence. We extend the standard sequence-to-sequence model to a conditional multi-sequence model, which explicitly models the relevance between multiple output sequences with the probabilistic chain rule. Based on this extension, our model can conditionally infer output sequences one-by-one by making use of both input and previously-estimated contextual output sequences. This model additionally has a simple and efficient stop criterion for the end of the transduction, making it able to infer the variable number of output sequences. We take speech data as a primary test field to evaluate our methods since the observed speech data is often composed of multiple sources due to the nature of the superposition principle of sound waves. Experiments on several different tasks including speech separation and multi-speaker speech recognition show that our conditional multi-sequence models lead to consistent improvements over the conventional non-conditional models. * equal contribution ? corresponding author Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many machine learning tasks can be formulated as a sequence transduction problem, where a system provides an output sequence given the corresponding input sequence. Examples of such tasks include machine translation, which maps text from one language to another, automatic speech recognition (ASR), which receives a speech waveform and produces a transcription, and video captioning, which generates the descriptions of given video scenes. In recent years, the development of neural sequenceto-sequence (seq2seq) models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> with attention mechanisms has led to significant progress in such tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In reality, the observed data contains various entangled components, making the one-to-many sequence transduction for mixture signals a common problem in machine learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>. This problem often happens in audio and speech processing due to the sequential properties and superposition principle of sound waves. For example, given the overlapped speech signal, speech separation is a problem of extracting individual speech sources, and multi-speaker speech recognition is a problem of decoding transcriptions of individual speakers. This type of problem is called the cocktail party problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref>. The existing methods to tackle this common sequence-to-multi-sequence problem can be roughly divided into two categories according to the correlation strength of multiple output sequences: serial mapping and parallel mapping. Serial mapping aims to learn the mappings through  <ref type="figure">Figure 1</ref>: Sequence-to-multi-sequence mapping approaches. (a) and (b) shows the serial mapping and parallel mapping used by existing methods respectively, and (c) refers to our Conditional Chain mapping strategy. a forward pipeline, as shown in <ref type="figure">Figure 1(a)</ref>. With the serial form, the output sequence of the first seq2seq model is fed into the following seq2seq model to output another sequence. This method is quite common when the logic and relationship between different output sequences are straightforward. For example, a cross-lingual speech translation system contains two components: speech recognition and machine translation. Serial mapping first recognizes the speech into the source language text and then use another model to translate it into the target language text. However, serial mapping methods usually suffer from some drawbacks. First, many of them need to be trained separately for different components, without taking advantage of the raw input information in the latter components. And the error accumulation through the pipeline will make the system suboptimal. The other category is parallel mapping, as shown in <ref type="figure">Figure 1(b)</ref>, which simultaneously outputs multiple sequences. This method is often used when the outputs are from the same domain. Speech separation and multi-speaker ASR are typical examples following this paradigm. Similar to serial mapping, parallel mapping could not effectively model the inherent relationship that exists between different outputs, and usually assumes the number of the output sequence is fixed (e.g., the fixed number of speakers in speech separation tasks), which limits its application scenarios.</p><p>In this paper, we propose a new unified framework aiming at the sequence-to-multi-sequence (seq2Mseq) transduction task, which can address the disadvantages of both the serial mapping and parallel mapping methods. For clarity, we refer to our methods as Conditional Chain (Cond-Chain) model, combining both the serial mapping and parallel mapping with the probabilistic chain rule. Simultaneous modeling for these two methods not only makes the framework more flexible but also encourages the model to automatically learn the efficient relationship between multiple outputs.</p><p>To instantiate the idea, as shown in <ref type="figure">Figure 1</ref>(c), we assume that the input sequence O can be mapped into N different sequences s i , i ? {1, .., N }. We take sequence O as the primary input for every output sequence. Meanwhile, the outputs will be generated one-by-one with the previous output sequence as a conditional input. We consider that the multiple outputs from the same input have some relevance at the information level. By combining both the serial and parallel connection, our model learns the mapping from the input to each output sequence as well as the relationship between the output sequences.</p><p>In this paper, we introduce the general framework in Section 2, and present a specific implementation for the tasks of speech separation and recognition in Section 3. We discuss some related work in Section 4 and describe our experiments in Section 5, and finally conclude in Section 6. Our source code will be available on our webpage: https://demotoshow.github.io/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General framework</head><p>We assume that the input sequence O ? O with length of T can be mapped into N different sequences s i , i ? {1, .., N }, where the output index i represents a particular domain D i . All the output sequences form a set S = {s i | i ? {1, ..., N }}. The basic formulation of our strategy is to estimate the joint probability of multiple output sequences, i.e., p(S|O). The joint probability is factorized into the product of conditional probabilities by using the probabilistic chain rule with/without the conditional independence assumption (denoted by ?), as follows:</p><formula xml:id="formula_0">p(S|O) = ? ? ? ? ? p(s 1 |O) N i=2 p(s i |s i?1 , O, s i?2 , ..., s 1 ) serial mapping N i=1 p(s i |O , s i?1 , ..., s 1 ) parallel mapping N i=1 p(s i |O , s i?1 , ..., s 1 ) conditional chain mapping (1)</formula><p>where, we also present the formulation of serial mapping and parallel mapping for comparison. As shown in Eq. 1, the serial mapping methods adopt a fixed order and the conditional distributions are only constructed with sequence from the previous step, i.e., p(S|O) = p(s 1 |O) N i=2 p(s i |s i?1 ). As a contrast, parallel mapping simplifies the joint distribution by the conditional independence assumption, which means all the output sequences are only conditioned on the raw input, i.e., p(S|O) = p(s i |O). For our conditional chain mapping, we manage to explicitly model the inherent relevance from the data, even if it seems very independent intuitively. To achieve this, we depart from the conditional independence assumption in parallel mapping or the Markov assumption in serial mapping. Instead, with the probabilistic chain rule, our method models the joint distribution of output sequences over an input sequence O as a product of conditional distributions. We can also apply the same methodology to the non-probabilistic regression output (e.g., speech separation).</p><p>In our model, each distribution p(s i |O, s i?1 , ..., s 1 ) in Eq. 1 is represented with a conditional encoderdecoder structure. Different from the conventional one-to-one sequence transduction for learning the mapping O ? D i , additional module in our model preserves the information from previous target sequences and takes it as a condition for the following targets. This process is formulated as follows:</p><formula xml:id="formula_1">E i = Encoder i (O) ? R D E i ?T E i ,<label>(2)</label></formula><formula xml:id="formula_2">H i = CondChain(E i ,? i?1 ) ? R D H i ?T H ,<label>(3)</label></formula><formula xml:id="formula_3">s i = Decoder i (H i ) ? D Ti i ,<label>(4)</label></formula><p>where, all the D i symbols are the number of dimensions for the features, and T E i , T H i , T i represent the size of temporal dimension. In the above equations, Encoder i and Decoder i refer to the specific networks designed for learning the mapping for the reference sequence s i . Note that the Encoder i and Decoder i here may also consist of linear layers, attention mechanism or other neural networks besides the standard RNN layer, so the lengths of the hidden embeddings E i and the estimation sequence? i may vary from the input, i.e., T E i , T H i , T i may not equal the T . For the i-th output, the E i in Encoder gets a length of T E i while the? i should get the same length with the reference s i ? D Ti i , where T i is the length of the sequence s i from domain D i . Different from the conventional seq2seq model, we utilize a conditional chain (CondChain in Eq. 3) to store the information from the previous sequences and regard them as conditions. This conditional chain is analogous to the design of memory cell in the LSTM model and the key component to realize <ref type="figure">Figure 1</ref>(c). Similarly, the conditional chain in Eq. 3 does not serve a specific target domain alone, it models some unified information for multi-sequence outputs. In other words, the encoder-decoder is specialized for each target sequence, but the conditional chain is shared by all the transduction steps i.</p><p>For most situations, when the logic and relationship between different output sequences is straightforward, we could set a fixed ordering of the outputted sequence, like the cross-lingual speech translation showed in <ref type="figure">Figure 1</ref>(a). Differently, for the outputs from the same domain, i.e., D i = D j , i = j, the Encoder and Decoder for each step could be shared with the same architecture and parameters, which yields less model parameters and better efficiency for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation for speech processing</head><p>This section describes our implementation of the proposed conditional chain model by using specific multi-speaker speech separation / recognition tasks as examples. Both of them are typical examples of seq2Mseq tasks with input from mixture signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic model</head><p>Multi-speaker speech separation / recognition aims at isolating individual speaker's voices from a recording with overlapped speech. <ref type="figure">Figure 2</ref> shows the network structure under our conditional chain mapping for these two tasks. For both of them, the input sequence is from the speech domain. Let us  <ref type="figure">Figure 2</ref>: Sequence-to-multi-sequence mapping with conditional model for multi speaker speech separation or recognition. In each sub-figure, the block with same name are all shared.</p><p>first assume this to be an audio waveform O ? R T . Another common feature for these two tasks lies in that the output sequences are from the same domain (D i = D j , i = j), which means we could use a shared model at each step, i.e., Encoder i in Eq. 2 and Decoder i in Eq. 4 are respectively the same networks with different i.</p><p>For speech separation, the target sequences s i ? R T are all from the same domain as the input, i.e., D i = O, and with the same length of the input mixture. Thus, we could use an identical basic Encoder (Enc in <ref type="figure">Figure 2</ref>(a)) to extract acoustic features from the input waveform O and predicted waveform? i . As a contrast, multi-speaker speech recognition outputs a predicted token sequence s i ? V T with token vocabulary V. We introduce an additional embedding layer, Embed, to map these predicted tokens as continuous representations, which is used for conditional representation.</p><p>In speech separation, as illustrated in <ref type="figure">Figure 2</ref>(a), both the mixed audio and the predicted source will go through an Encoder (Enc) to extract some basic auditory features. For the mixture waveform O, another separator (Separator) will also be used as the function to learn some hidden representation which is suitable for separation. And both the Enc and the Separator form the process in Eq. 2. For the Fusion block, due to the same lengths from input and output, a simple concatenation operation is used to stack the feature dimension for each frame. For the CondChain in Eq. 3, we use a unidirectional LSTM. At each source step i, the Decoder (Dec) is used to map the hidden state H i into the final separated speech source. Multi-speaker ASR is also performed in a similar form, as illustrated in <ref type="figure">Figure 2</ref>(b). Note that we use connectionist temporal classification (CTC) <ref type="bibr" target="#b11">[12]</ref> as a multi-speaker ASR network, since CTC is simple but yet powerful end-to-end ASR, and also the CTC output tokens without removing blank and repetition symbols can have the same length with the auditory feature sequence. Thus, we can realize the Fusion processing with a simple concatenation operation, similarly to speech separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stop criterion</head><p>One benefit of the conventional seq2seq model is the ability to output a variable-length sequence by predicting the end of the sequence ( EOS symbol) as a stop criterion. This advantage is inherited in our model to tackle the variable numbers of multiple sequences. For example, current speech separation or recognition models are heavily depending on a fixed number of speakers <ref type="bibr" target="#b18">[19]</ref> or require extra clustering steps <ref type="bibr" target="#b12">[13]</ref>. Thanks to the introduction of the above stop criterion, we can utilize the mixture data with various numbers of speakers during training, and can be applied to the case of unknown numbers of speakers during inference.</p><p>In our implementation, when we have the total number of output sequences as N , we attach an extra sequence to reach the stop condition during training. The target of this last sequence prediction for both speech separation and recognition tasks must be the silence, and we use the silent waveform and silent symbol (an entire blank label sequence in CTC), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training strategy with teacher-forcing and ordering</head><p>Like the conventional seq2seq approach <ref type="bibr" target="#b1">[2]</ref>, we use a popular teacher-forcing <ref type="bibr" target="#b44">[45]</ref> technique by exploiting the ground-truth reference as a conditional source s i?1 . Teacher-forcing provides proper guidance and makes training more efficient, especially at the beginning of the training, when the model is not good enough to produce reasonable estimation. Considering the unordered nature of multiple sources in multi-speaker speech separation or recognition, we adopt a greedy search method to choose the appropriate permutation of the reference sequences. This method achieves good performance in practice while maintaining high efficiency. More details about teacher-forcing and reference permutation search could be found in Section B in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Speech Separation As the core part of the cocktail party problem <ref type="bibr" target="#b6">[7]</ref>, speech separation draws much attention recently <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>. The common design of this task is to disentangle overlapped speech signals from a mixture speech with a fixed number of speakers, which is a typical example of the sequence-to-multi-sequence problem. Most existing approaches in this area follow the Parallel-mapping paradigm mentioned in Section 1, trained with permutation invariant training (PIT) technique <ref type="bibr">[51]</ref>. This design should know the number of speakers in advance and could only tackle the data with the same number of speakers <ref type="bibr" target="#b33">[34]</ref>. These constraints limit their application to real scenes, while our proposed structure can provide a solution to the variable and unknown speaker number issues. This study is inspired by recurrent selective attention networks (RSAN) <ref type="bibr" target="#b17">[18]</ref>, which has been proposed to tackle the above variable number of speakers in speech separation by iteratively subtracting a source spectrogram from a residual spectrogram. Similar ideas have also been proposed in time-domain speech separation <ref type="bibr" target="#b38">[39]</ref> and speaker diarization <ref type="bibr" target="#b9">[10]</ref>. However, the RSAN is based on the strong assumption of acoustic spectral subtraction in the time-frequency domain, and its application is quite limited. On the other hand, our conditional chain model reformulates it as a general sequence to multi-sequence transduction problem based on the probabilistic chain rule, which is applicable to the other problems including multi-speaker ASR than time-frequency domain speech separation. In addition, the relevance between current estimation and the former is learned by a conditional chain network in Eq. 3, which is more flexible and even applied to time-domain speech separation, making it totally end-to-end transduction from waveform to waveforms.</p><p>Multi-speaker speech recognition Multi-speaker speech recognition <ref type="bibr" target="#b43">[44,</ref><ref type="bibr">50,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, which aims to directly recognize the texts of each individual speaker from the mixture speech, has recently become a hot topic. Similar to the speech separation task, most of the previous methods follow the parallel mapping paradigm mentioned in Section 1. These methods could only tackle the data with the fixed number of speakers and require external speaker counting modules (e.g., speaker diarization <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32]</ref>), which lose an end-to-end transduction function, unlike our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We tested the effectiveness of our framework with speech data as our primary testing ground, where the sequence mapping problem is quite common and important. To be specific, the following sections describe the performance of our conditional chain model towards multi-speaker speech separation and speech recognition tasks, compared to other baselines. Furthermore, we also evaluated a joint model of speech separation and recognition, using multiple conditions from both waveform and text domains. In the Section A of Appendix, we provide the implementation details about all our experiments, and we also extend our model to one iterative speech denoising task in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For the speech mixtures, i.e., the input O for our tasks, with different numbers of speakers, data from the Wall Street Journal (WSJ) corpus is used by us. In the two-speaker scenario, we use the common benchmark called WSJ0-2mix dataset introduced by <ref type="bibr" target="#b12">[13]</ref>. The 30 h training set and the 10 h validation set contains two-speaker mixtures generated by randomly selecting speakers and utterances from the WSJ0 training set si_tr_s, and mixing them at various signal-to-noise ratios (SNRs) uniformly chosen between 0 dB and 10 dB. The 5 h test set was similarly generated using utterances from 18 speakers   from the WSJ0 validation set si_dt_05 and evaluation set si_et_05. For three-speaker experiments, similar methods are adopted except the number of speakers is three. The WSJ0-2mix and WSJ0-3mix datasets have become the de-facto benchmarks for multi-speaker source separation, and we compare our results to alternative methods. Besides the separation task, we also instantiate our conditional chain model on multi-speaker speech recognition with the same WSJ0-2mix dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-speaker speech separation</head><p>First, we investigate the application of our conditional chain model to speech separation benchmarks. We take TasNet as the main base model in <ref type="figure">Figure 2</ref>(a), which is a simple but powerful de-factostandard method in speech separation. <ref type="table" target="#tab_3">Table 1</ref> reports the results with different training settings (the number of speakers), compared with the same base model. By following the convention of this benchmark, we use the downsampled 8 kHz WSJ0-2mix set to reduce the memory consumption of separation. We retrained the TasNet with the same settings and hyper-parameters from an open  The scale-invariant source-to-noise ratio improvement (SI-SNRi) results from <ref type="table" target="#tab_3">Table 1</ref> show that our conditional strategy achieves better performance than the base model (TasNet) under the same configuration. For the fixed number of speakers (2 or 3), our model improves on the original results, especially when there are more speakers (0.2 dB gains in WSJ0-2mix while 0.5 dB in WSJ0-3mix). Moreover, thanks to the chain strategy in our model, the WSJ0-2&amp;3 mix datasets could be concurrently taken to train the model, and the performance is better than the training with each dataset. Compared with the RSAN <ref type="bibr" target="#b17">[18]</ref> and OR-PIT <ref type="bibr" target="#b38">[39]</ref> as mentioned in Section 4, which also could handle variable number of sources, our model achieves significant performance improvements as a result of our end-to-end speech transduction in time domain. To further verify the upper limit that our model can reach with more speakers in speech separation task, we remixed two datasets, which we refer to as WSJ0-4mix and WSJ0-5mix, by simply concatenating the given mixture list from the standard WSJ0-2&amp;3mix. As we expect, without adding any additional speech sources, the performance trained with WSJ0-2to5mix gains further improvement in WSJ0-2&amp;3 mix and get reasonable SI-SNRi results in even 4 and 5 speaker mixtures.</p><p>Besides the baseline models related to our methods, we also report the results from two strong works, i.e., DPRNN <ref type="bibr" target="#b23">[24]</ref> and Voice Separation <ref type="bibr" target="#b26">[27]</ref>, which two upgrade the model architecture from the TasNet. Especially, Voice Separation <ref type="bibr" target="#b26">[27]</ref> achieves the SOTA results in speech separation. However, their methods require multiple models (or multiple submodels in <ref type="bibr" target="#b26">[27]</ref>) for each number of speakers in advance, which import additional model complexities or training procedures, and also cannot be applied to more speakers than the training setup. Even though, with the suboptimal base model (TasNet), our conditional chain model gets better results in 5 speaker mixtures. And we could clearly observe that as the number of speakers increases, our model achieves less performance degradation. We expect to realize further gains with our conditional chain model by improving the base models to DPRNN or Vocice Separation, which will be a part of our future work.</p><p>Furthermore, <ref type="table" target="#tab_4">Table 2</ref> reports the estimation counting accuracy with the trained WSJ0 2-5mix datasets.</p><p>Here, we set the threshold of the energy value per frame in the estimated speech as 3 ? 10 ?4 to judge whether to stop the iteration, resulting in the overall accuracy of 94.8%, which is significantly better than the Voice Separation <ref type="bibr" target="#b26">[27]</ref> (? 62%). It is worth mentioning that the upper bound of speaker number was set as 5 in our trained model, so there is a strong tendency to predict silence at the 6-th step (similar with the observation from RSAN <ref type="bibr" target="#b17">[18]</ref>), leading to higher accuracy for 5-speaker mixtures than the 4-speaker ones. We also visualize two examples from the WSJ0-4mix test set in <ref type="figure" target="#fig_0">Figure 3</ref>, where we observe clear spectrograms with the estimated sources from pretty tangled mixtures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi-speaker speech recognition</head><p>Second, we evaluate our proposed method on the multi-speaker speech recognition task with the same WSJ0-mix corpus. We use the Transformer <ref type="bibr" target="#b40">[41]</ref> as the basic Enc architecture block in <ref type="figure">Figure 2</ref>(b) to build speech recognition models, optimized with the CTC criterion <ref type="bibr" target="#b11">[12]</ref>. Unlike the separation experiment in Section 5.2, which used 8 kHz, this section used the sampling rate as 16 kHz, which is the default setup for ASR to achieve better performance like most other previous works. An LSTMbased word-level language model with shallow fusion is used during decoding <ref type="bibr" target="#b13">[14]</ref>. We compare our  <ref type="formula" target="#formula_2">(3)</ref>: DPCL+DNN-HMM <ref type="bibr" target="#b25">[26]</ref>, and the attention-based systems <ref type="formula" target="#formula_3">(4)</ref>: PIT-RNN <ref type="bibr" target="#b5">[6]</ref> and <ref type="formula" target="#formula_5">(5)</ref>: PIT-Transformer. Note that all the PIT based methods correspond to the parallel mapping method in <ref type="figure">Figure 1(b)</ref>, and they cannot deal with variable numbers of speakers.</p><p>We compare the effectiveness of the proposed conditional chain models with the same CTC architecture based on PIT in <ref type="table" target="#tab_5">Table 3</ref>. Our conditional-chain Transformer <ref type="formula">(7)</ref> is significantly better than the corresponding PIT based system <ref type="bibr" target="#b5">(6)</ref>. Furthermore, the proposed conditional chain model can straightforwardly utilize the mixture speech data that has a variable number of speakers. To show this benefit, we train our model using the combination of the single and multi-speaker mixture WSJ0 training data. It can be seen that the conditional chain model trained with the combination of 1, 2 and 3-speaker speech (8) achieves the best WERs, 14.9%, on the 2-speaker mixture evaluation set among all the other systems including (1) - <ref type="bibr" target="#b6">(7)</ref>. Also, the proposed method can be applied to the 3-speaker mixture evaluation set and achieves reasonable performance (37.9%) in this very challenging condition. This result is consistent with the observation in the speech separation experiment in Section 5.2. Interestingly, by analyzing the hypothesis generation process, we found that the speaker with the longest text is predicted first generally, described in Section C in the Appendix.  In previous experiments of separation and multi-speaker speech recognition tasks, we explore the effectiveness of our conditional chain model with output sequences from the same domain, as depicted in <ref type="figure">Figure 2</ref>. In contrast, in this subsection, we evaluate a combination of cross-domain conditions by using the ASR output to guide separation learning. This direction is motivated by the so-called informational masking effect in the cocktail party problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>, where the linguistic clue may exert a strong influence on the outcome of speech-to-speech perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cross-domain condition in joint separation and recognition</head><p>Specifically, as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, in each step of our conditional chain model, both the separated waveform and the CTC alignment from the ASR model are utilized as the conditions for the next step, encouraging CondChain in Eq. 3 to jointly capture the information spanning multiple sources. The baseline conditional TasNet is trained using utterance-level waveform instead of chunk-level as in Section 5.2. In addition, unlike Section 5.3, we use a pre-trained ASR based on a single speaker Transformer-based CTC ASR system trained on the WSJ training set SI284 without overlap with WSJ0-2mix test set to stabilize our joint model training. We also use the downsampled 8 kHz data for the reduction of the memory consumption in separation and non-truncation of the speech mixture for appropriate ASR evaluation. Details of framework are explained in the Appendix A.3.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. Note that the numbers listed in this experiment cannot be strictly compared with those in Sections 5.2 and 5.3 due to the above different configuration requirements between separation and recognition. When directly feeding the separated waveform from the baseline conditional TasNet to the pre-trained ASR, we get 25.1% WER. Finetuning of the TasNet with both waveform and CTC alignment conditions achieved 0.3dB improvement of SI-SNRi. The improvement shows that the semantic condition, such as CTC alignment, provides a good guide to the separation learning. Finetuning of both TasNet and ASR models with both waveform and CTC alignment conditions yields the best WER of 14.4%, while waveform-only conditioning obtains the WER of 15.3%. Note that this joint training severely degraded the SI-SNRi result, but the WER result gets a significant improvement. This intriguing phenomenon demonstrates that the separation evaluation metric (SI-SNRi) does not always give a good indication of ASR performance, as studied in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Implementation for iterative speech denoising</head><p>Former experiments on multi-speaker speech separation and recognition show the effectiveness of our conditional chain to disentangle the input mixture signals into several components. For our proposed tasks above, actually, there is a mutually exclusive relationship between the outputs. However, the seq2Mseq tasks also cover some instances that the output sequences get a positive correlation. In this section, we manage to verify the ability to model this positive correlation, besides the proposed tasks above.</p><p>In the speech domain, the problem of a positive correlation between multiple outputs is also reflected in some problems. In this section, we take the speech denoising task as an example to verify the effectiveness of our conditional chain model in the case of positive correlation between iterative steps. The iterative estimation of some signals is an effective technique in speech processing, which could be used in speech enhancement <ref type="bibr" target="#b10">[11]</ref>, i-vector estimation <ref type="bibr" target="#b24">[25]</ref>, speech separation <ref type="bibr" target="#b18">[19]</ref>. Similar to this technique for speech denoising, we implement our conditional chain model with two iteration in the chain to denoise the noisy input speech. This formulation is very similar to the iterative re-estimation of the clean signal. That is to say that our conditional chain method is trained with two identical references as objects, i.e., s 1 = s 2 = s. And the output of the second step is conditioned on the estimation from the first step, similar to the structure shown in <ref type="figure">Figure 2</ref>(a).</p><p>To evaluate this, we conduct the speech denoising task based on a recently published dataset from the DNS-Challenge 2020 <ref type="bibr" target="#b28">[29]</ref>, which consists of 60,000 no-reverberant noisy clips in training and 300 in evaluation set. Here, we compare the results with the official baseline model <ref type="bibr" target="#b46">[47]</ref> and our implemented baseline based on TasNet. And, we also report the performance of our conditional chain model with the same architecture and hyper-parameters with the base TasNet model.</p><p>From the results in <ref type="table" target="#tab_7">Table 5</ref>, we could see that, with the iterative estimation of the clean speech signal in our conditional chain model, the performance gets obvious improvement over the same base model (TasNet). And, the estimation of the second step is better than the first step. These results show that our conditional chain learns to refine the condition from former steps, which further proves that our model has good adaptability and generalization performance when learning the relationship between multiple output sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we introduced conditional chain model, a unified method to tackle a one-to-many sequence transduction problem for mixture signals. With the probabilistic chain rule, the standard sequence-to-sequence model is extended to output multiple sequences with explicit modeling of the relevance between multiple output sequences. Our experiments on speech separation and multispeaker speech recognition show that our model led to consistent improvements with negligible increase in the model size compared with the conventional non-conditional methods,</p><p>In terms of the application scope, although we verify the proposed method with two specific tasks, speech separation and recognition, this conditional chain model can be flexibly extended to other sequence-to-multi-sequence problems for mixture signals, as a general machine learning framework. Therefore, as a future work, it might be interesting to adopt this method to other sequential tasks including natural language processing and video analysis. Another exciting direction would be introducing the attention mechanism into the fusion and conditional chain part, which could flexibly capture the implicit relationship between input and output from variable domains.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Speech separation experiments</head><p>For our conditional TasNet, we used the original configure from TasNet <ref type="bibr" target="#b20">[21]</ref> with N = 256, L = 20, B = 256, H = 512, P = 3, X = 8, R = 4. More specifically, TasNet contains three parts: (1) a linear 1-D convolutional encoder that encapsulates the input mixture waveform into an adaptive 2-D front-end representation, (2) a separator that estimates a fixed number of masking matrices, and (3) a linear 1-D transposed convolutional decoder that converts the masked 2-D representations back to waveforms. We use the same encoder and decoder design as in <ref type="bibr" target="#b20">[21]</ref>, referring to the Enc and Dec in <ref type="figure">Figure 2</ref>(a) respectively. For separator, we set the channel number of the last 1 ? 1 Conv as one, making it outputs single speech? i at each step. And we move the last 1 ? 1 Conv into the Dec part. For the Fusion and LSTM, E ? R D E ?T E is concatenated with the conditional state from the previous step Enc(? i?1 ) ? R D E ?T E at the feature's dimension. Then, a single layer of LSTM (2D E ?D H ) is carried out to mapping the fused feature into D H dimension at each frame.</p><p>Also, we noticed the update of the base model could further improve the performance like the same tendency in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>. In this paper, we mainly focus on the relative performance over the original TasNet. For separation-related tasks, all the speeches are re-sampled to 8 kHz to make a fair comparison with other works.</p><p>For the training loss calculation, the negative SI-SNR metric is widely used in [48, <ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> and achieves satisfying performance. However, the SI-SNR will lead the predicted sources as a scaled signal compared with the ground-truth, making it totally mismatch in inference phase. To address this problem, we change the training loss from negative SI-SNR to negative SDR, forcing the prediction of speech signals as similar as possible with the ground-truth.</p><p>For the training strategy, we set the initial learning rate of 1 ? 10 ?3 , which is multiplying by 0.9 every 8 epochs. In practice, we find the original ground-truth signals used as condition result in faster training speed, but a decrease in generalization ability. Therefore, to make the separation more robust, we add Gaussian noise with a standard deviation of 0.25 on these ground-truth source vectors (waveforms) during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Multi-speaker speech recognition experiments</head><p>We basically use the open source package ESPnet <ref type="bibr" target="#b42">[43]</ref> for the implementation of our ASR model. In the conditional Transformer-based CTC ASR model, there is a total of 16 Transformer layers, 8 before and 8 after the conditional chain LSTM. For the baseline Transformer-based CTC with PIT, there is a total of 12 Transformer layers in the acoustic model. The configuration of each Transformer layer is as follows: the dimension of attention is d att = 256, the dimension of feed-forward is d ff = 2048, number of heads is d head = 4. Before feeding the input to the Transformers, the log mel-filterbank features are encoded by two CNN blocks. The CNN layers have a kernel size of 3 ? 3 and the number of feature maps is 64 in the first block and 128 in the second block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Multi-speaker joint speech separation and recognition</head><p>In the joint training experiments, we first pre-train two models for both separation and ASR tasks. The parameters of conditional TasNet are the same as our previous setup. For the pre-trained ASR, we train a single speaker Transformer-based CTC ASR model on the WSJ training set, which is a clean close-mic read speech corpus with about 80h. When jointly finetuning these two parts, we use the separated wave of TasNet as the input of ASR model and feedback the predicted CTC alignments of ASR into TasNet as an additional condition. <ref type="figure" target="#fig_2">Figure 4</ref> shows an overview of our joint model. Besides, we also introduce two extra teacher-forcing hyperparameters to control the optimization part of the joint model, which are ss wav and ss ctc . The parameter ss wav is the probability of feeding separated wave to ASR, while ss ctc is the probability of inputting predicted CTC alignments as the condition. When aiming to optimize TasNet with multiple conditions, we fixed the parameters of ASR and set ss wav = 0 and ss ctc = 1, which means we only feed ground-truth wave to ASR and use the generated CTC alignments to guide the separation learning. When joint training both part to improve the performance of ASR, we set ss wav = 0.5 and ss ctc = 0.3. All experiments only have access to the predicted wave and CTC alignments during inference.</p><p>B Training strategy with teacher-forcing and ordering</p><formula xml:id="formula_4">1 2 3 4 S ? 1 ? 2 ? 3 ? 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Model</head><p>Step=1</p><p>Step=2</p><p>Step=3</p><p>Step=4 STOP = [ 2 , 3 , 1 , 4 ]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Available Option</head><p>Optimal Option <ref type="figure">Figure 5</ref>: The procedure of selecting and sorting the target sequences using a greedy algorithm in our proposed methods. At each step i, the ground-truth sequence s * i is selected from all the available references. In this example, the final order ? is s 2 ? s 3 ? s 1 ? s 4 .</p><p>In Eq. 3, the neural network accepts the hidden state of the previous sequence that is estimated at the previous iteration. However, the estimation error at the previous iteration hurts the performance at the next iteration. To reduce the error, we use the teacher-forcing <ref type="bibr" target="#b44">[45]</ref> technique, which boosts the performance by exploiting ground-truth reference. During training, Eq. 3 is replaced with as follows:</p><formula xml:id="formula_5">H i = CondChain(E i , s * i?1 ),<label>(5)</label></formula><p>Here, s * i?1 is a ground-truth sequence of index i ? 1. The teach-forcing technique is commonly used in conventional seq2seq methods. However, target sequence in seq2seq is determined and has an immutable order, so the previous approaches also generate the sequence through a fixed order, either from the beginning to the end, or the reverse order. But for many seq2MSeq problems, the multiple reference sequences are unordered. There arises a problem about how to select the s * i from the S to process the next iteration, which is also how to determine the best order ? of target sequences.</p><p>One most straightforward method is to use the permutation invariant training (PIT) strategy to traverse all the permutations and select the optimal one to update the parameters. However, with the teacherforcing technique attending each step of the output, we must go through the whole feedforward process for each permutation, which takes too much computational complexity. To alleviate this problem, we examine one simple greedy search strategy. As shown in <ref type="figure">Figure 5</ref>, for each output iteration i, the optimal target index is selected by minimizing the difference (distance) with? i among a set of the remaining target set, and the selected ground-truth sequence s * i is fed into the next decoding iteration. With this greedy strategy, the repetitive computation only occurs at the calculation of distance, and there is no need to re-run the feedforward process. In addition, the total number of repetitive computation is i = N (N + 1)/2, compared with the N ! in PIT based strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis on Speech Recognition Outputs</head><p>Our method is trained in a greedy fashion, which does not address the label permutation problem from the formulation as other methods do, such as permutation invariant training (PIT) or deep clustering (DPCL), etc. We further look into the speech recognition results in terms of the order following which our model generates the hypotheses. In <ref type="table" target="#tab_9">Table 6</ref>, we show the confusion matrix of the prediction order and the text length ranking for the two-speaker scenario. We observe that the order is somehow correlated to the length of the text. In 88% of evaluation samples, the generation order is consistent with the length ranking. By considering two texts sequences that may have very close lengths or some words are much simpler than the others, we loosen the constraint for the length ranking. If we simply accept those cases where the reference text of the first hypothesis is shorter than the second, but within a range, the pattern is more obvious. For example, the result is shown in <ref type="table" target="#tab_10">Table 7</ref> when the range is 5. In 99% evaluation samples, the generation order is consistent with the length ranking. In the three-speaker scenario, we find a similar pattern: the first hypothesis is significantly longer than the other two, 98% when the range is 5. We also have the same conclusion on the Transformer-based CTC ASR system trained with PIT in two speaker scenario. It is a Parallel-mapping framework without such conditional dependency. However, we found that the output of each head is highly dependent on the lengths. In our model, 87% of output from one head is longer than that from the other. If we further consider the range of 5, the ratio becomes 99%. Perhaps this can be the heuristic information to address the label permutation problem.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of two examples (mix1&amp;mix2) with 4-speaker mixture from our WSJ0-4mix testset. For the mixture, both the waveform and spectrogram are showed. More examples and audios are available on our webpage: https://demotoshow.github.io</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Conditional Chain Model for multi-speaker joint speech separation and recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>(c) Conditional Chain Mapping</head><label></label><figDesc>.g., mixture wav ? separated sources</figDesc><table><row><cell cols="2">(a) Serial Mapping</cell><cell></cell><cell>(b) Parallel Mapping</cell><cell></cell></row><row><cell cols="2">e.g., wav ? speech recognition</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">? machine translation ? text</cell><cell></cell><cell>Output</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sequence 1</cell><cell>Sequence 1</cell></row><row><cell>Input</cell><cell>Output</cell><cell>Output</cell><cell>Input</cell><cell>Input</cell></row><row><cell>Sequence</cell><cell>Sequence 1</cell><cell>Sequence 2</cell><cell>Sequence</cell><cell>Sequence</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Output</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sequence 2</cell><cell>Sequence 2</cell></row></table><note>e</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance for speech separation on WSJ0-mix dataset, compared with the same base model and SOTA methods. The</figDesc><table><row><cell>Eval (OC) SI-SNRi</cell></row><row><cell>in WSJ0-Nmix</cell></row></table><note>* means the same base model with identical settings and hyper- parameters. The N in WSJ0-N mix means the dataset with fixed N speakers. All the "multiple architectures" based methods are trained specifically towards each specific N .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The estimation counting results on WSJ0-mix test set with variable number of speakers (2 to 5 in our experiments). The overall accuracy of the counting is 94.8%. Here we set the threshold of the energy value per frame in the estimated speech as 3 ? 10 ?4 to judge whether to stop the iteration.</figDesc><table><row><cell>Ref</cell><cell>Est</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell cols="2">6 Sum Acc (%)</cell></row><row><cell>2</cell><cell></cell><cell>2961</cell><cell>39</cell><cell>0</cell><cell>0</cell><cell>0 3000</cell><cell>98.7</cell></row><row><cell>3</cell><cell></cell><cell>12</cell><cell>2884</cell><cell>104</cell><cell>0</cell><cell>0 3000</cell><cell>96.1</cell></row><row><cell>4</cell><cell></cell><cell>0</cell><cell>42</cell><cell cols="3">2658 300 0 3000</cell><cell>88.6</cell></row><row><cell>5</cell><cell></cell><cell>0</cell><cell>0</cell><cell cols="3">125 2875 0 3000</cell><cell>95.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>WER (%) for multi-speaker speech recognition on WSJ0-N mix-16 kHz dataset. It should be noticed that the TasNet and most speech separation methods could only be trained and used in the same number of speakers, while our conditional chain model removes this limitation. In terms of the architecture of the model, we only added a single layer of LSTM compared with the base model, resulting in a negligible increase in the number of parameters.</figDesc><table><row><cell>Methods</cell><cell>System</cell><cell>Training Data</cell><cell cols="2">WER WSJ0-2mix WSJ0-3mix</cell></row><row><cell>(1) DPCL + GMM-HMM [15]</cell><cell>HMM</cell><cell>WSJ0-2mix</cell><cell>30.8%</cell><cell>-</cell></row><row><cell>(2) PIT-DNN-HMM [28]</cell><cell>HMM</cell><cell>WSJ0-2mix</cell><cell>28.2%</cell><cell>-</cell></row><row><cell>(3) DPCL + DNN-HMM [26]</cell><cell>HMM</cell><cell>WSJ0-2mix</cell><cell>16.5%</cell><cell>-</cell></row><row><cell>(4) PIT-RNN [6]</cell><cell>Attention-based</cell><cell>WSJ0-2mix</cell><cell>25.4%</cell><cell>-</cell></row><row><cell>(5) PIT-Transformer</cell><cell>Attention-based</cell><cell>WSJ0-2mix</cell><cell>17.7%</cell><cell>-</cell></row><row><cell>(6) PIT-Transformer</cell><cell>CTC</cell><cell>WSJ0-2mix</cell><cell>31.2%</cell><cell>-</cell></row><row><cell>(7) Conditional-Chain-Transformer</cell><cell>CTC</cell><cell>WSJ0-2mix</cell><cell>24.7%</cell><cell>-</cell></row><row><cell>(8) Conditional-Chain-Transformer</cell><cell>CTC</cell><cell>WSJ0-1&amp;2&amp;3mix</cell><cell>14.9%</cell><cell>37.9%</cell></row><row><cell>source implementation [48].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>SI-SNRi (dB) and WER (%) for multi-speaker joint training on WSJ0-mix-8 kHz dataset.</figDesc><table><row><cell>Methods</cell><cell>Finetune Part</cell><cell>Condition</cell><cell>Training Data</cell><cell>WSJ0-2mix SI-SNRi WER</cell></row><row><cell>Conditional TasNet + Pre-trained ASR</cell><cell>-</cell><cell>Wave</cell><cell>WSJ0-2mix</cell><cell>15.2 dB 25.1%</cell></row><row><cell>+ With Multiple Condition</cell><cell>Separation</cell><cell>Wave + CTC</cell><cell>WSJ0-2mix</cell><cell>15.5 dB 17.2%</cell></row><row><cell>+ With Joint Training</cell><cell>Separation, ASR</cell><cell>Wave</cell><cell>WSJ0-2mix</cell><cell>12.0 dB 15.3%</cell></row><row><cell>+ With Multiple Condition</cell><cell cols="2">Separation, ASR Wave + CTC</cell><cell>WSJ0-2mix</cell><cell>10.3 dB 14.4%</cell></row><row><cell cols="5">conditional chain Transformer-based CTC with the other systems including the HMM systems (1):</cell></row><row><cell cols="4">deep clustering (DPCL)+GMM-HMM [15], (2): PIT-DNN-HMM [28] , and</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The SDR performance on no-reverberant testset in DNS-challenge 2020.</figDesc><table><row><cell>Methods</cell><cell cols="2">SDR SDRi</cell></row><row><cell>Official baseline[47]</cell><cell>13.4</cell><cell>4.2</cell></row><row><cell>Our implementation for [47]</cell><cell>14.6</cell><cell>5.5</cell></row><row><cell>TasNet</cell><cell>17.3</cell><cell>8.2</cell></row><row><cell>Conditional TasNet (1st step)</cell><cell>17.8</cell><cell>8.7</cell></row><row><cell cols="2">Conditional TasNet (2nd step) 18.0</cell><cell>8.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>48] Kaituo Xu. Project webpage. https://github.com/kaituoxu/Conv-TasNet. [49] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and Aaron Courville. Describing videos by exploiting temporal structure. In Proc. ICCV, pages 4507-4515. IEEE, 2015. [50] Dong Yu, Xuankai Chang, and Yanmin Qian. Recognizing multi-talker speech with permutation invariant training. In Proc. INTERSPEECH, pages 2456-2460, 2017.</figDesc><table><row><cell>[51] Dong Yu, Morten Kolbaek, Zheng-Hua Tan, and Jesper Jensen. Permutation invariant training</cell></row><row><cell>of deep models for speaker-independent multi-talker speech separation. In Proc. ICASSP, pages</cell></row><row><cell>241-245, 2017.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Confusion matrix between the hypothesis (Hyp.) generation order and the order of reference (Ref.) text length in 2-speaker case.</figDesc><table><row><cell>Hyp.</cell><cell cols="2">Ref. long short</cell></row><row><cell cols="2">1st output</cell><cell>2627 373</cell></row><row><cell cols="2">2nd output</cell><cell>373 2627</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Confusion matrix between the hypothesis (Hyp.) generation order and the order of reference (Ref.) text length in 2-speaker case with loosing range 5.</figDesc><table><row><cell>Hyp.</cell><cell cols="3">Ref. long short</cell></row><row><cell cols="2">1st output</cell><cell>2965</cell><cell>35</cell></row><row><cell cols="2">2nd output</cell><cell>35</cell><cell>2965</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speaker diarization: a review of recent research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bozonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinne</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="356" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Auditory scene analysis: the perceptual organization of sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albert S Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Informational and energetic masking effects in the perception of two simultaneous talkers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brungart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1101" to="1109" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: a neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end monaural multispeaker asr system without pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6256" to="6260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural speaker diarization with speaker-wise chain rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Nagamatsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01796</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative and sequential kalman filterbased speech enhancement algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="373" to="385" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep clustering: discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition with word-based rnn language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Singlechannel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extended ICA removes artifacts from electroencephalographic recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzyy-Ping</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te-Won</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Makeig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Iragui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="894" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Informational masking in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steven Colburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Auditory System at the Cocktail Party</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="75" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Listening to each speaker one by one with recurrent selective hearing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5064" to="5068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hua Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hua Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time single-channel dereverberation and separation with time-domain audio separation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="342" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Korenevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Prisyach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Khokhlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariya</forename><surname>Korenevskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Timofeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mitrofanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Andrusenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Podluzhny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07272</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Menne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sklyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2638" to="2642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-channel multi-talker speech recognition with permutation invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harishchandra</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishak</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiy</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashkan</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sam T Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="793" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A purely end-to-end system for multi-speaker speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2620" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diarization is hard: some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2808" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-toend multi-speaker speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4819" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Listen, think and listen again: capturing top-down auditory attention for speaker-independent speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blind source separation-semiparametric statistical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Shun-Ichi Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speech enhancement using end-to-end speech recognition objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Aswin Shanmugam Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murali</forename><forename type="middle">Karthick</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="234" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Far-field location guided target speech extraction using end-to-end speech recognition objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Aswin Shanmugam Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7299" to="7303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recursive Speech Separation for Unknown Number of Speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudarsanam</forename><surname>Parthasaarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1348" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An overview of automatic speaker diarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas A</forename><surname>Tranter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1557" to="1565" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Espnet: end-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep neural networks for singlechannel multi-talker speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1670" to="1679" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weighted speech distortion losses for neural-network-based real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harishchandra</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="871" to="875" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
