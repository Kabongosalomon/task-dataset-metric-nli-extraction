<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Repetitive Reprediction Deep Decipher for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Hua</forename><surname>Wang</surname></persName>
							<email>wangguohua@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Repetitive Reprediction Deep Decipher for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recent semi-supervised deep learning (deep SSL) methods used a similar paradigm: use network predictions to update pseudo-labels and use pseudo-labels to update network parameters iteratively. However, they lack theoretical support and cannot explain why predictions are good candidates for pseudo-labels. In this paper, we propose a principled end-toend framework named deep decipher (D2) for SSL. Within the D2 framework, we prove that pseudo-labels are related to network predictions by an exponential link function, which gives a theoretical support for using predictions as pseudo-labels. Furthermore, we demonstrate that updating pseudo-labels by network predictions will make them uncertain. To mitigate this problem, we propose a training strategy called repetitive reprediction (R2). Finally, the proposed R2-D2 method is tested on the large-scale ImageNet dataset and outperforms state-of-the-art methods by 5 percentage points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep learning has achieved state-of-the-art results on many visual recognition tasks. However, training these models often needs large-scale datasets such as ImageNet <ref type="bibr" target="#b9">(Russakovsky et al. 2015)</ref>. Nowadays, it is easy to collect images by search engines, but image annotation is expensive and time-consuming. Semi-supervised learning (SSL) is a paradigm to learn a model with a few labeled data and massive amounts of unlabeled data. With the help of unlabeled data, the model performance may be improved.</p><p>With a supervised loss, unlabeled data can be used in training by assigning pseudo-labels to them. Many stateof-the-art methods on semi-supervised deep learning used pseudo-labels implicitly. Temporal Ensembling (Laine and Aila 2017) used the moving average of network predictions as pseudo-labels. Mean Teacher <ref type="bibr" target="#b12">(Tarvainen and Valpola 2017)</ref> and Deep <ref type="bibr">Co-training (Qiao et al. 2018</ref>) employed another network to generate pseudo-labels. However, they produced or updated pseudo-labels in ad-hoc manners. Although these methods worked well in practice, there are few theories to support them. A mystery in deep SSL arises: why can predictions work well as pseudo-labels?</p><p>In this paper, we propose an end-to-end framework called deep decipher (D2). Inspired by <ref type="bibr" target="#b14">(Yi and Wu 2019)</ref>, we treat pseudo-labels as variables and update them by backpropagation, which is also learned from data. The D2 framework specifies a well-defined optimization problem, which can be properly interpreted as a maximum likelihood estimation over two set of random variables (the network parameters and the pseudo-labels). With deep decipher, we prove that there exists an exponential relationship between pseudo-labels and network predictions, leading to a theoretical support for using network predictions as pseudo-labels. Then, we further analyze the D2 framework and prove that pseudo-labels will become flat (i.e., their entropy is high) during training and there is an equality constraint bias in it. To mitigate these problems, we propose a simple but effective strategy, repetitive reprediction (R2). The improved D2 framework is named R2-D2 and obtaines state-of-the-art results on several SSL problems.</p><p>Our contributions are as follows. ? We propose D2, a deep learning framework that deciphers the relationship between predictions and pseudo-labels. D2 updates pseudo-labels by back-propagation. To the best of our knowledge, D2 is the first deep SSL method that learns pseudo-labels from data end-to-end. ? Within D2, we prove that pseudo-labels are exponentially transformed from the predictions. Hence, it is reasonable for previous works to use network predictions as pseudolabels. Meanwhile, many SSL methods can be considered as special cases of D2 in certain aspects. ? To further boost D2's performance, we find some shortcomings of D2. In particular, we prove that pseudo-labels will become flat during the optimization. To mitigate this problem, we propose a simple but effective remedy, R2. We tested the R2-D2 method on ImageNet and it outperforms state-of-the-arts by a large margin. On small-scale datasets like CIFAR-10 (Krizhevsky and Hinton 2009), R2-D2 also produces state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>We first briefly review deep SSL methods and the related works that inspired this paper.  (Lee 2013) is an early work on training deep SSL models by pseudo-labels, which picks the class with the maximum predicted probability as pseudo-labels for unlabeled images and tested only on a samll-scale dataset <ref type="bibr">MNIST (LeCun et al. 1998). Label propagation (Zhu and</ref><ref type="bibr" target="#b14">Ghahramani 2002)</ref> can be seen as a form of pseudo-labels. Based on some metric, label propagation pushes the label information of each sample to the near samples. <ref type="bibr" target="#b13">(Weston et al. 2012</ref>) applies label propagation to deep learning models. <ref type="bibr" target="#b2">(Iscen et al. 2019</ref>) use the manifold assumption to generate pseudo-labels for unlabeled data. However, their method is complicated and relies on other SSL methods to produce state-of-the-art results.</p><p>Several recent state-of-the-art deep SSL methods can be considered as using pseudo-labels implicitly. Temporal ensembling (Laine and Aila 2017) proposes making the current prediction and the pseudo-labels consistent, where the pseudo-labels take into account the network predictions over multiple previous training epochs. Extending this idea, Mean Teacher <ref type="bibr" target="#b12">(Tarvainen and Valpola 2017</ref>) employs a secondary model, which uses the exponential moving average weights to generate pseudo-labels. Virtual Adversarial Training <ref type="bibr" target="#b5">(Miyato et al. 2018</ref>) uses network predictions as pseudo-labels, then they want the network predictions under adversarial perturbation to be consistent with pseudo-labels. Deep <ref type="bibr">Co-Training (Qiao et al. 2018</ref>) employs many networks and uses one network to generate pseudo-labels for training other networks.</p><p>We notice that they all use the network predictions as pseudo-labels but a theory explaining its rationale is missing. With our D2 framework, we demonstrate that pseudolabels will indeed be related to network predictions. That gives a theoretical support to using network predictions as pseudo-labels. Moreover, pseudo-labels of previous works were designed manually and ad-hoc, but our pseudo-labels are updated by training the end-to-end framework. Many previous SSL methods can also be considered as special cases of the D2 framework in certain aspects.</p><p>There are some previous works in other fields that inspired this work. Deep label distribution learning <ref type="bibr" target="#b0">(Gao et al. 2017)</ref> inspires us to use label distributions to encode the pseudolabels. <ref type="bibr" target="#b11">(Tanaka et al. 2018</ref>) studies the label noise problem. They find it is possible to update the noisy label to make them more precise during the training. PENCIL <ref type="bibr" target="#b14">(Yi and Wu 2019)</ref> proposes an end-to-end framework to train the network and optimize the noisy labels together. Our method is inspired by PENCIL <ref type="bibr" target="#b14">(Yi and Wu 2019)</ref>. In addition, inspired by <ref type="bibr" target="#b4">(Liu et al. 2018)</ref>, we analyze our algorithm from the gradient perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The R2-D2 Method</head><p>We define the notations first. Column vectors and matrices are denoted in bold (e.g., x, X). When</p><formula xml:id="formula_0">x ? R d , x i is the i-th element of vector x, i ? [d], where [d] := {1, 2, . . . , d}. w i denote the i-th column of matrix W ? R d?l , i ? [l]</formula><p>. And, we assume the dataset has N classes. <ref type="figure" target="#fig_0">Figure 1</ref> shows the D2 pipeline, which is inspired by <ref type="bibr" target="#b14">(Yi and Wu 2019)</ref>. Given an input image x, D2 can employ any backbone network to generate feature f ? R D . Then, the linear activation? ? R N is computed as? = W T f , where W ? R D?N are weights of the FC layer and we omit the bias term for simplicity. The softmax function is denoted as ?(y) : R N ? R N and ?(y) i = exp(yi) N j=1 exp(yj ) . Then, the predictionp is calculated asp = ?(?) , henc?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep decipher</head><formula xml:id="formula_1">p n = ?(?) n = ?(W T f ) n = exp(w T n f ) N i=1 exp(w T i f ) .<label>(1)</label></formula><p>We define? as the pseudo logit which is an unconstrained variable and can be updated by back-propagation. Then, the pseudo label is calculated asp = ?(?) and it is a probability distribution.</p><p>In the training, the D2 framework is initialized as follows. Firstly, we train the backbone network using only labeled examples, and use this trained network as the backbone network and FC in <ref type="figure" target="#fig_0">Figure 1</ref>. For labeled examples,? is initialized by Ky, in which K = 10 and y is the groundtruth label in the one-hot encoding. Note that? of labeled examples will not be updated during D2 training. For unlabeled examples, we use the trained network to predict?. That means we use the FC layer activation? as the initial value of?. The process of initializing pseudo-labels is called predicting pseudo-labels in this paper. In the testing, we use the backbone network with FC layer to make predictions and the branch of pseudo-labels is removed.</p><p>Our loss function consists of L c and L e . L c is the classification loss and defined as KL(p||p) as in <ref type="bibr" target="#b14">(Yi and Wu 2019)</ref>, which is different from the classic KL-loss KL(p||p). L c is used to make the network predictions match the pseudolabels. L e is the entropy loss, defined as ? N j=1p j log(p j ). Minimizing the entropy of the network prediction can encourage the network to peak at only one category. So our loss function is defined as</p><formula xml:id="formula_2">L = ?L c + ?L e = ? N j=1p j [log(p j ) ? log(p j )] ? ? N j=1p j log(p j ) , (2)</formula><p>where ? and ? are two hyperparameters. Although there are two hyperparameters in D2, we always set ? = 0.1 and ? = 0.03 in all our experiments.</p><p>Then, we show that we can decipher the relationship between pseudo-labels and network predictions in D2, as shown by Theorem 1. Theorem 1 Suppose D2 is trained by SGD with the loss function L = ?L c + ?L e . Letp denote the prediction by the network for one example andp n is the largest value inp. After the optimization algorithm converges, we hav?</p><formula xml:id="formula_3">p n ? exp(? L ? ) (p n ) 1? ? ? . Proof.</formula><p>First, the loss function can be rewritten by</p><formula xml:id="formula_4">L = (? ? ?) N j=1 ? W T f j log ? W T f j ? ? N j=1 ? W T f j log(p j ) .<label>(3)</label></formula><p>It is easy to see</p><formula xml:id="formula_5">?? W T f j ?w n = I(j = n)? W T f j f ? ? W T f j ? W T f n f .<label>(4)</label></formula><p>Now we can compute the gradient of L with respect to w n :</p><formula xml:id="formula_6">?L ?w n = [(? ? ?) log (p n ) ? ? log(p n ) ? L]p n f . (5)</formula><p>During training, we expect the optimization algorithm can converge and finally ?L ?wn ? 0. Because f will not be 0, we conclude that</p><formula xml:id="formula_7">[(? ? ?) log (p n ) ? ? log(p n ) ? L]p n ? 0. Because N i=1p i = 1, consider the fact thatp n is the largest value in {p 1 ,p 1 , . . . ,p N }, thenp n ? 0 at the end of training. So we have [(? ? ?) log (p n ) ? ? log(p n ) ? L] ? 0, which states thatp n ? exp(? L ? ) (p n ) 1? ? ? .</formula><p>Theorem 1 tells usp n converges to exp(? L ? ) (p n ) 1? ? ? during the optimization. And at last, we expect thatp n = exp(? L ? ) (p n ) 1? ? ? , in which n is the class predicted by the network.</p><p>In other words, we have deciphered that there is an exponential link between pseudo-labels and predictions. From</p><formula xml:id="formula_8">p n ? exp(? L ? ) (p n ) 1? ? ? , we notice thatp n is approxi- mately proportional top 1? ? ? n .</formula><p>That gives a theoretical support to use network predictions as pseudo-labels. And, it is required that 1 ? ? ? &gt; 0 to make pseudo-labels and network predictions consistent. We must set ? &gt; ?. In our experiments, if we set ? &lt; ?, the training will indeed fail miserably.</p><p>Next, we analyze how? is updated in D2. With the loss function L, the gradients of L with respect to? n is</p><formula xml:id="formula_9">?L ?? n = ??? (?) n + ?? (?) n .<label>(6)</label></formula><p>By gradient descent, the pseudo logit? is updated b?</p><formula xml:id="formula_10">y ?? ? ? ?L ?? =? ? ??? (?) + ??? (?) ,<label>(7)</label></formula><p>where ? is the learning rate for updating?. The reason we use one more hyperparameter ? rather than the overall learning rate is that ?L ?? = ??? (?) + ?? (?) is smaller than? (in part due to the sigmoid transform) and the overall learning rate is too small to update the pseudo logit. We set ? = 4000 in all our experiments.</p><p>The updating formulas in many previous works can be considered as special cases of that of D2. In Temporal Ensembling (Laine and Aila 2017), the pseudo-labelsp is a moving average of the network predictionsp during training. The updating formula is P ? ?P + (1 ? ?)p. To correct for the startup bias, thep is needed to be divided by factor (1 ? ? t ), where t is the number of epochs. So the updating formula ofp isp ? P/(1 ? ? t ). In Mean Teacher <ref type="bibr" target="#b12">(Tarvainen and Valpola 2017)</ref>, thep is the prediction of a teacher model which uses the exponential moving average weights of the student model. <ref type="bibr" target="#b11">(Tanaka et al. 2018)</ref> proposed using the running average of the network predictions to estimate the groundtruth of the noisy label. However, their updating formula were designed manually and ad-hoc. In contrast, we treat pseudo-labels as random variables like the network parameters. These variables are learned by minimizing a well-defined loss function (cf. equation 2). From a probabilistic perspective, it is well known that minimizing the KL loss is equivalent to maximum likelihood estimation, in which the backbone network's architecture defines the estimation's functional space while SGD optimizes over these random variables (both the network parameters and the pseudo-labels). We do not need to manually specify how the pseudo-labels are generated. This process is natural and principled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A toy example</head><p>Now, we use a toy example to explain how the D2 framework works intuitively. Inspired by <ref type="bibr" target="#b4">(Liu et al. 2018)</ref>, we use the LeNet (LeCun et al. 1998) as backbone structure and add two FC layers, in which the first FC layer learns a 2-D feature and the second FC layer projects the feature onto the class space. The network was trained on MNIST. Note that MNIST has 50000 images for training. We only used 1000 images as labeled images to train the network. <ref type="figure" target="#fig_1">Figure 2a</ref>  2-D feature distribution of these 1000 images. We observe that features belonging to the same class will cluster together. <ref type="figure" target="#fig_1">Figure 2b</ref> shows the feature distribution of both these 1000 labeled and other 49000 unlabeled images. Although the network did not train on the unlabeled images, features belonging to the same class are still roughly clustered. Pseudo-labels in our D2 framework are probability distributions and initialized by network predictions. As <ref type="figure" target="#fig_1">Figure 2b</ref> shows, features near the cluster center will have confident pseudo-labels and can be learned safely. However, features at the boundaries between clusters will have a pseudo-label whose corresponding distribution among different classes is flat rather than sharp. By training D2, the network will learn confident pseudo-labels first. Then it is expected that uncertain pseudo-labels will become more and more precise and confident by optimization. At last, each cluster will become more compact and the boundaries between different classes' features will become clear. <ref type="figure" target="#fig_1">Figure 2d</ref> depicts the feature distribution of all images after D2 training. Because the same class features of unlabeled images get closer, the same class features of labeled images will also get closer (cf. <ref type="figure" target="#fig_1">Figure 2c</ref>). That is how unlabeled images help the training in our D2 framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repetitive reprediction</head><p>Although D2 has worked well in practice (cf. Table 1 column a), there are still some shortcomings in it. We will discuss two major ones. To mitigate these problems and further boost the performance, we propose a simple but effective strategy, repetitive reprediction (R2), to improve the D2 framework.</p><p>First, we expect pseudo-labels can become more confident along with D2's learning process. Unfortunately, we observed that more and more pseudo-labels become flat during training. Below, we prove Theorem 2 to explain why this adverse effect happens.</p><p>Theorem 2 Suppose D2 is trained by SGD with the loss function L = ?L c + ?L e . Ifp n = exp(? L ? ) (p n ) 1? ? ? , we must havep n ?p n .</p><p>Proof. First, according to the loss function we defined, we have</p><formula xml:id="formula_11">L = ? N j=1p j [log(p j ) ? log(p j )] ? ? N j=1p j log(p j ) (8) ? ?? N j=1p j log(p j ) ? ?? N j=1p j log(p n ) (9) = ?? log(p n ) ,<label>(10)</label></formula><p>wherep n is the largest value in {p 1 ,p 2 , . . . ,p N }. Then, from</p><formula xml:id="formula_12">p n = exp ? L ? p 1? ? ? n and L ? ?? log(p n ), we hav? p n = exp ? L ? p 1? ? ? n ? exp ? log(p n ) ? p 1? ? ? n =p n .<label>(11)</label></formula><p>From Theorem 1, we getp n ? exp(? L ? ) (p n ) 1? ? ? , wher? p gets the largest value atp n . And Theorem 2 tells us if p n = exp(? L ? ) (p n ) 1? ? ? thenp n will be smaller thanp n . Becausep andp are probability distributions, ifp andp get their largest value at n,p is more flat thanp whenp n ?p n . That is, along with the training of D2, there is a tendency that pseudo-labels will be more flat than the network predictions.</p><p>Second, we find an unsolicited bias in the D2 framework. From the updating formula, we can get</p><formula xml:id="formula_13">N i=1? i ? N i=1? i ? ?? N i=1 ? (?) i + ?? N i=1 ? (?) i (12) = N i=1? i ? ?? + ?? = N i=1? i .<label>(13)</label></formula><p>That is, N i=1? i will not change after initialization. Although we define? as the variable which is not constrained, the softmax function and SGD set an equality constraint for it. On the other hand, in practice, N i=1? i become more and more concentrated. Later, we will use an ablation study to demonstrate this bias is harmful.</p><p>We propose a repetitive reprediction (R2) strategy to overcome these difficulties, which repeatedly perform repredic-tions (i.e., using the prediction? to re-initialize the pseudolabels? several times) during training D2. The benefits of R2 are two-fold. First, we want to make pseudo-labels confident. According to our analysis, the network predictions are sharper than pseudo-labels when the algorithm converges. So repredicting pseudo-labels can make them sharper. Second, N i=1? i will not change during D2 training. Reprediction can reduce the impact of this bias. Furthermore, the validation accuracy often increase during training. A repeated reprediction can make pseudo-labels more accurate than that of the last reprediction.</p><p>Apart from the repredictions, we also reduce the learning rate to boost the performance. If the D2 framework is trained by a fixed learning rate as in <ref type="bibr" target="#b14">(Yi and Wu 2019)</ref>, the loss L did not descend in experiments. Reducing the learning rate can make the loss descend. We can get some benefits from a lower loss. On one hand, L c is the KL divergence between pseudo-labels and the network predictions. Minimizing this term makes pseudo-labels as sharp as the network predictions. On the other hand, minimizing L e can decrease the entropy of network predictions. So when it comes to next reprediction, pseudo-labels will be more confident according to sharper predictions.</p><p>Finally, repredicting pseudo-labels frequently is harmful for performance. By using the R2 strategy every epoch, the network predictions and pseudo-labels are always the same and D2 cannot optimize pseudo-labels anymore. In CIFAR-10 experiments, we repredict pseudo-labels every 75 epochs and reduce the learning rate after each reprediction. Using the R2 strategy can make pseudo-labels more confident at the end of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The overall R2-D2 algorithm</head><p>Now we propose the overall R2-D2 algorithm. The training can be divided into three stages. In the first stage, we only use labeled images to train the backbone network with cross entropy loss as in common network training. In the second stage, we use the backbone network trained in the first stage to predict pseudo-labels for unlabeled images. Then we use D2 to train the network and optimize pseudo-labels together. It is expected that this stage can boost the network performance and make pseudo-labels more precise. But according to our analysis, it is not enough to train D2 by only one stage. With the R2 strategy, D2 will be repredicted and trained for several times. In the third stage, the backbone network is finetuned by all images whose labels come from the second stage. For unlabeled images, we pick the class which has the maximum value in pseudo-labels and use the cross entropy loss to train the network. And pseudo-labels are not updated anymore. For labeled images, we use their groundtruth labels.</p><p>In general, R2-D2 is a simple method. It requires only one single network (versus two in Mean Teacher and the loss function consists of two terms (versus three in Mean Teacher). The training processes in different stages are identical (share the same code), just need to change the value of two switch variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we use four datasets to evaluate our algorithm: ImageNet <ref type="bibr" target="#b9">(Russakovsky et al. 2015)</ref>, CIFAR-100 (Krizhevsky and Hinton 2009), CIFAR-10 (Krizhevsky and Hinton 2009), SVHN <ref type="bibr" target="#b6">(Netzer et al. 2011)</ref>. We first use an ablation study to investigate the impact of the R2 strategy. We then report the results on these datasets to compare with state-of-the-arts. All experiments were implemented using the PyTorch framework and run on a computer with TITAN Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Note that we trained the network using stochastic gradient descent with Nesterov momentum 0.9 in all experiments. We set ? = 0.1, ? = 0.03 and ? = 4000 on all datasets, which shows the robustness of our method to these hyperparameters. Other hyperparameters (e.g., batch size, learning rate, and weight decay) were set according to different datasets.</p><p>ImageNet is a large-scale dataset with natural color images from 1000 categories. Each category typically has 1300 images for training and 50 for evaluation. Following the prior work <ref type="bibr" target="#b7">(Qiao et al. 2018;</ref><ref type="bibr" target="#b10">Sajjadi, Javanmardi, and Tasdizen 2016;</ref><ref type="bibr" target="#b7">Pu et al. 2016;</ref><ref type="bibr" target="#b12">Tarvainen and Valpola 2017)</ref>, we uniformly choose 10% data from training images as labeled data. That means there are 128 labeled data for each category. The rest of training images are considered as unlabeled data. We test our model on the validation set. The backbone network is <ref type="bibr">ResNet-18 (He et al. 2016)</ref>.</p><p>CIFAR-100 contains 32 ? 32 natural images from 100 categories. There are 50000 training images and 10000 testing images in CIFAR-100. Following <ref type="bibr" target="#b2">(Laine and Aila 2017;</ref><ref type="bibr" target="#b7">Qiao et al. 2018;</ref><ref type="bibr" target="#b2">Iscen et al. 2019)</ref>, we use 10000 images (100 per class) as labeled data and the rest 40000 as unlabeled data. We report the error rates on the testing images. The backbone network is ConvLarge (Laine and Aila 2017).</p><p>CIFAR-10 contains 32 ? 32 natural images from 10 categories. Following (Laine and Aila 2017; <ref type="bibr" target="#b5">Miyato et al. 2018;</ref><ref type="bibr" target="#b12">Tarvainen and Valpola 2017;</ref><ref type="bibr" target="#b7">Qiao et al. 2018;</ref><ref type="bibr" target="#b8">Robert, Thome, and Cord 2018)</ref>, we use 4000 images (400 per class) from 50000 training images as labeled data and the rest images as unlabeled data. We report the error rates on the full 10000 testing images. The backbone network is Shake-Shake <ref type="bibr" target="#b1">(Gastaldi 2017)</ref>.</p><p>SVHN consists of 32?32 house number images belonging to 10 classes. The category of each image is the centermost digit. There are 73257 training images and 26032 testing images in SVHN. Following (Laine and Aila 2017; <ref type="bibr" target="#b12">Tarvainen and Valpola 2017;</ref><ref type="bibr" target="#b5">Miyato et al. 2018;</ref><ref type="bibr" target="#b7">Qiao et al. 2018)</ref>, we use 1000 images (100 per class) as labeled data and the rest 72257 training images as unlabeled data. The backbone network is ConvLarge (Laine and Aila 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation studies</head><p>Now we validate our framework by an ablation study on CIFAR-10 with the Shake-Shake backbone and 4000 labeled images. All experiments used the same data splits and ran once. And they all used the first stage to initialize D2 and the third stage to finetune the network. <ref type="table" target="#tab_1">Table 1</ref>   results and the error rates are produced by the last epoch of the third stage. Different columns denote using different strategies to train D2 in the second stage. First, without R2 (column a), the error rate of a basic D2 learning is 6.71%, which is already competitive with state-of-the-arts. Next, we repeated the second stage without reprediction or reducing learning rate (column b). That means the network is trained by the first stage, the second stage, repeat the second stage, and the third stage. This network achieved a 6.37% error rate, which demonstrates training D2 for more epochs can boost performance and the network will not overfit easily. Repeating the second stage with reprediction (column c) could make the error rate even lower, to 6.23%. But, without reducing the learning rate, L did not decrease. On the other hand, repeating the second stage and reducing the learning rate (column d) can get better results (5.94%). However, only reducing the learning rate cannot remove the impact of the equality constraint bias. At last, applying both strategies (column e) improved the results by a large margin to 5.78%. <ref type="table" target="#tab_2">Table 2</ref> presents the results with different ?. ? is 0.03 in all experiments. We find setting ? = 0.2 will achieve a better performance and a large ? may degrade the performance. <ref type="table" target="#tab_3">Table 3</ref> shows the results with different ? when setting ? = 0.1. Compared with ?, our method is robust to ?. The highest error rate is 5.83% and the lowest error rate is 5.62%. There is only a roughly 0.2% difference between them. Overall, R2-D2 is robust to these hyperparameters. And when apply R2-D2, we suggest that ? = 0.1, ? = 0.03 is a safe starting point to tune these hyperparameters. All experiments in the rest of our paper used ? = 0.1, ? = 0.03. Please note that we did not carefully tune these hyperparameters. Error rates of R2-D2 may be lower than those reported in this paper if we tune them carefully.   <ref type="table" target="#tab_4">Table 4</ref> shows the results with different L c . Note that our loss function is defined as L = ?L c + ?L e . The loss function determines how the network parameters and pseudo-labels update. That means different L c result in different updating formulas of pseudo-labels. The default L c is KL(p||p) and the updating formula is Equation 7. When set L c = KL(p||p), the gradients of L with respect to? n is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>presents the</head><formula xml:id="formula_14">?L ?? n = ?p n [logp n ? logp n ? N k=1p k (logp k ? logp k )] ,<label>(14)</label></formula><formula xml:id="formula_15">wherep = ?(?) andp = ?(?). With L c = p ?p 2 2 , the gradients of L with respect to? n is ?L ?? n = 2?p n [p n ?p n ? N k=1p k (p k ?p k )] .<label>(15)</label></formula><p>The experimental results demonstrates superior performance of R2-D2 with L c = KL(p||p). It obtains 2.28% lower error rate than KL(p||p) and 0.57% lower error rate than p ?p 2 2 .</p><p>Results on ImageNet     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on CIFAR-100</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on CIFAR-10</head><p>We evaluated the performance of R2-D2 on CIFAR-10 with 4000 labeled samples. <ref type="table" target="#tab_9">Table 7</ref> presents the results. Following <ref type="bibr" target="#b12">(Tarvainen and Valpola 2017;</ref><ref type="bibr" target="#b8">Robert, Thome, and Cord 2018)</ref>, we used the Shake-Shake network <ref type="bibr" target="#b1">(Gastaldi 2017)</ref> as the backbone network. Overall, using Shake-Shake backbone network can achieves lower error rates than using ConvLarge. Our experiment was repeated five times with different random subsets of labeled training samples. We used the test </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on SVHN</head><p>We tested R2-D2 on SVHN with 1000 labeled samples. The results are shown in <ref type="table" target="#tab_10">Table 8</ref>. Following previous works (Laine and Aila 2017; <ref type="bibr" target="#b12">Tarvainen and Valpola 2017;</ref><ref type="bibr" target="#b5">Miyato et al. 2018;</ref><ref type="bibr" target="#b7">Qiao et al. 2018)</ref>, we used the ConvLarge network as the backbone network. The result we report is average error rate of the last epoch over five random data splits. On this task, the gap between 100% supervised and many SSL methods (e.g., VAT+EntMin <ref type="bibr" target="#b5">(Miyato et al. 2018</ref>), VAdD (KL)+VAT <ref type="bibr" target="#b7">(Park et al. 2018</ref><ref type="bibr">), Deep Co-Training (Qiao et al. 2018</ref>, and R2-D2) is less than 1%. Only Deep Co-Training with 8 Views <ref type="bibr" target="#b7">(Qiao et al. 2018)</ref> and VAdD (KL)+VAT slightly outperform R2-D2. Compared with other methods (e.g., Temporal Ensembling, Mean Teacher, and VAT, R2-D2 produces a lower error rate. Note that on the large-scale ImageNet, R2-D2 significantly outperformed Deep Co-Training. VAdD have not be evaluated on ImageNet in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed R2-D2, a method for semisupervised deep learning. D2 uses label probability distributions as pseudo-labels for unlabeled images and optimizes them during training. Unlike previous SSL methods, D2 is an end-to-end framework, which is independent of the backbone network and can be trained by back-propagation. Based on D2, we give a theoretical support for using network predictions as pseudo-labels. However, pseudo-labels will become flat during training. We analyzed this problem both theoretically and experimentally, and proposed the R2 remedy for it. At last, we tested R2-D2 on different datasets. The experiments demonstrated superior performance of our proposed methods. On large-scale dataset ImageNet, R2-D2 achieved about 5% lower error rates than that of previous state-ofthe-art. In the future, we will explore the combination of unsupervised feature learning and semi-supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The pipeline of D2. Solid lines and dashed lines represent the forward and back-propagation processes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Feature distribution on MNIST. First, LeNet was trained by labeled data. (a) shows the the feature distribution of labeled images. Points with the same color belong to the same class. (b) shows the feature distribution of both labeled and unlabeled images. Then, we used LeNet as the backbone network and trained the D2 framework. After training, (c) and (d) show the feature distribution of labeled images and all images, respectively. This figure needs to be viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies when using different strategies to train our end-to-end framework.</figDesc><table><row><cell></cell><cell>a</cell><cell>b</cell><cell>c</cell><cell>d</cell><cell>e</cell></row><row><cell>The 2nd stage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Repeat the 2nd stage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reprediction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reducing LR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error rates (%)</cell><cell cols="5">6.71 6.37 6.23 5.94 5.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">: Ablation studies when using different ? to train our</cell></row><row><cell cols="3">end-to-end framework. (? = 0.03)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell cols="6">Error rates (%) 5.78 5.44 5.81 5.90 6.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies when using different ? to train our end-to-end framework. (? = 0.1)</figDesc><table><row><cell>?</cell><cell>0.01 0.02 0.03 0.04 0.05</cell></row><row><cell cols="2">Error rates (%) 5.62 5.75 5.78 5.83 5.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies when using different L c to train our end-to-end framework. (? = 0.1, ? = 0.03)</figDesc><table><row><cell>Lc</cell><cell cols="2">KL(p||p) KL(p||p)</cell><cell>p ?p 2 2</cell></row><row><cell>Error rates (%)</cell><cell>5.78</cell><cell>8.06</cell><cell>6.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows our results on ImageNet with 10% labeled</cell></row><row><cell>samples. The setup followed that in (Qiao et al. 2018). The</cell></row><row><cell>image size in training and testing is 224 ? 224. For the fair-</cell></row><row><cell>ness of comparisons, the error rate is from single model</cell></row><row><cell>without ensembling. We use the result of the last epoch. Our</cell></row><row><cell>experiment is repeated three times with different random sub-</cell></row><row><cell>sets of labeled training samples. The Top-1 error rates are</cell></row><row><cell>41.64, 41.35, and 41.65, respectively. The Top-5 error rates</cell></row><row><cell>are 19.53, 19.60, and 19.44, respectively. R2-D2 achieves sig-</cell></row><row><cell>nificantly lower error rates than Stochastic Transformations</cell></row><row><cell>(Sajjadi, Javanmardi, and Tasdizen 2016) and VAE (Pu et al.</cell></row><row><cell>2016), albeit they used the larger input size 256 ? 256. With</cell></row><row><cell>the same backbone and input size, R2-D2 obtains roughly</cell></row><row><cell>5% lower Top-1 error rate than that of DCT (Qiao et al.</cell></row><row><cell>2018) and 7.5% lower Top-1 error rate than that of Mean</cell></row><row><cell>Teacher (Tarvainen and Valpola 2017). R2-D2 outperforms</cell></row><row><cell>the previous state-of-the-arts by a large margin. The perfor-</cell></row><row><cell>mances of Mean Teacher (Tarvainen and Valpola 2017) with</cell></row><row><cell>ResNet-18 (He et al. 2016) is quoted from (Qiao et al. 2018).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>presents experimental results on CIFAR-100 with 10000 labeled samples. All methods used ConvLarge for fairness of comparisons and did not use ensembling. The error rate of R2-D2 is the average error rate of the last epoch over five random data splits. The results of 100% Supervised is quoted from (Laine and Aila 2017). Using 10000</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Error rates (%) on the validation set of ImageNet benchmark with 10% images labeled. "-" means that the original papers did not report the corresponding error rates.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Backbone</cell><cell cols="3">#Param Top-1 Top-5</cell></row><row><cell>Supervised</cell><cell>100% Supervised 10% Supervised</cell><cell>ResNet-18 ResNet-18</cell><cell>11.6M 11.6M</cell><cell cols="2">30.43 10.76 52.23 27.54</cell></row><row><cell></cell><cell>Stochastic Transformations</cell><cell>AlexNet</cell><cell>61.1M</cell><cell>-</cell><cell>39.84</cell></row><row><cell></cell><cell>VAE with 10% Supervised</cell><cell>Customized</cell><cell>30.6M</cell><cell cols="2">51.59 35.24</cell></row><row><cell>Semi-supervised</cell><cell>Mean Teacher</cell><cell>ResNet-18</cell><cell>11.6M</cell><cell cols="2">49.07 23.59</cell></row><row><cell></cell><cell cols="2">Dual-View Deep Co-Training ResNet-18</cell><cell>11.6M</cell><cell cols="2">46.50 22.73</cell></row><row><cell></cell><cell>R2-D2</cell><cell>ResNet-18</cell><cell>11.6M</cell><cell cols="2">41.55 19.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Error rates (%) on CIFAR-100 benchmark with 10000 images labeled.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Backbone</cell><cell>Error rates (%)</cell></row><row><cell>Supervised</cell><cell cols="3">100% Supervised Using 10000 labeled images only ConvLarge 38.36 ? 0.27 ConvLarge 26.42 ? 0.17</cell></row><row><cell></cell><cell>Temporal Ensembling</cell><cell cols="2">ConvLarge 38.65 ? 0.51</cell></row><row><cell></cell><cell>LP</cell><cell cols="2">ConvLarge 38.43 ? 1.88</cell></row><row><cell>Semi-supervised</cell><cell>Mean Teacher LP + Mean Teacher</cell><cell cols="2">ConvLarge 36.08 ? 0.51 ConvLarge 35.92 ? 0.47</cell></row><row><cell></cell><cell>DCT</cell><cell cols="2">ConvLarge 34.63 ? 0.14</cell></row><row><cell></cell><cell>R2-D2</cell><cell cols="2">ConvLarge 32.87 ? 0.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Error rates (%) on CIFAR-10 benchmark with 4000 images labeled.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Error rates (%)</cell></row><row><cell>100% Supervised</cell><cell>Shake-Shake</cell><cell>2.86</cell></row><row><cell cols="3">Only 4000 labeled images Shake-Shake 14.90 ? 0.28</cell></row><row><cell>Mean Teacher</cell><cell>ConvLarge</cell><cell>12.31 ? 0.28</cell></row><row><cell>Temporal Ensembling</cell><cell>ConvLarge</cell><cell>12.16 ? 0.24</cell></row><row><cell>VAT+EntMin</cell><cell>ConvLarge</cell><cell>10.55 ? 0.05</cell></row><row><cell>DCT with 8 Views</cell><cell>ConvLarge</cell><cell>8.35 ? 0.06</cell></row><row><cell>Mean Teacher</cell><cell>Shake-Shake</cell><cell>6.28 ? 0.15</cell></row><row><cell>HybridNet</cell><cell>Shake-Shake</cell><cell>6.09</cell></row><row><cell>R2-D2</cell><cell>Shake-Shake</cell><cell>5.72 ? 0.06</cell></row><row><cell cols="3">labeled images achieved 38.36% error rates in our experi-</cell></row><row><cell cols="3">ments. With unlabeled images, R2-D2 produced a 32.87%</cell></row><row><cell cols="3">error rate, which is lower than others (e.g., Temporal En-</cell></row><row><cell cols="3">sembling, LP (Iscen et al. 2019), Mean Teacher, LP + Mean</cell></row><row><cell cols="3">Teacher (Iscen et al. 2019), and DCT). The performances of</cell></row><row><cell cols="3">Mean Teacher (Tarvainen and Valpola 2017) is quoted from</cell></row><row><cell>(Iscen et al. 2019).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Error rates (%) on SVHN benchmark with 1000 images labeled. Only 1000 labeled images ConvLarge 11.27 ? 0.85 error rates of the last epoch. After the first stage, the backbone network produced the error rates 14.90%, which is our baseline using 4000 labeled samples. With the help of unlabeled images, R2-D2 obtains an error rate of 5.72%.Compared with Mean Teacher<ref type="bibr" target="#b12">(Tarvainen and Valpola 2017)</ref> and Hy-bridNet<ref type="bibr" target="#b8">(Robert, Thome, and Cord 2018)</ref>, R2-D2 achieves lower error rate and produces state-of-the-art results.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Error rates (%)</cell></row><row><cell>100% Supervised</cell><cell>ConvLarge</cell><cell>2.88 ? 0.03</cell></row><row><cell>Temporal Ensembling</cell><cell>ConvLarge</cell><cell>4.42 ? 0.16</cell></row><row><cell>VAdD (KL)</cell><cell>ConvLarge</cell><cell>4.16 ? 0.08</cell></row><row><cell>Mean Teacher</cell><cell>ConvLarge</cell><cell>3.95 ? 0.19</cell></row><row><cell>VAT+EntMin</cell><cell>ConvLarge</cell><cell>3.86 ? 0.11</cell></row><row><cell>VAdD (KL) + VAT</cell><cell>ConvLarge</cell><cell>3.55 ? 0.05</cell></row><row><cell>DCT with 8 Views</cell><cell>ConvLarge</cell><cell>3.29 ? 0.03</cell></row><row><cell>R2-D2</cell><cell>ConvLarge</cell><cell>3.64 ? 0.20</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Natural Science Foundation of China (61772256, 61921006).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Shake-shake regularization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transductive centroid projection for semisupervised large-scale recognition</title>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11209</biblScope>
			<biblScope unit="page" from="72" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Miyato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Netzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="142" to="159" />
		</imprint>
	</monogr>
	<note>The European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HybridNet: Classification and reconstruction cooperation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thome</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="158" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Russakovsky et al. 2015</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javanmardi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade: Second Edition</title>
		<editor>Montavon, G.</editor>
		<editor>Orr, G. B.</editor>
		<editor>and M?ller, K.-R.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Learning from labeled and unlabeled data with label propagation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
