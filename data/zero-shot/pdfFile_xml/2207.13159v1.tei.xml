<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TINYCD: A (NOT SO) DEEP LEARNING MODEL FOR CHANGE DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Codegoni</surname></persName>
							<email>andrea.codegoni01@ateneopv.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Matematica &quot;F. Casorati&quot;</orgName>
								<orgName type="institution">University of Pavia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Lombardi</surname></persName>
							<email>gabriele.lombardi@argo.vision</email>
							<affiliation key="aff1">
								<orgName type="institution">ARGO Vision Milano</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Ferrari</surname></persName>
							<email>alessandro.ferrari@argo.vision</email>
							<affiliation key="aff1">
								<orgName type="institution">ARGO Vision Milano</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TINYCD: A (NOT SO) DEEP LEARNING MODEL FOR CHANGE DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Change Detection (CD) ? Remote Sensing (RS) ? Convolutional Neural Network (CNN)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of change detection (CD) is to detect changes occurred in the same area by comparing two images of that place taken at different times. The challenging part of the CD is to keep track of the changes the user wants to highlight, such as new buildings, and to ignore changes due to external factors such as environmental, lighting condition, fog or seasonal changes. Recent developments in the field of deep learning enabled researchers to achieve outstanding performance in this area. In particular, different mechanisms of space-time attention allowed to exploit the spatial features that are extracted from the models and to correlate them also in a temporal way by exploiting both the available images. The downside is that the models have become increasingly complex and large, often unfeasible for edge applications. These are limitations when the models must be applied to the industrial field or in applications requiring real-time performances. In this work we propose a novel model, called TinyCD, demonstrating to be both lightweight and effective, able to achieve performances comparable or even superior to the current state of the art with 13-150X fewer parameters. In our approach we have exploited the importance of low-level features to compare images. To do this, we use only few backbone blocks. This strategy allow us to keep the number of network parameters low. To compose the features extracted from the two images, we introduce a novel, economical in terms of parameters, mixing block capable of cross correlating features in both space and time domains. Finally, to fully exploit the information contained in the computed features, we define the PW-MLP block able to perform a pixel wise classification. Source code, models and results are available here: https://github.com/AndreaCodegoni/Tiny_model_4_CD</p><p>Keywords Change Detection (CD) ? Remote Sensing (RS) ? Convolutional Neural Network (CNN)</p><p>Thanks to the increasing number of available high resolution aerial images datasets, such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, data driven methods like deep Convolutional Neural Networks (CNN) found successful applicability <ref type="bibr" target="#b7">[8]</ref>. The well known ability of deep CNNs to extract complex and relevant features from images is the key factor for their early promising results <ref type="bibr" target="#b8">[9]</ref>. In the CD scenario, complex features are important, but are not sufficient to accomplish the task. To highlight the occurred changes it is in fact crucial to model the spatio-temporal dependencies between the two images. Plain CNNs have as drawback a limited receptive field caused by the use of fixed kernels in convolutions. To overcome this issue, recent works has focused their attention to enlarging the receptive fields by using different kernel types [10], or adding attention</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the Remote Sensing community, Change Detection (from now denoted with CD) is one of the main research topics. The main purpose of CD is to construct a model able to identify changes occurred in a scene between two different times. To this aim, a CD model compares two co-registered images acquired at times t 1 and t 2 <ref type="bibr" target="#b0">[1]</ref>. Once the relevant changes have been identified, such as urban expansion, deforestation or post disaster damage assessment <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, the challenge is to let the CD model ignore other irrelevant changes. Examples of irrelevant changes are, but not limited to, light conditions, shadows, and seasonal variations. mechanisms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. However, most of them failed to explicitly relate data in the temporal domain, since attention mechanisms are applied separately on the two images. The self attention mechanism adopted in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> shows promising results relating images spatio-temporally, but do that in a very computationally inefficient way. More recently, Transformers have been introduced in CD because of their receptive fields spatially covering the whole image <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Notice that, by applying multi-headed attention layers in the decoder part of the network, the receptive field covers the temporal domain too.</p><p>The CD field finds applicability also outside the remote sensing world. Our work is inspired by several industrial needs that we are facing. In that field one additional constraint is to keep low the complexity of the model, thus reducing machine response times. Unfortunately, the majority of State-Of-The-Art (SOTA) models proposed in literature are millions-parameters-sized. In industrial scenarios, using such big models is not always possible. The first issue with those models is related to the training phase. Training-time is clearly affected by the size of the model, thus making model tuning and Hyper-Parameters-Optimization (HPO) times incompatible with the resources available in medium-small companies. Moreover, the bigger the network, the bigger the datasets needed to prevent overfitting the data. The dataset size is a common issue in industrial applications where collecting data is a time-consuming and costly task. Finally, big networks require dedicated hardware both for training and inference. This is in contrast with production requirements and project budgets.</p><p>The main purpose of our work is to develop a neural network having comparable performances with respect to the SOTA CD models, but requiring a lower computational complexity.</p><p>The majors contributions of our work are the following:</p><p>? We explore the effectiveness of using low-level features in the problem of comparing images. This approach allowed us to validate our intuition that in this context the low-level features are sufficiently expressive. Moreover, this allowed us to significantly limit the number of parameters of our model.</p><p>? We introduce a novel strategy to mix the features between the two images. To this aim, we leverage the grouped convolutions and a specific type of concatenation. The mixing block operates a spatio-temporal cross-correlation between the pixels of the two input images.</p><p>? A novel block called MAMB is introduced to produce skip connections. More precisely, we define an attention mechanism that uses pixel-wise information to retrieve a skip connection mask on a per-resolution basis. The obtained masks are exploited in the up sampling phase to refine the carried information.</p><p>? To face the problem of per-pixel classification, in the last block we use the obtained channel wise information to generate the final mask by classifying each pixel separately.</p><p>Our architecture exploits the information contained in the channels of the feature vectors generated by the backbone. For this reason, it can effectively exploit low level features such that a relative small backbone can be adopted. Being the backbone 1 the most time-consuming and parameter-demanding component in the architecture, maintaining it small allows us to achieve our goal. In particular, this allows us to maintain the total number of parameters below 0.3 millions.</p><p>Finally, we compare the quality of the model with SOTA architectures, and we demonstrate that it has performances comparable if not even superior to other SOTA models in the CD field. We have extensively tested our model on public and proprietary datasets. In order to validate and make reproducible our results, in this paper we highlight the results obtained in the field of aerial images on public datasets. Similar results in terms of efficiency and effectiveness have been found in non-public datasets, in application fields other than that which is the subject of this paper.</p><p>The paper is organized as follows: in Chapter 2 we present some related works; in Chapter 3 we describe our proposed model; in Chapter 4 we report the results of our model on two public available datasets, and in Chapter 5 we highlight some future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Deep learning models, and in particular CNN, have been applied with great success in image comparison tasks <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, in pixel-level image classification <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, and they represent the SOTA in many other Computer Vision tasks <ref type="bibr" target="#b22">[23]</ref>.</p><p>Models in the context of the CD must manage two inputs: one image acquired at time t 1 , and another one acquired at time t 2 . The correct use of these two inputs and the features extracted from them are extremely important for the well behavior of the CD model. To the best of our knowledge, the first work that applied CNN to the CD problem is <ref type="bibr" target="#b8">[9]</ref>. In this work the authors propose two different approaches. In the first case they use a U-Net <ref type="bibr" target="#b20">[21]</ref> type network with the Early Fusion Strategy (FC-EF), i.e. they concatenate the two images taken at times t 1 and t 2 , and then they feed the U-Net with the concatenated tensor. In the second case they use a Siamese U-Net type network <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> where the two images are processed separately, and subsequently the features are fused in two different ways: concatenation (FC-Siam-conc) and subtraction (FC-Siam-diff). These fused features are then used as skip connections in the decoder. After this seminal work, an entire research line investigated both the Early Fusion Strategy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, and the Feature Fusion Strategy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>.</p><p>To take full advantage of the large amount of spatial information, deeper CNNs such as ResNet <ref type="bibr" target="#b32">[33]</ref> or VGG16 <ref type="bibr" target="#b33">[34]</ref> have been used <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> in order to extract spatial information and group them in a hierarchical way. Although, standard convolution has a fixed receptive field that limits the capacity of modelling the context of the image. To face this issue, atrous convolutions <ref type="bibr" target="#b34">[35]</ref> have been experimented <ref type="bibr" target="#b9">[10]</ref>: they are able to enlarge the receptive field of the single convolutional kernel without increasing the number of parameters. To definitively overcome the problem of fixed receptive field, attention mechanisms, in the form of spatial attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, channel wise attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, and also self-attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, have been introduced. In <ref type="bibr" target="#b5">[6]</ref>, the attention mechanisms are used in the decoder part: the channel wise attention is used to re-weight each pixel after the fusion with the skip connections, while the spatial attention is adopted to spatially re-weight the pixels containing misleading information due to the up sampling step.</p><p>To further exploit the interconnection between spatial and channel information, in <ref type="bibr" target="#b10">[11]</ref> a dual attention module is introduced. The co-attention module introduced in <ref type="bibr" target="#b12">[13]</ref> tries to leverage correlation between features extracted from both images. Also in <ref type="bibr" target="#b12">[13]</ref> a co-layer aggregation and a pyramid structure is used to make full use of the features extracted at each level and with different receptive fields. In <ref type="bibr" target="#b1">[2]</ref>, the non-local self attention introduced in <ref type="bibr" target="#b35">[36]</ref>, is used. This mechanism consists in stacking the features extracted from a Siamese backbone to apply them both a basic spatial attention mechanism, and a pyramidal attention mechanism. Since these two attention blocks are applied to stacked t 1 and t 2 features, these are correlated in a non-local spatio-temporal way.</p><p>Finally, we mention that the global attention mechanisms introduced with Transformers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> have also been applied to the CD problem. In <ref type="bibr" target="#b14">[15]</ref> the authors employ a modified ResNet18 as Siamese backbone to extract features. Then, to better justify the use of Transformer blocks, they follow a parallelism between the natural language processing field, and the image processing one, by introducing the semantic tokens. Roughly speaking, semantic tokens are the pixels of the last feature tensor extracted by the backbone. The authors use this concept to illustrate that concatenating single pixels and then processing them with a transformer encoder-decoder, a pair of features tensors can be obtained that incorporates both global spatial information, and global temporal information. On the other side, in <ref type="bibr" target="#b15">[16]</ref> the authors replace the CNN backbone with a transformer in order to exploit the global information contained in the images right from the start. In this model, the temporal aggregation is done only in the final multilayer perceptron decoder.</p><p>Our work is inspired by <ref type="bibr" target="#b14">[15]</ref>. In fact, we believe that the information contained in each single pixel at different resolutions, i.e. the semantic tokens, are essential for the final classification, and also they are more important than the global context provided by spatial attention. Moreover, our intuition is that, since we can compare two images, we can use only low level features to highlight the changes occurred in time. To these extents, we designed a Siamese U-Net type network where the backbone is represented by the first 4 blocks of EfficientNetb4 <ref type="bibr" target="#b38">[39]</ref>. To better fuse the information contained in channels in a spatio-temporal way, we introduce a mixing strategy that forces the network to fuse and compare features in a semantically consistent way between the features extracted at times t 1 and t 2 . Finally, to exploit all the information contained in each pixel/semantic token, we heavily applied multi layer perceptrons (MLP) on each pixel/semantic token. MLP are used to create spatial attention mask skip connections in the U-Net structure. Moreover, they have been applied in the classification block to classify each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed model</head><p>In this section we describe and motivate the structure of our model. We use a Siamese U-Net like model consisting of 4 main components:</p><p>? Siamese encoders constituted by a pre-trained backbone (see Section 3.2).</p><p>? Mix and Attention Mask Block (MAMB) and bottleneck mixing block to compose backbone results (see Section 3.3).</p><p>? Up-sample decoder to refine low resolution results incorporating higher resolution data from the skip connections (see Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Pixel level classifier (see Section 3.5)</head><p>In what follows, we denote with X ? R (C?H?W ) the reference tensor (image at time t 1 ) and with Y ? R (C?H?W ) the comparison tensor (image at time t 2 ). C is the number of channels, H is the height and W the width of the tensors. We omit the batch dimension for the ease of notation. We denote with Conv the convolution operator, with PReLU the Parametric Rectified Linear Unit <ref type="bibr" target="#b39">[40]</ref>, with IN2d the Instance Normalization <ref type="bibr" target="#b40">[41]</ref>, and with Sigmoid the Sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model overview</head><p>Indicating with f k the composition of the backbone blocks up to the k th one, the high-level features X k = f k (X) and The decoder consists of a series of up-layers, one for each block of the backbone. Each up-layer increases the spatial dimensions of the tensor received from the previous layer to reach the same resolution of the corresponding skipconnection. Furthermore, the up sampled tensors and the skip-connections are composed to generate the next layer inputs. This composition is the attention mask application to the features obtained from the previous layer.</p><formula xml:id="formula_0">Y k = f k (Y )</formula><p>Finally, the last block of our model classifies each pixel of the obtained tensor through a Pixel-Wise Multi-Layer Perceptron (PW-MLP). The PW-MLP associates to each pixel the probability that it belongs to the anomaly class. Applying a threshold to this tensor we obtain the binary mask of changes.</p><p>In the following subsections we describe each component separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Siamese encoders with pre-trained backbone</head><p>The purpose of the Siamese encoder is to extract simultaneously features from both images in a semantic coherent way.</p><p>In deep neural networks, training the first layers of the model is sometimes difficult due to the well-known phenomenon of vanishing gradients <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. To overcome this problem, several tricks have been introduced such as the residual connections of ResNet <ref type="bibr" target="#b32">[33]</ref> or the skip connections of U-Net <ref type="bibr" target="#b20">[21]</ref>. However, training deep backbones remains a difficult, time-consuming, or even impossible task to accomplish if the dataset is too small.</p><p>For these reasons, pre-trained backbones are often preferred, even in CD problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. The disadvantage of this approach is that the backbones are not always trained on images that are similar to the ones we are dealing with. However, CNN backbones work by layering information. Low-level features, such as lines, black/white spots, points, edges, can be considered general-purpose being common to all images.</p><p>In our intuition, in the faced task the comparison between two images acquired at times t 1 and t 2 can be accomplished by using just the low-level features extracted from the first few layers of a pre-trained backbone.</p><p>We therefore decided to use one EfficientNet backbone <ref type="bibr" target="#b38">[39]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b43">[44]</ref>. We allowed the training phase to tune all the backbone parameters during the update of all the other network parameters. We have chosen the EfficientNet backbone family due to both its efficacy and its efficiency. Moreover, the resolution reduction in the first EfficientNet layers is sufficiently slow in order to create skip connections of different spatial dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mix and Attention Mask Block (MAMB) and bottleneck mixing block</head><p>The purpose of this block is to merge the features (X k , Y k ) extracted from one of the blocks of the Siamese encoder. It creates a mask M k that is then used as skip connection to refine the information obtained during the up sampling phase.</p><p>The mask we create can also be understood as a pixel-level attention mechanism. The idea of pixel-wise attention has been already studied in <ref type="bibr" target="#b44">[45]</ref>. Here we specifically designed a pixel-wise attention mechanism exploiting both spatial and temporal information.</p><p>The MAMB can be divided into two sub-blocks: the Mixing block (see Section 3.3.1), and the Pixel level mask generator (see Section 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Mixing block</head><p>As the name suggests, in this sub-block we compose the features generated by the k th backbone block (X k , Y k ). To this aim, we observe that the features X k and Y k , share both the same shape C k , H k , W k , and the same arrangement in terms of features. This means that the features in channel c of X k have the same semantic meaning with respect to the corresponding features in channel c of Y k , being the Siamese encoder weights shared. In view of this observation, we decided to concatenate the tensors X k and Y k in the tensor Z k ? R 2C k ?H k ?W k using the following rule:</p><formula xml:id="formula_1">Z c k := X c/2 k if c is even Y (c?1)/2 k otherwise ?c ? {0, 1..., 2C k ? 1}.<label>(1)</label></formula><p>To mix the features coming from X k and Y k both spatially and temporally, we used a group convolution. By choosing the number of groups equal to C k we obtain C k kernels of depth 2 which process the tensor Z k in pairs of channels. These kernels perform at the same time both spatial and temporal convolution using the cross-correlation between semantically similar features.</p><p>The new tensor Z k ? R C k ?H k ?W k is defined as:</p><formula xml:id="formula_2">Z k = Mix(X k , Y k ) := PReLU [IN2d [Conv(Z k , ch in = 2C k , ch out = C k , groups = C k )]] .<label>(2)</label></formula><p>An illustration of our concatenation strategy, and the following grouped convolution, is reported in <ref type="figure" target="#fig_2">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Pixel-level mask generator</head><p>Fixing the spatial coordinates of a single pixel, the C k values in the tensor Z k contain spatial information related to both times t 1 and t 2 . Our idea is to use the PW-MLP in order to process this information and generate a score that acts as a spatio-temporal attention.</p><p>To this aim, the PW-MLP is designed to produce a mask tensor M k ? R H?W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">PW-MLP</head><p>To implement a pixel-wise Multi-Layer Perceptron, that is an MLP working on all the channels of one single pixel at a time, we use 1 ? 1 convolutions. The MLP is composed by N blocks each containing one 1 ? 1 convolution and one activation function. As activation, we used the PReLU, being this able to propagate gradients also on the negative side of the real axis. The last convolution contains just one filter, thus producing a tensor M k with dimensions 1, H k , W k .</p><p>The use of 1 ? 1 convolutions to implement an MLP is not a new idea. In <ref type="bibr" target="#b45">[46]</ref> this strategy has been used to substitute layers such as convolutions with small, trainable, networks. As pointed out in <ref type="bibr" target="#b45">[46]</ref>, we have very poor prior information on the latent concepts in pixel vectors. Hence, we have decided to use this universal function approximator to separate different semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">The bottleneck mixing block</head><p>We applied the tensor mixing strategy reported in Section 3.3.1 to compute the bottleneck of the U-Net like network. More precisely, we compute: U e = Z e = Mix(X e , Y e ).</p><p>U e represents the output of the encoder and the input to be processed by the decoder. U e contains the higher level features extracted by the backbone and correlated both spatially and temporarily. Given that, our intuition is that U e contains enough information in order to classify each pixel at the bottleneck resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Up-sample decoder with skip connections</head><p>The general k?th decoder block takes as input the tensor U k+1 of shape C k+1 , H k+1 , W k+1 and a mask M k of shape 1, H k , W k generated by one MAMB. Firstly, an up sampling operation is performed in order to transform U k+1 so that its shape matches the one of M k . We call the up sampled tensor U k . Then, we define U k with</p><formula xml:id="formula_3">U k = Up(U k+1 , M k ) := PReLU [IN2d [Conv(U k M k )]] ,</formula><p>where we have denoted with the symbol the Hadamard product. This represents the skip connection attention mechanism at the pixel level.</p><p>As we already mentioned in Section 3.3.4, U e contains enough information to classify each pixel at its spatial resolution. By multiplying the mask M k we are re-weighting each pixel in order to alleviate the misleading information generated by up sampling.</p><p>Notice that, in this Up block we employ the depth wise separable convolution <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pixel level classifier</head><p>Finally, since the change detection problem is a binary classification problem, we decided to use as last layer a PW-MLP with output classes {0, 1} representing respectively normal and change pixels. With respect to what reported in Section 3.3.3, in this case we used as the last activation layer a Sigmoid function instead of the PReLU, thus enforcing the result of the network to contain values in [0, 1]. In this case, the PW-MLP is used as a non-linear classifier which separates pixels in normal or changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Settings and Results</head><p>In this section we present the settings used in our experiments, and we report the achieved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>As already stated in Section 1, we cannot share the dataset related to our industrial application. Moreover, in order to fairly evaluate our model, and to compare it with other works in the CD field, we used the following public and widely adopted aerial images building datasets: LEVIR-CD <ref type="bibr" target="#b1">[2]</ref> and WHU-CD <ref type="bibr" target="#b6">[7]</ref> 2 . Notice that the task defined by these datasets is particularly close to the faced industrial one, that is the driver of our research work. In these two datasets the model has to track some specific patters, those corresponding to buildings, and carefully segments the eventually occurred changes.</p><p>LEVIR-CD contains 637 pairs of high resolution aerial images. Starting from these images, patch pairs of size 256 ? 256 each have been extracted. After that, the pair instances have been partitioned accordingly to the authors' original indications. This step produced 7120, 1024, and 2048 pair instances for the train, validation, and test dataset respectively.</p><p>WHU-CD contains just one pair of images having resolution 32507 ? 15354 as a crop of a wider geographic area 3 . Following <ref type="bibr" target="#b48">[49]</ref>, the images have been split in non overlapping patches with resolution 256 ? 256. After that, a randomly partitioning of the dataset have been performed obtaining 5947, 743, and 744 pairs for train, validation, and test respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We implemented our model using PyTorch <ref type="bibr" target="#b49">[50]</ref> and we trained it on an NVIDIA GeForce RTX 2060 6GB GPU. As described in Section 3.2, we selected the first four blocks of the EfficientNet version b4 backbone pretrained on the ImageNet dataset. All other weights of the model have been initialized randomly <ref type="bibr" target="#b3">4</ref> .</p><p>As optimizer we adopted AdamW <ref type="bibr" target="#b50">[51]</ref>, tuning its hyperparameters with the package Neural Network Intelligence (NNI) <ref type="bibr" target="#b51">[52]</ref>. More precisely, we tuned the learning-rate, the weight decay, and the usage of the optimizer amsgrad variant. Due to computational resource limitations, no other hyperparameters have been tuned. We have not experimented any network architecture search technique (NAS). To dynamically adjust the learning rate during the training, we opted for the cosine annealing strategy as described in <ref type="bibr" target="#b52">[53]</ref>, but avoiding the warm restart.</p><p>Since aerial images are spatially registered, we applied as data augmentation both Random Flip and Random Rotation simultaneously to the reference/comparison images and their associated ground-truth mask. Moreover, Gaussian Blur and Random Brightness/Contrast changes have been applied independently on the reference and the comparison images respectively. <ref type="bibr" target="#b4">5</ref> Finally, due to the limited GPU memory capacity and computational power, we fixed the batch size to 8, and trained for just 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss function and evaluation metrics</head><p>Since the CD problem is a binary classification problem, we used the Binary Cross Entropy (BCE) loss function, namely:</p><formula xml:id="formula_4">L(G, P ) := ? 1 |H| ? |W | h?H,w?W g h,w log(p h,w ) + (1 ? g h,w ) log(1 ? p h,w ),</formula><p>where we denoted G the ground truth mask, P the model prediction, and with a little abuse of notation, H and W the set of indices relative to height and width and whose cardinalities are |H| and |W | respectively.</p><p>To evaluate the performances achieved by our model, we calculated the following indicators with respect to the change class:</p><p>Precision P r := T P T P + F P ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head><p>Rc := T P T P + F N ,</p><formula xml:id="formula_5">F1 score F 1 := 1 P r ?1 + Rc ?1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intersection over Union</head><p>IoU := T P F N + F P + T P ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Accuracy</head><p>OA := T P + T N F N + F P + T P + T N ,</p><p>where T P , T N , F P , F N are computed on the change class and represent the true positives, true negatives, false positives, and false negatives respectively. To retrieve the change mask we applied a 0.5 threshold to the output mask. <ref type="bibr" target="#b2">3</ref> The whole dataset depicts the city of Christchurch, in New Zealand. The crop, aimed to be used in CD tasks, is a sub-area acquired in two different times. <ref type="bibr" target="#b3">4</ref> To make our results reproducible, we fixed the random seed at the beginning of each experiment. <ref type="bibr" target="#b4">5</ref> To achieve all the adopted augmentations, we used the Albumentation library <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state-of-the-art</head><p>To demonstrate the effectiveness of our approach, we compared our results with those reported in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. As baseline, we used the three models present in <ref type="bibr" target="#b8">[9]</ref>. Moreover, to compare our model with other works adopting both spatial and channel attention mechanisms, we dealt with <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>. Finally, given the success achieved by Transformers applied to the computer vision field, we also compared the results obtained in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion of results</head><p>The results reported in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show the superior performance of our model on these two building change detection datasets. The baseline models FC-Siam-diff and FC-Siam-conc <ref type="bibr" target="#b8">[9]</ref> are probably the architectures most similar to ours. With respect to these two baseline models, we increased the F1 score by 4.73 points on LEVIR-CD, and by more than 20 points on the WHU-CD. With respect to the best model we found in the literature <ref type="bibr" target="#b15">[16]</ref>, the performance increment on the LEVIR-CD dataset is smaller. However, as we can see from <ref type="table" target="#tab_2">Table 3</ref>, our model is 146.50 times smaller. In view of these results, we can conclude that our model, despite the lower complexity and the lower number of employed parameters, is very effective on the buildings CD task. Moreover, having not used any global attention mechanism, we have a confirmation of our intuitions: in the faced CD task, low level information is sufficient to reach high-quality results. Also, the information contained in each single pixel at different resolutions is very rich and can be exploited to effectively classify changes. In <ref type="figure" target="#fig_3">Figure 3</ref> we reported an example of the intermediate masks that our model creates in skip-connections. As can be seen, the intermediate resolution masks have captured some semantics of the scene partially classifying the pixels' content. Hence, as can be seen comparing the ground truth binary mask (GT) with that generated by our model, the final result is very sharp and detailed. Moreover, unlike usual skip connections, our mechanism is lighter in terms of parameters, it does not impose major limitations on the number of used features, and finally it makes the role of skip connections more interpretable.</p><p>To quantitatively confirm the usefulness of skip connections, we trained a model without them and compared the achieved results in <ref type="table" target="#tab_3">Table 4</ref>. As can be seen, all the metrics confirm the beneficial effects of skip connections in the model. Since the Recall index increased, we deduce that the skip connection model lowered the number of false negatives by better segmenting the various changes between the two images.</p><p>In <ref type="table" target="#tab_4">Table 5</ref> we compare our bottleneck mixing block described in Section 3.3.1 with other possible and simple feature fusion block. We replace the mixing block Section 3.3.1 with:</p><p>? Subtraction;</p><p>? Concatenation + Depth wise separable convolution;</p><p>? Concatenation + Convolution.</p><p>Our mixing strategy is a generalization of the pixel-wise subtraction <ref type="bibr" target="#b5">6</ref> . However, our mixing block Section 3.3.1 is fully trainable with the spirit of feature re-use <ref type="bibr" target="#b54">[55]</ref>. Also, concatenation with depth-wise separable convolution and standard convolution can be seen as generalizations of subtraction, but the number of parameters to achieve these generalizations is much bigger than ours. Generally speaking, the number of parameters needed by our mixing block is c(2 ? k h ? k w ), where c is the number of channels, k h , k w are the convolutional kernel sizes. The parentheses are highlighting the size of each kernel and the number of kernels. By comparison, a convolution working on the concatenated feature tensors requires c(2c?k h ?k w ) parameters. Even with a more cheap depth wise separable convolution we have 2c(?k h ?k w )+c(2c) parameters, that means 2c 2 more than our proposed strategy. In addition to the savings on the number of parameters, from <ref type="table" target="#tab_4">Table 5</ref> we see that our strategy appears to be the one that achieves the best performance. This shows that our feature fusion strategy is effective and also very efficient and cheap in terms of number of parameters. In <ref type="figure">Figure 4</ref> a visual/qualitative comparison between the masks created by our model and those created by BIT <ref type="bibr" target="#b14">[15]</ref> on the LEVIR-CD test dataset is reported. Generally speaking, both models perform well and we end up our analysis by conjecturing that the performance difference reported in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> are more related to missing or hallucinated change regions, than region quality issues. Nevertheless, we can find some examples were there are significant differences between the ground truth mask (GT) and that created by the two models. In <ref type="figure">Figure 4</ref> it is interesting to note that there are examples where both models fail similarly in the same region, despite the two models are based on very different approaches (local versus global).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future works</head><p>Guided by industrial needs, we proposed a tiny convolutional change-detection Siamese U-Net like model. Our model exploits low-level features by comparing and classifying them to obtain a binary map of detected changes. We propose a mixing block, introducing the ability to compare/compose features on both spatial and temporal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>Visual comparison between our model and BIT. We highlighted with red bounding boxes those regions containing significant differences between the ground-truth and the generated masks.</p><p>The proposed PW-MLP block, shows a great ability in extracting features useful to classify occurred changes on a per-pixel basis. The composition of these proposed blocks, here referred as MAMB, show the ability to estimate masks useful to enrich features used in the U-Net decoder part. We have shown that an effective way to generate the output mask is to process low-level backbone features in a PW-MLP block, effectively facing the change-detection task as a per-pixel classification problem.</p><p>We tested our model on public change-detection datasets containing aerial images acquired in two different times. Furthermore, we compared the achieved results with state-of-the-art models proposed in the change-detection literature.</p><p>Our tests demonstrated that our model performs comparably or better than the current state-of-the-art models, remaining at the same time the smaller and faster one.</p><p>The ideas employed in this work can be also applied to other fields. For this reason, we will investigate the application of MAMB and PW-MLP blocks to tasks such as anomaly-detection, surveillance and semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Siamese U-Net like architecture including MAMB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are extracted from each level k of the backbone. These features are used both to compute the resulting output at each level of the U-Net encoder, and to estimate the attention masks. The last backbone block produces the embeddings X e and Y e representing the bottleneck inputs. Every backbone intermediate output pair (X k , Y k ) is processed by means of the MAMB producing spatial attention masks M k . These masks are used as skip-connections and composed in the decoder. The last mixed tensor is obtained by composing (X e , Y e ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visual representation of our concatenation strategy (1) and the grouped convolution<ref type="bibr" target="#b1">(2)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the intermediate masks at different resolutions and the final binary mask for one example image pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance metrics on the LEVIR-CD dataset. To improve results readability, we adopted a color ranking convention representing the First, Second, and Third results. The metrics are reported in percentage.</figDesc><table><row><cell></cell><cell cols="2">LEVIR-CD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Pr</cell><cell>Rc</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell></row><row><cell>FC-EF [9]</cell><cell cols="5">86.91 80.17 83.40 71.53 98.39</cell></row><row><cell>FC-Siam-diff [9]</cell><cell cols="5">89.53 83.31 86.31 75.92 98.67</cell></row><row><cell>FC-Siam-conc [9]</cell><cell cols="5">91.99 76.77 83.69 71.96 98.49</cell></row><row><cell>DTCDSCN [11]</cell><cell cols="5">88.53 86.83 87.67 78.05 98.77</cell></row><row><cell>STANet [2]</cell><cell cols="5">83.81 91.00 87.26 77.40 98.66</cell></row><row><cell>IFNet [6]</cell><cell cols="5">94.02 82.93 88.13 78.77 98.87</cell></row><row><cell>SNUNet [31]</cell><cell cols="5">89.18 87.17 88.16 78.83 98.82</cell></row><row><cell>BIT [15]</cell><cell cols="5">89.24 89.37 89.31 80.68 98.92</cell></row><row><cell cols="6">Changeformer [16] 92.05 88.80 90.40 82.48 99.04</cell></row><row><cell>Ours</cell><cell cols="5">92.68 89.47 91.05 83.57 99.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance metrics on the WHU-CD dataset. To improve results readability, we adopted a color ranking convention representing the First, Second, and Third results. The metrics are reported in percentage.</figDesc><table><row><cell></cell><cell></cell><cell>WHU-CD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Pr</cell><cell>Rc</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell></row><row><cell>FC-EF [9]</cell><cell cols="5">71.63 67.25 69.37 53.11 97.61</cell></row><row><cell>FC-Siam-diff [9]</cell><cell cols="5">47.33 77.66 58.81 41.66 95.63</cell></row><row><cell>FC-Siam-conc [9]</cell><cell cols="5">60.88 73.58 66.63 49.95 97.04</cell></row><row><cell>DTCDSCN [11]</cell><cell cols="5">63.92 82.30 71.95 56.19 97.42</cell></row><row><cell>STANet [2]</cell><cell cols="5">79.37 85.50 82.32 69.95 98.52</cell></row><row><cell>IFNet [6]</cell><cell cols="5">96.91 73.19 83.40 71.52 98.83</cell></row><row><cell>SNUNet [31]</cell><cell cols="5">85.60 81.49 83.50 71.67 98.71</cell></row><row><cell>BIT [15]</cell><cell cols="5">86.64 81.48 83.98 72.39 98.75</cell></row><row><cell>Ours</cell><cell cols="5">92.22 90.74 91.48 84.30 99.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Parameters, complexity and performance comparison. The metrics are reported in percentage, parameters in millions (M) and complexity in GigaFLOPs (G).</figDesc><table><row><cell>Model</cell><cell>Parameters (M)</cell><cell>Parameters ratio</cell><cell>FLOPs (G)</cell><cell>LEVIR-CD F1</cell><cell>WHU-CD F1</cell></row><row><cell>DTCDSCN [11]</cell><cell>41.07</cell><cell>146.67</cell><cell>7.21</cell><cell>87.67</cell><cell>71.95</cell></row><row><cell>STANet [2]</cell><cell>16.93</cell><cell>60.46</cell><cell>6.58</cell><cell>87.26</cell><cell>82.32</cell></row><row><cell>IFNet [6]</cell><cell>50.71</cell><cell>181.10</cell><cell>41.18</cell><cell>88.13</cell><cell>83.40</cell></row><row><cell>SNUNet [31]</cell><cell>12.03</cell><cell>42.96</cell><cell>27.44</cell><cell>88.16</cell><cell>83.50</cell></row><row><cell>BIT [15]</cell><cell>3.55</cell><cell>12.67</cell><cell>4.35</cell><cell>89.31</cell><cell>83.98</cell></row><row><cell>Changeformer [16]</cell><cell>41.02</cell><cell>146.50</cell><cell>N.D.</cell><cell>90.40</cell><cell>N.D.</cell></row><row><cell>Ours</cell><cell>0.28</cell><cell>1</cell><cell>1.54</cell><cell>91.04</cell><cell>91.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between the model with/without skip connections on both datasets LEVIR-CD and WHU-CD.</figDesc><table><row><cell></cell><cell cols="2">LEVIR-CD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model type</cell><cell>Pr</cell><cell>Rc</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell></row><row><cell>No Skip</cell><cell cols="5">92.35 88.50 90.38 82.45 99.04</cell></row><row><cell>Skip</cell><cell cols="5">92.68 89.47 91.05 83.57 99.10</cell></row><row><cell></cell><cell cols="2">WHU-CD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model type</cell><cell>Pr</cell><cell>Rc</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell></row><row><cell>No Skip</cell><cell cols="5">90.56 89.77 90.16 82.09 99.22</cell></row><row><cell>Skip</cell><cell cols="5">92.22 90.74 91.48 84.30 99.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison between the model with our mixing strategy and a simple concatenation and convolution on both datasets LEVIR-CD and WHU-CD.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LEVIR-CD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model type</cell><cell>Pr</cell><cell>Rc</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell><cell>Param. tot.</cell></row><row><cell>Subtraction</cell><cell cols="5">92.36 89.11 90.70 82.99 99.06</cell><cell>284063</cell></row><row><cell>Depth. Sep.</cell><cell cols="5">92.81 89.01 90.87 83.27 99.08</cell><cell>291513</cell></row><row><cell>Concat + conv</cell><cell cols="5">92.90 88.23 90.81 83.18 99.08</cell><cell>340568</cell></row><row><cell>Mix + grouped</cell><cell cols="5">92.68 89.47 91.05 83.57 99.10</cell><cell>285128</cell></row><row><cell></cell><cell></cell><cell cols="2">WHU-CD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model type</cell><cell>Pr</cell><cell>Rc</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell><cell>Param. tot.</cell></row><row><cell>Subtraction</cell><cell cols="5">90.32 87.95 89.12 80.38 99.14</cell><cell>284063</cell></row><row><cell>Depth. Sep.</cell><cell cols="5">91.33 89.05 90.18 82.11 99.23</cell><cell>291513</cell></row><row><cell>Concat + conv</cell><cell cols="5">91.31 90.66 90.98 83.46 99.28</cell><cell>340568</cell></row><row><cell>Mix + grouped</cell><cell cols="5">92.22 90.74 91.48 84.30 99.32</cell><cell>285128</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Notice that the backbone is evaluated twice in Siamese architectures.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Both the adopted dataset have been obtained from https://github.com/wgcban/SemiCD in an already pre-processed version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In fact, if we initialize all of our 2-depth kernels with the "central" weights to 1 and ?1 and all the rest to 0, we have the standard subtraction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors want to thank the whole Argo Vision team, professor Stefano Gualandi, Gabriele Loli and Gennaro Auricchio for the useful discussion and comments. We also want to thank all those who have provided their codes in an accessible and reproducible way. The scholarship of Andrea Codegoni is founded by SeaVision s.r.l..</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review article digital change detection techniques using remotely-sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Change detection of deforestation in the brazilian amazon using landsat data and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>De Carvalho Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Fontes</forename><surname>Guimar?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">901</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Satellite change detection analysis of deforestation rates and patterns along the colombia-ecuador border</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vi?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Echavarria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Rundquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMBIO: A Journal of the Human Environment</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="118" to="125" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Building damage detection in satellite imagery using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zaytseva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06444</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tapete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khelifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mignotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="126" to="385" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triplet-based semantic relation learning for aerial remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="270" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building change detection for remote sensing images using a dual-task constrained deep siamese convolutional network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="811" to="815" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end change detection for high resolution satellite images using improved unet++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1382</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pga-siamnet: Pyramid feature-based attention-guided siamese network for remote sensing orthoimagery building change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dasnet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1194" to="1206" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Remote sensing image change detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A transformer-based siamese network for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G C</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01293</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting change for multi-view, long-term surface inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning advances in computer vision with 3d data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chatzilari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Change detection in remote sensing images using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>International Archives of the Photogrammetry</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using adversarial network for multiple change detection in bitemporal remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optical remote sensing image change detection based on attention mechanism and image difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7296" to="7307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ppcnet: A combined patch-level and pixel-level end-to-end deep network for high-resolution remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1797" to="1801" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From w-net to cdgan: Bitemporal change detection via deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Change detection based on deep siamese convolutional network for optical aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dual learning-based siamese framework for change detection using bi-temporal vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1292</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial instance augmentation for building change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>A field guide to dynamical recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1687</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Revisiting consistency regularization for semi-supervised change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G C</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08454</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Neural Network Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/nni" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
