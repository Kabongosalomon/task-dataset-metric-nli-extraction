<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Monjurul</forename><surname>Karim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruwen</forename><surname>Qin</surname></persName>
						</author>
						<title level="a" type="main">A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. XX, NO. X, MM YYYY 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Dynamic temporal attention</term>
					<term>dynamic spatial attention</term>
					<term>model fusion</term>
					<term>autonomous vehicle</term>
					<term>human-inspired AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapid advancement of sensor technologies and artificial intelligence are creating new opportunities for traffic safety enhancement. Dashboard cameras (dashcams) have been widely deployed on both human driving vehicles and automated driving vehicles. A computational intelligence model that can accurately and promptly predict accidents from the dashcam video will enhance the preparedness for accident prevention. The spatial-temporal interaction of traffic agents is complex. Visual cues for predicting a future accident are embedded deeply in dashcam video data. Therefore, the early anticipation of traffic accidents remains a challenge. Inspired by the attention behavior of humans in visually perceiving accident risks, this paper proposes a Dynamic Spatial-Temporal Attention (DSTA) network for the early accident anticipation from dashcam videos. The DSTA-network learns to select discriminative temporal segments of a video sequence with a Dynamic Temporal Attention (DTA) module. It also learns to focus on the informative spatial regions of frames with a Dynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) is trained jointly with the attention modules to predict the probability of a future accident. The evaluation of the DSTA-network on two benchmark datasets confirms that it has exceeded the state-of-the-art performance. A thorough ablation study that assesses the DSTA-network at the component level reveals how the network achieves such performance. Furthermore, this paper proposes a method to fuse the prediction scores from two complementary models and verifies its effectiveness in further boosting the performance of early accident anticipation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>widely deployed dashcams even just a few seconds ahead would effectively increase the situational awareness of human drivers, Advanced Driver Assistance Systems (ADAS), and autonomous vehicles to trigger a higher level of preparedness for accident prevention.</p><p>Many causal factors contribute to traffic accidents <ref type="bibr" target="#b4">[5]</ref>, including but not limited to human factors, environmental conditions, road and traffic characteristics, temporal-spatial factors, and vehicle types <ref type="bibr" target="#b5">[6]</ref>. Detecting accident causal factors can help improve the awareness of accident risks and develop methods to mitigate their negative impacts on safety. Dashcams capture many of these causal factors in the form of video data. Computational methods that can translate the easily obtained dashcam video data into the perception of accident risks are largely desired <ref type="bibr" target="#b6">[7]</ref>.</p><p>Some pioneering studies of computer vision-based accident anticipation use conventional recurrent neural networks to capture causal factors by distributing soft attention to agents in the traffic scene <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Dashcam videos contain not only relevant information but irrelevant information. Without explicitly considering the spatial and temporal importance of traffic agents and the driving context along with their dynamic changes, it is impossible to effectively learn the video representation for anticipating accidents.</p><p>Humans can turn their attention to risky regions in the driving scene. They use their peripheral vision to assess the scene and then fixate on regions that seem of high salient values. Drivers' choice of attended regions and their levels of attention to them (e.g., measured by the fixation duration) vary over time. Their attention level increases when they perceive accident risks visually. Inspired by humans' dynamic visual attention in perceiving traffic risks, this paper developed a computer vision-based deep neural network for the early anticipation of traffic accidents. The network has embedded spatial-temporal attention modules that reinforce its ability to anticipate traffic accidents. The technical contributions of this paper are threefold:</p><p>? A new framework named Dynamic Spatial-Temporal Attention (DSTA) network that learns to dynamically attend to the salient temporal segments and spatial regions of the driving scene video for the early accident anticipation. The network has outperformed the state-of-the-art; ? A new method named score-level late-fusion that can be implemented on any set of complementary trained networks. The fusion method achieves higher prediction accuracy than the constituent networks; ? A detailed analysis of the applicability and limitations of representative traffic accident datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2106.10197v2 [cs.CV] 21 Dec 2021</head><p>The remainder of this paper summarizes the literature in Section II, delineates the proposed DSTA-network in Section III, and discusses the implementation detail and the experimental assessment of the network in Section IV. At the end, the paper summarizes findings and important future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE LITERATURE</head><p>Accident anticipation, in general, falls into the category of problems that predict the probability of a future event. This problem is studied thoroughly to anticipate human actions. The computer vision-based approach to human action anticipation commonly uses appearance features as cues for prediction, including those at the object, activity, and context levels, respectively <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Other cues that may further strengthen the prediction ability include the temporal relationship of sub-activities or activity-related entities, and the spatial relationship between humans, other entities, and the environment <ref type="bibr" target="#b4">[5]</ref>. Furthermore, the design of the loss function for training prediction neural networks is a mechanism for encouraging early predictions <ref type="bibr" target="#b16">[17]</ref>. While these studies have built a strong methodological foundation for the probabilistic prediction of future events, they analyze video data captured by static surveillance cameras, not applicable to mobile cameras mounted on vehicles.</p><p>Various efforts are made to anticipate traffic accidents from dashcam videos. Yao et al. <ref type="bibr" target="#b17">[18]</ref> developed an unsupervised approach that utilizes the ego-vehicle motion information to monitor and predict future locations of traffic agents. Takimoto et al. <ref type="bibr" target="#b19">[19]</ref> incorporated physical location data with video data to predict the occurrence of traffic accidents. Closely related to <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[20]</ref>, Suzuki et al. <ref type="bibr" target="#b9">[10]</ref> proposed an adaptive loss function for promoting the early anticipation of accidents. The loss function assigns penalty coefficients according to the achieved mean time-to-accident during training.</p><p>Recently, attention mechanisms are receiving growing interest. Like humans, a neural network with attention units can learn to selectively concentrate on most relevant features. Chan et al. <ref type="bibr" target="#b7">[8]</ref> introduced the dynamic soft-attention to the traffic accident anticipation, which fuses the weighted sum of objectlevel features with the frame-level features of each video frame. The weights represent the attention levels on different objects. Inspired by <ref type="bibr" target="#b7">[8]</ref>, Zeng et al. <ref type="bibr" target="#b8">[9]</ref> proposed a softattention RNN that models the nonlinear interaction between traffic agents and locates risky regions where the agents may involve in a future accident. Fatima et al. <ref type="bibr" target="#b10">[11]</ref> introduced a feature aggregation block that calculates the weights for aggregating object-level features to capture the inter-object interactions. The above-discussed studies mainly focus on learning attentions to spatially distributed agents that may be related to accidents. The temporal importance of appearance features are ignored either at the frame-level, the object-level, or both. From a different application domain, Cui et al. <ref type="bibr" target="#b21">[21]</ref> integrated both a spatial attention module and a temporal attention module with a GRU <ref type="bibr" target="#b22">[22]</ref> to estimate the state-ofhealth of batteries. The model in <ref type="bibr" target="#b21">[21]</ref> processes the feature vector measured at any time using a one-dimensional (1D) convolution layer, assigns "spatial" weights to filter kernels, and learns a set of temporal weights that do not update over time. This approach is not applicable to the recurring task of accident anticipation from 2D video frames.</p><p>Very recently, Bao et al. <ref type="bibr" target="#b23">[23]</ref> used a graph convolutional recurrent neural network (GCRNN) to capture the spatialtemporal relations among candidate objects. By measuring the spatial distance between objects in each frame, a graph is created to capture their spatial relationships. However, spatial distances between objects in a frame do not capture their true spatial relationships in the real world. The study further used a folded vector of all the hidden representations as temporal attention. Although it is beneficial to model training to a certain extent, temporal attention does not update dynamically to reflect the latest temporal information received.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The proposed network for accident anticipation integrates a Gated Recurrent Unit (GRU) with a Dynamic Spatial Attention (DSA) module, a Dynamic Temporal Attention (DTA) module, and Temporal Self-Attention Aggregation (TSAA) module, as <ref type="figure">Fig.1</ref> illustrates. First, the network reads a dashcam video. Each frame of the video flows into an object detector to get multiple objects detected. Then, a feature extractor extracts both the frame-level features and object-level features. The weighted aggregation of the object-level features is concatenated with the frame-level features, becoming the overall feature input to the GRU. The GRU reads the input feature and the weighted aggregation of multiple hidden representations in the past to generate the hidden representation of the current frame. This hidden representation is used to predict the probability of seeing an accident in future frames. The DTA module learns the attention weights for aggregating hidden representations in the past, and the DSA module learns the attention weights for aggregating object-level features. Additionally, the appended auxiliary network TSAA learns the weights to aggregate all the hidden representations of each training video to predict the class of the video only in the training phase. Details of the proposed network are delineated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamic Spatial-Temporal Attention Framework</head><p>The Dynamic Spatial-Temporal Attention (DSTA) network uses a GRU, a simple yet well-performed type of recurrent neural network <ref type="bibr" target="#b22">[22]</ref>, to recurrently predict the probability of seeing an accident in future frames. The spatial-temporal relations of objects and the context information provide important cues for the accident anticipation. The GRU integrates a DSA module and a DTA module to learn the impact of the spatialtemporal relations to accident risks.</p><p>1) Feature Extraction and Aggregation: An object detector detects spatially distributed objects from each video frame and keeps the top N objects of the highest detection score for consideration. The VGG-16 <ref type="bibr" target="#b24">[24]</ref> feature extractor extracts both frame-level and object-level features from each frame through multiple convolution and pooling operations. Then, two fully connected layers flatten the extracted feature maps to become a D dimensional feature vector. This study adds an additional fully connected layer to further reduce the dimension of the <ref type="figure">Fig. 1</ref>. Overview of the DSTA-framework feature vector to d (&lt; D). Therefore, the extracted objectlevel features are o t ? R d?N , and the frame-level features are f t ? R d . After passing through the DSA module to be introduced in the next section, o t is turned into a weighted aggregation, o t ? R d . Then, o t is concatenated with the framelevel feature f t ,</p><formula xml:id="formula_0">X t = [o t ; f t ],<label>(1)</label></formula><p>to form the overall feature vector of each frame, X t ? R 2d .</p><p>2) Dynamic Spatial Attention (DSA): Attentions to the spatially distributed objects in a frame are unequal. This fact is modeled by the spatial attention weights, ? t ? R N , calculated using the weighted aggregation of hidden representations from the last step, h t?1 , to be introduced in the next section and the object-level features extracted from the current frame, o t :</p><formula xml:id="formula_1">? t = ?(W T sa tanh(W g h t?1 + W ? o t + B ? )),<label>(2)</label></formula><p>where W sa ? R d , W g ? R d?d , W ? ? R d?d , and B ? ? R d are parameters of the DSA module, which are learned during training. tanh is an activation function that regulates the values flowing through the network, and ? in this section is the softmax operator that normalizes the attention scores as spatial attention weights. Given the weights, the object-level features are turned into a weighted aggregation, o t ? R d :</p><formula xml:id="formula_2">o t = o t ? t .<label>(3)</label></formula><p>3) Dynamic Temporal Attention (DTA): Frames in a video are not equally important for the accident anticipation. Some frames may contain more discriminative information for accident prediction, whereas others mainly provide contextual information. Motivated by this fact, the study designs the DTA module to provide the temporal attention weights for aggregating the hidden representations of the most recent M frames. Denote H t?1 ? R d?M as the hidden representations of the past M frames indexed by t ? M to t ? 1:</p><formula xml:id="formula_3">H t?1 = [h t?1 , . . . , h t?M ].<label>(4)</label></formula><p>The sliding window size M is chosen experimentally. A very long temporal window may distract the network from the accident anticipation task and a very short window would not provide sufficient temporal context for the task. The temporal attention weights, ? t?1 ? R d?M , are computed as</p><formula xml:id="formula_4">? t?1 = ?(W ta tanh(H t?1 )),<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">W ta ? R d?d are learnable parameters. Then, ? t?1 is used to turn H t?1 into a weighted aggregation, h h h t?1 ? R d : h t?1 = ? t?1 , H t?1 r ,<label>(6)</label></formula><p>where , r represents the row-wise inner product of two equally-sized matrix. 4) Spatial-Temporal Relational Learning with GRU: At any frame t, the GRU takes the feature vector X X X t and the hidden representation h h h t?1 as the inputs to obtain the hidden representation of the current frame, h h h t . GRU has two gates, a reset gate and an update gate, which retain the most relevant information, g g g (r) t and g g g <ref type="bibr">(u)</ref> t , respectively from the video sequence by filtering out irrelevant information. The data flow through the GRU are expressed mathematically in equations (7-10):</p><formula xml:id="formula_6">g g g (r) t = ?(W W W (r) g X X X t + B B B (r) g h h h t?1 ),<label>(7)</label></formula><formula xml:id="formula_7">r r r t = tanh(W W W r X X X t + B B B r (g g g (r) t ? h h h t?1 )),<label>(8)</label></formula><formula xml:id="formula_8">g g g (u) t = ?(W W W (u) g X X X t + B B B (u) g h h h t?1 ),<label>(9)</label></formula><formula xml:id="formula_9">h h h t = (1 ? g g g (u) t ) ? r r r t + g g g (u) t ? h h h t?1 ,<label>(10)</label></formula><p>wherein ? represents the sigmoid activation function, ? is the element-wise product operator, W W W 's and B B B's (? R d?d ) are learnable parameters. After passing two fully connected layers, ?, the hidden representation h h h t is turned into the scores of positive and negative classes. The scores are further normalized by the softmax operator to become the accident anticipation probability, a t :</p><formula xml:id="formula_10">a t = ?[?(?(h h h t ; W W W 0 , B B B 0 ); W W W 1 , B B B 1 )],<label>(11)</label></formula><p>where W W W 's and B B B's (? R d?d ) are learnable parameters of the fully connected layers.</p><p>5) Temporal Self-Attention Aggregation (TSAA): To help train the hidden layers of GRU better, the auxiliary module TSAA is included in the training stage only. The TSAA module provides learnable weights W saa ? R T to perform a temporal self-attention aggregation <ref type="bibr" target="#b25">[25]</ref> of all the T hidden representations of each training video to predict the video class. As illustrated in <ref type="figure">Fig. 1</ref>, all the T hidden representations of the training video indexed by v are stored as a matrix H H H v :</p><formula xml:id="formula_11">H v = [h 1 , . . . , h T ] v ,<label>(12)</label></formula><p>Then, the self-attention operation is applied to obtain the weighted aggregation of all hidden representations, Z Z Z v ? R d :</p><formula xml:id="formula_12">Z v = H v ?(H T v H v )W saa ,<label>(13)</label></formula><p>where W saa ? R T are learnable parameters of TSAA. This aggregated video-level representation flows into two additional fully connected layers and one softmax operator to predict the video-level score, a v :</p><formula xml:id="formula_13">a v = ?[?(?(Z Z Z v ; W W W v0 , B B B v0 ); W W W v1 , B B B v1 )],<label>(14)</label></formula><p>where W W W v 's and B B B v 's (? R d?d ) are learnable parameters of the fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Procedure</head><p>The goal of the training process is to fit GRU, DSA, DTA, and TSAA to the training data by determining their learnable parameters described in Section III-A. A video dataset of size V , indexed by v, is used to train the DSTA-network. Each video contains T frames in total, captured at the rate of f per second. The training dataset has two classes: positive and negative. If a video contains an accident starting from frame ? (&lt; T ), it belongs to the positive class and the video-level label is l v = 1. Otherwise, it belongs to the negative class and the label l v = 0 meaning that the video contains no accident. Prediction results of the DSTA-network are compared against the ground truth of training data to determine the loss that guides the learning process. a t,v , calculated in Eq. <ref type="formula" target="#formula_0">(11)</ref>, denotes the probability that video v contains an accident, anticipated by the DSTA-network at frame t of the video. The frame-level loss function on the training dataset is calculated as:</p><formula xml:id="formula_14">L F = V v=1 ?l v T t=1 e ? max( ? ?t f ,0) log(a t,v ) ?(1 ? l v ) T t=1 log(1 ? a t,v ) .<label>(15)</label></formula><p>The first term within the square bracket in Eq. (15) is the loss of a positive video, and the second term is the loss of a negative video. Negative videos use a regular binary cross entropy loss function. But the frame-level loss coefficient for positive videos increases exponentially as approaching the accident (e.g., t &lt; ? ). After that, it reaches the same loss coefficient for negative videos. The use of the exponentially increasing loss coefficient for positive videos encourages the early anticipation of accidents.</p><p>The auxiliary TSAA module is trained to predict the probability that a video contains an accident using the video-level loss function:</p><formula xml:id="formula_15">L V = V v=1 [?l v log(a v ) ? (1 ? l v ) log(1 ? a v )] ,<label>(16)</label></formula><p>where a v , calculated in Eq. <ref type="formula" target="#formula_0">(14)</ref>, is the predicted probability that video v is positive. The video-level loss function is a regular binary cross entropy loss function. The first term within the square bracket in Eq. <ref type="formula" target="#formula_0">(14)</ref> is the loss of a positive video and the second term is the loss of a negative video. The objective to achieve in training the DSTA-network is to minimize the following loss function:</p><formula xml:id="formula_16">L = L F + w a L V .<label>(17)</label></formula><p>Here, w a is a hyper-parameter that helps adjust the relative importance of the auxiliary loss L V relative to the primary loss L F to achieve an appropriate balance between them. This study chooses 15 as the value for w a experimentally. Finally, the training progresses by backpropagating the loss L to update the learnable parameters of DSA, DTA, GRU, and TSAA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Score-Level Late Fusion</head><p>Any model for the early anticipation of traffic accidents, including the proposed DSTA-network, faces an unavoidable trade-off between the earliness of prediction and the correctness. Different models have their respective trade-off points. A deep ensemble of two or more complimentary models may further improve performance. Motivated by this hypothesis, the study proposes a method that fuses the prediction scores of two models that can complement each other to derive a more reliable prediction. This study found that models of the same architecture, but different parameters may complement each other, and so those of different architectures.</p><p>Algorithm 1 below summarizes the proposed score-level late fusion method. Consider a video sequence flowing into two models. On each frame t, the two models give their own independent frame-level prediction scores, a (1) t and a <ref type="bibr" target="#b1">(2)</ref> t , respectively. If both models give scores greater than or equal to their respective classification thresholds,? <ref type="bibr" target="#b0">(1)</ref> and? <ref type="bibr" target="#b1">(2)</ref> , the driving scene is anticipated to involve in a future accident confidently. Therefore, the fusion method takes the maximum score from the two models as the new anticipation probability. Similarly, if both the models return scores lower than their respective threshold values, the driving scene is anticipated to be safe confidently. Accordingly, the fusion method takes the minimum score from the two model as the new anticipation probability. If one model returns a score greater than or equal to its threshold while the other returns a score lower than its threshold, The two models give conflict classification. In such a case, the fusion method takes the mean of their scores as the new anticipation probability.</p><p>The threshold values of the two constituent models in Algorithm 1 are design parameters that impact the effectiveness of the fusion method. Different models have different prediction behavior. For example, one model may focus more on the earliness of prediction, while the other may focus more on the Algorithm 1 The Score-level Late Fusion Method Notation:</p><p>T : number of frames in a video sequence, indexed by t a (1) t and a <ref type="bibr" target="#b1">(2)</ref> t : prediction scores by models 1 and 2, respectively, at frame t a <ref type="bibr" target="#b0">(1)</ref> and? <ref type="bibr" target="#b1">(2)</ref> : classification thresholds of models 1 and 2, respectively Input: Frames of a video sequence Output: Accident prediction scores, {a t |t = 1, . . . , T } for t = 1 to T do if a</p><p>t ?? <ref type="bibr" target="#b0">(1)</ref> and a</p><formula xml:id="formula_18">t ?? (2) then a t = max(a (1) t , a (2) t ) else if a (1) t &lt;? (1) and a (2) t &lt;? (2) then a t = min(a (1) t , a (2) t ) else a t = mean(a (1) t , a<label>(2)</label></formula><p>t ) end if end for correctness. Therefore, using separate threshold values for the constituent models allow for taking advantage of their respective strengths. The values of? <ref type="bibr" target="#b0">(1)</ref> and? <ref type="bibr" target="#b1">(2)</ref> are determined in a numerical optimization approach. That is, given a pair of threshold values, the corresponding performance of the fusion method on the testing dataset is obtained. The decision is to select a pair of threshold values which leads to the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION AND EXPERIMENTAL RESULTS</head><p>The study illustrates the implementation and evaluation of the proposed DSTA-network for the early accident anticipation on two publicly available datasets.</p><p>A. Datasets 1) Dashcam Accident Dataset (DAD) <ref type="bibr" target="#b7">[8]</ref>: It contains a diverse set of videos captured across different cities in Taiwan. The frame rate of the video is 20 frames per second (fps). Each video lasts 5 seconds and thus consists of 100 (T ) frames. This dataset has 1,130 negative videos without any accident and 620 positive videos that each contains an accident in the last 0.5 seconds. The training dataset includes 455 positive and 829 negative videos, whereas the testing dataset comprises 165 positive and 301 negative videos.</p><p>2) Car Crash Dataset (CCD) <ref type="bibr" target="#b23">[23]</ref>: It is a dashcam dataset with diverse environmental attributes. It contains 4,500 videos. 80% are training data and 20% are testing data. For both training and testing data, the ratio of positive videos to negative videos is 1:2. Each video lasts 5 seconds with 50 frames in total (i.e., fps is 10). For a positive video, the accident starting time is randomly placed in the last 2 seconds of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Detail of Implementation</head><p>The study implemented the proposed approach using Py-Torch <ref type="bibr" target="#b26">[26]</ref>. Training and testing were performed using an Nvidia Tesla V100 GPU with 32GB of memory. In use of the DAD dataset, this study adopted the features of 19 (N ) candidate objects and the frame-level appearance features provided by <ref type="bibr" target="#b7">[8]</ref>. The candidate object classes are human, bicycle, motorbike, car, and bus which were detected using Faster R-CNN <ref type="bibr" target="#b27">[27]</ref>. For the CCD dataset, the study also directly uses the features provided by <ref type="bibr" target="#b23">[23]</ref> for a fair result comparison, where candidate objects were detected using Cascade R-CNN <ref type="bibr" target="#b28">[28]</ref>. The dimension of VGG-16 features in both datasets is 4,096 (D). These features were passed through fully-connected embedding layers to reduce the dimension to 512 (d). The dimension of hidden representations returned by GRU is 512 too. Features and hidden representations of this dimension can represent input video frames well and they are learned within a reasonable amount of training time. Parameters of the DSTA-network were initialized randomly, by taking values from a normal distribution with 0 mean and 0.01 standard deviation. The temporal sliding window for the DTA module is 0.5 seconds (M = 0.5f ), which is verified experimentally to be suitable. Similar to Chan et al. <ref type="bibr" target="#b7">[8]</ref>, a learning rate of 0.0001 and a batch size 10 were used to train the network. ReduceLROnPlateau was used as the learning rate scheduler. Adam optimizer was used to optimize the network for 60 epochs that are sufficiently long for this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>The evaluation of a model for the early accident anticipation needs to consider two aspects: the earliness and the correctness of prediction.</p><p>1) Correctness: The ability of a model to correctly anticipate accidents can be measured by its performance in classifying a set of testing videos. This study adopts the following three classic metrics <ref type="bibr" target="#b29">[29]</ref>.</p><p>? Recall (R): the ratio of correctly predicted positives over the total number of positive videos ? Precision (P): the ratio of correctly predicted positive videos over the total number of positive predictions ? Average Precision (AP): The recall value and the precision value are changing when the classification threshold changes. The average precision is the area below the precision-recall curve:</p><formula xml:id="formula_20">AP = P R dR,<label>(18)</label></formula><p>where P R is the precision corresponding to the recall R. AP is independent of the choice of classification threshold. A high recall value certainly is critical for accident anticipation due to the severe consequence of false negatives. But a very high recall could be unrealistic, especially when it is at the cost of very low precision. In accident anticipation studies <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[23]</ref>, 80% represents a reasonably good recall value for evaluating the corresponding precision. This study adopts the precision corresponding to 80% recall, denoted by P 80R , as another metric for the performance evaluation.</p><p>2) Earliness: The sooner a model can anticipate an accident, the more capable the accident prevention is. This study measures the earliness of accident anticipation using the following metrics [8]- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[23]</ref>:</p><p>? Time-to-Accident (TTA) : The first time when the framelevel anticipation probability a t goes across the classification threshold? is the time to classify a video as a positive. TTA is the period between this time and the starting time of accident, ? :</p><formula xml:id="formula_21">TTA = max{? ? t|a t ??, 1 ? t ? ? }<label>(19)</label></formula><p>? mean Time-to-Accident (mTTA): TTA is changing if the classification threshold? changes. The study thus calculates the mean value of TTA:</p><formula xml:id="formula_22">mTTA = E[TTA].<label>(20)</label></formula><p>mTTA is independent of the classification threshold. AP and mTTA are commonly used as a pair of metrics for the model assessment. TTA 80R is the TTA at 80% recall. TTA 80R reflects the TTA at a relatively high requirement on recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation of the DSTA-Network 1) Experimental Evaluation:</head><p>A learning behavior is observed from training the DSTA-network, which attempts to seek models that have larger AP and longer mTTA over multiple epochs. A trade-off between the earliness and the correctness of accident anticipation presents because the exponentially increasing loss coefficient for positive videos in Eq. (15) encourages predicting accidents earlier. The network learns a set of models that are positioned at different places on the mTTA-AP diagram. Models on the Pareto-optimal frontier <ref type="bibr" target="#b30">[30]</ref> are efficient models in that neither their mTTA nor AP can be further improved without sacrificing the other. <ref type="figure">Fig. 2</ref> is the mTTA-AP diagram that displays a group of models the proposed DSTA-network learnt on the DAD dataset. It keeps models with at least 1 second of mTTA and at least 50% of AP because a very short mTTA or a very low AP is not applicable in real-world applications. As seen from the figure, the efficient model with the longest mTTA (3.75 seconds) achieves 53.7% AP. Increasing the AP of an efficient model requires to shorten the mTTA. The rightmost point in this mTTA-AP diagram is the efficient model with the best AP, which achieves 1.5 seconds of mTTA at 72.3% AP. A similar relationship between mTTA and AP is also observed when training the DSTA-network on the CCD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Relationship between mTTA and AP on the DAD dataset</head><p>Selection of an efficient model from the Pareto-optimal frontier for the implementation should consider the specific need of users. Requirements on accident anticipation systems can vary, depending on the weather conditions, time of the day, and areas where vehicles are running <ref type="bibr" target="#b4">[5]</ref>. Some may demand super early prediction where false alarms are handled well. Others may require a high accuracy where earliness is not essential.</p><p>2) Comparison to the State-of-the-Art Models: The DSTAnetwork is compared to the state-of-the-art models targeting longer mTTA <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[23]</ref>. TABLE I summarizes the results of the comparative study. The performance of <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[23]</ref> are cited from <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b23">[23]</ref>. On the DAD dataset, DSA published in 2016 <ref type="bibr" target="#b7">[8]</ref> achieved 48.1% AP and 1.34 seconds mTTA. Within four years, other studies have progressively increased the AP to 53.7% and extended the mTTA to 3.66 seconds <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[23]</ref>. The proposed DSTA-network further leverages the AP by another 2.4% and mTTA by 0.13 seconds, achieving 56.1% AP and 3.66 secconds mTTA. This confirms that DSTA can improve both AP and mTTA on such a challenging dataset. On the CCD dataset, the state-of-theart performance seems has saturated already. But DSTA still outperforms it with a small margin, achieving 99.6% AP with 4.87 seconds mTTA. On both DAD and CCD datasets, the proposed DSTA-network has outperformed the existing stateof-the-art performance. TABLE II further compares the best AP model that the DSTA-network attains to that of GCRNN <ref type="bibr" target="#b23">[23]</ref>. Compared to GCRNN, the best AP model of DSTA extends the mTTA by 13% and TTA 80R by 16%. Meanwhile, it increases AP slightly, to 72.34 %. Not to mention that extending the TTA even for a fraction of second can create better opportunities for accident prevention. Results in TABLES I and II verify the competitiveness of the DSTA-network in meeting various requirements for prediction correctness and earliness. The above-cited performances are associated with individual models. The proposed fusion method, to be discussed in Section IV-E, will achieve further improvement.  Experiment 1 is the DSTA-network with all the components in place. The attained AP is higher than that of any other experiment. It shall be mentioned that the DSTA-network can provide another model (see <ref type="figure">Fig. 2</ref>), whose AP is 70.5% and mTTA is 1.74 seconds, longer than that of any other experiment in TABLE III.</p><p>In experiment 2, the sliding window of the temporal attention model, M , is reduced to one frame (i.e., the last frame). This change reduces the AP for 1.6%, which signifies the effectiveness of using several recent frames' hidden representations to learn the temporal relation.</p><p>In experiment 3, a GCRNN replaces the GRU, resulting a drop of the AP for 1.87% and a decrease of mTTA for 0.28 seconds. This is mostly because the edge weights of the GCRNN are less effective than the dynamic spatial attention weights that the DSTA-network learns. Similarly, the replacement of the GRU by an LSTM in experiment 4 lowers the AP for 3.26% and reduces the mTTA slightly, for 0.02 seconds, despite of increasing the number of parameters. Additionally, the training time of experiment 3 is about five times longer than the training times of experiments 1 and 4.</p><p>Experiment 5 drops the TSAA module, which decreases the AP for 2.45% but increases the mTTA for 0.20 seconds. Experiment 6 drops the DTA module, resulting a decrease in the AP for 2.62% and the mTTA for 0.07 seconds. Experiment 10 drops both modules, which decreases the AP for 7.31% and but slightly increases the mTTA for 0.05 seconds. These experiments verify the importance of the temporal attention modules in improving AP.</p><p>Compared to experiment 1, experiment 7 drops the DSA module. This change reduces the AP for 4.19% and the mTTA for 0.17 seconds. Similar observations are seen from the comparison between experiments 9 and 5 and the comparison between experiments 8 and 6. These experiments verify that the DSA module in the DSTA-network is effective in increasing both the AP and the mTTA.</p><p>The ablation study verifies the merits of the DSTA-network design. Each of the attention modules positively contributes to the improvement of AP. Their net effect on mTTA is also positive. This study also confirms that, given all these attention modules, GRU is a better choice than both LSTM and GCRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Qualitative Results:</head><p>The DSTA-network can focus on the most semantically significant spatial and temporal regions in a video sequence. <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates the dynamics of the spatial and temporal attentions using a few sample frames from a positive video wherein two vehicles (cars) are approaching to each other in an intersection. The color bar on the right indicates the magnitude of the attention weights.</p><p>The first row is the original sample frames. In the second row, the top two attended objects in each frame are indicated by colored rectangular boxes. The object receiving the highest spatial attention weight and the magnitude of this weight may vary from one frame to another. In this example, the top two attended objects in frames #50, #54, and #56 are traffic agents that will soon involve in an accident. In frames #48 and #52, only the object with the top attention weight is a related traffic agent. The observation indicates that the network will get more contextual information as time goes by and it will become more accurate in assigning the spatial attention.</p><p>According to Eq.(5), the temporal attention assigned to the hidden representation of frame t ? j, ? ? ? t?j , for j = 1, . . . , M , is a vector. This study calculates the mean value of this vector and uses it to visualize the temporal attention on this frame assigned at t. The third row of <ref type="figure" target="#fig_0">Fig. 3</ref> shows the frames overlaid with their averaged temporal attention weights assigned at frame #58. Frame #52 receives the highest attention whereas frame #50 receives the lowest attention. The fourth row further shows the frames overlaid with their averaged temporal attention weights obtained at frame #60. The change of attention weights from row three to row four illustrates the dynamic nature of the temporal attention. <ref type="figure" target="#fig_1">Fig. 4</ref> illustrates three examples of accident anticipation by the proposed DSTA-network. a) and b) are successful examples on analyzing a positive video and a negative video, respectively. c) is a false-negative example. For the illustration purpose, a threshold value of 0.5 is used to trigger the prediction of a future accident. In each example, sample frames of the video are shown on the top and the time series of the accident anticipation probability (i.e., the red colored curve) is displayed at the bottom. In a), the probability of anticipating an accident has reached the threshold at frame #54, yielding 1.75 seconds TTA. From the sample frames, it is clear that the network successfully identifies the risk of accident when a few motorcycles are coming to the direct lane of the car involved in the accident later on. In b), the network does not predict an accident because the probability is always below 50%. In c), the accident happens between a yellow car and a motorcycle, which are relatively far from the ego vehicle. The prediction score never exceeds the selected threshold value 0.5. The long-distance of the accident-involved agents from the ego vehicle might cause difficulty in extracting salient spatial-temporal relational dynamics of the two vehicles from their features. Therefore, the network missed predicting the accident ahead of time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance of the Fusion Method</head><p>The study evaluates the effectiveness of the proposed scorelevel late-fusion method on the DAD dataset. The top portion of TABLE IV lists four candidate models for fusion, which are efficient and may complement one and another. The first two candidate models are learnt by the DSTA-network, which only differ in model parameters. DSTA(I) is the model yielding the best AP (see <ref type="figure">Fig. 2</ref>) and DSTA(II) is the one achieving the best P 80R . The third candidate model is the best AP model of GCRNN <ref type="bibr" target="#b23">[23]</ref>, and its performance is attained by reproducing the model using the publicly available code 1 . The last candidate model is a modification of GCRNN, which is trained using the adaptive loss function of AdaLEA <ref type="bibr" target="#b9">[10]</ref>. Here, AdaLEA provides another degree of flexibility to explore a higher AP. Compared to GCRNN, GCRNN+AdaLEA attains higher AP and P 80R , but shorter mTTA and TTA 80R . The last two candidate models have their respective network architecture, different than the DSTA-network.</p><p>The bottom portion of <ref type="table" target="#tab_0">Table IV</ref> summarizes three selected fusion results. Fusing the predicted scores of DSTA(I) and GCRNN+AdaLEA achieves a remarkably high AP, 74.1%, with the corresponding mTTA 1.08 seconds. This result is achieved by selecting the prediction threshold 0.72 for DSTA(I), and 0.55 for GCRNN+AdaLEA. Compared to the first fusion result, the fusion of DSTA(I) with GCRNN slightly lowers the AP for 0.3% but extends the mTTA for 0.39 seconds. Fusing DSTA(I) and DSTA(II) effectively leverages the P 80R to 56.4%, equivalent to a margin of 9.8% compared to 1 https://github.com/Cogito2012/UString DSTA(I) and 3.3% compared to DSTA(II). It is noted that the fusion of DSTA(I) and DSTA(II) provides a practical solution to real-world applications. This fusion result dominates all of the four individual models on the mTTA-AP diagram. Meantime, it provides probably the most attractive combination of precision and TTA, at 80% recall. This fusion result further highlights the contribution of the proposed DSTA-network. The study found that the improvement from fusing more than two efficient models is marginal. I shows that performances of the accident anticipation models on the DAD dataset are significantly lower than on the CCD dataset. The literature has not revealed the underlying reasons for the observed dramatic difference. In attempt to reveal some of the unknowns, a thorough experimental study is conducted to evaluate the applicability and limitations of representative datasets for the early accident anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Impact of the Datasets</head><p>CCD is over 2.5 times larger than DAD. To determine if the performance gap is caused by the data size difference, a subset of CCD, which is in the same size as DAD and named as CCD-small, is created by randomly selecting videos from the original CCD dataset. Despite of its reduced size, the DSTAnetwork trained on CCD-small still achieves 98.51% AP and 4.83 seconds mTTA. This verifies that the larger size of CCD is not the main reason for the superior performance of early accident anticipation on CCD.</p><p>DAD and CCD datasets are dramatically different. Accidents in the DAD dataset always start from the last 0.5 seconds of positive videos, only 10% of the total video length. Most importantly, 90% of the positive videos in DAD contains extremely complex urban traffic accidents, where ego-vehicles are not involved in accidents, and traffic agents involved in, or affected by accidents, may appear in the video for a very short time in relative to the full length of the training videos. In the CCD dataset, 53.4% of the accidents are ego-vehicle involved. Changes in motion, size, and other appearance features in accident videos are relatively large. The starting time of accidents in CCD is placed randomly in the last 2 seconds, and the mean starting time is in the last 1.28 seconds. These differences are important reasons that accident anticipation models have largely different performance on the two datasets.</p><p>To determine if the above-discussed data differences would In these experiments, the frequency of DAD videos is reduced to 10 fps to be compatible with CCD. The first two experiments in this table are cross-testing. Both of the tests achieve a low AP value, near 36%. The dramatically reduced AP implies a challenge for the early accident anticipation arising from differences in traffic scenes and accident types. The network is further trained on the dataset that mixes the CCD-small and the DAD training dataset. This model performs better than in the cross-testing experiments, achieving 98.89% AP and 4.48 secconds mTTA when tested on the CCD dataset, and 69.67% AP and 1.40 sec mTTA on DAD. But the performances are below those in TABLE I. This experimental study verifies that the data foundation for the early accident anticipation is crucial. A comprehensive dataset is desired, which has balanced ego-vehicle involved and uninvolved accidents and diverse scene configurations from different regions and countries.</p><p>V. CONCLUSION This paper presented a novel end-to-end DSTA-network for the early anticipation of traffic accidents from widely deployed dashcams. Through designing a new model fusion method and analyzing existing datasets, this paper identifies opportunities for further advancing the early accident anticipation.</p><p>Computer vision has been playing vital roles in traffic safety enhancement and autonomous driving. It can augment or substitute for human vision in the transportation system. Just like human vision that is one, but not the only, sensing and learning mechanism, computer-vision based early accident anticipation should be further integrated with other safetyenhancement technologies and methods such as LiDAR, internet of things, and transportation safety surrogate measurements (SSM) to deliver a multimodal, multifunctional system for the early accident anticipation. The convergence of these technologies with the proposed method is an important direction that will bring exciting opportunities for safety enhancement. This study has envisaged multiple lines of future work. The proposed DSTA-network has a common limitation as many other deep learning models, which is the unexplainability of the rationale behind the network's decision-making process. A follow-up study is to reveal the risk perception mechanism of the network to make it explainable and trustworthy to humans. In transportation, various surrogate safety measurements are effectively used to identify accident risks. An exciting future direction is to develop sensor-fusion based, SSM-informed deep learning networks for safety enhancement. This study also evidences the need for a strong data foundation for the accident anticipation. Finally, accident anticipation is also a desired capability for mobile robots such as drones and remotely operated underwater vehicles. The application of the DSTA-network to those robots faces additional challenges. This paper lays a foundation for exploring the above-discussed exciting opportunities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Illustrating the dynamics of the spatial and temporal attentions of the DSTA-network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of accident anticipation on the DAD dataset: a) a true-positive sample, b) a true-negative sample, and c) a false-negative sample. For better visualization, white bounding boxes with a shade are added to the top two attended objects. The red curve indicates the prediction probability at each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF MODELS SEEKING LONGER MTTA ON DAD AND CCD</figDesc><table><row><cell>Dataset</cell><cell>Year</cell><cell>Model</cell><cell>AP(%)</cell><cell>mTTA(s)</cell></row><row><cell></cell><cell>2016</cell><cell>DSA [8]</cell><cell>48.1</cell><cell>1.34</cell></row><row><cell></cell><cell>2017</cell><cell>L-RAI [9]</cell><cell>51.4</cell><cell>3.01</cell></row><row><cell>DAD</cell><cell>2018</cell><cell>adaLEA [10]</cell><cell>52.3</cell><cell>3.43</cell></row><row><cell></cell><cell>2020</cell><cell>GCRNN [23]</cell><cell>53.7</cell><cell>3.53</cell></row><row><cell></cell><cell>2021</cell><cell>DSTA (Ours)</cell><cell>56.1</cell><cell>3.66</cell></row><row><cell></cell><cell>2016</cell><cell>DSA [8]</cell><cell>99.6</cell><cell>4.52</cell></row><row><cell>CCD</cell><cell>2020</cell><cell>GCRNN [23]</cell><cell>99.5</cell><cell>4.74</cell></row><row><cell></cell><cell>2021</cell><cell>DSTA (Ours)</cell><cell>99.6</cell><cell>4.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>An ablation study comprising 10 experiments is performed to assess the architecture of the proposed DSTA-network. A decrease in the model's performance due to the removal or replacement of a key component measures the component's contribution. TABLE III summarizes the best AP and corresponding mTTA attained in each of the experiments.</figDesc><table><row><cell>3) Ablation Study:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">OF BEST AP MODELS ON DAD</cell></row><row><cell>Model</cell><cell>AP(%)</cell><cell>mTTA(s)</cell><cell>TTA 80R (s)</cell></row><row><cell>GCRNN [23]</cell><cell>72.22</cell><cell>1.33</cell><cell>1.56</cell></row><row><cell>DSTA (Ours)</cell><cell>72.34</cell><cell>1.50</cell><cell>1.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">STUDY ON THE DAD DATASET</cell><cell></cell></row><row><cell>Experiment</cell><cell>RNN</cell><cell>DSA</cell><cell>DTA</cell><cell>TSAA</cell><cell>AP (%)</cell><cell>mTTA (s)</cell></row><row><cell>1</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>72.34</cell><cell>1.50</cell></row><row><cell>2</cell><cell>GRU</cell><cell></cell><cell>(M =1)</cell><cell></cell><cell>70.74</cell><cell>1.36</cell></row><row><cell>3</cell><cell>GCRNN</cell><cell></cell><cell></cell><cell></cell><cell>70.47</cell><cell>1.22</cell></row><row><cell>4</cell><cell>LSTM</cell><cell></cell><cell></cell><cell></cell><cell>69.08</cell><cell>1.48</cell></row><row><cell>5</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>69.89</cell><cell>1.70</cell></row><row><cell>6</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>69.72</cell><cell>1.43</cell></row><row><cell>7</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>68.15</cell><cell>1.33</cell></row><row><cell>8</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>66.04</cell><cell>1.39</cell></row><row><cell>9</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>65.99</cell><cell>1.42</cell></row><row><cell>10</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell>65.03</cell><cell>1.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">ASSESSMENT OF THE FUSION METHOD</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model-1</cell><cell cols="3">Model-2? (1)?(2)</cell><cell>AP (%)</cell><cell>P 80R (%)</cell><cell>mTTA (s)</cell><cell>TTA 80R (s)</cell></row><row><cell></cell><cell>DSTA (I)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.3</cell><cell>46.6</cell><cell>1.50</cell><cell>1.81</cell></row><row><cell>Individual</cell><cell>DSTA (II) GCRNN</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>68.6 68.3</cell><cell>53.1 50.8</cell><cell>1.50 1.31</cell><cell>1.69 1.58</cell></row><row><cell></cell><cell>GCRNN+AdaLEA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.4</cell><cell>50.6</cell><cell>1.02</cell><cell>1.18</cell></row><row><cell></cell><cell>DSTA (I)</cell><cell>GCRNN+AdaLEA</cell><cell>0.72</cell><cell>0.55</cell><cell>74.1</cell><cell>53.1</cell><cell>1.08</cell><cell>1.18</cell></row><row><cell>Fusion</cell><cell>DSTA (I)</cell><cell>GCRNN</cell><cell>0.40</cell><cell>0.20</cell><cell>73.8</cell><cell>48.9</cell><cell>1.47</cell><cell>1.60</cell></row><row><cell></cell><cell>DSTA (I)</cell><cell>DSTA (II)</cell><cell>0.42</cell><cell>0.50</cell><cell>72.6</cell><cell>56.4</cell><cell>1.50</cell><cell>1.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V THE</head><label>V</label><figDesc></figDesc><table><row><cell cols="4">IMPACT OF DATASET DIFFERENCES</cell></row><row><cell>Training</cell><cell>Testing</cell><cell>AP(%)</cell><cell>mTTA (s)</cell></row><row><cell>DAD</cell><cell>CCD</cell><cell>35.99</cell><cell>4.98</cell></row><row><cell>CCD</cell><cell>DAD</cell><cell>35.74</cell><cell>4.99</cell></row><row><cell>CCD-small?DAD</cell><cell>CCD</cell><cell>98.89</cell><cell>4.48</cell></row><row><cell>CCD-small?DAD</cell><cell>DAD</cell><cell>69.67</cell><cell>1.40</cell></row></table><note>impact the generalization capability of the proposed DSTA- network, four experiments listed in TABLE V are conducted.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT Qin, Karim, and Li receive support from National Science Foundation (NSF) through the grant ECCS-#2026357. Yin and Karim receive support from NSF through ECCS-#2025929.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Research advances and challenges of autonomous and connected ground vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eskandarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for safe autonomous driving: Current challenges and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Del</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H C</forename><surname>De Albuquerque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Autonomous vehicle collision reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmv</forename><surname>California</surname></persName>
		</author>
		<ptr target="https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/autonomous-vehicle-collision-reports/" />
		<imprint>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Global status report on road safety 2018: Summary</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crash report data analysis for creating scenario-wise, spatio-temporal attention guidance to support computer vision-based perception of fatal crash risks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accident Analysis &amp; Prevention</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">105962</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Critical reasons for crashes investigated in the national motor vehicle crash causation survey</title>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>NHTSA ; US Department of Transportation, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computer vision for autonomous vehicles: Problems, datasets and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="1" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anticipating accidents in dashcam videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Agent-centric risk assessment: Accident anticipation and risky region localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anticipating traffic accidents with adaptive loss and large-scale incident db</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3521" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global feature aggregation for accident anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fatima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U K</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Kyung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2809" to="2816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="29" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>M?nguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fern?ndez-Llorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">?</forename><surname>Sotelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1803" to="1814" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal recurrent networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5532" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Peeking into the future: Predicting future person activities and locations in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5725" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encouraging LSTMs to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised traffic accident detection in first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Atkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="273" to="280" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting traffic accidents with event recorder data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility</title>
		<meeting>the 3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning activity progression in LSTMs for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A dynamic spatial-temporal attention-based gru model with healthy features for state-of-health estimation of lithiumion batteries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Joe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="27" to="374" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty-based traffic accident anticipation with spatio-temporal relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2682" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pareto optimality in multiobjective problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Censor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Optimization</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
