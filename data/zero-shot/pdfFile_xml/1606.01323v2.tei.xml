<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Coreference Resolution by Learning Entity-Level Distributed Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
							<email>kevclark@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Coreference Resolution by Learning Entity-Level Distributed Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A long-standing challenge in coreference resolution has been the incorporation of entity-level information -features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features. arXiv:1606.01323v2 [cs.CL] 8 Jun 2016 does not use entity-level information. We also show that using an easy-first strategy improves the performance of the cluster-ranking model. Our final system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese, substantially outperforming other state-of-the-art systems. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>Our cluster-ranking model is a single neural network that learns which coreference cluster merges are desirable. However, it is helpful to think of the network as being composed of distinct subnetworks. The mention-pair encoder produces distributed representations for pairs of mentions by passing relevant features through a feedforward neural network. The cluster-pair encoder produces distributed representations for pairs of clusters by applying a pooling operation over the representations of relevant mention pairs, i.e., pairs where one mention is in each cluster. The clusterranking model then scores pairs of clusters by passing their representations through a single neural network layer.</p><p>We also train a mention-ranking model that scores pairs of mentions by passing their representations through a single neural network layer. Its parameters are used to initialize the clusterranking model, and the scores it produces are used to prune which candidate cluster merges the cluster-ranking model considers, allowing the cluster-ranking model to run much faster. The system architecture is summarized in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention-Pair Encoder</head><p>Cluster-Pair Encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster-Ranking Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention-Ranking Model</head><p>Pretraining, Search space pruning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution, the task of identifying which mentions in a text refer to the same realworld entity, is fundamentally a clustering problem. However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together <ref type="bibr" target="#b23">Martschat and Strube, 2015;</ref><ref type="bibr" target="#b1">Wiseman et al., 2015)</ref>.</p><p>An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity. Such systems can take advantage of entity-level information, i.e., features between clusters of mentions instead of between just two mentions. As an example for why this is useful, it is clear that the clusters {Bill Clinton} and {Clinton, she} are not referring to the same entity, but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent.</p><p>Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters <ref type="bibr" target="#b31">(Raghunathan et al., 2010;</ref>. In this work, we instead train a deep neural network to build distributed representations of pairs of coreference clusters. This captures entity-level information with a large number of learned, continuous features instead of a small number of hand-crafted categorical ones.</p><p>Using the cluster-pair representations, our network learns when combining two coreference clusters is desirable. At test time it builds up coreference clusters incrementally, starting with each mention in its own cluster and then merging a pair of clusters each step. It makes these decisions with a novel easy-first cluster-ranking procedure that combines the strengths of cluster-ranking (Rahman and <ref type="bibr" target="#b32">Ng, 2011)</ref> and easy-first <ref type="bibr" target="#b34">(Stoyanov and Eisner, 2012)</ref> coreference algorithms.</p><p>Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made. We address this by using a learning-to-search algorithm inspired by SEARN <ref type="bibr" target="#b12">(Daum? III et al., 2009)</ref> to train our neural network. This approach allows the model to learn which action (a cluster merge) available from the current state (a partially completed coreference clustering) will eventually lead to a high-scoring coreference partition.</p><p>Our system uses little manual feature engineering, which means it is easily extended to multiple languages. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset. The cluster-ranking model significantly outperforms a mention-ranking model that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Building Representations</head><p>In this section, we describe the neural networks producing distributed representations of pairs of  mentions and pairs of coreference clusters. We assume that a set of mentions has already been extracted from each document using a method such as the one in <ref type="bibr" target="#b31">Raghunathan et al. (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mention-Pair Encoder</head><p>Given a mention m and candidate antecedent a, the mention-pair encoder produces a distributed representation of the pair r m (a, m) ? R d with a feedforward neural network, which is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent. We also experimented with models based on Long Short-Term Memory recurrent neural networks (Hochreiter and Schmidhuber, 1997), but found these to perform slightly worse when used in an end-to-end coreference system due to heavy overfitting to the training data.</p><p>Input Layer. For each mention, the model extracts various words and groups of words that are fed into the neural network. Each word is represented by a vector w i ? R dw . Each group of words is represented by the average of the vectors of each word in the group. For each mention and pair of mentions, a small number of binary features and distance features are also extracted. Distances and mention lengths are binned into one of the buckets <ref type="bibr">[0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]</ref> and then encoded in a one-hot vector in addition to being included as continuous features. The full set of features is as follows:</p><p>Embedding Features: Word embeddings of the head word, dependency parent, first word, last word, two preceding words, and two following words of the mention. Averaged word embeddings of the five preceding words, five following words, all words in the mention, all words in the mention's sentence, and all words in the mention's document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Mention Features:</head><p>The type of the mention <ref type="bibr">(pronoun, nominal, proper, or list)</ref>, the mention's position (index of the mention divided by the number of mentions in the document), whether the mentions is contained in another mention, and the length of the mention in words.</p><p>Document Genre: The genre of the mention's document (broadcast news, newswire, web data, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance Features:</head><p>The distance between the mentions in sentences, the distance between the mentions in intervening mentions, and whether the mentions overlap.</p><p>Speaker Features: Whether the mentions have the same speaker and whether one mention is the other mention's speaker as determined by string matching rules from <ref type="bibr" target="#b31">Raghunathan et al. (2010)</ref>.</p><p>String Matching Features: Head match, exact string match, and partial string match.</p><p>The vectors for all of these features are concatenated to produce an I-dimensional vector h 0 , the input to the neural network. If a = NA, the features defined over mention pairs are not included. For this case, we train a separate network with an identical architecture to the pair network except for the input layer to produce anaphoricity scores.</p><p>Our set of hand-engineered features is much smaller than the dozens of complex features typically used in coreference systems. However, we found these features were crucial for getting good model performance. See Section 6.1 for a feature ablation study.</p><p>Hidden Layers. The input gets passed through three hidden layers of rectified linear (ReLU) units <ref type="bibr" target="#b26">(Nair and Hinton, 2010)</ref>. Each unit in a hidden layer is fully connected to the previous layer:</p><formula xml:id="formula_0">h i (a, m) = max(0, W i h i?1 (a, m) + b i ) where W 1 is a M 1 ? I weight matrix, W 2 is a M 2 ? M 1 matrix, and W 3 is a d ? M 2 matrix.</formula><p>The output of the last hidden layer is the vector representation for the mention pair: r m (a, m) = h 3 (a, m). <ref type="figure">Figure 3</ref>: Cluster-pair encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster-Pair Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention-Pair Representations</head><formula xml:id="formula_1">Pooling ! ! ! c 2 c 1 Mention-Pair Encoder ! ! ! ! ! ! r c (c 1 , c 2 ) R m (c 1 , c 2 ) ! ! !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cluster-Pair Encoder</head><p>Given two clusters of mentions c i = {m i 1 , m i 2 , ..., m i |c i | } and c j = {m j 1 , m j 2 , ..., m j |c j | }, the cluster-pair encoder produces a distributed representation r c (c i , c j ) ? R 2d . The architecture of the encoder is summarized in <ref type="figure">Figure 3</ref>.</p><p>The cluster-pair encoder first combines the information contained in the matrix of</p><formula xml:id="formula_2">mention-pair representations R m (c i , c j ) = [r m (m i 1 , m j 1 ), r m (m i 1 , m j 2 ), ..., r m (m i |c i | , m j |c j | )] to produce r c (c i , c j )</formula><p>. This is done by applying a pooling operation. In particular it concatenates the results of max-pooling and average-pooling, which we found to be slightly more effective than using either one alone:</p><formula xml:id="formula_3">r c (c i , c j ) k = max {R m (c i , c j ) k,? } for 0 ? k &lt; d avg {R m (c i , c j ) k?d,? } for d ? k &lt; 2d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mention-Ranking Model</head><p>Rather than training a cluster-ranking model from scratch, we first train a mention-ranking model that assigns each mention its highest scoring candidate antecedent. There are two key advantages of doing this. First, it serves as pretraining for the cluster-ranking model; in particular the mentionranking model learns effective weights for the mention-pair encoder. Second, the scores produced by the mention-ranking model are used to provide a measure of which coreference decisions are easy (allowing for an easy-first clustering strategy) and which decisions are clearly wrong (these decisions can be pruned away, significantly reducing the search space of the cluster-ranking model).</p><p>The mention-ranking model assigns a score s m (a, m) to a mention m and candidate an-tecedent a representing their compatibility for coreference. This is produced by applying a single fully connected layer of size one to the representation r m (a, m) produced by the mention-pair encoder:</p><formula xml:id="formula_4">s m (a, m) = W m r m (a, m) + b m where W m is a 1 ? d weight matrix. At test time,</formula><p>the mention-ranking model links each mention with its highest scoring candidate antecedent.</p><p>Training Objective. We train the mentionranking model with the slack-rescaled maxmargin training objective from <ref type="bibr" target="#b1">Wiseman et al. (2015)</ref>, which encourages separation between the highest scoring true and false antecedents of the current mention. Suppose the training set consists of N mentions m 1 , m 2 , ..., m N . Let A(m i ) denote the set of candidate antecedents of a mention m i (i.e., mentions preceding m i and NA), and T (m i ) denote the set of true antecedents of m i (i.e., mentions preceding m i that are coreferent with it or {NA} if m i has no antecedent). Lett i be the highest scoring true antecedent of mention m i :t</p><formula xml:id="formula_5">i = argmax t?T (m i ) s m (t, m i )</formula><p>Then the loss is given by</p><formula xml:id="formula_6">N i=1 max a?A(m i ) ?(a, m i )(1 + s m (a, m i ) ? s m (t i , m i ))</formula><p>where ?(a, m i ) is the mistake-specific cost function</p><formula xml:id="formula_7">?(a, m i ) = ? ? ? ? ? ? ? ? ? ? ? ? FN if a = NA ? T (m i ) = {NA} ? FA if a = NA ? T (m i ) = {NA} ? WL if a = NA ? a / ? T (m i ) 0 if a ? T (m i )</formula><p>for "false new," "false anaphoric," "wrong link," and correct coreference decisions. The different error penalties allow the system to be tuned for coreference evaluation metrics by biasing it towards making more or fewer coreference links.</p><p>Finding Effective Error Penalties. We fix ? WL = 1.0 and search for ? FA and ? FN out of {0.1, 0.2, ..., 1.5} with a variant of grid search. Each new trial uses the unexplored set of hyperparameters that has the closest Manhattan  <ref type="bibr" target="#b17">(Hinton and Tieleman, 2012)</ref>. To regularize the network, we applied L2 regularization to the model weights and dropout  with a rate of 0.5 on the word embeddings and the output of each hidden layer.</p><p>Pretraining. As in <ref type="bibr" target="#b1">Wiseman et al. (2015)</ref>, we found that pretraining is crucial for the mentionranking model's success. We pretrained the network in two stages, minimizing the following objectives from Clark and Manning <ref type="formula">(2015)</ref>:</p><formula xml:id="formula_8">All-Pairs Classification ? N i=1 [ t?T (m i ) log p(t, m i ) + f ?F (m i ) log(1 ? p(f, m i ))] Top-Pairs Classification ? N i=1 [ max t?T (m i ) log p(t, m i ) + min f ?F (m i ) log(1 ? p(f, m i ))]</formula><p>Where F(m i ) is the set of false antecedents for m i and p(a, m i ) = sigmoid(s(a, m i )). The top-pairs objective is a middle ground between the all-pairs classification and mention ranking objectives: it only processes high-scoring mentions, but is probabilistic rather than max-margin. We first pretrained the network with all-pairs classification for 150 epochs and then with top-pairs classification for 50 epochs. See Section 6.1 for experiments on the two-stage pretraining.</p><p>Although a strong coreference system on its own, the mention-ranking model has the disadvantage of only considering local information between pairs of mentions, so it cannot consolidate information at the entity-level. We address this problem by training a cluster-ranking model that scores pairs of clusters instead of pairs of mentions. Given two clusters of mentions c i and c j , the cluster-ranking model produces a score s c (c i , c j ) representing their compatibility for coreference. This is produced by applying a single fully connected layer of size one to the representation r c (c i , c j ) produced by the cluster-pair encoder:</p><formula xml:id="formula_9">s c (c i , c j ) = W c r c (c i , c j ) + b c</formula><p>where W c is a 1 ? 2d weight matrix. Our cluster-ranking approach also uses a measure of anaphoricity, or how likely it is for a mention m to have an antecedent. This is defined as</p><formula xml:id="formula_10">s NA (m) = W NA r m (NA, m) + b NA where W NA is a 1 ? d matrix.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cluster-Ranking Policy Network</head><p>At test time, the cluster ranker iterates through every mention in the document, merging the current mention's cluster with a preceding one or performing no action. We view this procedure as a sequential decision process where at each step the algorithm observes the current state x and performs some action u.</p><p>Specifically, we define a state x = (C, m) to consist of C = {c 1 , c 2 , ...}, the set of existing coreference clusters, and m, the current mention being considered. At a start state, each cluster in C contains a single mention. Let c m ? C be the cluster containing m and A(m) be a set of candidate antecedents for m: mentions occurring previously in the document. Then the available actions</p><formula xml:id="formula_11">U (x) from x are ? MERGE[c m , c],</formula><p>where c is a cluster containing a mention in A(m). This combines c m and c into a single coreference cluster.</p><p>? PASS. This leaves the clustering unchanged.</p><p>After determining the new clustering C based on the existing clustering C and action u, we consider another mention m to get the next state x = (C , m ).</p><p>Using the scoring functions s c and s NA , we define a policy network ? that assigns a probability distribution over U (x) as follows:</p><formula xml:id="formula_12">?(MERGE[c m , c]|x) ? e sc(cm,c) ?(PASS|x) ? e sNA(m)</formula><p>During inference, ? is executed by taking the highest-scoring (most probable) action at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Easy-First Cluster Ranking</head><p>The last detail needed is the ordering in which to consider mentions. Cluster-ranking models in prior work order the mentions according to their positions in the document, processing them leftto-right <ref type="bibr" target="#b32">(Rahman and Ng, 2011;</ref><ref type="bibr" target="#b22">Ma et al., 2014)</ref>. However, we instead sort the mentions in descending order by their highest scoring candidate coreference link according to the mention-ranking model. This causes inference to occur in an easyfirst fashion where hard decisions are delayed until more information is available. Easy-first orderings have been shown to improve the performance of other incremental coreference strategies <ref type="bibr" target="#b31">(Raghunathan et al., 2010;</ref><ref type="bibr" target="#b34">Stoyanov and Eisner, 2012)</ref> because they reduce the problem of errors compounding as the algorithm runs.</p><p>We also find it beneficial to prune the set of candidate antecedents A(m) for each mention m. Rather than using all previously occurring mentions as candidate antecedents, we only include high-scoring ones, which greatly reduces the size of the search space. This allows for much faster learning and inference; we are able to remove over 95% of candidate actions with no decrease in the model's performance. For both of these two preprocessing steps, we use s(a, m) ? s(NA, m) as the score of a coreference link between a and m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deep Learning to Search</head><p>We face a sequential prediction problem where future observations (visited states) depend on previous actions. This is challenging because it violates the common i.i.d. assumption made in machine learning. Learning-to-search algorithms are effective for this sort of problem, and have been applied successfully to coreference resolution <ref type="bibr" target="#b11">(Daum? III and Marcu, 2005;</ref><ref type="bibr" target="#b10">Clark and Manning, 2015)</ref> as well as other structured prediction tasks in natural language processing <ref type="bibr" target="#b13">(Daum? III et al., 2014;</ref><ref type="bibr"></ref> Algorithm 1 Deep Learning to Search for i = 1 to num epochs do Initialize the current training set ? = ? for each example (x, y) ? D do Run the policy ? to completion from start state x to obtain a trajectory of states {x 1 , x 2 , ..., x n } for each state x i in the trajectory do for each possible action u ? U (x i ) do Execute u on x i and then run the reference policy ? ref until reaching an end state e Assign u a cost by computing the loss on the end state: l(u) = L(e, y) end for Add the state x i and associated costs l to ? end for end for Update ? with gradient descent, minimizing (x,l)?? u?U (x) ?(u|x)l(u) end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chang et al., 2015a).</head><p>We train the cluster-ranking model using a learning-to-search algorithm inspired by SEARN <ref type="bibr" target="#b12">(Daum? III et al., 2009)</ref>, which is described in Algorithm 1. The algorithm takes as input a dataset D of start states x (in our case documents with each mention in its own singleton coreference cluster) and structured labels y (in our case gold coreference clusters). Its goal is to train the policy ? so when it executes from x, reaching a final state e, the resulting loss L(e, y) is small. We use the negative of the B 3 coreference metric for this loss <ref type="bibr" target="#b4">(Bagga and Baldwin, 1998</ref>). Although our system evaluation also includes the MUC <ref type="bibr" target="#b35">(Vilain et al., 1995)</ref> and CEAF ? 4 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF ? 4 is slow to compute.</p><p>For each example (x, y) ? D, the algorithm obtains a trajectory of states x 1 , x 2 , ..., x n visited by the current policy by running it to completion (i.e., repeatedly taking the highest scoring action until reaching an end state) from the start state x. This exposes the model to states at train time similar to the ones it will face at test time, allowing it to learn how to cope with mistakes.</p><p>Given a state x in a trajectory, the algorithm then assigns a cost l(u) to each action u ? U (x) by executing the action, "rolling out" from the resulting state with a reference policy ? ref until reaching an end state e, and computing the resulting loss L(e, y). This rolling out procedure allows the model to learn how a local action will affect the final score, which cannot be otherwise computed because coreference evaluation metrics do not de-compose over cluster merges. The policy network is then trained to minimize the risk associated with taking each action: u?U (x) ?(u|x)l(u).</p><p>Reference policies typically refer to the gold labels to find actions that are likely to be beneficial. Our reference policy ? ref takes the action that increases the B 3 score the most each step, breaking ties randomly. It is generally recommended to use a stochastic mixture of the reference policy and the current learned policy during rollouts when the reference policy is not optimal <ref type="bibr">(Chang et al., 2015b)</ref>. However, we find only using the reference policy (which is close to optimal) to be much more efficient because it does not require neural network computations and is deterministic, which means the costs of actions can be cached.</p><p>Training details. We update ? using RMSProp and apply dropout with a rate of 0.5 to the input layer. For most experiments, we initialize the mention-pair encoder component of the clusterranking model with the learned weights from the mention-ranking model, which we find to greatly improve performance (see Section 6.2).</p><p>Runtime. The full cluster-ranking system runs end-to-end in slightly under 1 second per document on the English test set when using a GPU (including scoring all pairs of mentions with the mention-ranking model for search-space pruning). This means the bottleneck for the overall system is the syntactic parsing required for mention detection (about 4 seconds per document on the English test set).  <ref type="bibr" target="#b30">(Pradhan et al., 2012)</ref>. The models are evaluated using three of the most popular coreference metrics: MUC, B 3 , and Entity-based CEAF (CEAF ? 4 ). We generally report the average F 1 score (CoNLL F 1 ) of the three, which is common practice in coreference evaluation. We used the most recent version of the CoNLL scorer (version 8.01), which implements the original definitions of the metrics.</p><p>Mention Detection. Our experiments were run using system-produced predicted mentions. We used the rule-based mention detection algorithm from <ref type="bibr" target="#b31">Raghunathan et al. (2010)</ref>, which first extracts pronouns and maximal NP projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities and pleonastic it pronouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Mention-Ranking Model Experiments</head><p>Feature Ablations. We performed a feature ablation study to determine the importance of the hand-engineered features included in our model. The results are shown in <ref type="table">Table 1</ref>. We find the small number of non-embedding features substantially improves model performance, especially the distance and string matching features. This is unsurprising, as the additional features are not easily captured by word embeddings and historically such features have been very important in coreference resolvers <ref type="bibr" target="#b5">(Bengtson and Roth, 2008)</ref>.</p><p>The Importance of Pretraining. We evaluate the benefit of the two-step pretraining for the    <ref type="bibr" target="#b1">Wiseman et al. (2015)</ref>, we find pretraining to greatly improve the model's accuracy. We note in particular that the model benefits from using both pretraining steps from Section 4, which more smoothly transitions the model from a mention-pair classification objective that is easy to optimize to a max-margin objective better suited for a ranking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Cluster-Ranking Model Experiments</head><p>We evaluate the importance of three key details of the cluster ranker: initializing it with the mentionranking model's weights, using an easy-first ordering of mentions, and using learning to search. The results are shown in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>Pretrained Weights. We compare initializing the cluster-ranking model randomly with initializing it with the weights learned by the mentionranking model. Using pretrained weights greatly improves performance. We believe the clusterranking model has difficulty learning effective weights from scratch due to noise in the signal coming from cluster-level decisions (an overall bad cluster merge may still involve a few cor-rect pairwise links) and the smaller amount of data used to train the cluster-ranking model (many possible actions are pruned away during preprocessing). We believe the score would be even lower without search-space pruning, which stops the model from considering many bad actions.</p><p>Easy-First Cluster Ranking. We compare the effectiveness of easy-first cluster-ranking with the commonly used left-to-right approach. Using a left-to-right strategy simply requires changing the preprocessing step ordering the mentions so mentions are sorted by their position in the document instead of their highest scoring coreference link according to the mention-ranking model. We find the easy-first approach slightly outperforms using a left-to-right ordering of mentions. We believe this is because delaying hard decisions until later reduces the problem of early mistakes causing later decisions to be made incorrectly.</p><p>Learning to Search. We also compare learning to search with the simpler approach of training the model on a trajectory of gold coreference decisions (i.e., training on a fixed cost-sensitive classification dataset). Using this approach significantly decreases performance. We attribute this to the model not learning how to deal with mistakes when it only sees correct decisions during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Capturing Semantic Similarity</head><p>Using semantic information to improve coreference accuracy has had mixed in results in previous research, and has been called an "uphill battle" in coreference resolution . However, word embeddings are well known for being effective at capturing semantic relatedness, and we show here that neural network coreference models can take advantage of this. Perhaps the case where semantic similarity is most important is in linking nominals with no head match (e.g., "the nation" and "the country"). We compare the performance of our neural network model with our earlier statistical system <ref type="bibr" target="#b10">(Clark and Manning, 2015)</ref> at classifying mention pairs of this type as being coreferent or not. The neural network shows substantial improvement (18.9 F 1 vs. 10.7 F 1 ) on this task compared to the more modest improvement it gets at classifying any pair of mentions as coreferent (68.7 F 1 vs. 66.1 F 1 ). Some example wins are shown in <ref type="table">Table 4</ref>. These types of coreference links are quite rare in the CoNLL data (about 1.2% of the positive coref-Antecedent Anaphor the country's leftist rebels the guerrillas the company the New York firm the suicide bombing the attack the gun the rifle the U.S. carrier the ship <ref type="table">Table 4</ref>: Examples of nominal coreferences with no head match that the neural model gets correct, but the system from <ref type="bibr" target="#b10">Clark and Manning (2015)</ref> gets incorrect.</p><p>erence links in the test set), so the improvement does not significantly contribute to the final system's score, but it does suggest progress on this difficult type of coreference problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Final System Performance</head><p>In <ref type="table" target="#tab_7">Table 5</ref> we compare the results of our system with state-of-the-art approaches for English and Chinese. Our mention-ranking model surpasses all previous systems. We attribute its improvement over the neural mention ranker from  to our model using a deeper neural network, pretrained word embeddings, and more sophisticated pretraining.</p><p>The cluster-ranking model improves results further across both languages and all evaluation metrics, demonstrating the utility of incorporating entity-level information. The improvement is largest in CEAF ? 4 , which is encouraging because CEAF ? 4 is the most recently proposed metric, designed to correct flaws in the other two <ref type="bibr" target="#b21">(Luo, 2005)</ref>. We believe entity-level information is particularly useful for preventing bad merges between large clusters (see <ref type="figure">Figure 4</ref> for an example). However, it is worth noting that in practice the much more complicated cluster-ranking model brings only fairly modest gains in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>There has been extensive work on machine learning approaches to coreference resolution <ref type="bibr">(Soon et al., 2001;</ref><ref type="bibr" target="#b27">Ng and Cardie, 2002)</ref>, with mentionranking models being particularly popular <ref type="bibr" target="#b13">(Denis and Baldridge, 2007;</ref><ref type="bibr" target="#b23">Martschat and Strube, 2015)</ref>.</p><p>We train a neural mention-ranking model inspired by <ref type="bibr" target="#b1">Wiseman et al. (2015)</ref> as a starting point, but then use it to pretrain a cluster-ranking model that benefits from entity-level information. Wise-   <ref type="figure">Figure 4</ref>: Thanks to entity-level information, the cluster-ranking model correctly declines to merge these two large clusters when running on the test set. However, the mention-ranking model incorrectly links the Russian President and President Clinton's, which greatly reduces the final precision score. man et al. <ref type="formula">(2016)</ref> extend their mention-ranking model by incorporating entity-level information produced by a recurrent neural network running over the candidate antecedent-cluster. However, this is an augmentation to a mention-ranking model, and not fundamentally a clustering model as our cluster ranker is. Entity-level information has also been incorporated in coreference systems using joint inference <ref type="bibr" target="#b24">(McCallum and Wellner, 2003;</ref><ref type="bibr" target="#b29">Poon and Domingos, 2008;</ref><ref type="bibr" target="#b16">Haghighi and Klein, 2010)</ref> and systems that build up coreference clusters incrementally <ref type="bibr" target="#b20">(Luo et al., 2004;</ref><ref type="bibr" target="#b38">Yang et al., 2008;</ref><ref type="bibr" target="#b31">Raghunathan et al., 2010)</ref>. We take the latter approach, and in particular combine the cluster-ranking (Rahman and <ref type="bibr" target="#b32">Ng, 2011;</ref><ref type="bibr" target="#b22">Ma et al., 2014)</ref> and easy-first <ref type="bibr" target="#b34">(Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b10">Clark and Manning, 2015)</ref> clustering strategies. These prior systems all express entity-level information in the form of hand-engineered features and constraints instead of entity-level distributed representations that are learned from data. We train our system using a learning-to-search algorithm similar to SEARN <ref type="bibr" target="#b12">(Daum? III et al., 2009)</ref>. Learning-to-search style algorithms have been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by <ref type="bibr" target="#b11">Daum? et al. (2005)</ref>, <ref type="bibr" target="#b22">Ma et al. (2014)</ref>, and <ref type="bibr" target="#b10">Clark and Manning (2015)</ref>. Other works use structured perceptron models for the same purpose <ref type="bibr" target="#b34">(Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b15">Fernandes et al., 2012;</ref><ref type="bibr" target="#b2">Bj?rkelund and Kuhn, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a coreference system that captures entity-level information with distributed representations of coreference cluster pairs. These learned, dense, high-dimensional feature vectors provide our cluster-ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones. The model is trained with a learning-to-search algorithm that allows it to learn how local decisions will affect the final coreference score. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task and report a substantial improvement over the current state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System architecture. Solid arrows indicate one neural network is used as a component of the other; the dashed arrow indicates other dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Mention-pair encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>distance to the best setting found so far on the dev set. We stopped the search when all immediate neighbors (within 0.1 distance) of the best setting had been explored. We found (? FN , ? FA , ? WL ) = (0.8, 0.4, 1.0) to be best for English and (? FN , ? FA , ? WL ) = (0.7, 0.4, 1.0) to be best for Chinese on the CoNLL 2012 data. We attribute our smaller false new cost from the one used by Wiseman et al. (they set ? FN = 1.2) to using more precise mention detection, which results in fewer links to NA.</figDesc><table><row><cell>Training Details. We initialized our word em-</cell></row><row><cell>beddings with 50 dimensional ones produced by</cell></row><row><cell>word2vec (Mikolov et al., 2013) on the Giga-</cell></row><row><cell>word corpus for English and 64 dimensional ones</cell></row><row><cell>provided by Polyglot (Al-Rfou et al., 2013) for</cell></row><row><cell>Chinese. Averaged word embeddings were held</cell></row><row><cell>fixed during training while the embeddings used</cell></row><row><cell>for single words were updated. We set our hid-</cell></row><row><cell>den layer sizes to M 1 = 1000, M 2 = d = 500</cell></row><row><cell>and minimized the training objective using RMS-</cell></row><row><cell>Prop</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: CoNLL F 1 scores of the mention-ranking</cell></row><row><cell cols="3">model on the dev sets with different pretraining</cell></row><row><cell>methods.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">English F 1 Chinese F 1</cell></row><row><cell>Full Model</cell><cell>66.01</cell><cell>64.86</cell></row><row><cell>-PRETRAINING</cell><cell>-5.01</cell><cell>-6.85</cell></row><row><cell>-EASY-FIRST</cell><cell>-0.15</cell><cell>-0.12</cell></row><row><cell>-L2S</cell><cell>-0.32</cell><cell>-0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: CoNLL F 1 scores of the cluster-ranking</cell></row><row><cell>model on the dev sets with various ablations.</cell></row><row><cell>-PRETRAINING: initializing model parameters</cell></row><row><cell>randomly instead of from the mention-ranking</cell></row><row><cell>model, -EASY-FIRST: iterating through mentions</cell></row><row><cell>in order of occurrence instead of according to their</cell></row><row><cell>highest scoring candidate coreference link, -L2S:</cell></row><row><cell>training on a fixed trajectory of correct actions in-</cell></row><row><cell>stead of using learning to search.</cell></row><row><cell>mention-ranking model and report results in Ta-</cell></row><row><cell>ble 2. Consistent with</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the current state-of-the-art approaches on the CoNLL 2012 test sets. NN Mention Ranker and NN Cluster Ranker are contributions of this work.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">incorrect link predicted by</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">the mention-ranking model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>{</cell><cell>Russian President Vladimir Putin, his,</cell><cell>? ,</cell><cell>the Russian President, ? ,</cell><cell>he</cell><cell>}</cell><cell>{</cell><cell>President Clinton's, Bill Clinton,</cell><cell>? ,</cell><cell>Mr. Clinton's</cell><cell>}</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and trained models are available at https:// github.com/clarkkev/deep-coref.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Will Hamilton, Jon Gauthier, and the anonymous reviewers for their thoughtful comments and suggestions. This work was supported by NSF Award IIS-1514268.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno>72.22 - - 60.50 - - 56.37 63.03</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiseman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Rfou</surname></persName>
		</author>
		<title level="m">Polyglot: Distributed word representations for multilingual NLP. Conference on Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
	<note>Bj?rkelund and Kuhn2014</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1503.05615</idno>
		<title level="m">Learning to search for dependencies</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to search better than your teacher</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>Chang et al.2015b] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum? III, and John Langford</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining the best of two worlds: A hybrid approach to multilingual coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A large-scale exploration of effective global features for a joint entity detection and tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1837</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Empirical Methods in Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decentralized entitylevel modeling for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Leo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wright</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="114" to="124" />
		</imprint>
	</monogr>
	<note>Durrett et al.2013</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Haghighi and Klein2010</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieleman2012] Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and J?rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A mention-synchronous coreference resolution algorithm based on the Bell tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prune-andscore: Learning for greedy coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Martschat and Strube2015</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward conditional models of identity uncertainty with application to proper noun coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wellner2003] Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI Workshop on Information Integration on the Web</title>
		<meeting>the IJCAI Workshop on Information Integration on the Web</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie2002] Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A joint framework for coreference resolution and mention head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint unsupervised coreference resolution with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
	<note>Poon and Domingos2008</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multi-pass sieve for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghunathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Narrowing the modeling gap: a clusterranking approach to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="469" to="521" />
		</imprint>
	</monogr>
	<note>Rahman and Ng2011</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrases</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Hwee Tou Ng, and Daniel Chung Yong Lim</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Wee Meng Soon</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Easy-first coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2519" to="2534" />
		</imprint>
	</monogr>
	<note>Stoyanov and Eisner2012</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A model-theoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vilain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Message understanding</title>
		<meeting>the 6th conference on Message understanding</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wiseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wiseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An entity-mention model for coreference resolution with inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
