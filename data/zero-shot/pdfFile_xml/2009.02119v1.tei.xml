<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech Gesture Generation from the Trimodal Context of Text, Audio, and Speaker Identity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12">December 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ETRI, KAIST</roleName><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bok</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ETRI</roleName><forename type="first">Minsu</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ETRI</roleName><forename type="first">Jaeyeon</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ETRI</roleName><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>KAIST</roleName><forename type="first">Geehyuk</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bok</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Haeng</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Jaehong</roleName><forename type="first">Jaeyeon</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bok</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Haeng</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geehyuk</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">ETRI JOO-HAENG LEE</orgName>
								<orgName type="institution" key="instit3">ETRI</orgName>
								<orgName type="institution" key="instit4">University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Speech Gesture Generation from the Trimodal Context of Text, Audio, and Speaker Identity</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Transactions on Graphics</title>
						<imprint>
							<biblScope unit="volume">39</biblScope>
							<biblScope unit="issue">6</biblScope>
							<biblScope unit="page">222</biblScope>
							<date type="published" when="2020-12">December 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3414685.3417838</idno>
					<note type="submission">Publication date: December 2020.</note>
					<note>0730-0301/2020/12-ART222 $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ?Computing methodologies ? Animation</term>
					<term>Supervised learning by regression</term>
					<term>Additional Key Words and Phrases: nonverbal behavior, co-speech gesture, neural generative model, multimodality, evaluation of a generative model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Overview of the proposed gesture generation model that considers the trimodality of speech text, audio, and speaker identity. The model is trained on online speech videos demonstrating co-speech gestures. At the synthesis phase, we can manipulate gesture styles by sampling a style vector from the learned style embedding space.</p><p>For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is di cult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches a empt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are humanlike and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or a liate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. that the proposed gesture generation model is be er than existing end-toend generation models. We further con rm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that di erent gesture styles can be generated for the same speech by specifying di erent speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at h ps://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Overview of the proposed gesture generation model that considers the trimodality of speech text, audio, and speaker identity. The model is trained on online speech videos demonstrating co-speech gestures. At the synthesis phase, we can manipulate gesture styles by sampling a style vector from the learned style embedding space.</p><p>For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is di cult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches a empt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are humanlike and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or a liate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. ? 2020 ACM. 0730-0301/2020/12-ART222 $15.00 <ref type="bibr">DOI: 10.1145/3414685.3417838</ref> that the proposed gesture generation model is be er than existing end-toend generation models. We further con rm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that di erent gesture styles can be generated for the same speech by specifying di erent speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at h ps://github.com/ai4r/Gesture-Generation-from-Trimodal-Context. CCS Concepts: ?Computing methodologies ? Animation; Supervised learning by regression;</p><p>1 INTRODUCTION e continued development of graphics and robotics technology has prompted the development of arti cial embodied agents, such as virtual avatars and social robots, as a popular interaction medium. One of the merits of the embodied agent is its nonverbal behavior, including facial expressions, hand gestures, and body gestures. In the present paper, we focus on upper-body gestures that occur with speech. Such co-speech gestures are a representative example of nonverbal communication between people. Appropriate use of gestures is helpful for understanding speech <ref type="bibr" target="#b42">(McNeill 1992)</ref> and increases persuasion and credibility <ref type="bibr" target="#b11">(Burgoon et al. 1990</ref>). Gestures are important not only in human-human interaction, but also in human-machine interaction. Gestures performed by arti cial agents help a listener to concentrate and understand u erances <ref type="bibr">(Bremner et al. 2011)</ref> and improve the intimacy between humans and agents <ref type="bibr" target="#b58">(Wilson et al. 2017)</ref>.</p><p>Interactive arti cial agents, such as game characters, virtual avatars, and social robots, need to generate gestures in real time in accord with their speech. Automatically generating co-speech gestures is a di cult problem because machines must be able to understand speech, gestures, and the relationship between them. Two representative gesture generation methods are rule-based and datadriven approaches <ref type="bibr" target="#b33">(Kipp 2005;</ref><ref type="bibr" target="#b35">Kopp et al. 2006</ref>). e rule-based approach, as the name suggests, de nes various rules mapping speech to gestures; it requires considerable human e ort to de ne the rules, but it is widely used in commercial robots because these models are relatively simple and intuitive. e data-driven approach learns gesticulation skills from human demonstrations. is approach requires more complex models and large amounts of data, but they do not require human e ort in designing rules. As large gesture datasets are becoming more available, research on data-driven approaches is increasing, e.g., <ref type="bibr" target="#b13">(Chiu et al. 2015;</ref><ref type="bibr" target="#b20">Ginosar et al. 2019;</ref><ref type="bibr" target="#b25">Huang and Mutlu 2014;</ref><ref type="bibr" target="#b33">Kipp 2005;</ref><ref type="bibr" target="#b60">Yoon et al. 2019)</ref>.</p><p>One data-driven approach, called the end-to-end method <ref type="bibr" target="#b20">(Ginosar et al. 2019;</ref><ref type="bibr" target="#b60">Yoon et al. 2019)</ref>, is unlike others in that it uses raw gesture data without intermediate representation such as predened unit gestures. Such less restrictive representation increases the method's expressive capacity, enabling it to generate more natural gestures. Previous studies have successfully demonstrated end-toend gesture generation methods. However, they were limited by their consideration of only a single modality, either speech audio or text. Since human gestures are associated with various factors, such as speech content, speech audio, interlocutor interaction, individual personality, and surrounding environment, generating gestures from a single speech modality can produce a very limited model. In the study of human gestures <ref type="bibr" target="#b42">(McNeill 1992)</ref>, researchers have de ned four categories, called iconic, metaphoric, deictic, and beat gestures, which are related to di erent contexts. Iconic gestures illustrate physical actions or properties (e.g., raising one's hands while saying "tall") and metaphoric gestures describes abstract concepts (e.g., moving one's hands up and down to depict a wall while saying "constraint"). Both iconic and metaphoric gestures are highly related to the speech lexicon. Deictic gestures are indicative motions that point to a speci c target or space, and are related to both the speech lexicon and the spatial context in which the gesture is made. Beat gestures are rhythmic movements that are closely related to the speech audio. In addition, even with the same speech and in the same surrounding environment, each person makes di erent gestures every time due to inter-person and intra-person variability of human gestures, and the inter-person variability may be a ributed to individual personality. Various modalities related to speech should be considered in order to generate more meaningful and human-like gestures.</p><p>In the present study, we propose an end-to-end gesture generation model that uses the multimodal context of text for speech content, audio for speech rhythm, and speaker identity (ID) for style variations. To integrate these multiple modalities, a temporally synchronized encoder-decoder architecture is devised based on the property of temporal synchrony found between speech and gestures in human gesture studies <ref type="bibr" target="#b15">(Chu and Hagoort 2014;</ref><ref type="bibr" target="#b43">McNeill 2008)</ref>. We experimentally con rm that each modality is e ective. Especially, a style embedding space is learned from speaker IDs to re ect inter-person variability, so we can create di erent styles of gestures for the same speech by sampling di erent points in the style embedding space. <ref type="figure">Figure 1</ref> provides an overview of the proposed gesture generation model and its training. e model is trained on a dataset derived from online videos exhibiting speech gestures with a training objective to generate human-like and diverse gestures. Our task is to develop a general gesture generator, a model that is supposed to generate convincing gestures for previously unseen speech.</p><p>A major hurdle in gesture generation studies is determining how to evaluate results. ere is no single ground truth in gesture generation and well-de ned evaluation methods are not yet available. Subjective human evaluation is the most reasonable method, but it is not cost e ective and di cult to reproduce results. Some studies have used the mean absolute error (MAE) of the positions of body joints between human gesture examples and generated gestures for the same speech <ref type="bibr" target="#b20">(Ginosar et al. 2019;</ref><ref type="bibr" target="#b29">Joo et al. 2019</ref>). e MAE evaluation method is objective and reproducible, though it is hard to ascertain to what extent the MAE between joints correlates with perceived gesture quality. In the present paper, we apply the Fr?chet inception distance (FID) concept proposed in image generation research <ref type="bibr" target="#b23">(Heusel et al. 2017)</ref> to our problem of gesture generation. FID compares ed distributions on a latent image feature space between the sets of real and generated images. We introduce the Fr?chet gesture distance (FGD), which compares samples on a latent gesture feature space. With synthetic noisy data and comparing to human judgements, we validate that the proposed metrics are more perceptually plausible than computing the MAE between gestures.</p><p>Our contributions can be summarized as follows:</p><p>? A new gesture generation model using a trimodal context of speech text, audio, and speaker identity. To the best of our knowledge, this is the rst end-to-end approach using trimodality to generate co-speech gestures. ? e proposal and validation of a new objective evaluation metric for gesture generation models.</p><p>? Extensive experiments to verify the usability of the proposed model. We show style manipulations with the trained style embedding space, the model's response to altered speech text, and the gestures' incorporation with synthesized audio. e remainder of this paper is organized as follows. We rst introduce related research (Section 2), then describe the proposed model (Section 3) and its training in detail (Section 4). Section 5 introduces a metric for evaluating gesture generative models and Section 6 describes human evaluation to validate the proposed metric. Section 7 presents qualitative and quantitative results. Finally, Section 8 concludes the paper with a discussion of the limitations and future direction of the present research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We rst review automatic co-speech gesture generation methods for arti cial agents. Next, we introduce previous data-driven gesture generation approaches. Related work discussing gesture styles, multimodality, and evaluation methods are also introduced.</p><p>Co-speech Gesture Generation for Arti cial Agents. Motion capture and retargeting human motions to arti cial agents is widely used to generate motions, especially in commercial systems, because of its high-quality motion from human actors <ref type="bibr" target="#b44">(Menache 2000)</ref>. Nonverbal behavior can also be generated by retargeting human motion <ref type="bibr" target="#b31">(Kim and Lee 2020)</ref>. However, the motion capture method has a critical limitation: the motion should be recorded beforehand. erefore, the motion capture method can only be used in movies or games that have speci ed scripts. Interactive applications, in which the agents interact with humans with various speech u erances in real time, mostly use automatic gesture generation methods. e typical automatic generation method is rule-based generation <ref type="bibr" target="#b12">(Cassell et al. 2004;</ref><ref type="bibr" target="#b35">Kopp et al. 2006;</ref><ref type="bibr" target="#b40">Marsella et al. 2013)</ref>. For example, the robots NAO and Pepper (So bank 2018) have a prede ned set of unit gestures and have rules that connect speech words and unit gestures. is rule-based method requires human e ort to design the unit gestures and hundreds of mapping rules. Research into data-driven methods has aimed to reduce the human e ort required for rule generation; these methods nd gesture generation rules in data using machine learning techniques. Probabilistic modeling for speech-gesture mapping has also been studied <ref type="bibr" target="#b25">(Huang and Mutlu 2014;</ref><ref type="bibr" target="#b33">Kipp 2005;</ref><ref type="bibr" target="#b39">Levine et al. 2010</ref>) and a neural classi cation model selecting a proper gesture for given speech context <ref type="bibr" target="#b13">(Chiu et al. 2015)</ref> was also proposed. e review paper <ref type="bibr" target="#b57">(Wagner et al. 2014</ref>) provides a comprehensive summary of the gesture generation research and rule-based approaches.</p><p>End-to-end Gesture Generation Methods. Gesture generation is a complex problem that requires understanding speech, gestures, and their relationships. To reduce the complexity of this task, previous data-driven models have divided speech into discrete topics <ref type="bibr" target="#b54">(Sadoughi and Busso 2019)</ref> or represented gestures as prede ned unit gestures <ref type="bibr" target="#b25">(Huang and Mutlu 2014;</ref><ref type="bibr" target="#b33">Kipp 2005;</ref><ref type="bibr" target="#b39">Levine et al. 2010</ref>). However, with recent advancements in deep learning, an end-toend approach using raw gesture data is possible. ere are studies using the end-to-end approach <ref type="bibr" target="#b18">(Ferstl et al. 2019;</ref><ref type="bibr" target="#b20">Ginosar et al. 2019;</ref><ref type="bibr" target="#b37">Kucherenko et al. 2019</ref><ref type="bibr" target="#b60">Yoon et al. 2019</ref>) that have formulated gesture generation as a regression problem rather than a classi cation problem. is continuous gesture generation does not require cra ing unit gestures and their rules and also removes the restriction that gesture expressions must be selected from predetermined unit gestures.</p><p>One study used an a entional Seq2Seq network that generates a sequence of upper body poses from speech text <ref type="bibr" target="#b60">(Yoon et al. 2019</ref>). e network consists of a text encoder that processes speech text and a gesture decoder that generates a pose sequence. Other studies generated gestures from speech audio <ref type="bibr" target="#b18">(Ferstl et al. 2019;</ref><ref type="bibr" target="#b20">Ginosar et al. 2019;</ref><ref type="bibr" target="#b37">Kucherenko et al. 2019)</ref>. ese audio-based generators also based on the neural architectures generating a sequence of poses, and some studies used adversarial loss to guide generated gestures to become similar to actual human gestures. e main difference between the previous models is the use of di erent speech modalities. Both semantics and acoustics are important for generating co-speech gestures <ref type="bibr" target="#b42">(McNeill 1992)</ref>, so, in this paper, we propose a model that uses multimodal speech information, audio and text together. Note that there is a concurrent work considering both audio and text information, but it trained and validated the generative model on a limited dataset of a single actor .</p><p>Learning Styles of Gestures. People make di erent gestures even when they say the same words (Hoste er and Po ho 2012). Similarly, arti cial agents must also learn di erent styles of gestures. e agents should be able to make extrovert-or introvert-style gestures according to their emotional states, interaction history, user preferences, and other factors. Stylized gestures also give the agents a unique identity similar to appearances and voices. Previous studies have a empted to generate such stylized gestures <ref type="bibr" target="#b20">(Ginosar et al. 2019;</ref><ref type="bibr" target="#b39">Levine et al. 2010;</ref><ref type="bibr" target="#b47">Ne et al. 2008)</ref>. In these studies, generative models were trained separately for each speaker or style.</p><p>is approach is an obvious way of learning individual styles, but requires a substantial amount of training data for each individual style. Because of this limitation, only three and ten individual styles were trained in <ref type="bibr" target="#b39">(Levine et al. 2010)</ref> and <ref type="bibr" target="#b20">(Ginosar et al. 2019</ref>), respectively. In the present study, we aim to build a style embedding space, so that we can manipulate styles through sampling the space into which di erent styles are embedded, rather than replicating a particular style as the previous papers did. Another study proposed more detailed style manipulation by using control signals of hand position, motion speed, or moving space .</p><p>Processing Multimodal Data. e present study considers four modalities: text, audio, gesture motion, and speaker identity. Generally, multimodal data processing includes the representation of each modality, alignment between modalities, and translation between modalities <ref type="bibr" target="#b5">(Baltru?aitis et al. 2018)</ref>. ere are two approaches to representation: one is that all modalities share the same representation and the other is that modalities are represented separately, and later alignment or translation stages integrate them. We can nd both representation approaches related to gesture generation. A study by <ref type="bibr" target="#b0">(Ahuja and Morency 2019)</ref> represented both human motion and descriptive text as vectors in the same embedding space. In other studies, di erent representations are used for di erent modalities <ref type="bibr" target="#b52">(Roddy et al. 2018;</ref><ref type="bibr" target="#b54">Sadoughi and Busso 2019)</ref>. We use separate representations, owing to the di culty of learning a cross-modal representation for co-speech gestures arising from the weak and ambiguous relationship between speech and gestures. Alignment between modalities is also an important factor for time-series data. In <ref type="bibr" target="#b20">(Ginosar et al. 2019</ref>), a feature vector encoding input speech was passed to a decoder to generate gestures, and the alignment between the modalities is not explicitly handled. A neural encoder and decoder implicitly processed the alignment as well as the translation from speech to gesture. In <ref type="bibr" target="#b60">(Yoon et al. 2019</ref>), a similar encoder-decoder architecture was used, but they guided the model to learn sequential alignment more explicitly by incorporating an a ention mechanism <ref type="bibr">(Bahdanau et al. 2015)</ref>. In , speech audio and text were aligned but not with gestures. Our model uses explicitly aligned speech and gesture because speech and gesture are synchronized temporally <ref type="bibr" target="#b15">(Chu and Hagoort 2014)</ref>, allowing the network to concentrate on the translation from input speech to gestures.</p><p>Evaluating Generative Models. Recently, as research into generative models has expanded, interest in evaluating generative models has increased. In a generation problem considering speech synthesis, image generation, and conversational text generation, human evaluation is the most plausible evaluation method because there is no clear ground truth to compare with. However, the results of human evaluation cannot easily be reproduced. A reliable computational evaluation metric is necessary for reproducible comparisons with state-of-the-art models and would accelerate research. Previous studies have measured gesture di erences between generated and human gestures <ref type="bibr" target="#b20">(Ginosar et al. 2019;</ref><ref type="bibr" target="#b29">Joo et al. 2019)</ref>, though this method is limited because pose-level di erences do not measure the perceptual quality of the generated gestures. Some studies have used other metrics to evaluate human motion, for example, the motion statistics of jerk and acceleration <ref type="bibr" target="#b37">(Kucherenko et al. 2019)</ref> and Laban parameters from a study of choreography <ref type="bibr" target="#b2">(Aristidou et al. 2015)</ref>. However, the aforementioned metrics compute distances for each sample, so they cannot measure how the generated results are diversi ed, which is crucial in generation problems. In the image generation problem, the inception score <ref type="bibr" target="#b55">(Salimans et al. 2016</ref>) and FID <ref type="bibr" target="#b23">(Heusel et al. 2017</ref>) have recently become de facto evaluation metrics because they can measure the diversity of generated samples as well as their quality, and this concept was successfully applied to other generation problems <ref type="bibr" target="#b30">(Kilgour et al. 2018;</ref><ref type="bibr">Unterthiner et al. 2019)</ref>. In this study, we have applied the concept of FID to the gesture generation problem to measure both perceptual quality and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Overall Architecture</head><p>Gesture generation in this paper is a translation problem that generates co-speech gestures from a given speech context. Our goal is to generate gestures that are human-like and match well with any given speech. We propose a neural network architecture consisting of three encoders for input speech modalities and a decoder for gesture generation. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall architecture. ree modalities-text, audio, and speaker identity (ID)-are encoded with di erent encoder networks and transferred to the gesture generator.</p><p>A gesture is represented as a sequence of human poses, and the generator, which is a recurrent neural network, generates poses frame-by-frame from an input sequence of feature vectors containing encoded speech context. Speech and gestures are temporally synchronized <ref type="bibr" target="#b15">(Chu and Hagoort 2014;</ref><ref type="bibr" target="#b43">McNeill 2008)</ref>, so we con gured the generator to use part of the speech text and audio near </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Pose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Poses</head><p>Discriminator Real / Generated</p><formula xml:id="formula_0">A B C D E F G H I J D</formula><p>... the current time step instead of the whole speech context. Gesture style does not change in the short term, so the same speaker ID is used throughout the synthesis. In addition, we used seed poses for the rst few frames for be er continuity between consecutive syntheses. See appendix A for the gures of detailed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Speech Context</head><p>is section describes how the speech modalities of text, audio, and speaker ID are represented and the details of the encoder networks. We have four modalities, including the output gesture, in di erent time resolution. We rst ensure that all input data have the same time resolution as the output gestures, so all modalities share the same time steps and the proposed sequential model ( <ref type="figure" target="#fig_0">Figure 2</ref>) can process speech input and generate poses frame by frame.</p><p>e speech text is a word sequence, with the number of words varying according to speech speed. We insert padding tokens ( ) into the word sequence to make a padded word sequence (word 1 , word 2 , ?, word t ) that is the same length as the gestures. Here, t is the number of poses in a synthesis ( xed as 34 throughout the paper, see Section 4). We assume the exact u erance time of words is known, so the padding token is inserted to make the words temporally match the gestures. For instance, for the speech text "I love you", if there were a short pause between "I" and "love", then the padded word sequence would be "I love you" when t is 5. All words in the padded word sequence are then transformed into word vectors in 300 dimensions via a word embedding layer. Next, these word vectors are encoded by a temporal convolutional network (TCN) <ref type="bibr" target="#b4">(Bai et al. 2018)</ref> to make 32-D feature vectors for speech text modality (f text 1 , f text 2 , ?, f text t ). TCN processes sequential data through convolutional operations, and showed competitive results over the recurrent neural networks in diverse problems <ref type="bibr" target="#b4">(Bai et al. 2018)</ref>. In this paper, we used a four-layered TCN, where each f text has a receptive eld of 16. us, f text i encodes 16 padded words around at time step i. For our training dataset the average and the largest number of non-padding words in this receptive eld were 3.9 and 16, respectively.</p><p>We used FastText <ref type="bibr" target="#b7">(Bojanowski et al. 2017</ref>), a pretrained word embedding, and update these embeddings during training. ere was the concern that word embeddings pretrained by lling a missing word in a sentence <ref type="bibr" target="#b45">(Mikolov et al. 2013</ref>) may not suitable to gesture generation. For instance, if we query words that are close to large, then small appears in the top-3 list in both GloVe <ref type="bibr" target="#b50">(Pennington et al. 2014)</ref> and FastText <ref type="bibr" target="#b7">(Bojanowski et al. 2017</ref>) even though they have opposite meanings. is problem with pretrained word embedding has also been raised in text-based sentiment analysis, where the sentiment of words is important <ref type="bibr" target="#b19">(Fu et al. 2018)</ref>. We tested three different se ings: 1) pretrained embeddings without weight updating, 2) pretrained embeddings with ne-tuning weights, and 3) learning word embeddings from scratch. In our problem, using pretrained embeddings with ne-tuning was the most successful. FastText <ref type="bibr" target="#b7">(Bojanowski et al. 2017</ref>) was favored over GloVe <ref type="bibr" target="#b50">(Pennington et al. 2014)</ref> since FastText is using subword information so that it gives accurate representation for unseen words.</p><p>For the speech audio modality, a raw audio waveform goes through cascaded one-dimensional (1D) convolutional layers to generate a sequence of 32-D feature vectors (f audio 1 , f audio 2 , ?, f audio t ). Audio frequency is usually xed, so we adjusted the sizes, strides, and padding in the convolutional layers to obtain equally many audio feature vectors as there were output motion frames. In our experiments, each feature vector had a receptive eld of about a quarter of a second. e quarter-second receptive eld may not be large enough to cover occasional asynchrony between speech and gesture (the standard deviation of the temporal di erences is about a half second according to <ref type="bibr" target="#b6">(Bergmann et al. 2011)</ref>), but our use of a bidirectional GRU in the gesture generator that sends information forwards and backwards can compensate for the asynchrony. e model also uses speaker IDs to learn a style embedding space. Human gestures are not the same even for the same speech. We utilize the speaker IDs to re ect characteristics of each speaker in the dataset, and we call this individuality as 'style' in the present paper. Note that our purpose is to build an embedding space capturing di erent styles not to replicate gestures of each speaker. e speaker IDs are represented as one-hot vectors where only one element of a selected speaker is nonzero. A set of fully connected layers maps a speaker ID to a style embedding space of much smaller dimension (8 in the present study). To make the style embedding space more interpretable, variational inference (Kingma and Welling 2014; Rezende et al. 2014) that uses a probabilistic sampling process is used. e same feature vector f style on the style embedding space is used for all time steps in a synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gesture Generator</head><p>e generator G(?) takes encoded features as input and generates gestures. e gesture is a sequence of human poses p i consisting of 10 upper body joints (spine, head, nose, neck, L/R shoulders, L/R elbows, and L/R wrists). All poses were spine-centered. When we train the model, we represent each pose as directional vectors which represent the relative positions of the child joints from the parent joints. ere are nine directional vectors for spine-neck, neck-nose, nose-head, neck-R/L shoulders, R/L shoulders-R/L elbows, and R/L elbows-R/L wrists. e directional vectors are favored for training the proposed model because this representation is less a ected by bone lengths and root motion. In the representation of joint coordinates, a small translation of neck, which is the parent joint of both arms, can have an excessive e ect on all coordinates of the arms. We denote human poses represented as directional vectors by d i , and all directional vectors were normalized to the unit length. We note that forearm twists were not considered in this paper.</p><p>For gesture generation, we use a multilayered bidirectional gated recurrent unit (GRU) network <ref type="bibr" target="#b14">(Cho et al. 2014)</ref>. Encoded features of speech text, audio, and speaker ID are concatenated to form a concatenated feature vector f i = (f text i , f audio i , f style ) for each time instant i. e generator takes the feature vector f i as input and generates the next posed i+1 iteratively.</p><p>For a long speech, the speech is divided into 2-second chunks and the generator synthesizes gestures for each chunk. e use of seed poses helps to make transitions between consecutive syntheses smooth. Seed poses d i=1, ...,4 , the last four frames of the previous synthesis, are concatenated with the feature vector for the early four frames of the next synthesis as (f i , d i ), and an additional bit is used to indicate the presence of a seed pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adversarial Scheme</head><p>An adversarial scheme <ref type="bibr" target="#b21">(Goodfellow et al. 2014</ref>) is applied in training the model to generate more realistic gestures. e adversarial scheme uses a discriminator, which is a binary classi er distinguishing between real and generated gestures. By alternate optimization of generator and discriminator, the generator improves its performance to fool the discriminator. For the discriminator, we use a multilayered bidirectional GRU that outputs binary output for each time step. A fully connected layer aggregate the t binary outputs and gives a nal binary (real or generated gesture) decision. 4 TRAINING WITH "IN-THE-WILD" VIDEOS 4.1 TED Gesture Dataset e gesture generation model is trained on the TED gesture dataset <ref type="bibr" target="#b60">(Yoon et al. 2019)</ref>, which is a large-scale, English-language dataset for data-driven gesture generation research. e dataset includes speech from various speakers, so it is suitable for learning individual gesture styles. We added 471 additional TED videos to the data of <ref type="bibr" target="#b60">(Yoon et al. 2019)</ref>, for a total of 1,766 videos. Extracted human poses from TED videos, speech audio, and transcribed English speech text are available. We further converted all human poses to 3D by using the 3D pose estimator <ref type="bibr" target="#b49">(Pavllo et al. 2019)</ref> which convert a sequence of 2D poses into 3D poses. e pose estimator uses temporal convolutions that lead to temporally coherent results despite of a few of inaccurate 2D poses. We used the manual speech transcriptions available on each TED talk, with onset timestamps of each word extracted using the Gentle forced aligner <ref type="bibr" target="#b48">(Ochshorn and Hawkins 2016)</ref> to insert padding tokens. e forced aligner reported successful alignment of 97% of the total words.</p><p>From the videos, only the sections of videos in which upper body gestures were clearly visible were extracted; the total duration of the valid data was 97 h. e gesture poses were resampled at 15 frames per second, and each training sample having 34 frames was sampled with a stride of 10 from the valid video sections. e initial four frames were used as seed poses and the model was trained to generate the remaining 30 poses (2 seconds). We excluded noninformative samples having li le motion (i.e., low variance of a sequence of poses) and erratic samples having lying poses (i.e., low angle of the spine-neck vector). e dataset was divided into training, validation, and test sets. e division was done at the video level. Because all presentations in the TED dataset were given by di erent speakers, the number of unique speaker IDs is the same as the number of videos and there is no overlap of speaker IDs between split sets. We used the training set for training the model, the validation set for tuning the systems, and the test set for qualitative results and human evaluation. e nal number of 34-frame sequences in each data partition were <ref type="bibr">199,384; 26,795; and 25,930.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Loss Function</head><p>e model is trained using the losses below. We use L G to train the encoders and gesture generator and L D to train the discriminator.</p><formula xml:id="formula_1">L G = ? ? L Huber G + ? ? L NSGAN G + ? ? L style G + ? ? L KLD G (1) L Huber G = E[ 1 t t i=1 HuberLoss(d i ,d i )]<label>(2)</label></formula><formula xml:id="formula_2">L NSGAN G = ?E[log(D(d))]<label>(3)</label></formula><formula xml:id="formula_3">L style G = ?E ? ? ? ? ? ? ? ? ? min HuberLoss(G(f text , f audio , f style 1 ) ? G(f text , f audio , f style 2 )) f style 1 ? f style 2 1 , ? ? ? ? ? ? ? ? ? ? (4) L D = ?E[log(D(d))] ? E[log(1 ? D(d))]<label>(5)</label></formula><p>where t is the length of the gesture sequence, d i represents the ith pose, represented as directional vectors, in a training sample. When training the encoder and gesture generator, we minimized the di erence between human poses d in the training examples and the corresponding generated posesd using the Huber loss <ref type="bibr" target="#b26">(Huber 1964)</ref>. is loss L Huber G can be interpreted as a once-di erentiable combination of the L1 and L2 losses, and is therefore sometimes called the smooth L1 loss. e adversarial losses L NSGAN G and L D are from the non-saturating generative adversarial network (NS-GAN) <ref type="bibr" target="#b21">(Goodfellow et al. 2014)</ref>. We use sample mean to approximate the expectation terms.</p><p>A generative model conditioned on multiple input contexts o en su ers from posterior collapse where weak context is ignored. In the proposed model, various gestures can be generated only from text and audio, so the style features from speaker IDs might be ignored during training. us, we use diversity regularization <ref type="bibr" target="#b59">(Yang et al. 2019</ref>) to avoid ignoring style features. L style G is the Huber loss between the gestures generated from di erent style features normalized by the di erences of the two style features, so it guides style features in the embedding space to generate di erent gestures. ? is for value clamping for numerical stability. In Equation 4, f style 1 is the style feature corresponding to the speaker ID of a training sample, and f style 2 is the style feature for a speaker ID selected randomly. L KLD G , the Kullback-Leibler (KL) divergence between N (0, I ) and the style embedding space assumed Gaussian, prevents the style embedding space from being too sparse (Kingma and Welling 2014).</p><p>L D is to train the discriminator D, and the generator and discriminator are alternately updated with L G and L D as in conventional GAN training <ref type="bibr" target="#b21">(Goodfellow et al. 2014)</ref>. D(?) is trained to output 1 for human gestures and 0 for generated gestures. e model was trained for 100 epochs. An Adam optimizer with ? 1 = 0.5 and ? 2 = 0.999 was used, and the learning rate was 0.0005. Weights for the loss terms were determined experimentally (? = 500, ? = 5, ? = 0.05, and ? = 0.1). In addition, there was a warm-up period of 10 epochs in which the adversarial loss was not used (? = 0). ? was 1000. e trained encoders and generator are used at the synthesis stage. As the model is lightweight enough, the synthesis can be done in real time. A single synthesis generating 30 poses takes 10 ms on a GPU (NVIDIA RTX 2080 Ti) and 80 ms on a CPU (Intel i7-5930K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OBJECTIVE EVALUATION METRIC</head><p>It is di cult to evaluate gesture generation models objectively because no perceptual quality metric is available for human gestures. Although a human evaluation method in which participants rate generated gestures subjectively is possible, objective evaluation metrics are still required for fair and reproducible comparisons between state-of-the-art models. No proper and widely used evaluation metric is yet available for the gesture generation problem.</p><p>Image generation studies have proposed the FID metric <ref type="bibr" target="#b23">(Heusel et al. 2017)</ref>. Latent image features are extracted from the generated images using a pretrained feature extractor and FID calculates the Fr?chet distance between the distributions of the features of real and generated images. Because FID uses feature vectors that describe visual characteristics well, FID is more perceptually appropriate than measurements over raw pixel spaces. FID can also measure the diversity of the generated samples by using the samples' distribution rather than simply averaging the di erences between the real and generated samples. e diversity of generation has been thought to be one of major factors in evaluating generative models <ref type="bibr" target="#b8">(Borji 2019)</ref>. Diversity is also crucial for the gesture generation problem because the use of repetitive gestures makes arti cial agents look dull.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fr?chet Gesture Distance</head><p>In applying the concept of FID to the gesture generation problem, there is a hurdle that no general feature extractor is available for gesture data. e paper proposing FID used an inception network trained on the ImageNet database for image classi cation, but there is no analog of the pretrained inception network for gesture motion data to the best of our knowledge. Accordingly, we trained a feature extractor based on autoencoding <ref type="bibr" target="#b53">(Rumelhart et al. 1985)</ref>, which can be trained in unsupervised manner. e feature extractor consists of a convolutional encoder and decoder; the encoder encodes a sequence of direction vectors d to a latent feature z estur e and the decoder then a empts to restore the original pose sequence from the latent z estur e (see appendix A for the detailed architecture).</p><p>is unsupervised learning is unlike the supervised learning of the inception network used in FID. However, both supervised and unsupervised learning have proven to be e ective for learning perceptual quality metrics <ref type="bibr" target="#b61">(Zhang et al. 2018</ref>). e encoder part of the trained autoencoder was used as a feature extractor. We de ned FGD(X ,X ) as the Fr?chet distance between the Gaussian mean and covariance of the latent features of human gestures X and the Gaussian mean and covariance of the latent features of the generated gesturesX as follows:</p><formula xml:id="formula_4">FGD(X ,X ) = ? r ? ? 2 + Tr(? r + ? ? 2(? r ? ) 1/2 )<label>(6)</label></formula><p>where ? r and ? r are the rst and second moments of the latent feature distribution Z r of real human gestures X , and ? and ? are the rst and second moment of the latent feature distribution Z of generated gesturesX . For training the feature extractor, we used the Human3.6M dataset <ref type="bibr" target="#b27">(Ionescu et al. 2013</ref>) containing motion capture data of 7 di erent actors and 17 di erent scenarios including discussion and making purchases showing co-speech gestures. e total duration of the training data was about 175 m. All poses were frontalized based on two hip joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment with Synthetic Noisy Data</head><p>We explored the properties of the proposed FGD metric using synthetic noisy data. Five types of noisy data were considered. Gaussian noise and Salt&amp;Pepper (S&amp;P) noise were added to the joint coordinates of poses; the same noise data were added to all poses in a sequence, so that there is no arti cial temporal discontinuity. Temporal noise was simulated by adding Gaussian noise to only a few time frames. Multiplicative transformation in "eigenposes" p ei en i <ref type="bibr" target="#b60">(Yoon et al. 2019</ref>) converted from p i using principal component analysis (PCA) was used to generate monotonous or exaggerated gestures. Mismatched gestures were also generated to examine how the metric responds to discrepancies between speech and gestures. e following shows how the noisy data were synthesized. e parameter ? controls the overall disturbance levels. e dimension of a pose, K, is 30 (10 joints in 3D coordinates). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Gaussian noise:p</head><formula xml:id="formula_5">i = p i + x; x ? N K (0, ? I ) (a) (b) (c) (d) (e) (f) (g)</formula><formula xml:id="formula_6">i = p i + x x k=1, ...,K = ? ? ? ? ? ? ? ? ? 0.2 if u ? ? /2; u ? U (0, 1) ?0.2 if ? /2 &lt; u ? ? 0 otherwise ? Temporal noise: p i = ? ? ? ? ? ? ? ? ? p i + x; x ? N K (0, 0.003I ) if r ? i &lt; r + ? ;</formula><p>r is a random time step p i otherwise ? Multiplicative transformation:p ei en i = ? ? p ei en i ? Mismatched samples: Select a fraction ? of all samples and associate the input speech to random gestures in the TED test dataset to make mismatched samples. <ref type="figure" target="#fig_1">Figure 3</ref> shows samples of the synthetically noisy data. e Gaussian noise introduced changes across all joints, whereas the S&amp;P noise produces impulsive noise in a few joints. e temporal noise introduced discontinuities in motion. Multiplicative transformation was applied to eigenposes, so it controls the overall motion range. e mismatch noise shows a sample of nonmatching content and speech rhythms.</p><p>We measured FGD and mean absolute error of joint coordinates (MAEJ) which is calculated as MAE(p, p). <ref type="figure" target="#fig_2">Figure 4</ref> shows the experimental results. For the Gaussian and S&amp;P noise, both FGD and MAEJ showed increasing distances as the disturbance level increases, but FGD showed larger distances for S&amp;P noise than Gaussian noise on average, unlike MAEJ. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>  with S&amp;P noise, so, in our view, having larger distances for S&amp;P noise is acceptable. Both FGD and MAEJ showed relatively low values for the temporal noise even though discontinuous motion is perceptually unnatural. MAEJ calculates errors in each time frame independently, so it is obvious that MAEJ is not able to capture motion discontinuity. However, FGD, which encodes a whole sequence, also showed low distances unexpectedly. e primary reason is that the feature extractor used in FGD were not able to discriminate enough between the sequences with and without temporal noise. When we examined the reconstructed motion from the autoencoder, we found that the autoencoder tended to remove temporal noise.</p><p>For the multiplicative transformation, both metrics showed increasing distances as the disturbance level increased (larger or smaller than ? = 1.0). MAEJ showed similar distances for ? = 0.0 and 2.0, but FGD showed a much larger distance when ? = 0.0. As shown in <ref type="figure" target="#fig_1">Figure 3</ref> (e) and (f), ? = 0.0 and 2.0 make mean and exaggerated poses. If we consider several results and their diversity, exaggerated poses are perceptually favored over having the same mean poses regardless of input speech. us, it is reasonable to have larger distances for ? = 0.0 for than 2.0, as FGD does.</p><p>Lastly, for the mismatched samples, both MAEJ and FGD showed increasing distances for more mismatched samples, but the increase in FGD was smaller than MAEJ. is result is not surprising since FGD considers a distribution formed by a set of gestures and is not aware of the input speech.</p><p>In this experiment, we found that FGD gives perceptually plausible results for the di erent gesture data of Gaussian noise, S&amp;P noise, and multiplicative transformation and has the limitation that it is not able to measure well enough temporal noise and match of speech and gestures. We found the characteristics of FGD; however, it is di cult to argue that the metric is suitable for use based on an experiment with synthetic data wherein only one human gesture example is used for each speech even though many-to-many mappings exist between speech and gestures. To further investigate the e ectiveness of FGD and MAEJ, we compare these metrics to human judgements in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDY TO VALIDATE EVALUATION METRICS</head><p>In this section, we validated FGD by comparing with subjective ratings from humans. We followed the overall experimental se ing in the paper introducing Fr?chet video distance <ref type="bibr">(Unterthiner et al. 2019</ref>), but we had two separate user study sessions with 14 noise models and 10 trained gesture generation models. In the rst session, 14 noise models (excluding Mismatched with ? = 0.2 and 0.5) were used. In the second session, 10 gesture generation models showing di erent FGD were selected among models trained in the course of this study. We tried to select models equidistant in terms of FGD. e selected models are in di erent architectures, con gurations, and training stages; see appendix B for the complete list of the selected models and their con gurations. We also included the human gesture in both sessions.</p><p>We made videos showing a dummy character making gestures for each model. In the evaluation, pairwise preference comparisons were used instead of a Likert-scale rating. Co-speech gestures are subtle, so participants would have struggled to rate them on a ve-or seven-point scale. Using pairwise preference comparisons reduces participants' cognitive load and yields reliable results, as discussed in <ref type="bibr" target="#b17">(Clark et al. 2018</ref>). e participants watched two videos of two different models and responded to one of three questions asking about their preference, human-likeness of motion, and speech-gesture matching: 1) "Which gesture motion do you prefer?, " 2) "Which gesture motion is more natural and human-like?, " and 3) "Which gesture motion is more appropriate with the speech audio and words?" e answer options were "Video A," "Video B," and "Undecidable." For the question on human-likeness of motion, the videos were played without speech audio to make the participants assess only the motion. Each participant was asked to answer one randomly selected question in all of his/her trials, since the three questions are substantially correlated and the participants are prone to give the same answer if we ask three questions at the same time.</p><p>For the evaluation, speech samples with lengths of 5-10 s were drawn randomly from the TED test dataset. We only reviewed the quality of the extracted 3D human poses of the samples to exclude faulty samples that may mislead the performance of human gestures (the top line). irty speech samples were used in the evaluation a er excluding four faulty samples where the speaker is manipulating an object, si ing on a chair, and occluded by a podium. Two models were randomly selected for each pairwise comparison to eliminate ordering e ects.</p><p>Native or bilingual English speakers were recruited from Amazon MTurk. Each participant responded to 30 pairwise comparisons which were chosen randomly among all possible pairwise combinations (30 sentences ? { 15 C 2 or 11 C 2 } ? 3 questions). ey took <ref type="table">Table 1</ref>. Agreement of the evaluation metric to human judgements in the user study on (a) noise models and (b) trained gesture generation models. We also report the agreements between human subjects as a top line. Higher numbers are be er.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agreement (%)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Preference Humanlikeness of motion 15-30 min to complete the task, and 2.5 USD was given as a reward. We also included an a ention check presenting two copies of the same video side by side. e participants who did not answer "undecidable" in this case were excluded. In the rst session with the noise models, a total of 28 subjects participated, but we analyzed the results from 22 subjects a er excluding six subjects failed the a ention check. ere were 13 male and 9 female subjects, and they were 36.9 ? 11.5 years old. In the second session with the trained generation models, a total of 51 subjects participated and 21 subjects were excluded. ere were 15 male and 15 female subjects, and they were 42.8 ? 13.2 years old. e total numbers of answers were 660 and 900 at the rst and second session, respectively. We evaluated the objective evaluation metrics by comparing those with human judgements, and the results are shown in <ref type="table">Table 1</ref>. MAE of acceleration was used to assess dance motion <ref type="bibr" target="#b2">(Aristidou et al. 2015)</ref> and gestures <ref type="bibr" target="#b37">(Kucherenko et al. 2019)</ref>, and it focuses on motion rather poses. e agreement values were calculated as the number of comparisons in which each metric agreed human judgement divided by the total number of comparisons. "Undecidable" responses were not included in the analysis. In both sessions, FGD showed greater agreement with human judgements than did MAE of joint coordinates and MAE of acceleration on all questions. However, FGD was performed less than the agreements between humans; in particular, FGD showed the lowest agreement of 53.5% for temporal noise as discussed in Section 5.2.</p><p>By considering both experimental results on synthetic noisy data and human judgements, FGD is a plausible objective metric. In addition, when we examine learning curves, which are shown in <ref type="figure">Figure   Training</ref>  5, FGD showed a decreasing trend when the distribution of generated gestures becomes more similar to the reference distribution as training continues. In contrast, MAEJ showed a at learning curve. e lowest MAEJ is at Epoch 6, in which only static mean poses appear for all speech contexts. In the following experiments, we use FGD to compare models.</p><p>All subjects were asked to write the reasons for their selection. Most of them said they preferred gestures that were t to speech words and audio, as we had assumed in the present paper. Opinions on gesture dynamics were mixed. Some participants liked dynamic or even exaggerated gestures, whereas some other participants preferred moderate gestures with a few large movements for emphasis.</p><p>is implies that the gesture styles must be adapted as per the users' preference.</p><p>7 EXPERIMENTS AND HUMAN EVALUATION 7.1 alitative Results <ref type="figure">Figure 6</ref> shows the gesture generation results for the speech in the test set of the TED gesture dataset. e gestures are depicted using a 3D dummy character. e poses represented as directional vectors were retarge ed to the character with xed bone lengths, and the gesture sequences were upsampled using cubic spline interpolation to 30 FPS. We used the same retargeting procedure for all animations. e character makes metaphoric gestures when saying "civil rights, " "30 million, " or "great leadership. " An iconic gesture also found for the words "to the point." Gesture generation depends on speech rhythm and presence or absence of speech as shown in the sample (a) and (e). A deictic gesture also appears in (c) when the character says "I. " Please see the supplementary video for the animated results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Comparisons with state-of-the-art models</head><p>We compared the proposed model with three models from previous studies. e rst model compared is a entional Seq2Seq, which generates gestures from speech text <ref type="bibr" target="#b60">(Yoon et al. 2019)</ref>. We followed the original implementation provided by the authors but the gesture representation was modi ed to be identical to the proposed model. e second comparison model is Speech2Gesture <ref type="bibr" target="#b20">(Ginosar et al. 2019)</ref>, which generates gestures from speech audio using an encoder-decoder neural architecture and learns to generate humanlike gestures by using an adversarial loss during training. Spectrograms were used to represent audio in this model. e third one is the joint embedding model <ref type="bibr" target="#b0">(Ahuja and Morency 2019)</ref>, which (a) as civil rights violation that it is we're seeing cities and states</p><formula xml:id="formula_7">(b) (c) (e)<label>(f)</label></formula><p>30 million hectares of land in Europe I thought he immediately wrote me back we could even see this moving to the point no doubt of great leadership but it is also the result of strong Arab women not giving up <ref type="bibr">(d)</ref> with these grades and our kids <ref type="figure">Fig. 6</ref>. Sample results of co-speech gesture generation from the trimodal speech context of text, audio, and speaker identity. Motion history images for some parts are depicted along with the speech text and audio signals. In (a), the character makes metaphoric gestures when saying "civil rights" and beat gestures for "cities and states. " In (b) and (d), there are metaphoric gestures for the words of "30 million, " "great leadership, " and "giving up. " In (c), a deictic gesture appears when the character says "I. " In (e), we can find the character does not gesture in the middle of the silence. An iconic gesture is also found in (f). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method FGD</head><p>A entional Seq2Seq <ref type="bibr" target="#b60">(Yoon et al. 2019)</ref> 18.154 Speech2Gesture <ref type="bibr" target="#b20">(Ginosar et al. 2019)</ref> 19.254 Joint embedding model <ref type="bibr" target="#b0">(Ahuja and Morency 2019)</ref> 22.083 Proposed 3.729</p><p>creates human motion from motion description text. is model maps text and motion to the same embedding space. We embedded the input speech text and audio together to the same space as the motion. e same encoders in our model were used to process the audio and text, and 4-layered GRUs were used for gesture generation. All models were trained on the same TED dataset for the same number of epochs. We modi ed the original architectures of the baselines to generate the same number of poses (i.e., 30) and to use four seed poses for consecutive syntheses. e learning rate and weights of loss terms in the baselines were optimized via grid search for best FGD. <ref type="figure" target="#fig_4">Figure 7</ref> shows sample results from each model for the same speech. e joint embedding model generated very static poses, failing to learn gesticulation skills. e relationship between speech and gestures are weak and subtle, making it di cult to map speech and gestures to a joint embedding space. All other models generated plausible motions, but there were di erences depending on the modality and training loss considered. A entional Seq2Seq generated di erent gestures for di erent input speech sentences, but the motion tended to be slow and we found a few discontinuities between the seed poses and generates poses. e Speech2Gesture model used an RNN decoder similar to a entional Seq2Seq, but it showed be er motion with the help of its adversarial loss component. However, because it uses only a single speech modality, audio, Speech2Gesture generated monotonous beat gestures. e proposed model successfully generated large and dynamic gestures as shown in the supplementary video. e proposed model performed the best in terms of FGD <ref type="table" target="#tab_2">(Table 2)</ref>. We also analysed the human evaluation results by computing ranks from pairwise comparisons using the Bradley-Terry model <ref type="bibr" target="#b16">(Chu and Ghahramani 2005)</ref>. Pairwise comparisons were collected from another 14 MTurk subjects that passed the same a ention check as before. e same se ings described in Section 6 were used, but only the four models in <ref type="table" target="#tab_2">Table 2</ref> and human gestures were compared. <ref type="figure" target="#fig_5">Figure 8</ref> shows the results. For all the questions, the proposed method achieved be er results than A entional Seq2Seq, Speech2Gesture, and joint embedding methods, but the di erences between the proposed method and Speech2Gesture were not distinct in the the human-likeness of motion and speech-gesture match questions. We also tested statistical signi cance between the proposed method and the others by using the Chi-Square Goodness of Fit test over the null hypothesis that the probabilities of the pairwise choices are equal to 50% (the choice of "undecidable" was not counted). In the preference, the di erence between the proposed and joint embedding method was signi cant (p ? 0.01). In the speech-gesture match, Seq2Seq</p><p>Speech Text "I started walking towards the public. I was a mess. I was half naked.</p><p>I was full of blood and tears were running down my face."   <ref type="bibr" target="#b16">(Chu and Ghahramani 2005)</ref>. S2G denotes the Speech2Gesture method. and joint embedding methods were signi cantly di erent from the proposed method (p ? 0.01 and ? 0.05, respectively). e proposed method showed be er results than the previous methods objectively and subjectively. Also, the proposed method is mostly tied with human gestures in the user study. is indicates the superiority of the proposed method, but we cannot conclude that the proposed method performed equally well as humans since the human gestures used in the experiments were based on automatically extracted poses from TED videos and all motion was retarge ed to a restricted character without face or hand expressions.</p><formula xml:id="formula_8">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Ablation Study</head><p>An ablation study was conducted to understand the proposed model in detail. We eliminated components from the proposed model that was used in the comparison with the state-of-the-art models. Table 3 summarizes the results of the ablation study. Removing each modality of text, audio, and speaker ID reduced the model's performance; this shows that all three modalities used in the proposed model had positive e ects on gesture generation. Among the loss terms, removing the adversarial term and regularization terms also worsened FGD. In particular, when we trained the model without the adversarial scheme, the model tended to generate static poses close to the mean pose.</p><p>Although, when ablating di erent modalities, excluding the speaker ID degraded the FGD the most, we could not nd a noticeable degradation in our subjective impression of motion quality than ablating text or audio modalities. In our view, this is a ributed to that overall diversity was reduced without the divergence regularization L style G and that the property of FGD that measures not only motion quality but also diversity. ere is no concrete way to disentangle the factors of quality and diversity in FGD as well as FID. However, we hypothesise that the covariance matrix of the ed Gaussian is more related to the diversity than to quality. e trace of the covariance matrix was 244, which is less than that of the human gestures and of the models without the text or audio modalities <ref type="bibr">(299, 258, and 250, respectively)</ref>. is indirectly suggests that generated gestures were less diverse without speaker IDs and L style G . e text modality had the least e ect on FGD. In the proposed model, speech text and audio are treated as independent modalities; however, strictly speaking, audio contains text information because we can transcribe text from audio. Although the above ablation study showed that the FGD worsened without the text modality, it was less signi cant than excluding audio or speaker IDs. We There are of sparrows few hundreds (a) Audio Only (b) Text and Audio <ref type="figure">Fig. 9</ref>. Visualization of how a gesture changes when a word is changed in a sentence. We compare the results of (a) the ablated model without text modality and (b) proposed model considering both text and audio. The generated gesture for the original and altered sentences are overlaid for five evenly sampled frames. When we consider both text and audio, the model generates more di erent gestures for the changes in speech content.</p><p>further veri ed the e ect of the text with an additional experiment. <ref type="figure">Figure 9</ref> shows how the generated gestures di ered when a word was altered in the input speech with the model considering both text and audio and the model considering only audio. Although the model considering both text and audio generated di erent gestures (widening arms) when the word "hundreds" replaced the words "few," there was only a slight change in motion when we used the audio-only model. We synthesized speech audio using Google Cloud TTS (Google 2018) for both original and altered text.</p><p>We also conducted the above text-altering experiment quantitatively. For 1,000 samples randomly selected from the validation set, a word in a speech sentence was changed to a synonym or antonym taken from WordNet <ref type="bibr" target="#b46">(Miller 1995)</ref>. If there were several synonyms or antonyms, the one closest in duration to the original word was selected to minimize the change in the length of the speech audio. Synthesized audio was used and the experiment was repeated 10 times due to the randomness in selecting samples and words. We report the FGD between the generated samples before and a er text alteration; this measure is unlike all other FGD measures, which compare human motion and generated motion, in the paper. e model considering text and audio (2.433 ? 0.483) showed a signicantly higher FGD than the model considering only audio (1.604 ? 0.275) (paired t-test, p ? 0.001), indicating that using text and audio modalities together helps to generate diverse gestures according to the changes in the speech text. is argument is also backed by the result that the FGD when a word was replaced by an antonym (2.567 ? 0.484) was signi cantly higher than when replaced by a synonym (2.299 ? 0.467) (paired t-test, p ? 0.05) in the model using both text and audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Incorporating Synthesized Audio</head><p>Many arti cial agents use synthesized audio since recording a human speaking for every word is infeasible. We tested that the proposed model, trained with human speech audio, also can work with synthesized audio. <ref type="figure">Figure 10</ref> and the supplementary video shows some results using synthesized audio with di erent voices. Google Cloud TTS <ref type="bibr" target="#b22">(Google 2018)</ref> was used in this experiment. e proposed model worked well with synthesized audio of di erent voices, prosody, speed, and pauses. When the speech is fast, the model generates rapid motion. e model also reacts to inserted speech pauses by generating static poses for the silence period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Analysis of the Learned Style</head><p>Embedding Space e proposed model can generate di erent gesture styles for the same speech. <ref type="figure">Figure 11</ref> visualizes the trained style embedding space and the gestures generated with di erent style vectors for the same input speech. To understand the style embedding space closely, we depict the motion statistics of the generated gestures for each style vector corresponding to speaker ID with the marker color and shape in the gure. Colors from red to blue correspond to higher and lower temporal motion variances. A larger motion variance can be called an extrovert style and the opposite is an introvert style. We also calculated the temporal motion variance for the right and le arms separately and used di erent marker shapes to indicate styles of handedness. Styles using the right and le arms more are depicted as and respectively, and the rest are depicted as ?. As shown in <ref type="figure">Figure 11</ref>, similar styles are clustered, and users can easily choose the desired style from the embedding space a er traversing it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS AND LIMITATIONS</head><p>In this paper, we presented a co-speech gesture generation model that generates upper-body gestures from input speech. We proposed a temporally synchronized architecture using the three input modalities of speech text, audio, and speaker ID. e trained model successfully generated various gestures matching the speech text and audio; di erent styles of gestures could be generated by sampling style vectors from a style embedding space. A new metric, FGD, was introduced to evaluate the generation results. e proposed metric was validated using synthetic noisy data and measuring the agreement with human judgements. e proposed generation method showed be er results than previous methods both objectively and subjectively as determined by the FGD metric and human evaluation. We also highlighted di erent properties of the proposed model through various experiments. e model can generate gestures with synthesized audio of various prosody se ings. Additionally, the style embedding space was trained to be a continuous space where similar styles are distributed closely.</p><p>ere is room for improvement in the present research. First, it is di cult to control the gesture generation process. Although style manipulation is possible, users are not able to set constraints on gestures. For example, we might want an avatar to make a deictic gesture when the avatar says a speci c word. Most end-to-end neural models have this controllability issue <ref type="bibr" target="#b28">(Jahanian et al. 2020)</ref>. It would be interesting to extend the current model to have further controllability, for example, by adding constraining poses in the middle of generation. Second, FGD need to be improved. In nonverbal behavior, subtle motion is as important as large motion, but the feature extractor trained by motion reconstruction might fail to capture subtle motion. It is also necessary to separately evaluate motion quality and diversity for in-depth comparisons between generation models. ird, we only considered the motion of upper body, whereas whole-body motion, including facial expressions and nger movements should be integrated. Taking a long-term view of creating an arti cial conversational agent, we would pursue integrating our model with other nonverbal behaviors and with a conversational model. Gestures are deeply related to verbalization according to the information packaging hypothesis <ref type="bibr" target="#b34">(Kita 2000)</ref>, so an integrated model generating speech and gestures together could deliver information more e ciently. A DETAILED ARCHITECTURES <ref type="figure" target="#fig_0">Figure 12</ref> shows the detailed architectures of the encoders, gesture generator, and discriminator. <ref type="figure" target="#fig_1">Figure 13</ref> shows the architecture of the feature extractor used in the Fr?chet gesture distance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MODELS IN HUMAN EVALUATION</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the proposed gesture generation model. The generator generates a sequence of human poses from a sequence of context feature vectors that contain the encoded features of speech text, speech audio, and speaker identity (ID). The features of text, audio, and speaker ID are depicted as red, blue, and green arrows, respectively. The seed poses are also used to ensure continuity between consecutive syntheses. The discriminator is a binary classifier that distinguishes between real human gestures and generated gestures. The number in parentheses indicates the data dimension. The poses are in 27 dimensions since there are nine directional vectors in 3D coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Samples of noisy gesture data to validate evaluation metrics. (a) None (original data), (b) Gaussian noise (? = 0.001), (c) Salt&amp;Pepper noise (? = 0.1), (d) Temporal noise (? = 5), (e-f) Multiplicative transformation in eigenposes (? = 0.0, 2.0), (g) Mismatched sample ? Salt&amp;Pepper noise:p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(b)  and (c), the sample with Gaussian noise still look like human poses, though with some distortions, whereas the sample with S&amp;P noise show unrealistic poses where the neck is out of the upper body. e samples with Gaussian noise are more perceptually plausible gestures than those Results of the metric validation experiment on the synthetic noisy dataset showing four types of noise. The disturbance level increases as ? increases except for the multiplicative transformation. The disturbance level is lowest when ? = 1.0 for the multiplicative transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Validation learning curves measured by mean absolute error of joint coordinates (MAEJ) and Fr?chet gesture distance (FGD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Sample results of (a) a entional Seq2Seq, (b) Speech2Gesture, (c) joint embedding, and (d) the proposed model for the same input speech. Seven evenly sampled frames are shown for the resulting pose sequences. The last column shows motion history images in which all frames are superimposed. Please see the supplementary video for animated results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>The results of human evaluation for the three questions about (a) preference, (b) human-likeness of motion, and (c) speech-gesture match. The ranking is calculated using the Bradley-Terry model and the horizontal axis represents the winning probability against the other methods. Mean and standard deviation are depicted through Bayesian inference for the Bradley-Terry model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig</head><label></label><figDesc>. once handed me a very thick book, it was his family's legacy" . 10. Co-speech gesture generation results with (a) original human speech audio, (b) synthesized audio of a male voice, (c) synthesized audio of female voice, and (d) synthesized audio of a female voice with pauses. The proposed model can generate gestures from synthesized audio of di erent voices and rhythm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Detailed architectures of the (a) audio encoder, (b) text encoder, (c) speaker embedding, (d) gesture generator (assumed two seed poses), and (e) discriminator. BN stands for batch normalization, FC for fully connected layer, and TCN for temporal convolutional network. Detailed architecture of the autoencoder of the Fr?chet gesture distance. BN stands for batch normalization, FC for fully connected layer, and ConvTr for transposed convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of comparisons with state-of-the-art models. Lower numbers indicate be er performance (Bold: best, Underline: second).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results of the ablation study for the proposed model. Lower numbers are be er. Ablations are not accumulated.</figDesc><table><row><cell>Con guration</cell><cell></cell><cell></cell><cell>FGD</cell></row><row><cell>Proposed (no ablation)</cell><cell></cell><cell></cell><cell>3.729</cell></row><row><cell>Without speech text modality</cell><cell></cell><cell></cell><cell>4.701</cell></row><row><cell cols="2">Without speech audio modality</cell><cell></cell><cell>4.874</cell></row><row><cell>Without speaker ID</cell><cell></cell><cell></cell><cell>6.275</cell></row><row><cell>Without adversarial scheme</cell><cell></cell><cell></cell><cell>9.712</cell></row><row><cell>Without regularization terms L</cell><cell>style G</cell><cell>and L KLD G</cell><cell>5.756</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The list of the gesture generation models used in the human evaluation. * denotes the epoch having the best FGD.</figDesc><table><row><cell>Model</cell><cell>Training stage</cell><cell>FGD</cell></row><row><cell></cell><cell>(epochs)</cell><cell></cell></row><row><cell>Proposed model</cell><cell>89  *</cell><cell>3.729</cell></row><row><cell>Proposed model without regulariza-</cell><cell>83  *</cell><cell>5.756</cell></row><row><cell>tion terms</cell><cell></cell><cell></cell></row><row><cell>Proposed model without adversarial</cell><cell>87  *</cell><cell>9.712</cell></row><row><cell>scheme</cell><cell></cell><cell></cell></row><row><cell>Proposed model without text modal-</cell><cell>20</cell><cell>12.144</cell></row><row><cell>ity</cell><cell></cell><cell></cell></row><row><cell>Proposed model without audio</cell><cell>16</cell><cell>16.558</cell></row><row><cell>modality</cell><cell></cell><cell></cell></row><row><cell>A entional Seq2Seq</cell><cell>66  *</cell><cell>18.054</cell></row><row><cell>Speech2Gesture</cell><cell>86  *</cell><cell>19.254</cell></row><row><cell>Joint embedding model</cell><cell>98  *</cell><cell>22.083</cell></row><row><cell>Proposed model without adversarial</cell><cell>17</cell><cell>26.328</cell></row><row><cell>scheme and audio modality</cell><cell></cell><cell></cell></row><row><cell>A entional Seq2Seq</cell><cell>20</cell><cell>28.273</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>lists the models used in the human evaluation. :16 ? Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee</figDesc><table><row><cell>(t x32)</cell><cell></cell></row><row><cell>Conv 1D</cell><cell>FC</cell></row><row><cell>Conv 1D / BN</cell><cell></cell></row><row><cell>Conv 1D / BN</cell><cell></cell></row><row><cell>Conv 1D / BN</cell><cell></cell></row><row><cell>(1x68267)</cell><cell></cell></row><row><cell>Speech Audio</cell><cell></cell></row></table><note>ACM Transactions on Graphics, Vol. 39, No. 6, Article 222. Publication date: December 2020.222</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Transactions on Graphics, Vol. 39, No. 6, Article 222. Publication date: December 2020.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENTS e authors thank the anonymous reviewers for their thorough and valuable comments. is work was supported by the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2017-0-00162, Development of Human-care Robot Technology for Aging Society). Resource supporting this work were provided by the 'Ministry of Science and ICT' and NIPA ("HPC Support" Project).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language2Pose: Natural Language Grounded Pose Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Simon Alexanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Folk Dance Evaluation Using Laban Movement Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stavrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><surname>Charalambous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephania</forename><forename type="middle">Loizidou</forename><surname>Himona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All speaker identities are mapped to style feature vectors f style , and we visualize the feature vectors in two dimensions by using UMAP (McInnes et al. 2018). The points represent degrees of motion variance via color and degree of handedness by its marker types. We labeled the sampled style vectors according to the overall motion variance and handedness as &apos;IB&apos; for the introvert style of moving both hands similarly, &apos;ER&apos; for the extrovert style of moving the right hand more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bok</forename><surname>? Youngwoo Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Haeng</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
	<note>Jaehong Kim, and Geehyuk Lee Fig. 11. Visualization of the style embedding space and sample generation results for the di erent style vectors. and so on. All gesture results are generated from the same speech</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<title level="m">Multimodal Machine Learning: A Survey and Taxonomy. IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation of Speech and Gestures: Temporal Synchrony Follows Semantic Synchrony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Aksu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Gesture and Speech in Interaction</title>
		<meeting>the 2nd Workshop on Gesture and Speech in Interaction</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pros and Cons of GAN Evaluation Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pipe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Melhuish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanian</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">E ects of Robot-Performed Co-Verbal Gesture on Listener Behaviour</title>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS International Conference on Humanoid Robots</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="458" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonverbal Behaviors, Persuasion, and Credibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Judee K Burgoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Birk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human communication research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="140" to="169" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BEAT: the Behavior Expression Animation Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>H?gni Vilhj?lmsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bickmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Life-Like Characters</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="163" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting Coverbal Gestures: A Deep and Temporal Modeling Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Intelligent Virtual Agents</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="152" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synchronization of Speech and Gesture: Evidence for Interaction in Action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hagoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page">1726</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<title level="m">Extensions of Gaussian Processes for Ranking: Semi-supervised and Active Learning. Learning to Rank</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Why Rate When You Could Compare? Using the &quot;EloChoice&quot; Package to Assess Pairwise Comparisons of Perceived Physical Strength</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">L</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Penton-Voak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-Objective Adversarial Gesture Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ylva</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motion, Interaction and Games</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Sentiment-Speci c Word Embedding via Global Sentiment Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengcheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Arti cial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Individual Styles of Conversational Gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gefen</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3497" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Google Cloud Text-to-Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<idno>google.com/text-to-speech Accessed: 2020-03-01</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Omas Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea L Po</forename><surname>Autumn B Hoste Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">E ects of Personality and Social Situation on Representational Gesture Production. Gesture</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="62" to="83" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning-Based Modeling of Multimodal Behaviors for Humanlike Robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Human-Robot Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust Estimation of a Location Parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">e Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Hu-man3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the &quot;Steerability&quot; of Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards Social Arti cial Intelligence: Nonverbal Social Signal Prediction in A Triadic Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Cikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10873" to="10883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Hew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shari</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08466</idno>
		<title level="m">Fr?chet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">C-3PO: Cyclic-ree-Phase Optimization for Human-Robot Motion Retargeting based on Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Haeng</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gesture Generation by Imitation: From Human Behavior to Computer Character Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kipp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Universal-Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How Representational Gestures Help Speaking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotaro</forename><surname>Kita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and gesture</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="162" to="185" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speech Gesture Generation from the Trimodal Context of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Brigi E Krenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pelachaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Towards a ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
	<note>Hannes Pirker, Kristinn R ?risson, and Hannes Vilhj?lmsson</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Common Framework for Multimodal Generation: e Behavior Markup Language</title>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Intelligent Virtual Agents</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="205" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing Input and Output Representations for Speech-Driven Gesture Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoshi</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Intelligent Virtual Agents</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gesticulator: A Framework for Semantically-Aware Speech-Driven Gesture Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Sanne Van Waveren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iolanda</forename><surname>Alexanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gesture Controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Sebastian Run</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Virtual Character Performance From Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaux</forename><surname>Lhommet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">UMAP: Uniform Manifold Approximation and Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gro?berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source So ware</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hand and Mind: What Gestures Reveal About ought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Gesture and ought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding Motion Capture for Computer Animation and Video Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Menache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gesture Modeling and Animation Based on a Probabilistic Recreation of Speaker Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Gentle: A Forced Aligner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ochshorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Hawkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Je Rey Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multimodal Continuous Turn-Taking Prediction Using Multiscale RNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Ma Hew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Skantze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Rey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Speech-Driven Animation with Meaningful Behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">FVD: A new Metric for Video Generation</title>
		<ptr target="//doc.aldebaran.com/2-5/indexdevguide" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<editor>omas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha?l Marinier, Marcin Michalski, and Sylvain Gelly</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
	<note>NAOqi API Documentation</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gesture and Speech in Interaction: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Zo A Malisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Special Iss.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hand Gestures and Verbal Acknowledgments Improve Human-Robot Rapport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nah</forename><forename type="middle">Young</forename><surname>Jason R Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saechao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="334" to="344" />
		</imprint>
	</monogr>
	<note>Sharon Hershenson, Ma hias Scheutz, and Linda Tickle-Degnen</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Diversity-Sensitive Conditional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo-Ri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geehyuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4303" to="4309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">e Unreasonable E ectiveness of Deep Features as a Perceptual Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
