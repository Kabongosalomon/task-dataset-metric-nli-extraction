<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Pal</surname></persName>
							<email>ankit.pal@saama.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Saama AI Research Chennai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logesh</forename><forename type="middle">Kumar</forename><surname>Umapathi</surname></persName>
							<email>logesh.umapathi@saama.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Saama AI Research Chennai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malaikannan</forename><surname>Sankarasubbu</surname></persName>
							<email>malaikannan.sankarasubbu@saama.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Saama AI Research Chennai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address realworld medical entrance exam questions. More than 194k high-quality AIIMS &amp; NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects &amp; topics. A detailed explanation of the solution, along with the above information, is provided in this study.</p><p>Data and Code Availability The dataset to reproduce these experiments and the leaderboard to track the progress of MedMCQA is available at medmcqa.github.io</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question Answering (QA) is an important and challenging research area in Natural Language Processing (NLP). QA systems enable efficient access to the vast amount of information available that exists in text format.</p><p>In recent times, a significant amount of work has been done on constructing a question-answer dataset <ref type="bibr">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b10">(Rajpurkar et al., , 2018</ref><ref type="bibr" target="#b11">Reddy et al., 2019;</ref><ref type="bibr">Kwiatkowski et al., 2019;</ref><ref type="bibr">Yang et al., 2015)</ref> reading comprehension datasets <ref type="bibr" target="#b18">(Yang et al., 2018;</ref><ref type="bibr">Lai et al., 2017;</ref><ref type="bibr" target="#b19">Zellers et al., 2018;</ref><ref type="bibr" target="#b16">Yagcioglu et al., 2018;</ref><ref type="bibr" target="#b6">Dua et al., 2019;</ref><ref type="bibr" target="#b1">Bajaj et al., 2018;</ref><ref type="bibr">Huang et al., 2019)</ref>, extractive question answering <ref type="bibr">(Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Trischler et al., 2017)</ref>, healthcare domain QA <ref type="bibr">(Jin et al., 2019;</ref><ref type="bibr">?uster and Daelemans, 2018</ref>; M?ller The Treatment of phimosis in children is dependent on the parent's preference, however preputial if phimosis is causing ballooning of prepuce, circumcision is strongly considered.</p><p>A five-year-old child presents with ballooning of prepuce after micturition. Examination reveals preputial adhesions. Which of the following is the best treatment? However, despite these successful efforts, automatic questions answering for real medical examination is still a challenge that is less explored. This type of real-world examination dataset on complex medical subjects like pharmacology, medicine, surgery, etc., is scarce. Apart from their scarcity, the requirement of a comprehensive understanding of the domain, matching human experts, makes them appealing for research pursuits. Before this attempt, very few works have been done to construct biomedical MCQA datasets <ref type="bibr">(Vilares and Gomez-Rodr, 2019)</ref>, and they are (1) mostly small, containing up to few thousand questions, and (2) cover a limited number of Medical topics and Subjects.</p><p>Thus, a large-scale, diverse medical QA dataset is needed to accelerate research and facilitate more consistent and effective open-domain QA models in Medical-QA. This paper addresses the aforementioned limitations by introducing MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. The dataset consists of 194k high-quality medical domain MCQs covering 2.4k healthcare topics and 21 medical subjects to provide a reliable and diverse benchmark. Apart from the question, the correct answer(s), and other options., it also consists of various ancillary data, the primary being a detailed explanation of the solution.</p><p>Questions are taken from AIIMS &amp; NEET PG entrance exam MCQs, where graduate medical students are evaluated on their professional knowledge. Questions in these exams are challenging and generally require deeper domain and language understanding as it tests the 10+ reasoning abilities across a wide range of medical subjects &amp; topics. Hence a model must be trained to find relevant information from the open domain knowledge base, reason over them, and choose the correct answer. <ref type="figure" target="#fig_1">Fig.1</ref> shows two example questions, their corresponding explanation, and answers from the study dataset.</p><p>An in-depth analysis &amp; a thorough evaluation of the dataset are conducted. The baseline experiments on this dataset with the current state-of-theart methods can only answer 47% of the question correctly, which is far behind the performance of human candidates (merit candidates of these exams score an average of 90% marks). Error analysis and results indicate possibilities for improvement in the current methods' reasoning and medical domain question answering. It is believed that this dataset would be an appropriate testbed for future research in this direction.</p><p>In brief, the contributions of this study are as follows. ? Quality Detailed statistics, analysis of the data, and fine-grained evaluation per medical subject are provided, yielding a more precise comparison between models. Each sample contains a question, correct answer(s), other options, and a detailed explanation of the solution.</p><p>? Evaluation of quality Extensive experiments are conducted using high-performance pretrained medical domain models. Error analysis is also provided to illustrate the major challenges of this task. The baseline experiments on this dataset with the most current state-ofthe-art methods answer only 47% of the question correctly, which is far behind the human performance of 90%, indicating possibilities for improvement in models' reasoning ability &amp; constitutes a challenging benchmark for future research.</p><p>?   <ref type="bibr">(1991-present)</ref> from the official websites are also used to create the MedMCQA.</p><p>The dataset contains MCQs with fine-grained human-labeled classes on various graduation level medical subjects. Each sample contains ID, question, correct answer, and options. Besides, an explanation of the solution is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Preprocessing &amp; Quality Checks</head><p>To ensure that all the questions are answerable using textual input only, the following steps were taken to clean the raw data, considering questions from several data sources,</p><p>? Questions with an inconsistent format were excluded, e.g., a question where the number of options was not four(excluding punctuation marks).</p><p>? Questions with no best answer and missing or null candidates were also omitted.</p><p>? Questions whose validity relied on external information were filtered, i.e., the articles and questions containing images or tables.</p><p>? Questions containing the keywords "equation", "India", "graph", "map" etc., were removed using a manually curated list of words.</p><p>? Further, heuristic rules were also used. For example, in some cases, the question contained HTML tags, special symbols, URLs, extra whitespaces, and missing options. Different tools were used, e.g., a spell checker, an HTML parser, to identify and correct these cases.</p><p>? A proofreading tool, 'Grammarly' was used for all the questions, options, and explanations in the dataset to fix the grammar, punctuation, and spelling mistakes. Appropriate suggestions from the tool were applied to the content with human supervision to improve the dataset's quality. As a result, many errors could be corrected</p><p>? Lastly, all duplicated questions were removed.</p><p>Additional data cleansing steps were carried out to ensure that the question has provided information that matches the data quality goals. In the dataset, leakages of similar questions from the training data to test and dev could artificially inflate the models' performance. This is avoided by building the development and test set to include sufficiently different training data questions.</p><p>The Levenshtein distance between each pair of questions was computed in the entire dataset. If the similarity between the two documents was larger than 0.9, the question was excluded from the development and test set. The final dataset contains 183K train examples, 6K in the development set, and 4K in the test set.  <ref type="table" target="#tab_6">table 2</ref> An additional informative statistic is the count of unique tokens in the dataset plotted in <ref type="figure" target="#fig_3">Fig. 4</ref>. Vocabulary size is a good measure of linguistic and domain complexity associated with a text corpus and influences the models' performance. It is observed    the question might be asked for the most probable diagnosis/the most appropriate treatment or examination required/mechanism of a certain condition, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data statistics</head><p>The majority of the dataset questions are nonfactoid and open-ended in nature and seek detailed information about the health condition. Questions in MedMCQA are fairly long, with a mean length of 12.77 words, indicating the compositional nature of questions and different levels of complexity and details covered.</p><p>To understand the types of questions in MedM-CQA, 25% of questions were sampled, and their properties were analyzed manually. It was observed that 68% of the questions started with an interrogative word, which generally tends to be open-ended. The dataset also contained many dichotomous questions, which often require explanations. The diversity of questions in the MedMCQA makes it a challenging dataset containing many aspects of medical knowledge. Another distinguishing factor of this dataset is that it has questions that were created for and by human domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Answer types</head><p>In the dataset, each question contains four options with an average length of 2.69 tokens. Out of which, 25% examples were sampled from the development set, and the answer types are presented in <ref type="figure" target="#fig_4">Fig. 5</ref>. As shown, MedMCQA covers a broad range of answer types, which matches the analysis on questions' contribution. The answers were manually categorized, and it was observed that answers re-garding drug/medicine's name accounted for 22.49%. Medical procedure/Treatment type aiming to determine, measure, or diagnose a condition or parameter accounted for 18.74% of answers. In comparison, 11.24% of answers were related to the quantity of dose(in unit). It was observed that side effects, causes &amp; affected body parts accounted for 12.74%, 10.49% &amp; 9.75% of the dataset. The rest of the answer groups contained fewer instances of the time period, adverse events &amp; other types. <ref type="figure">Fig. 8(A)</ref> in the Appendix presents the distribution of medical topics per subject for the datasets. Almost 95% of the subjects contain above 50 topics, while 70% of subjects exceed 100 topics exhibiting a plethora of medical content. Topics range from Medicine (Endocrinology, Infection, Haematology, Respiratory, etc.), Surgery (General Surgery, Endocrinology, breast, and Vascular surgery, etc.) to Radiology &amp; Biochemistry. This wide range of topics increases the dataset's difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Subject &amp; Topic Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Reasoning Types</head><p>To provide a detailed &amp; better understanding of the dataset? reasoning types, 25% of questions from MedMCQA were sampled randomly. The reasoning types required to answer were manually analyzed. The procedure was followed, and the annotation types presented in <ref type="bibr" target="#b4">(Clark et al., 2018)</ref> were re-used to categorize them into the following reasoning types:  ? Question logic In this, the reasoning is tested by excluding the distractor.</p><p>? Factual These are the questions that have facts as answers.</p><p>? Explanation/definition The questions that require selection of definition or explanation or a term/phenomenon.</p><p>? MultiHop Reasoning To answer these questions, the reasoning is required from multiple passages.</p><p>? Analogy In these types of questions, the responder must select the most similar/analogous answer.</p><p>? Teleology/purpose Requires understanding of the purpose of a phenomenon/a thing.</p><p>? Comparison Questions that require reasoning by comparing multiple options.</p><p>? Fill in the blanks The responder selects the most appropriate answer suitable to fill the blanks.</p><p>? Natural language inference Determining whether a hypothesis is true, false (contradiction), or neutral given an assumption.</p><p>? Mathematical Questions that require mathematical critical thinking and logical reasoning.</p><p>? Treatment Questions that require selection of a correct treatment method for a given ailment / condition.</p><p>? Diagnosis Questions that require selection of a correct cause of a given ailment / condition.  Natural language inference 0.82% <ref type="figure" target="#fig_5">Figure 6</ref>: Relative sizes of Reasoning Types in MedMCQA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Baseline Models</head><p>The primary motivation of the baseline experiments is to understand the adequacy of the current models in answering multiple-choice questions meant for human domain experts (post-graduate medical students) and to understand the level of domain specificity required in the models. Therefore, models and knowledge sources with varying levels of specificity are selected. We consider four existing models in our baseline experiments. They are based on different pre-trained language models using Transformers architecture <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref> , including BERT <ref type="bibr">(Devlin et al., 2019)</ref> , SciBERT <ref type="bibr">(Beltagy et al., 2019)</ref>, <ref type="bibr">BioBERT (Lee et al., 2020)</ref> and PubmedBERT <ref type="bibr">(Gu et al., 2022)</ref>. We finetuned these models on our training dataset in a multiclass classification fashion. We consider models of base size. BERT is evaluated for its out-domain pretraining, SciBERT and BioBERT for their mixed domain and in-domain continual training, and Pubmed-BERT for its in-domain pretraining. These models are explained in detail in the following section,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">SciBERT</head><p>SciBERT <ref type="bibr">(Beltagy et al., 2019</ref>) is a pretrained language model based on BERT. The model has been pre-trained from scratch on 1.14M papers on the semantic scholar. Even though SciBERT has been pretrained from scratch, it has a mix of computer science (18%) and biomedical domain (82%), making it a mix-domain pretrained model. The uncased version of the model that uses a vocabulary called scivocab is used, which is a domain-specific vocabulary of size 30K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">BioBERT</head><p>BioBERT <ref type="bibr">(Lee et al., 2020)</ref> is the first biomedical domain-specific pretrained language model based on BERT. The model is initialized with standard BERT weights (pretrained from Wikipedia and BookCorpus), and continual pretraining is performed with PubMed abstracts and full texts. The model uses the same vocabulary as the standard BERT model. The base variant of the 1.1 version of the model is used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">PubMedBERT</head><p>PubMedBERT <ref type="bibr">(Gu et al., 2022)</ref> is a recent domainspecific pre-trained language model that is first to pretrain only on in-domain texts (PubMed abstracts and full texts). The base version of the model trained with both abstracts and full texts is used in the experiments. This model is used to evaluate the performance of a fully in-domain pre-trained model on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Retriever models</head><p>With the recent success of neural retrievers, dense passage retrieval <ref type="bibr">(Karpukhin et al., 2020)</ref>, and Pub-MedBERT <ref type="bibr">(Gu et al., 2022)</ref> were utilized to evaluate Wikipedia and PubMed as knowledge bases, respectively. Dense passage retriever follows a siamese/biencoder architecture; One encoder encodes the documents and another to encode the query, originally trained with Maximum inner product search objective. The pretrained DPR model and Wikipedia index from Transformer's library <ref type="bibr">(Wolf et al., 2020)</ref> were used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>To complement the motivation stated in section 5, The reader models were chosen with varying domain specificity levels. The contribution of external knowledge sources (Wikipedia and PubMed) was evaluated by providing these sources as contexts. Furthermore, an ablation study was also performed on context by training and evaluating all the models without context. This was done to understand the contribution of external context and the usefulness of the internal knowledge stored in these domain-specific models. The baseline experiments are broadly classified as follows,</p><p>? Out-Domain: Pre-trained models trained on out-domain corpora like Wikipedia and Book corpus were used in this experiment type.</p><p>? Mix domain (continual): Pre-trained models trained on out-domain initially and later adapted to in-domain or trained from scratch on both out-domain and in-domain corpora were used in this experiment.</p><p>? In-Domain: Pre-trained models trained from scratch on in-domain corpora like PubMed abstracts and full texts were used in this experiment type.</p><p>All these experiments were repeated with and without external knowledge context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Pubmed Data Preprocessing</head><p>Before encoding the passages, the passages were truncated to 250 token lengths to fit the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Retriever</head><p>For the experiments that involve context, a re-triever+reader pipeline approach was opted (as introduced in <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>). The out-of-the-box retriever models were used (explained in the section 6.2) from Huggingface's Transformers library <ref type="bibr">(Wolf et al., 2020)</ref> to encode the passages and questions. The passage with the highest cosine similarity was retrieved and used as a context for training the reader models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Reader finetuning</head><p>The finetuning approach was followed as in <ref type="bibr">(Devlin et al., 2019)</ref> to finetune the reader models. The highest scoring contexts for each question are retrieved from the retriever. These contexts are combined by <ref type="bibr">[SEP]</ref> token with the concatenation of question and answer pair. This creates four input sequences per question.</p><p>[CLS] Context <ref type="bibr">[SEP]</ref> Question <ref type="bibr">[SEP]</ref> Option <ref type="bibr">[SEP]</ref> A linear layer with softmax is applied over the output of the <ref type="bibr">[CLS]</ref> token of the encoder. This is to select the most appropriate option for a question and context pair.</p><p>For the experiments that do not use context, question and answer pair concatenation is encoded, and a linear layer with softmax is applied over the output of the <ref type="bibr">[CLS]</ref> token of the encoder to select the most appropriate option for a question.</p><p>[CLS] Question <ref type="bibr">[SEP]</ref> Option <ref type="bibr">[SEP]</ref> The models were finetuned on two Tesla T4 GPUs for 5 epochs with a learning rate of 2e-4 and a batch  <ref type="figure">Figure 7</ref>: The Retriever+Reader Pipeline for Open-Domain Question Answering system used in our experiments.Dense passage retrieval <ref type="bibr">(Karpukhin et al., 2020)</ref> and PubMedBERT <ref type="bibr">(Gu et al., 2022)</ref> are used to evaluate Wikipedia and PubMed as knowledge bases respectively, while different transformer models (explained in section 5) as reader models.</p><p>size of 16. The model checkpoint with the highest validation score in the 5 epochs was selected and used to evaluate the Test Set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Error Analysis</head><p>The error analysis details on a sample set of mispredictions by the best baseline model (PubMedBERT) is given in this section. The analysis was done manually for about 100 mispredictions that were sampled.This could be used for further research to improve the models/methods on the dataset.</p><p>? Multi-hop reasoning: It was observed that the model often mispredicted the questions related to the cause of an event (diagnosis) and the right course of action (treatment) in a given medical situation. Such questions typically require information on multiple symptoms, ailments, and treatments to select the most appropriate choice. This multiplicity of information is not likely to be present in one passage, possibly the reason for the mispredictions.</p><p>? Incorrect context passages: It is observed that inadequate contexts from the retriever are also major contributors to the mispredictions.</p><p>? It is found that the models mispredicted the questions requiring arithmetic reasoning. This is in line with the observations in <ref type="bibr" target="#b6">(Dua et al., 2019)</ref> on BERT-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Result &amp; Discussion</head><p>In this section, the results from the evaluation of the methods discussed in section 6 are presented.</p><p>? It is observed that PubMedBERT performs better than other models in all the categories. This aligns with the results from <ref type="bibr">(Gu et al., 2022)</ref> where PubMedBERT surpasses all other biomedical models in the majority of BLURB tasks. Examples of correct and incorrect predictions of the model is presented in <ref type="table">Table A</ref> ? PubMedBERT is followed by SciBERT (mix domain pretraining) and BioBERT (continual pretraining) in accuracy. From this result, it can be inferred that the model's performance decreases with a decrease in domain specificity of the models and external knowledge sources.</p><p>? It is observed that there is an insignificant improvement in the model's performance when Wikipedia is used as context compared to without context results, and the model variants trained on PubMed, which have a 4-7% improvement in the performance. This can be attributed to the domain specificity of the external knowledge source required by the dataset. The majority of the reasoning types (Diagnosis, treatment, etc.) mentioned in 4.4 require domain expertise as these questions are intended for post-graduate medical students.</p><p>? The subject wise accuracies of the top PubMed-BERT model is presented in   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>of vitamin B12 results in megaloblastic anemia and demyelination. It can cause subacute combined degeneration of the spinal cord and peripheral neuritis. E E Surgery</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Samples from the MedMCQA dataset, along with the answer's explanation. ( : the correct answer) et al., 2020) and the organization of workshops &amp; competitions such as the Question Answering in the medical domain &amp; BioASQ Challenge (Abacha et al., 2019; Nentidis et al., 2020)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) distribution of Pubmed context length (b) Distribution of question length (c) Distribution of answer length (d) Distribution of explanation length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Distribution of unique tokens &amp; Cumulative Frequency Graph in the union of Train, Test, and Development split in MedMCQA dataset. The vocabulary size in the AIIMS PG exams (Test Set) is larger than that of the NEET exams (Dev. Set). Thus indicating the correlation between vocabulary size and difficulty level of the exam.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Relative sizes of Answer Types in MedMCQA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>shows statistics &amp; examples of major reasoning types in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Comparison of MedMCQA with several existing MCQA datasets(MedQA(Zhang et al., 2018),</cell></row><row><cell>HEAD-QA(Vilares and Gomez-Rodr, 2019)) in the medical domain. represents the dataset that has the</cell></row><row><cell>feature and represents it does not</cell></row><row><cell>the text, O represents the candidate options, mul-</cell></row><row><cell>tiple candidate answers are given for each question</cell></row><row><cell>O = {O 1 , O 2 , ..., O n }. The goal is to select the single</cell></row><row><cell>or multiple answers from the option set.The ground</cell></row><row><cell>truth label of a data point is y ? R n where y i =</cell></row><row><cell>{0, 1} and n is the number of options, the objective</cell></row><row><cell>is to learn a prediction function f : X ? y</cell></row><row><cell>2.2. Dataset collection</cell></row><row><cell>All India Institute of Medical Sciences (AIIMS PG) &amp;</cell></row><row><cell>National Eligibility cum Entrance Test (NEET PG)</cell></row><row><cell>are the two medical entrance exams conducted by</cell></row><row><cell>All India Institute for Medical Sciences (AIIMS) &amp;</cell></row><row><cell>National Board of Examinations (NBE), respectively,</cell></row><row><cell>for providing admission to the postgraduate medical</cell></row><row><cell>courses. The applicants must have obtained an Bach-</cell></row><row><cell>elor of Medicine and Bachelor of Surgery (MBBS)</cell></row><row><cell>from a recognized institute to appear for the exams.</cell></row><row><cell>The exams are used to evaluate the candidates in</cell></row><row><cell>a structured format, namely, Diagnostic Reasoning</cell></row><row><cell>and Treatment, Pharmacology, Psychology, Biology,</cell></row><row><cell>Physical Examination, General Management Strate-</cell></row><row><cell>gies, Medical Knowledge, and many other aspects of</cell></row><row><cell>health and general attitude demeanor of the patient</cell></row><row><cell>and the examiners. These exams are a comprehen-</cell></row><row><cell>sive evaluation of the professional skills of a medical</cell></row><row><cell>practitioner.</cell></row><row><cell>In this paper, the raw data is collected from open</cell></row><row><cell>websites and books that put together several mock</cell></row><row><cell>tests and online test series created by medical pro-</cell></row><row><cell>fessionals. In addition to the collected data, AIIMS</cell></row><row><cell>&amp; NEET PG examination questions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The final dataset contains 193,155 questions.</figDesc><table><row><cell></cell><cell>which 29.16%</cell></row><row><cell>what 23.36%</cell><cell></cell></row><row><cell></cell><cell>of 3.89% was 1.17% has 1.95%</cell></row><row><cell>is 9.26% in 13.39%</cell><cell>to 6.89% are 5.1% by 5.84%</cell></row><row><cell cols="2">Figure 2: Relative sizes of Question Types in MedMCQA</cell></row><row><cell>2.4. Split Criteria</cell><cell></cell></row></table><note>The goal of MedMCQA is to emulate the rigor of real word medical exams. To enable that, a predefined split of the dataset is provided. The split is by exams instead of the given questions. This also ensures the reusability and generalization ability of the models. The training set of MedMCQA consists of all the collected mock &amp; online test series, whereas the test set consists of all AIIMS PG exam MCQs (years 1991- present). The development set consists of NEET PG exam MCQs (years 2001-present) to approximate real exam evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This dataset covers many medical subjects based on the AIIMS &amp; NEET PG entrance exams. The train, development, and test set consist of 182,822 , 4,183 &amp; 6,150 questions with an average token length of 12.35, 13.91 &amp; 9.68, respectively. The general statistics of preprocessed data are summarized in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>that the length of questions and the vocabulary size in the AIIMS PG exams (test set) are larger than that of the NEET PG exams (dev. set). Hence, it can be inferred that questions from AIIMS are more complex than NEET.</figDesc><table><row><cell></cell><cell>Train</cell><cell>Test</cell><cell>Dev</cell><cell>Total</cell></row><row><cell>Question #</cell><cell>182,822</cell><cell>6,150</cell><cell>4,183</cell><cell>193,155</cell></row><row><cell>Vocab</cell><cell>94,231</cell><cell cols="2">11,218 10,800</cell><cell>97,694</cell></row><row><cell>Max Q tokens</cell><cell>220</cell><cell>135</cell><cell>88</cell><cell>220</cell></row><row><cell>Max A tokens</cell><cell>38</cell><cell>21</cell><cell>25</cell><cell>38</cell></row><row><cell>Max E tokens</cell><cell>3,155</cell><cell>651</cell><cell>695</cell><cell>3,155</cell></row><row><cell>Avg Q tokens</cell><cell>12.77</cell><cell>9.93</cell><cell>14.09</cell><cell>12.71</cell></row><row><cell>Avg A tokens</cell><cell>2.69</cell><cell>2.58</cell><cell>3.19</cell><cell>2.70</cell></row><row><cell>Avg E tokens</cell><cell>67.52</cell><cell>46.54</cell><cell>38.44</cell><cell>66.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>MedMCQA dataset statistics, where Q, A, E represents the Question, Answer, and Explanation, respectively4. Data AnalysisAn analysis of the dataset is presented in the subsequent sections. The difficulty and diversity of questions and the answers were analyzed to understand the MedMCQA dataset's properties. The complexity of MedMCQA is demonstrated by considering the question and reasoning types covered in the dataset.4.1. Difficulty and Diversity of QuestionsIn clinical medicine, a diverse number of questions are possible as it is spread over a range of topics. For example, given the description of a patient's condition,</figDesc><table><row><cell>20000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train Test</cell></row><row><cell>15000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dev</cell></row><row><cell>10000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>Ph ar m ac ol og y EN T</cell><cell>PS M</cell><cell>FM</cell><cell>De nt al</cell><cell>Su rg er y</cell><cell>Pe di at ric s O ph th al m ol og y Sk in</cell><cell>O rth op ae di cs Bi oc he m ist ry</cell><cell>Un kn ow n</cell><cell>An at om y</cell><cell>Ra di ol og y O &amp;G</cell><cell cols="2">M ed ici ne</cell><cell cols="2">M icr ob io lo gy</cell><cell cols="2">Ph ys io lo gy</cell><cell>Ps yc hi at ry</cell><cell>Pa th ol og y</cell><cell>An ae st he sia</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.00</cell><cell>Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.75</cell><cell>Test Dev</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell>Ph ar m ac ol og y EN T</cell><cell>PS M</cell><cell>FM</cell><cell cols="2">De nt al Su rg er y</cell><cell>Pe di at ric s O ph th al m ol og y Sk in</cell><cell>O rth op ae di cs Bi oc he m ist ry</cell><cell>Un kn ow n</cell><cell>An at om y</cell><cell cols="2">Ra di ol og y O &amp;G M ed ici ne</cell><cell cols="2">M icr ob io lo gy</cell><cell cols="2">Ph ys io lo gy</cell><cell cols="2">Ps yc hi at ry</cell><cell>Pa th ol og y</cell><cell>An ae st he sia</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 :</head><label>3</label><figDesc>Fine-grained evaluation per medical subject in test and dev set</figDesc><table><row><cell></cell><cell cols="2">w/o Context</cell><cell>Wiki</cell><cell cols="2">PubMed</cell></row><row><cell>Model</cell><cell>Test</cell><cell cols="4">Dev Test Dev Test Dev</cell></row><row><cell>BertBase</cell><cell>0.33</cell><cell>0.35</cell><cell>0.33</cell><cell>0.35 0.37</cell><cell>0.35</cell></row><row><cell>BioBert</cell><cell>0.37</cell><cell>0.38</cell><cell>0.39</cell><cell>0.37 0.42</cell><cell>0.39</cell></row><row><cell>SciBert</cell><cell>0.39</cell><cell>0.39</cell><cell>0.38</cell><cell>0.39 0.43</cell><cell>0.41</cell></row><row><cell>PubMedBERT</cell><cell>0.41</cell><cell cols="4">0.40 0.42 0.41 0.47 0.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4</head><label>4</label><figDesc>Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: A pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240, February 2020. ISSN 1367-4803. doi: 10.1093/ bioinformatics/btz682. Association for Computational Linguistics, 7 2020. URL https://www.aclweb.org/ anthology/2020.nlpcovid19-acl.18. Predictions from the best model A.1. Correct Predictions</figDesc><table><row><cell>: Performance of all baseline models in</cell></row><row><cell>accuracy (%) on MedMCQA test-dev set</cell></row><row><cell>&amp; topics. It is demonstrated that the dataset is</cell></row><row><cell>challenging for the current state-of-the-art methods</cell></row><row><cell>and domain-specific models, with the best baseline</cell></row><row><cell>achieving only 47% accuracy. It is expected that this</cell></row><row><cell>dataset would facilitate future research in this direc-</cell></row><row><cell>tion.</cell></row><row><cell>Institutional Review Board (IRB)</cell></row><row><cell>This research does not require IRB approval.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022 A. Pal, L.K. Umapathi &amp; M. Sankarasubbu.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5039</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-5039" />
	</analytic>
	<monogr>
		<title level="m">MEDIQA</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/D19-1371</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1371" />
	</analytic>
	<monogr>
		<title level="m">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge. The Allen Institute for Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Tristan Naumann, Jianfeng Gao, and Hoifung Poon</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Results of the seventh edition of the bioasq challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-43887-6_51</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-43887-6_51" />
	</analytic>
	<monogr>
		<title level="j">Communications in Computer and Information Science</title>
		<imprint>
			<biblScope unit="page" from="553" to="568" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/D16-1264</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>URL https</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Head-qa: A healthcare dataset for complex reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gomez-Rodr</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/P19-1092</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">HEAD-QA: A Healthcare Dataset for Complex Reasoning</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19" to="1092" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Transformers: State-ofthe-Art Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Erkut Erdem, and Nazli Ikizler-Cinbis. Recipeqa: A challenge dataset for multimodal comprehension of cooking recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yagcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1237</idno>
		<ptr target="https://www.aclweb.org/anthology/D15-1237" />
	</analytic>
	<monogr>
		<title level="m">WikiQA: A Challenge Dataset for Open-Domain Question Answering</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical exam question answering with largescale reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xien</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clicr: A dataset of clinical case reports for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Simon?uster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
