<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localization Uncertainty Estimation for Anchor-Free Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Won</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Il</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjin</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Bae</surname></persName>
							<email>ysbae@etri.re.krsjhwang82@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localization Uncertainty Estimation for Anchor-Free Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since many safety-critical systems, such as surgical robots and autonomous driving cars operate in unstable environments with sensor noise and incomplete data, it is desirable for object detectors to take the localization uncertainty into account. However, there are several limitations of the existing uncertainty estimation methods for anchor-based object detection. 1) They model the uncertainty of the heterogeneous object properties with different characteristics and scales, such as location (center point) and scale (width, height), which could be difficult to estimate. 2) They model box offsets as Gaussian distributions, which is not compatible with the ground truth bounding boxes that follow the Dirac delta distribution. 3) Since anchor-based methods are sensitive to anchor hyper-parameters, their localization uncertainty could also be highly sensitive to the choice of hyper-parameters. To tackle these limitations, we propose a new localization uncertainty estimation method called UAD for anchor-free object detection. Our method captures the uncertainty in four directions of box offsets (left, right, top, bottom)  that are homogeneous, so that it can tell which direction is uncertain, and provide a quantitative value of uncertainty in [0, 1]. To enable such uncertainty estimation, we design a new uncertainty loss, negative power log-likelihood loss, to measure the localization uncertainty by weighting the likelihood loss by its IoU, which alleviates the model misspecification problem. Furthermore, we propose an uncertainty-aware focal loss for reflecting the estimated uncertainty to the classification score. Experimental results on COCO datasets demonstrate that our method significantly improves FCOS [32], by up to 1.8 points, without sacrificing computational efficiency. We hope that the proposed uncertainty estimation method can serve as a crucial component for the safety-critical object detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Example of 4-directional uncertainty for anchor-free object detection. C L, C R, C T, and C B denote the estimated certainty in [0, 1] value with respect to left, right, top, and bottom. For example, the proposed UAD estimates low top-directional certainty due to its ambiguous head boundary of the cat wearing a hat. This demonstrates that our method enables the detection network to quantify which direction is uncertain due to unclear or obvious objects.</p><p>the object detection is, in addition to achieving good performance. Although object detection is a task that requires both object localization and classification, most of the state-of-the-art methods <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b3">3]</ref> utilize the classification scores as detection scores without considering the localization uncertainty. As a result, these methods output mislocalized detection boxes that are highly overconfident <ref type="bibr" target="#b10">[10]</ref>. Thus the confidence of the bounding box localization should also be taken into consideration when estimating the uncertainty of the detection.</p><p>Recently, there have been many attempts <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">8</ref>] that try to model localization uncertainty for object detection. All of these efforts model the uncertainty of location (center point) and scale (width, height) by modeling their distributions as Gaussian distributions <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b26">26]</ref> with extra channels in the regression output. However, since the center points, widths, and heights have semantically different characteristics <ref type="bibr" target="#b15">[15]</ref>, this approach which considers each value equally is inappropriate for modeling localization uncertainty. For example, the estimated distributions of the center point and scale are largely different according to <ref type="bibr" target="#b15">[15]</ref>. In terms of the loss function for uncertainty modeling, conventional methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">8]</ref> use the negative log-likelihood loss to regress outputs as Gaussian distributions. He et al. <ref type="bibr" target="#b10">[10]</ref> introduce KL divergence loss by modeling the box prediction as Gaussian distribution and the ground-truth box as Dirac delta function. In the perspective of cross-entropy, however, these methods face the model misspecification problem <ref type="bibr" target="#b14">[14]</ref> in that the Dirac delta function can-not be exactly represented with Gaussian distributions, i.e., for any ? and ?, ?(x) ? = N (x|?, ? 2 ).</p><p>Recently, anchor-free methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b32">32]</ref> that do not require heuristic anchor-box tuning (e.g., scale, aspect ratio) have outperformed conventional anchor-box based methods such as Faster R-CNN <ref type="bibr" target="#b26">[26]</ref>, RetinaNet <ref type="bibr" target="#b21">[21]</ref>, and their variants <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b38">38]</ref>. As a representative anchor-free method, FCOS <ref type="bibr" target="#b32">[32]</ref> adopts the concept of centerness as an implicit localization uncertainty that measures how well the center of the predicted bounding boxes fits the ground-truth boxes. The centerness score can be multiplied by the classification score to calibrate the box quality score during the test phase. However, the centerness faces the problem of inconsistency <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b36">36]</ref> between the training and test phase as it is trained separately with the classification network. Besides, the centerness does not fully account for localization uncertainty of bounding boxes (e.g., scale or direction).</p><p>To deal with these limitations, in this paper, we propose Uncertainty-Aware Detection (UAD), which explicitly estimates the localization uncertainty for an anchor-free object detection model. The proposed method estimates the uncertainty of the four values that define the box offsets (left, right, top, bottom) to fully describe the localization uncertainty. It is advantageous to estimate the uncertainty of the four box offsets having a similar semantic characteristic compared to conventional algorithms that estimate localization uncertainty for anchor-based detection <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">8]</ref>. Our method can also capture richer information for localization uncertainty than just the centerness of FCOS.</p><p>The proposed method enables to inform which direction of a box boundary is uncertain as a quantitative value in [0,1] independently from the overall box uncertainty as shown in <ref type="figure">Figure 1</ref> (please refer to more examples in <ref type="figure" target="#fig_1">Fig. 3</ref>). To this end, we model the box offset and its uncertainty as Gaussian distributions by introducing a newly designed uncertainty loss and an uncertainty network.</p><p>To resolve the aforementioned model misspecification <ref type="bibr" target="#b14">[14]</ref> between Dirac delta and Gaussian distribution, we design a novel uncertainty loss, negative power loglikelihood loss (NPLL), inspired by Power likelihood <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31]</ref>, to enable the uncertainty network to learn to estimate localization uncertainty by weighing the log-likelihood loss by Intersection-over-Union (IoU).</p><p>To handle the inconsistency between the training and test phase, we also propose an uncertainty-aware classification by reflecting the estimated uncertainty into the classification score in both the training/inference phase. To this end, we introduce a Certainty-aware representation Network (CRN) to represent features with the classification network jointly. We also define an uncertaintyaware focal loss (UFL) that adjusts the loss contributions of examples differently based on their estimated uncertainties. UFL focuses on the high-quality examples obtaining lower uncertainty (i.e., higher certainty) by weighting the estimated certainty, which enables to generate uncertainty-reflected localization score. The difference from other focal losses such as QFL <ref type="bibr" target="#b19">[19]</ref> and VFL <ref type="bibr" target="#b36">[36]</ref> is that we use the estimated uncertainty as a weighting factor instead of IoU. The 4-directions uncertainty captures the object localization quality better than IoU (i.e., scale), and the uncertainty-based methods empirically yield better performance.</p><p>By introducing the uncertainty network with NPLL and uncertainty-aware classification with UFL to FCOS <ref type="bibr" target="#b33">[33]</ref>, we build an uncertainty-aware detector, UAD. We validate UAD on the challenging COCO <ref type="bibr" target="#b22">[22]</ref> benchmarks. Through extensive experiments, we find that the Gaussian modeling with the proposed NPLL and the newly defined focal loss, UFL, yields better performance over baseline methods. Besides, UAD improves the FCOS <ref type="bibr" target="#b33">[33]</ref> baseline by 1?1.8 gains in AP using different backbone networks without additional computation burden. In addition to detection performance, the proposed UAD accurately estimates the uncertainty in 4-directions as well as the detection performance as shown in <ref type="figure" target="#fig_1">Fig. 1, 3</ref>.</p><p>The main contributions of our work can be summarized as follows:</p><p>-We propose a simple and effective method to measure the localization uncertainty for anchor-free object detectors that can serve as a detection quality measure and provide confidences in [0, 1] for four directions (left, right, top, bottom) from the center of the object. -We propose a novel uncertainty-aware loss function, inspired by power likelihood that weighs the negative log-likelihood loss by the IoU, which resolves the model misspecification problem. -We introduce an uncertainty-aware classification scheme with the proposed uncertainty-aware focal loss by leveraging the estimated uncertainty during both the training and the test phase in a consistent manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anchor-Free Object Detection</head><p>Recently, anchor-free object detectors <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b32">32]</ref> have attracted attention beyond anchor-based methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b38">38]</ref> that need to tune sensitive hyperparameters related to anchor box (e.g., scale, aspect ratio, etc). CornerNet <ref type="bibr" target="#b17">[17]</ref> predicts an object location as a pair of keypoints (top-left and bottom-right). CenterNet <ref type="bibr" target="#b3">[3]</ref> extends CornerNet as a triplet instead of a pair of key points to boost performance. This idea is extended by CenterNet <ref type="bibr" target="#b3">[3]</ref> that utilizes a triplet instead of a pair of key points to boost performance. ExtremeNet <ref type="bibr" target="#b40">[40]</ref> locates four extreme points (top, bottom, left, right) and one center point to generate the object box. Zhu et al. <ref type="bibr" target="#b39">[39]</ref> utilizes keypoint estimation to predict center point objects and regresses to other attributes, including size, orientation, pose, and 3D location. FCOS <ref type="bibr" target="#b32">[32]</ref> views all points (or locations) inside the ground-truth box as positive samples and regresses four distances (left, right, top, bottom) from the points. We propose to endow FCOS with localization uncertainty due to its simplicity and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty Estimation</head><p>Uncertainty in deep neural networks can be estimated in two types <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b18">18]</ref>: epistemic (sampling-based) and aleatoric (sampling-free) uncertainty. Epistemic uncertainty measures the model uncertainty in the models' parameters through Bayesian neural networks <ref type="bibr" target="#b30">[30]</ref>, Monte Carlo dropout <ref type="bibr" target="#b5">[5]</ref>, and Bootstrap Ensemble <ref type="bibr" target="#b16">[16]</ref>. As they need to be re-evaluated several times and store several sets of weights for each network, it is hard to apply them for real-time applications. Aleatoric uncertainty is data and problems inherent such as sensor noise and ambiguities in data. It can be estimated by explicitly modeling it as model output.</p><p>Recent works <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b16">16</ref>] have adopted uncertainty estimation for object detection. Lakshminarayanan et al. <ref type="bibr" target="#b16">[16]</ref> and Harakeh et al. <ref type="bibr" target="#b8">[8]</ref> use Monte Carlo dropout in Epistemic based methods. As described above, since epistemic uncertainty needs to be inferred several times, it is not suitable for real-time object detection. Le et al. <ref type="bibr" target="#b18">[18]</ref> and Choi et al. <ref type="bibr" target="#b2">[2]</ref> are aleatoric based methods and jointly estimate the uncertainties of four parameters of bounding box from SSD <ref type="bibr" target="#b23">[23]</ref> and YOLOv3 <ref type="bibr" target="#b25">[25]</ref>. He <ref type="bibr" target="#b10">[10]</ref> estimates the uncertainty of the bounding box by minimizing the KL-divergence loss for the Gaussian distribution of the predicted box and Dirac delta distribution of the ground-truth box on the Faster R-CNN <ref type="bibr" target="#b26">[26]</ref> (anchor-based method). From the cross-entropy perspective, however, Dirac delta distribution cannot be represented as a Gaussian distribution, which results in a misspecification problem <ref type="bibr" target="#b14">[14]</ref>. To overcome this problem, we design a new uncertainty loss function, negative power log-likelihood loss, inspired by the power likelihood concept <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31]</ref>. The latest concurrent work is the Generalized Focal loss (GFocal) <ref type="bibr" target="#b19">[19]</ref> that represents jointly localization quality, classification, and model bounding box as arbitrary distribution. The distinct difference from GFocal <ref type="bibr" target="#b19">[19]</ref> is that our method estimates 4-directions uncertainties as quantitative values in the range [0, 1] thus, these estimated values can be used as an informative cue for decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Uncertainty-Aware Detection (UAD)</head><p>To estimate uncertainty of object detection, we use an anchor-free detector, FCOS <ref type="bibr" target="#b32">[32]</ref> for the following reasons: 1) Simplicity. FCOS directly regresses the target bounding boxes in a pixel-wise prediction manner without heuristic anchor tuning (aspect ratio, scales, etc). 2) 4-directions uncertainty. anchorbased methods <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b16">16]</ref> regress center point (x,y), width, and height based on each anchor box, while FCOS directly regresses four boundaries (left, right, top, bottom) of a bounding box at each location. Besides, the center, width, and height from the anchor-based methods have different characteristics, whereas the distances between four boundaries and each location are semantically symmetric. In terms of modeling, it is easier to model the values sharing semantic meanings that have similar properties. Furthermore, it enables to notify which direction of a box boundary is uncertain separately from the overall box uncertainty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Feature pyramid FCOS also adopts centerness to suppress the low-quality predicted boxes in the inference stage, which estimates how well center of the predicted box fits any of the objects. However, centerness is an implicit score of uncertainty which is insufficient in measuring localization uncertainty, since it does not fully capture the box quality such as scale or directions. It also has the inconsistency problem, since centerness as localization uncertainty is independently obtained regardless of the classification score and multiplied to the classification score only at the inference phase.</p><p>To overcome such limitations of the centerness-based uncertainty estimation, we propose a novel method, Uncertainty-Aware Detection (UAD), to endow FCOS with a localization uncertainty estimator that reflects the box quality along the 4-directions of the bounding box. This approach allows the network to estimate not only a single measure of object localization uncertainty but also explicitly output the uncertainty in each of the four directions. To this end, firstly, we introduce an uncertainty network to FCOS with a newly proposed uncertainty loss, negative power log-likelihood loss (NPLL). Next, we propose an uncertainty-aware focal loss (UFL) to jointly represent the estimated uncertainty with the classification network during both training and test step. The overview of UAD is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Power Likelihood</head><p>In FCOS <ref type="bibr" target="#b32">[32]</ref>, if the location belongs to the ground-truth box area, it is regarded as a positive sample and as a negative sample otherwise. On each location (x, y), the box offsets are regressed as 4D vector B x,y = [l, r, t, b] ? that is the distances from the location to four sides of the bounding box (i.e., left, right, top, and bottom). The regression targets B g</p><p>x,y = [l g , r g , t g , b g ] ? are computed as,</p><formula xml:id="formula_0">l g = x ? x g lt , r g = x g rb ? x, t g = y ? y g lt , b g = y g rb ? y,<label>(1)</label></formula><p>where (x g lt , y g lt ) and (x g rb , y g rb ) denote the coordinates of the left-top and rightbottom corners of the ground-truth box, respectively. Then, for all locations of positive samples, the IoU loss <ref type="bibr" target="#b35">[35]</ref> is measured between the predicted B x,y and the ground truth B g x,y for the regression loss. To better estimate localization uncertainty than centerness, it is necessary to consider the four directions that represent the object boundary. Therefore, we introduce an uncertainty network that estimates the localization uncertainty of the box based on the regressed box offsets (l, r, t, b). To predict the uncertainties of four box offsets, we model the box offsets as Gaussian distributions and train the network to estimate their uncertainties (standard deviation). Assuming that each instance of box offsets is independent, we use multivariate Gaussian distribution of output B * with diagonal covariance matrix ? B to model each box offset B:</p><formula xml:id="formula_1">P ? (B * |B) = N (B * ; ? B , ? B ),<label>(2)</label></formula><p>where ? is the learnable network parameters.</p><formula xml:id="formula_2">? B = [? l , ? r , ? t , ? b ] ? and ? B = diag(? 2 l , ? 2 r , ? 2 t , ? 2 b</formula><p>) denote the predicted box offset and its uncertainty, respectively.</p><p>Existing works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b2">2]</ref> model ground-truth as Dirac delta distribution and box offset as Gaussian estimation, respectively. <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b2">2]</ref> adopt negative loglikelihood loss (NLL) and <ref type="bibr" target="#b10">[10]</ref> use KL-divergence loss (KL-Loss). In cross-entropy perspective, minimizing NLL and KL-loss is equivalent as below:</p><formula xml:id="formula_3">L = ? 1 N x P D (x) ? logP ? (x),<label>(3)</label></formula><p>where P D and P ? are Dirac delta function and Gaussian probability density function, respectively. When the box offset is located in a ground-truth box, the P D is 1 then Eq. 3 becomes the negative log-likelihood loss. However, this is problematic since the Dirac delta distribution does not belong to the family of Gaussian distributions, which results in the model misspecification problem <ref type="bibr" target="#b14">[14]</ref>. In a number of statistical literature <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31]</ref>, to estimate parameters of interest in a robust way when the model is misspecified, the Power likelihood (P ? (?) w ) is often used to replace the original likelihood, which raises the likelihood (P ? (?)) to a power (w) to reflect how influential a data instance is. To fill the gaps between Dirac delta distribution and Gaussian distribution, we utilize this Power likelihood and introduce a novel uncertainty loss, negative power log-likelihood loss (NPLL), that exploits Intersection-over-Union (IoU) as the power term since the offset that has higher IoU should be more influential. By the property of the logarithm, the log-likelihood is multiplied by the IoU. Thus, the new uncertainty loss is defined as :</p><formula xml:id="formula_4">L uc = ? k?{l,r,t,b} IoU ? logP ? B g k |? k , ? 2 k (4) = IoU ? k?{l,r,t,b} (B g k ? ? k ) 2 2? 2 k + 1 2 log? 2 k + 2log2? ,<label>(5)</label></formula><p>where IoU is the intersection-over-union between the predicted box and the ground-truth box and k is ? {l, r, t, b}. With this uncertainty loss, when the predicted coordinate ? k from the regression branch is inaccurate, the network will output larger uncertainty ? k . Note that unlike centerness, our network is trained to directly estimate the localization uncertainties for the four directions ? u = (? l , ? r , ? t , ? b ) that define the box offsets. This allows to estimate which direction of a box boundary is uncertain, separately from the overall box uncertainty.</p><p>To realize this idea, we add a 3?3 Conv layer with 4 channels as an uncertainty network for FCOS <ref type="bibr" target="#b32">[32]</ref> (w/o centerness) as shown in <ref type="figure">Fig. 2</ref>. Our network predicts a probability distribution in addition to the box coordinates. The mean values ? k of each box offset are predicted from the regression branch, and the new uncertainty network with the sigmoid function outputs four uncertainty values ? k ? [0, 1]. The regression and the uncertainty networks share the same feature (4 Conv layers) as their inputs to estimate the mean ? k and the standard deviation ? k . Note that the computation burden of adding the uncertainty network is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty-Aware Classification</head><p>Dense object detectors including FCOS generate several predicted boxes and then utilize non-maximum suppression (NMS) to filter out redundant boxes by ranking the boxes with classification confidence. However, as the classification score does not account for the quality of the detected bounding box, centerness <ref type="bibr" target="#b32">[32]</ref> or IoU <ref type="bibr" target="#b34">[34]</ref> is often used to weigh the classification confidence at the inference phase to calibrate the detection score. To address the misalignment problem between confidence and localization quality, GFL <ref type="bibr" target="#b19">[19]</ref> or VFL <ref type="bibr" target="#b36">[36]</ref> suggests a method to reflect the box quality (e.g., IoU ) into the classification score in both the training and test phase. Certainty Representation Network. As the localization uncertainty is negatively correlated to the detected box quality, it is natural to apply the estimated localization uncertainty to the classification network for uncertainty-aware classification. The higher the box quality is, the closer the predicted probability is to 1, so that the uncertainty is taken in reverse. Thus, we convert the uncertainty features X uc ? R 4?W ?H from the uncertainty network into certainty features X c ? R 1?W ?H through the certainty representation network (CRN), to combine the confidence scores with the classification features. The CRN consists of two fully-connected layers, W 1 ? R 4?4 and W 2 ? R 1?4 . Specific process is described <ref type="table">Table 1</ref>: Comparison of various focal loss. y is target IoU between the predicted box and the ground-truth as a soft label instead of one-hot category. p denotes the predicted classification score, ? is a weighting factor, and L BCE means binary cross-entropy loss. UFL denotes the proposed uncertainty-aware focal loss. f (? u ) and ? u denote a certainty function and the estimated localization uncertainties from the uncertainty branch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss type</head><formula xml:id="formula_5">y &gt; 0 y = 0 QFL [19] |y ? p| ? ? LBCE ?p ? log(1 ? p) VFL [36] y ? LBCE ??p ? log(1 ? p) UFL (ours) f (?u) ? LBCE ??p ? log(1 ? p)</formula><p>as below:</p><formula xml:id="formula_6">X c = ?(W 2 (?(W 1 (1 ? X uc )))),<label>(6)</label></formula><p>where ? and ? denote the Sigmoid and ReLU activation function, respectively. The certainty feature is multiplied by the classification features X cls ? R 256?W ?H to obtain a representation that accounts for both certainty and the classification score.</p><p>Uncertainty-Aware Focal Loss. Both GFL <ref type="bibr" target="#b19">[19]</ref> and VarifocalNet <ref type="bibr" target="#b36">[36]</ref> introduce IoU-classification representation and new classification losses, Quality focal loss (QFL) and Varifocal loss (VFL), that inherit Focal loss (FL) <ref type="bibr" target="#b21">[21]</ref> for addressing class imbalance between positive/negative examples. <ref type="table">Table 1</ref> describes the detail definitions of focal losses. QFL and VFL utilize IoU score between the predicted box and the ground-truth as a soft label (e.g., y ? [0, 1]), instead of one-hot category (e.g., y ? {0, 1}). ? is a weighting factor and p denotes the predicted classification score. Following FL, QFL focuses on hard positive examples by down-weighting the loss contribution of easy examples with a modulating factor (|y ? p| ? ), while VFL pays more attention to easy examples (i.g., higher-IoU) by weighting the target IoU (i.e., y) and only reduces the loss contribution from negative examples by scaling the loss with a weighting factor of ?p ? . That is, QFL and VFL adjust the loss contribution with respect to training samples by weighting box quality measure (e.g., IoU). Unlike GFL and VFL based on deterministic detection results (e.g., only box boundaries), our method can estimate object boundaries as probabilistic distributions. Specifically, our method estimates the means (? k ) and its uncertainties as standard deviations (? k ), of the four offset distributions. In this way, our method utilizes the estimated uncertainty as a box quality measure instead of IoU as in QFL and VFL. From this perspective, we design a new focal loss, uncertainty-aware focal loss (UFL) for learning uncertainty-aware classification as defined in the third row of <ref type="table">Table 1</ref>. p is the estimated probability from the uncertainty-aware classification network. f (? u ) and ? u denote certainty function and the estimated localization uncertainties from the uncertainty branch, respectively. Instead of weighting the IoU, we use the certainty score obtained from the certainty function f (? u ) to give more attention to high quality examples. In this paper, f (? u ) is defined as below:</p><formula xml:id="formula_7">f (? u ) = 1 4</formula><p>k?{l,r,t,b}</p><formula xml:id="formula_8">(1 ? ? k ),<label>(7)</label></formula><p>which averages (1 ? ? k ) for all k ? {l, r, t, b}. Thus, we weigh the positive examples with the estimated certainty score instead of the target IoU (y). For negative samples, we find that down-weighting the loss contribution yields better performance with a smaller scale factor of ? than VFL (e.g., 0.25 vs. 0.75) in <ref type="table" target="#tab_2">Table 3</ref> (right). Also, following QFL or VFL, we set ? to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><p>We define the total loss L of UADet as below:</p><formula xml:id="formula_9">L = 1 N pos i L uac + 1 N pos i 1 {c * i &gt;i} (L bbox + ?L uc ),<label>(8)</label></formula><p>where L uc is NPLL ( ?3.1), L bbox is the GIoU loss <ref type="bibr" target="#b27">[27]</ref>, and L uac is UFL ( ?3.2). N pos denotes the number of positive samples, 1 is the indicator function, c * i is class label of the location i, and ? is to balance weight for L uc . The summation is calculated over all positive locations i on the feature maps. Since our baseline is FCOS, We follow the sampling strategy of FCOS. With the proposed NPLL and UFL, UADet is learned to estimate uncertainties in four directions as well as the uncertainty-aware classification score. In the test phase, the predicted uncertainty-aware classification score is utilized in the NMS post-processing step for ranking the detected boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experimental setup. In this section, we evaluate the effectiveness of UADet on the challenging COCO <ref type="bibr" target="#b22">[22]</ref> dataset which has 80 object categories. We use COCO train2017 set for training and val2017 set for ablation studies. Final results are evaluated on test-dev2017 in the evaluation server for comparison with state-of-the-art. Since FCOS <ref type="bibr" target="#b32">[32]</ref> without centerness is our baseline, we use the default hyper-parameters of FCOS. We train UADet by stochastic gradient descent algorithm with a mini-batch of size 16 and the initial learning rate is 0.01. For the ablation study, we use ResNet-50 backbone with ImageNet pre-trained weights and 1? schedule <ref type="bibr" target="#b9">[9]</ref> without multi-scale training. For comparison with state-of-the-art methods, we adopt 2? schedule with multi-scale augmentation where the shorter image side is randomly sampled from [640, 800] pixels. We implement the proposed UAD based on Detectron2 <ref type="bibr" target="#b6">[6]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation study</head><p>Power likelihood. We investigate the effectiveness of the proposed Gaussian modeling with the proposed uncertainty loss (i.e., NPLL) for localization uncertainty. <ref type="table" target="#tab_1">Table 2</ref> shows different data representation methods. The Dirac delta distribution in the first row is the baseline which is FCOS without centerness. Compared to naive Gaussian distribution methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15]</ref>, our method obtains more accuracy gain (+1.2% vs. +0.5%), which shows the proposed IoU power term effectively overcomes the model misspecification problem <ref type="bibr" target="#b14">[14]</ref>. Besides, our method shows similar performance compared to general distribution with DFL <ref type="bibr" target="#b19">[19]</ref>, demonstrating that Gaussian distribution with NPLL effectively models the underlying distribution of the object localization as well. Meanwhile, we test the hyper-parameter ? of NPLL in <ref type="table" target="#tab_2">Table 3</ref> (left) and 0.05 shows the best performance, thus we use the value in the rest experiments. It is also worth noting that the proposed method allows the network to well estimate which direction is uncertain as a quantified value ? [0, 1] as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Uncertainty-Aware Classification. We compare the proposed uncertaintyaware classification using UFL with other box quality estimation methods in <ref type="table" target="#tab_3">Table 4</ref>. We can find that the quality jointly representation methods including QFL <ref type="bibr" target="#b19">[19]</ref>, VFL <ref type="bibr" target="#b36">[36]</ref>, and UFL (ours) surpass the centerness <ref type="bibr" target="#b32">[32]</ref> and IoU branch <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b12">12]</ref> methods which combine quality score only during the test phase. The proposed UFL (39.5%) consistently achieves better performance than both QFL(39.0%) and VFL(39.0%). We also investigate the effects of various focal losses on UAD as shown in <ref type="table" target="#tab_4">Table 5</ref>. UFL still outperforms IoU-based focal losses such as QFL and VFL, which demonstrates our uncertainty modeling strategy  Components of UAD. As shown in <ref type="table" target="#tab_5">Table 6</ref>, we investigate the effect of each component of the proposed UAD. In addition, we measure the GPU inference time at NVIDIA V100 GPU with a batch size of 1. We start FCOS without centerness as a baseline (37.8 AP). Adding the uncertainty network and learning with NPLL improve the baseline to 39.0 AP (+1.2). Replacing focal loss with the proposed UFL obtains 0.5 AP gain, and adding CRN boosts further performance gain (+0.3). These results demonstrate that the proposed method not only effectively estimates 4-directions localization uncertainty but also boosts detection performance. We also emphasize that all these components require negligible computational overhead as shown in <ref type="table" target="#tab_5">Table 6</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>GFL <ref type="bibr" target="#b19">[19]</ref> also estimates the general distributions along with four directions like UAD. GFL only qualitatively interprets the directional uncertainty according to the shape of the distribution (e.g., sharp or flatten). However, different from GFL, our UAD not only can capture the overall uncertainty of the objects, but also estimate the four-directional uncertainties as quantitative values in [0, 1]. Specifically, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, UAD can estimate lower certainty values on unclear and ambiguous boundaries due to occlusion and shadow. For examples, UAD estimates the lower bottom-directional certainty value (e.g., C B: 34%) of the dog in the center-bottom image due to the its shadow. For the upper-mid image, the left-directional certainty value (C L) of the bird is estimated by only 25% because the body of the bird is occluded by the tree branch. Besides, as the giraffe in the upper-rightmost image is occluded by another giraffe, it has the lower right-and bottom-directional certainties (57% and 36%). Hence, UAD can captures lower certainties on unclear or cloaked sides. From these results, we might expect that the estimated 4-directional quantitative values can be used as crucial information for the safety-critical application or decision-making system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed UAD that estimates 4-directions uncertainty for anchor-free object detectors. To this end, we design the new uncertainty loss, negative Power log-likelihood loss, to train the network that produces the localization uncertainty and enables accurate localization. Our uncertainty estimation method captures not only the quality of the detected box but also which direction is uncertain as a quantified value in [0, 1]. Furthermore, we also propose an uncertaintyaware focal loss and the certainty representation network for uncertainty-aware classification. It helps to correctly rank detected objects in the NMS step. Experiments on challenging COCO datasets demonstrate that UAD improves our baseline, FCOS, without the additional computational overhead. We hope the proposed UAD can serve as a component providing localization uncertainty as an essential cue and improving the performance of the anchor-free object detection methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Estimated uncertainty examples of the proposed UAD. Since there is no supervision of uncertainty, we analyze the estimated uncertainty qualitatively. UAD estimates lower certainties on unclear and ambiguous boundaries due to occlusion and shadow. For example, Both the surfboard and the person in the left-bottom image have much lower bottom-directional certainties (i.e., C B : 55% and 22%) as their shapes are quite unclear due to the water. Also, the right-directional certainty (C R) of the woman in the right-bottom image is estimated only by 1% because it is covered by the tail of a cat on the TV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; Communications Technology Planning &amp; Evaluation(IITP) grant funded by the Korea government(MSIT) (No.2014-3-00123, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis, No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Architecture of the proposed UAD. Differently from FCOS<ref type="bibr" target="#b32">[32]</ref>, our UAD can estimate localization uncertainty from a separate uncertainty network that outputs the uncertainty on four values that describes a bounding box (left, right, top, and bottom). In addition, the estimated uncertainty is utilized for uncertainty-aware classification as a box-quality confidence.</figDesc><table><row><cell>x4</cell><cell>? H x W x 1</cell><cell>Uncertainty-Aware Classification H x W x C</cell></row><row><cell>H x W x 256</cell><cell></cell><cell></cell><cell>Sigmoid</cell><cell>H x W x 1</cell></row><row><cell></cell><cell>CRN</cell><cell></cell><cell>FC, 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ReLU</cell><cell>H x W x 4</cell></row><row><cell>Input</cell><cell></cell><cell>Uncertainty H x W x 4</cell><cell>FC, 4 H x W x 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>H x W x 4</cell></row><row><cell></cell><cell></cell><cell>Regression</cell></row><row><cell>x4</cell><cell></cell><cell>H x W x 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CRN</cell></row><row><cell>H x W x 256</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Detection head</cell><cell></cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with data representations of box regression on FCOS. NPLL denotes the proposed negative power log-likelihood loss.</figDesc><table><row><cell>Distribution</cell><cell>AP AP75 APS APM APL</cell></row><row><cell>Dirac delta [32]</cell><cell>37.8 40.8 21.2 42.1 48.2</cell></row><row><cell>Gaussian w/ NLL [2,15,10]</cell><cell>38.3 42.2 21.2 42.5 49.4</cell></row><row><cell>General w/ DFL [19]</cell><cell>39.0 42.3 22.6 43.0 50.6</cell></row><row><cell cols="2">Gaussian w/ NPLL (ours) 39.0 42.9 21.8 43.2 50.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Varying ? and ? for NPLL and UFL, respectively.</figDesc><table><row><cell>?</cell><cell>AP</cell><cell>? AP</cell></row><row><cell cols="2">0.100 39.5</cell><cell>1.00 39.4</cell></row><row><cell cols="2">0.075 39.5</cell><cell>0.75 39.4</cell></row><row><cell cols="2">0.050 39.8</cell><cell>0.50 39.6</cell></row><row><cell cols="2">0.025 39.3</cell><cell>0.25 39.8</cell></row><row><cell cols="2">0.010 39.3</cell><cell>0.10 39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison between box quality estimation methods on FCOS.</figDesc><table><row><cell cols="2">Box quality methods AP AP75 APS APM APL</cell></row><row><cell cols="2">FCOS w/o centerness 37.8 40.8 21.2 42.1 48.2</cell></row><row><cell cols="2">centerness-branch [32] 38.5 41.6 22.4 42.4 49.1</cell></row><row><cell>IoU-branch [12,34]</cell><cell>38.7 42.0 21.6 43.0 50.3</cell></row><row><cell>QFL [19]</cell><cell>39.0 41.9 22.0 43.1 51.0</cell></row><row><cell>VFL [36]</cell><cell>39.0 41.9 21.9 42.6 51.0</cell></row><row><cell>UFL (ours)</cell><cell>39.5 42.6 22.1 43.4 51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different focal losses on UAD (ours).</figDesc><table><row><cell cols="2">Focal loss type AP AP75 APS APM APL</cell></row><row><cell>QFL [19]</cell><cell>39.1 42.9 21.9 43.2 50.8</cell></row><row><cell>VFL [36]</cell><cell>39.3 42.5 21.6 43.2 51.1</cell></row><row><cell cols="2">UFL (ours) 39.8 43.0 22.0 44.0 51.4</cell></row><row><cell cols="2">on 4-directions effectively captures more degrees (l, r, t, b) of the box quality</cell></row><row><cell cols="2">compared to only an overall quality IoU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The effect of each component on UAD (ours). The baseline is FCOS without centerness. The inference time is measured at V100 GPU with a batch size of 1. CRN denotes the certainty-aware representation network in theFig. 2.</figDesc><table><row><cell cols="4">NPLL UFL CRN AP Inference time (s)</cell></row><row><cell></cell><cell></cell><cell>37.8</cell><cell>0.037</cell></row><row><cell>?</cell><cell></cell><cell>39.0</cell><cell>0.038</cell></row><row><cell>?</cell><cell>?</cell><cell>39.5</cell><cell>0.038</cell></row><row><cell>?</cell><cell>?</cell><cell>? 39.8</cell><cell>0.038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>UAD performance on COCO test-dev2017. These results are tested with single-model and single-scale. Note that the results of FCOS is the latest update version in<ref type="bibr" target="#b33">[33]</ref>. FCOS and UAD are trained with the same training protocols such as multi-scale augmentation and 2? schedule in<ref type="bibr" target="#b9">[9]</ref>. DCN:Deformable Convolutional Network v2.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Epoch AP</cell><cell>AP50 AP75 APS APM APL</cell></row><row><cell>anchor-based:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FPN [20]</cell><cell>ResNet-101</cell><cell>24 36.2</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compared to FCOS [33], UAD achieves consistent performance gains based on various backbones, such as ResNet-50/101 and ResNeXt-101-32x8d/64x4d. We note that UAD further boosts performance on deeper and more complex backbone ResNeXt than ResNet. Specifically, ResNeXt-101-32x8d / 64x4d / 32x8d-DCN obtains +1.3 / +1.4 / +1.8 AP gains while ResNet-R-50 / 101 get 1.0 AP gain, respectively. These results show the uncertainty representation of UAD makes the network result in a synergy effect with the deeper feature representation</title>
	</analytic>
	<monogr>
		<title level="m">Using different backbones, we compare the proposed UAD with FCOS and other methods on COCO [22] test-dev2017. Table 7 summarizes the results</title>
		<imprint/>
	</monogr>
	<note>37]) based on anchor-based detection, UAD shows lower AP on ResNet-101. However. UAD achieves 48.4 AP, which is higher than GFL (48.2) when using the ResNeXt-101-32x8d-DCN backbone</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2019) 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inconsistency of bayesian inference for misspecified linear models, and a proposal for repairing it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gr?nwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Ommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesod: A bayesian approach for uncertainty estimation in deep object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03838</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019) 2, 3, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Assigning a value to a power likelihood in a general bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The bernstein-von-mises theorem under misspecification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J K</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uncertainty estimation in one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITSC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uncertainty estimation for deep neural object detectors in safety-critical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITSC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS (2020) 3, 5, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2017) 2, 3, 4</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">General bayesian updating and the loss-likelihood bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lyddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalized intersection over union</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpreting statistical evidence by using imperfect models: Robust adjusted likelihood functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Royall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Guru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02731</idno>
		<title level="m">A comprehensive guide to bayesian convolutional neural network with variational inference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Calibrating general posterior credible regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Syring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FCOS: A simple and strong anchor-free object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE T. Pattern Analysis and Machine Intelligence (TPAMI) (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Iou-aware single-stage object detector for accurate localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MM</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13367</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchorbased and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12448</idno>
		<title level="m">Soft anchor-point object detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
