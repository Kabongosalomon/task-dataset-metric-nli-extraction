<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressive Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<email>aarnab@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
							<email>canny@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
							<email>iansf@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Compressive Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning effective visual representations that generalize well without human supervision is a fundamental problem in order to apply Machine Learning to a wide variety of tasks. Recently, two families of self-supervised methods, contrastive learning and latent bootstrapping, exemplified by SimCLR and BYOL respectively, have made significant progress. In this work, we hypothesize that adding explicit information compression to these algorithms yields better and more robust representations. We verify this by developing SimCLR and BYOL formulations compatible with the Conditional Entropy Bottleneck (CEB) objective, allowing us to both measure and control the amount of compression in the learned representation, and observe their impact on downstream tasks. Furthermore, we explore the relationship between Lipschitz continuity and compression, showing a tractable lower bound on the Lipschitz constant of the encoders we learn. As Lipschitz continuity is closely related to robustness, this provides a new explanation for why compressed models are more robust. Our experiments confirm that adding compression to SimCLR and BYOL significantly improves linear evaluation accuracies and model robustness across a wide range of domain shifts. In particular, the compressed version of BYOL achieves 76.0% Top-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with ResNet-50 2x. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Individuals develop mental representations of the surrounding world that generalize over different views of a shared context. For instance, a shared context could be the identity of an object, as it does not change when viewed from different perspectives or lighting conditions. This ability to represent views by distilling information about the shared context has motivated a rich body of self-supervised learning work <ref type="bibr" target="#b34">[54,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr">47]</ref>. For a concrete example, we could consider an image from the ImageNet training set <ref type="bibr" target="#b40">[60]</ref> as a shared context, and generate different views by repeatedly applying different data augmentations. Finding stable representations of a shared context corresponds to learning a minimal high-level description since not all information is relevant or persistent. This explicit requirement of learning a concise representation leads us to prefer objectives that are compressive and only retain the relevant information.</p><p>corresponds to a general shared context that is invariant to various transformations of the input, and it is assumed that such invariant features will be effective for various downstream higher-level tasks. However, although existing contrastive approaches maximize mutual information between augmented views of the same input, they do not necessarily compress away the irrelevant information from these views <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. As shown in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, retaining irrelevant information often leads to less stable representations and to failures in robustness and generalization, hampering the efficacy of the learned representations. An alternative state-of-the-art self-supervised learning approach is BYOL <ref type="bibr" target="#b29">[30]</ref>, which uses a slow-moving average network to learn consistent, view-invariant representations of the inputs. However, it also does not explicitly capture relevant compression in its objective.</p><p>In this work, we modify SimCLR <ref type="bibr" target="#b11">[12]</ref>, a state-of-the-art contrastive representation method, by adding information compression using the Conditional Entropy Bottleneck (CEB) <ref type="bibr" target="#b26">[27]</ref>. Similarly, we show how BYOL <ref type="bibr" target="#b29">[30]</ref> representations can also be compressed using CEB. By using CEB we are able to measure and control the amount of information compression in the learned representation <ref type="bibr" target="#b25">[26]</ref>, and observe its impact on downstream tasks. We empirically demonstrate that our compressive variants of SimCLR and BYOL, which we name C-SimCLR and C-BYOL, significantly improve accuracy and robustness to domain shifts across a number of scenarios. Our primary contributions are:</p><p>? Reformulations of SimCLR and BYOL such that they are compatible with information-theoretic compression using the Conditional Entropy Bottleneck <ref type="bibr" target="#b25">[26]</ref>. ? An exploration of the relationship between Lipschitz continuity, SimCLR, and CEB compression, as well as a simple, tractable lower bound on the Lipschitz constant. This provides an alternative explanation, in addition to the information-theoretic view <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, for why CEB compression improves SimCLR model robustness. ? Extensive experiments supporting our hypothesis that adding compression to the state-of-theart self-supervised representation methods like SimCLR and BYOL can significantly improve their performance and robustness to domain shifts across multiple datasets. In particular, linear evaluation accuracies of C-BYOL are even competitive with the supervised baselines considered by SimCLR <ref type="bibr" target="#b11">[12]</ref> and BYOL <ref type="bibr" target="#b29">[30]</ref>. C-BYOL reaches 76.0% and 78.8% with ResNet-50 and ResNet-50 2x respectively, whereas the corresponding supervised baselines are 76.5% and 77.8% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we describe the components that allow us to make distributional, compressible versions of SimCLR and BYOL. This involves switching to the Conditional Entropy Bottleneck (CEB) objective, noting that the von Mises-Fisher distribution is the exponential family distribution that corresponds to the cosine similarity loss function used by SimCLR and BYOL, and carefully identifying the random variables and the variational distributions needed for CEB in SimCLR and BYOL. We also note that SimCLR and CEB together encourage learning models with a smaller Lipschitz constant, although they do not explicitly enforce that the Lipschitz constant be small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Conditional Entropy Bottleneck</head><p>In order to test our hypothesis that compression can improve visual representation quality, we need to be able to measure and control the amount of compression in our visual representations. To achieve this, we use the Conditional Entropy Bottleneck (CEB) <ref type="bibr" target="#b25">[26]</ref>, an objective function in the Information Bottleneck (IB) <ref type="bibr" target="#b47">[66]</ref> family.</p><p>Given an observation X, a target Y , and a learned representation Z of X, CEB can be written as:</p><formula xml:id="formula_0">CEB ? min Z ?I(X; Z|Y ) ? I(Y ; Z)<label>(1)</label></formula><p>= min </p><p>where H(?) and H(?|?) denote entropy and conditional entropy respectively. We can drop the H(Y ) term because it is constant with respect to Z. I(Y ; Z) is the useful information relevant to the task, or the prediction target Y . I(X; Z|Y ) is the residual information Z captures about X when we already know Y , which we aim to minimize. Compression strength increases as ? increases.</p><p>We define e(z|x) as the true encoder distribution, where z is sampled from; b(z|y), a variational approximation conditioned on y; d(y|z), the decoder distribution (also a variational approximation) which predicts y conditioned on z. As shown in <ref type="bibr" target="#b25">[26]</ref>, CEB can be variationally upper-bounded:</p><p>vCEB ? min e(z|x),b(z|y),d(y|z) E x,y?p(x,y),z?e(z|x) ?(log e(z|x) ? log b(z|y)) ? log d(y|z) <ref type="bibr" target="#b3">(4)</ref> There is no requirement that all three distributions have learned parameters. At one limit, a model's parameters can be restricted to any one of the three distributions; at the other limit, all three distributions could have learned parameters. If e(?) has learned parameters, its distributional form may be restricted, as we must be able to take gradients through the z samples. <ref type="bibr" target="#b1">2</ref> The only requirement on the b(?) and d(?) distributions is that we be able to take gradients through their log probability functions.</p><p>InfoNCE. As shown in <ref type="bibr" target="#b25">[26]</ref>, besides parameterizing d(y|z), it is possible to reuse b(z|y) to make a variational bound on the H(Y |Z) term. As I(Y ; Z) = H(Y ) ? H(Y |Z) and H(Y ) is a constant with respect to Z:</p><formula xml:id="formula_2">H(Y |Z) ? E x,y?p(x,y),z?e(z|x) log b(z|y) K k=1 b(z|y k )<label>(5)</label></formula><p>where K is the number of examples in a minibatch. Eq. (5) is also known as the contrastive InfoNCE bound <ref type="bibr" target="#b34">[54,</ref><ref type="bibr" target="#b36">56]</ref>. The inner term,</p><formula xml:id="formula_3">d(y|z) ? b(z|y) K k=1 b(z|y k ) ,<label>(6)</label></formula><p>is a valid variational approximation of the true but unknown p(y|z). Fischer <ref type="bibr" target="#b25">[26]</ref> calls Eq. (6) the CatGen decoder because it is a categorical distribution over the minibatch that approximates the generative decoder distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">C-SimCLR: Compressed SimCLR</head><p>The InfoNCE bound <ref type="bibr" target="#b34">[54]</ref> enables many contrastive visual representation methods to use it to capture shared context between different views of an image as a self-supervised objective <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">40]</ref>.</p><p>In this work, we show how to compress the SimCLR <ref type="bibr" target="#b11">[12]</ref> model, but the method we discuss is generally applicable to other InfoNCE-based models.</p><p>SimCLR applies randomized augmentations to an image to create two different views, x and y (which we also refer to as x ), and encodes both of them with a shared encoder, producing representations r x and r y . Both r x and r y are l 2 -normalized. The SimCLR version of the InfoNCE objective is:</p><formula xml:id="formula_4">L N CE (r x , r y ) = ? log e 1 ? r T y rx K k=1 e 1 ? r T y k rx<label>(7)</label></formula><p>where ? is a temperature term and K is the number of views in a minibatch. SimCLR further makes its InfoNCE objective bidirectional, such that the final objective becomes</p><formula xml:id="formula_5">L N CE (r x , r y ) + L N CE (r y , r x ) = ? log e 1 ? r T y rx K k=1 e 1 ? r T y k rx ? log e 1 ? r T x ry K k=1 e 1 ? r T x k ry<label>(8)</label></formula><p>We can observe the following: exp( 1 ? r T y r x ) in Eq. <ref type="formula" target="#formula_4">(7)</ref> corresponds to the unnormalized b(z|y) in Eq. <ref type="bibr" target="#b4">(5)</ref>. e(?|x) generates z = r x , whilst r y and r y k are distribution parameters of b(?|y) and b(?|y k ) respectively. e(?|x) and b(?|y) share model parameters.</p><p>von Mises-Fisher Distributional Representations. The cosine-similarity-based loss (Eq. <ref type="formula" target="#formula_4">(7)</ref>) is commonly used in contrastive learning and can be connected to choosing the von Mises-Fisher (vMF) distribution for e(?|x) and b(?|y) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b51">70]</ref>. vMF is a distribution on the (n ? 1)-dimensional hyper-sphere. The probability density function is given by f n (z, ?, ?) = C n (?)e ?? T z , where ? and ? are denoted as the mean direction and concentration parameter respectively. We assume ? is a constant. The normalization term C n (?) is a function of ? and equal to ? n/2?1 (2?) n/2 I n/2?1 (?) , where I v denotes the modified Bessel function of the first kind at order v.</p><p>By setting the mean direction ? to r y , concentration ? b of b(?|y) to 1/? , and r x to z, we can connect the SimCLR objective (Eq. <ref type="formula" target="#formula_4">(7)</ref>) to the distributional form of InfoNCE (Eq. <ref type="formula" target="#formula_2">(5)</ref>)</p><formula xml:id="formula_6">e 1 ? r T y rx K k=1 e 1 ? r T y k rx = C n (? b )e ? b r T y rx K k=1 C n (? b )e ? b r T y k rx = f n (r x , r y , ? b ) K k=1 f n (r x , r y k , ? b ) = b(r x |y) K k=1 b(r x |y k )<label>(9)</label></formula><p>z = r x is a deterministic unit-length vector, so we can view e(?|x) as a spherical delta distribution, which is equivalent to a vMF with r x as the mean direction and ? e ? ?. We can further extend the forward encoder to have non-infinite ? e , which results in a stochastic z. These allow us to have SimCLR in a distributional form with explicit distributions e(?|x) and b(?|y) and satisfy the requirements of CEB discussed in Sec. 2.1.</p><p>Compressing SimCLR with Bidirectional CEB. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the Compressed SimCLR (C-SimCLR) model. The model learns a compressed representation of an view X that only preserves information relevant to predicting a different view Y by switching to CEB. As can be seen in Eq. (3), the CEB objective treats X and Y asymmetrically. However, as shown in <ref type="bibr" target="#b25">[26]</ref>, it is possible to learn a single representation Z of both X and Y by having the forward and backward encoders act as variational approximations of each other:</p><formula xml:id="formula_7">CEB bidir ? min Z ? X I(X; Z|Y ) ? I(Y ; Z) + ? Y I(Y ; Z|X) ? I(X; Z) (10) ? min Z ? X (?H(Z|X) + H(Z|Y )) + H(Y |Z) (11) + ? Y (?H(Z|Y ) + H(Z|X)) + H(X|Z) ? min e(?|?),b(?|?),c(?|?),d(?|?) E x,y?p(x,y)<label>(12)</label></formula><formula xml:id="formula_8">E zx?e(zx|x) ? X (log e(z x |x) ? log b(z x |y)) ? log d(y|z x ) + E zy?e(zy|y) ? Y (log(e(z y |y) ? log(b(z y |x)) ? log c(x|z y )</formula><p>where d(?|?) and c(?|?) are the InfoNCE variational distributions of b(?|?) and e(?|?) respectively. e and b use the same encoder to parameterize mean direction in SimCLR setting. Since SimCLR is trained with a bidirectional InfoNCE objective, Eq. <ref type="bibr" target="#b11">(12)</ref> gives an easy way to compress its learned representation. As in SimCLR, the deterministic h x (in <ref type="figure" target="#fig_1">Fig. 1</ref>) is still the representation used on downstream classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">C-BYOL: Compressed BYOL</head><p>In this section we will describe how to modify BYOL to make it compatible with CEB, as summarized in <ref type="figure" target="#fig_8">Fig. 2</ref>. BYOL <ref type="bibr" target="#b29">[30]</ref> learns an online encoder that takes x, an augmented view of a given image, as input and predicts outputs of a target encoder which encodes x , a different augmented view of the same image. The target encoder's parameters are updated not by gradients but as an exponential moving average of the online encoder's parameters. The loss function is simply the mean square error, which is equivalent to the cosine similarity between the online encoder output ? e and the target encoder output y as both ? e and y are l 2 -normalized:  <ref type="figure" target="#fig_8">Figure 2</ref>: C-BYOL. The upper online encoder path takes an augmented view x as input and produces e(?|x) and d(?|z). The lower two paths use the same target encoder (shaded), which is a moving average of the online encoder (Conv + Projection). The target encoder maps x and another view x to r t and r t . sg(r t ) (sg: stop gradients) is our target y. y leads to b(?|y). sg(r t ) is our perturbed target y . r t , r t , ? e , ? b ,? are l 2 -normalized. These yield the components required by CEB. We highlight changes to BYOL in blue.</p><formula xml:id="formula_9">L byol = ||? e ? y || 2 2 = ? T e ? e + y T y ? 2? T e y = 2 ? 2? T e y<label>(13)</label></formula><p>This iterative "latent bootstrapping" allows BYOL to learn a view-invariant representation. In contrast to SimCLR, BYOL does not rely on other samples in a batch and does not optimize the InfoNCE bound. It is a simple regression task: given input x, predict y . To make BYOL CEB-compatible, we need to identify the random variables X, Y , Z, define encoder distributions e(z|x) and b(z|y), and define the decoder distribution d(y|z) (see Equation <ref type="formula">(4)</ref>).</p><p>We define e(z|x) to be a vMF distribution parameterized by ? e , and sample z from e(z|x):</p><formula xml:id="formula_10">e(z|x) = C n (? e )e ?ez T ?e<label>(14)</label></formula><p>We use the target encoder to encode x and output r t , an l 2 -normalized vector. We choose r t to be y. We then add a 2-layer MLP on top of y and l 2 -normalize the output, which gives ? b . We denote this transformation as ? b = m(y) and define b(z|y) to be the following vMF parameterized by</p><formula xml:id="formula_11">? b : b(z|y) = C n (? b )e ? b z T ? b<label>(15)</label></formula><p>For d(y|z), we add a linear transformation on z with l 2 -normalization,? = l(z), and define a vMF parameterized by?:</p><formula xml:id="formula_12">d(y|z) = C n (? d )e ? d y T?<label>(16)</label></formula><p>In the deterministic case where z is not sampled, this corresponds to adding a linear layer with l 2 -normalization on ? e which does not change the model capacity and empirical performance.</p><p>In principle, we can use any stochastic function of Z to generate Y . In our implementation, we replace the generative decoder log d(y|z) with log d(y |z), where we use the target encoder to encode x and output y . Given that X ? X is a stochastic transformation and both X and X go through the same the target encoder function, Y ? Y is also a stochastic transformation. d(y |z) can be considered as having a stochastic perturbation to d(y|z). Our vCEB objective becomes</p><formula xml:id="formula_13">L cbyol (x, x ) = ?(log e(z|x) ? log b(z|y)) ? log d(y |z).<label>(17)</label></formula><p>We empirically observed the best results with this design choice. d(y |z) can be directly connected the standard BYOL regression objective:</p><formula xml:id="formula_14">When ? d = 2, ? log(d(y |z)) = ?? d y T? ? log(C n (? d ))</formula><p>is equivalent to Eq. (13) when constants are ignored.</p><p>Although it seems that we additionally apply the target encoder to x compared to BYOL, this does not increase the computational cost in practice. As in BYOL, the learning objective is applied symmetrically in our implementation: L cbyol (x, x ) + L cbyol (x , x). Therefore, the target encoder has to be applied to both x and x no matter in BYOL or C-BYOL. Finally, note that like in BYOL, h <ref type="figure" target="#fig_8">(Fig. 2)</ref> is the deterministic representation used for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Lipschitz Continuity and Compression</head><p>Lipschitz continuity provides a way of measuring how smooth a function is. For some function f and a distance measure D(f (x 1 ), f (x 2 )), Lipschitz continuity defines an upper bound on how quickly f can change as x changes:</p><formula xml:id="formula_15">L||?x|| ? D(f (x), f (x + ?x)),<label>(18)</label></formula><p>where L is the Lipschitz constant, ?x is the vector change in x, and ||?x|| &gt; 0. If we define f (x) to be our encoder distribution e(z|x) (which is a vMF and always positive), and the distance measure, D, to be the absolute difference of the logs of the functions, we get a function of z of Lipschitz value, such that:</p><formula xml:id="formula_16">L(z) ? 1 ||?x||</formula><p>| log e(z|x) ? log e(z|x + ?x)| <ref type="bibr" target="#b18">(19)</ref> As detailed in Sec. G, by taking expectations with respect to z, we can obtain a lower bound on the encoder distribution's squared Lipschitz constant: 3</p><formula xml:id="formula_17">L 2 ? 1 ||?x|| 2 max KL[e(z|x)||e(z|x + ?x)], KL[e(z|x + ?x)||e(z|x)]<label>(20)</label></formula><p>To guarantee smoothness of the encoder distribution, we would like to have an upper bound on L, rather than a lower bound. Minimizing a lower bound does not directly yield any optimality guarantees relative to the bounded quantity. However, in this case, minimizing the symmetric KL below is consistent with learning a smoother encoder function: </p><p>By consistent, we mean that, if we could minimize this symmetric KL at every pair (x, x + ?x) in the input domain, we would have smoothed the model. In practice, for high-dimensional input domains, that is not possible, but minimizing Eq. (21) at a subset of the input domain still improves the model's smoothness, at least at that subset.</p><p>The minimization in Eq. (21) corresponds almost exactly to the CEB compression term in the bidirectional SimCLR models. We define y = x + ?x. At samples of the augmented observed variables, X, Y , the C-SimCLR models minimize upper bounds on the two residual informations:</p><formula xml:id="formula_19">I(X; Z|Y ) + I(Y ; Z|X) ? E x,y?p(x,y) KL[e(z|x)||e(z|y)] + KL[e(z|y)||e(z|x)]<label>(22)</label></formula><p>The only caveat to this is that we use b(z|y) instead of e(z|y) in C-SimCLR. b and e share weights but have different ? values in their vMF distributions. However, these are hyperparameters, so they are not part of the trained model parameters. They simply change the minimum attainable KLs in Eq. <ref type="bibr" target="#b21">(22)</ref>, thereby adjusting the minimum achievable Lipschitz constant for the models (see Sec. G).</p><p>Directly minimizing Equation <ref type="bibr" target="#b19">(20)</ref> would require normalizing the symmetric KL per-example by ||?x|| 2 . The symmetric CEB loss does not do this. However, the residual information terms in Equation <ref type="formula" target="#formula_19">(22)</ref> are multiplied by a hyperparameter ? ? 1. Under a simplifying assumption that the ||?x|| values generated by the sampling procedure are typically of similar magnitude, we can extract the average 1 ||?x|| 2 into the hyperparameter ?. We note that in practice using per-example values of ||?x|| 2 would encourage the optimization process to smooth the model more strongly at observed (x, ?x) pairs where it is least smooth, but we leave such experiments to future work.</p><p>Due to Eq. (22), we should expect that the C-SimCLR models are locally more smooth around the observed data points. We reiterate, though, that this is not a proof of increased global Lipschitz smoothness, as we are minimizing a lower bound on the Lipschitz constant, rather than minimizing an upper bound. It is still theoretically possible to learn highly non-smooth functions using CEB in this manner. It would be surprising, however, if the C-SimCLR were somehow less smooth than the corresponding SimCLR models.</p><p>The Lipschitz continuity property is closely related to model robustness to perturbations <ref type="bibr" target="#b8">[9]</ref>, including robustness to adversarial examples <ref type="bibr" target="#b52">[71,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">73]</ref>. Therefore, we would expect to see that the C-SimCLR models are more robust than SimCLR models on common robustness benchmarks. It is more difficult to make the same theoretical argument for the C-BYOL models, as they do not use exactly the same encoder for both x and y. Thus, the equivalent conditional information terms from Eq. (22) are not directly minimizing a lower bound on the Lipschitz constant of the encoder. Nevertheless, we empirically explore the impact of CEB on both SimCLR and BYOL models next in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We first describe our experimental set-up in Sec. 3.1, before evaluating the image representations learned by our self-supervised models in linear evaluation settings in Sec. 3.2. We then analyse the robustness and generalization of our self-supervised representations by evaluating model accuracy across a wide range of domain and distributional shifts in Sec. 3.3. Finally, we analyse the effect of compression strength in Sec. <ref type="bibr">3.4</ref>. Additional experiments and ablations can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Set-up</head><p>Implementation details. Our implementation of SimCLR, BYOL, and their compressed versions is based off of the public implementation of SimCLR <ref type="bibr" target="#b11">[12]</ref>. Our implementation consistently reproduces BYOL results from <ref type="bibr" target="#b29">[30]</ref> and outperforms the original SimCLR, as detailed in Sec. A.</p><p>We use the same set of image augmentations as in BYOL <ref type="bibr" target="#b29">[30]</ref> for both BYOL and SimCLR, and also use BYOL's (4096, 256) two-layer projection head for both methods. We follow SimCLR and BYOL to use the LARS optimizer <ref type="bibr" target="#b55">[74]</ref> with a cosine decay learning rate schedule [49] over 1000 epochs with a warm-up period, as detailed in Sec. A.4. For ablation experiments we train for 300 epochs instead. As in SimCLR and BYOL, we use batch size of 4096 split over 64 Cloud TPU v3 cores. Except for ablation studies of compression strength, ? is set to 1.0 for both C-SimCLR and C-BYOL. We follow SimCLR and BYOL in their hyperparameter choices unless otherwise stated, and provide exhaustive details in Sec. A. Pseudocode can be found in Sec. H.</p><p>Evaluation protocol. We assess the performance of representations pretrained on the ImageNet training set <ref type="bibr" target="#b40">[60]</ref> without using any labels. Then we train a linear classifier on different labeled datasets on top of the frozen representation. The final performance metric is the accuracy of these classifiers. As our approach builds on SimCLR <ref type="bibr" target="#b12">[13]</ref> and BYOL <ref type="bibr" target="#b29">[30]</ref>, we follow the same evaluation protocols. Further details are in Sec. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linear Evaluation of Self-supervised Representations</head><p>Linear evaluation on ImageNet. We first evaluate the representations learned by our models by training a linear classifier on top of frozen features on the ImageNet training set, following standard practice <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">43,</ref><ref type="bibr">44]</ref>. As shown in <ref type="table">Table 1</ref>, our compressed objectives provide strong improvements to state-of-the-art SimCLR <ref type="bibr" target="#b11">[12]</ref> and BYOL <ref type="bibr" target="#b29">[30]</ref> models across different ResNet architectures [34] of varying widths (and thus number of parameters) <ref type="bibr" target="#b56">[75]</ref>. Our reproduction of the SimCLR baseline (70.7% top-1 accuracy) outperforms that of the original paper (69.3%). Our implementation of BYOL, which obtains a mean Top-1 accuracy of 74.2% (averaged over three trials) matches that of <ref type="bibr" target="#b29">[30]</ref> within a standard deviation.</p><p>Current self-supervised methods benefit from longer training schedules <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>. <ref type="table">Table 1</ref> shows that our improvements remain consistent for both 300 epochs, and the longer 1000 epoch schedule which achieves the best results. In addition to the Top-1 and Top-5 accuracies, we also compute the Brier score <ref type="bibr" target="#b7">[8]</ref> which measures model calibration. Similar to the predictive accuracy, we observe that our compressed models obtain consistent improvements.</p><p>Learning with a few labels on ImageNet. After self-supervised pretraining on ImageNet, we learn a linear classifier on a small subset (1% or 10%) of the ImageNet training set, using the class labels this time, following the standard protocol of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. We expect that with strong feature representations, we should be able to learn an effective classifier with limited training examples. <ref type="table" target="#tab_2">Table 2</ref> shows that the compressed models once again outperform the SimCLR and BYOL counterparts. The largest improvements are observed in the low-data regime, where we improve upon the state-of-the-art BYOL by 5.1% and SimCLR by 1.8%, when using only 1% of the ImageNet labels. Moreover, note that self-supervised representations significantly outperform a fully-supervised ResNet-50 baseline which overfits significantly in this low-data scenario.</p><p>Comparison to other methods. <ref type="table" target="#tab_3">Table 3</ref> compares C-SimCLR and C-BYOL to other recent selfsupervised methods from the literature (in the standard setting of using two augmented views) on ImageNet linear evaluation accuracy. We present accuracy for models trained for 800 and 1000 <ref type="table">Table 1</ref>: ImageNet accuracy of linear classifiers trained on representations learned with SimCLR <ref type="bibr" target="#b11">[12]</ref> and BYOL <ref type="bibr" target="#b29">[30]</ref>, with and without CEB compression. A lower Brier score corresponds to better model calibration. We report mean accuracy and standard deviations over three trials.   Comparison to supervised baselines. As shown in <ref type="table" target="#tab_3">Table 3</ref>, SimCLR and BYOL use supervised baselines of 76.5% for ResNet-50 and 77.8% for ResNet-50 2x <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> respectively. In comparison, the corresponding compressed BYOL models achieve 76.0% for ResNet-50 and 78.8% for ResNet-50 2x, effectively matching or surpassing reasonable supervised baselines. <ref type="bibr" target="#b3">4</ref> The results in Tables 1 to 3 support our hypothesis that compression of SSL techniques can improve their ability to generalize in a variety of settings. These results are consistent with theoretical understandings of the relationship between compression and generalization <ref type="bibr" target="#b42">[61,</ref><ref type="bibr" target="#b49">68,</ref><ref type="bibr" target="#b21">22]</ref>, as are the results in <ref type="table" target="#tab_5">Table 5</ref> that show that performance improves with increasingly strong compression (corresponding to higher values of ?), up to some maximum amount of compression, after which performance degrades again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation of Model Robustness and Generalization</head><p>In this section, we analyse the robustness of our models to various domain shifts. Concretely, we use the models, with their linear classifier, from the previous experiment, and evaluate them on a suite  of robustness benchmarks that have the same label set as the ImageNet dataset. We use the public robustness benchmark evaluation code of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. As a result, we can evaluate our network and report Top-1 accuracy, as shown in <ref type="table" target="#tab_4">Table 4</ref>, without any modifications to the network.</p><p>We  <ref type="table" target="#tab_4">Table 4</ref> shows that SimCLR and BYOL models trained with CEB compression consistently outperform their uncompressed counterparts across all seven robustness benchmarks. This is what we hypothesized in the SimCLR settings based on the Lipschitz continuity argument in Sec. 2.4 and the appendix. All models performed poorly on ImageNet-A, but this is not surprising given that ImageNet-A was collected by <ref type="bibr">[37]</ref> according to images that a ResNet-50 classifier trained with full supervision on ImageNet misclassified, and we evaluate with ResNet-50 models too. <ref type="table" target="#tab_5">Table 5</ref> studies the effect of the CEB compression term, ? on linear evaluation accuracy on ImageNet, as well as on the same suite of robustness datasets. We observe that ? = 0, which corresponds to no explicit compression, but a stochastic representation, already achieves improvements across all datasets. Further improvements are observed by increasing compression (?), with ? = 1 obtaining the best results. But overly strong compression can be harmful. Large values of ? correspond to high levels of compression, and can cause training to collapse, which we observed for ? = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Effect of Compression Strength</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Most methods for learning visual representations without additional annotation can be roughly grouped into three families: generative, discriminative, and bootstrapping. Generative approaches build a latent embedding that models the data distribution, but suffer from the expensive image generation step <ref type="bibr" target="#b50">[69,</ref><ref type="bibr" target="#b39">59,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">39,</ref><ref type="bibr">42]</ref>. While many early discriminative approaches used heuristic pretext tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">53]</ref>, multi-view contrastive methods are among the recent state-of-the-art <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">54,</ref><ref type="bibr">35,</ref><ref type="bibr">48,</ref><ref type="bibr" target="#b45">64,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Some previous contributions in the multi-view contrastive family <ref type="bibr" target="#b46">[65,</ref><ref type="bibr" target="#b57">76,</ref><ref type="bibr" target="#b44">63,</ref><ref type="bibr" target="#b20">21]</ref> can be connected to the information bottleneck principle <ref type="bibr" target="#b47">[66,</ref><ref type="bibr" target="#b48">67,</ref><ref type="bibr" target="#b2">3]</ref> but in a form of unconditional compression as they are agnostic of the prediction target, i.e. the target view in multiview contrastive learning. As discussed in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, CEB performs conditional compression that directly optimizes for the information relevant to the task, and is shown theoretically and empirically better <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. A multiview self-supervised formulation of CEB, which C-SimCLR can be linked to, was described in <ref type="bibr" target="#b25">[26]</ref>. Federici et al. <ref type="bibr" target="#b23">[24]</ref> later proposed a practical implementation of that, leveraging either label information or data augmentations. In comparison to <ref type="bibr" target="#b23">[24]</ref>, we apply our methods with large ResNet models to well-studied large-scale classification datasets like ImageNet and study improvements in robustness and generalization, rather than using two layer MLPs on smaller scale tasks. This shows that compression can still work using state-of-the-art models on challenging tasks. Furthermore, we use the vMF distribution rather than Gaussians in high-dimensional spaces, and extend beyond contrastive learning with C-BYOL.</p><p>Among the bootstrapping approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> which BYOL <ref type="bibr" target="#b29">[30]</ref> belongs to, BYORL <ref type="bibr" target="#b28">[29]</ref> modified BYOL <ref type="bibr" target="#b29">[30]</ref> to leverage Projected Gradient Descent [50] to learn a more adversarially robust encoder. The focus is, however, different from ours as we concentrate on improving the generalization gap and robustness to domain shifts.</p><p>A variety of theoretical work has established that compressed representations yield improved generalization, including <ref type="bibr" target="#b42">[61,</ref><ref type="bibr" target="#b49">68,</ref><ref type="bibr" target="#b21">22]</ref>. Our work demonstrates that these results are valid in practice, for important problems like ImageNet, even in the setting of self-supervised learning. Our theoretical analysis linking Lipschitz continuity to compression also gives a different way of viewing the relationship between compression and generalization, since smoother models have been found to generalize better (e.g., <ref type="bibr" target="#b8">[9]</ref>). Smoothness is particularly important in the adversarial robustness setting <ref type="bibr" target="#b52">[71,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">73]</ref>, although we do not study that setting in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced compressed versions of two state-of-the-art self-supervised algorithms, SimCLR <ref type="bibr" target="#b11">[12]</ref> and BYOL <ref type="bibr" target="#b29">[30]</ref>, using the Conditional Entropy Bottleneck (CEB) <ref type="bibr" target="#b26">[27]</ref>. Our extensive experiments verified our hypothesis that compressing the information content of self-supervised representations yields consistent improvements in both accuracy and robustness to domain shifts. These findings were consistent for both SimCLR and BYOL across different network backbones, datasets and training schedules. Furthermore, we presented an alternative theoretical explanation of why C-SimCLR models are more robust, in addition to the information-theoretic view <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, by connecting Lipschitz continuity to compression.</p><p>Limitations. We note that using CEB often requires explicit and restricted distributions. This adds certain constraints on modeling choices. It also requires additional effort to identify or create required random variables, and find appropriate distributions for them. Although we did not need additional trainable parameters for C-SimCLR, we did for C-BYOL, where we added a linear layer to the online encoder, and a 2-layer MLP to create b(?|y). It was, however, not difficult to observe the von Mises-Fisher distribution corresponds to loss function of BYOL and SimCLR, as well as other recent InfoNCE-based contrastive methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Potential Negative Societal Impact. Our work presents self-supervised methods for learning effective and robust visual representations. These representations enable learning visual classifiers with limited data (as shown by our experiments on ImageNet with 1% or 10% training data), and thus facilitates applications in many domains where annotations are expensive or difficult to collect.</p><p>Image classification systems are a generic technology with a wide range of potential applications. We are unaware of all potential applications, but are cognizant that each application has its own merits and societal impacts depending on the intentions of the individuals building and using the system. We also note that training datasets contain biases that may render models trained on them unsuitable for certain applications. It is possible that people use classification models (intentionally or not) to make decisions that impact different groups in society differently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>We provide additional implementation details in Section A, details of our linear evaluation protocol in Section B, experiments on transfer of the compressive representations to other classification tasks in Section C, and further ablations in Section E. We provide further details about our robustness evaluation in Section F. Finally, we provide a more detailed explanation of the relation between Lipschitz continuity and SimCLR with CEB compression, introduced in Section 2.4 of the main paper, in Section G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation details and hyperparameters</head><p>In this section, we further describe our implementation details. Our implementation is based off of the public implementation of SimCLR <ref type="bibr" target="#b11">[12]</ref>. In general, we closely follow the design choices of BYOL <ref type="bibr" target="#b29">[30]</ref> for both of our SimCLR and BYOL implementations. Despite having different objectives, BYOL and SimCLR share many components, including image augmentations, network architectures, and optimization settings. As explained in the original paper <ref type="bibr" target="#b29">[30]</ref>, BYOL itself may be considered as a modification to SimCLR with a slow moving average target network, an additional predictor network, and switching the learning target from InfoNCE to a regression loss. Therefore, many of the design choices and hyperparameters are applicable to both. As explained in Section 3.1, we align SimCLR with BYOL on the choices of image augmentations, network architecture, and optimization settings in order to reduce the number of variables in comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Image augmentations</head><p>During self-supervised training, we use the set of image augmentations from BYOL <ref type="bibr" target="#b29">[30]</ref> for all our models.</p><p>? Random cropping: randomly select a patch of the image, with an area uniformly sampled between 8% and 100% of that original image, and an aspect ratio logarithmically sampled between 3/4 and 4/3. Then this patch is resized to 224 ? 224 using bicubic interpolation.</p><p>? Left-to-right flipping: randomly flip the patch.</p><p>? Color jittering: brightness, contrast, saturation and hue of an image are shifted by a uniformly random offset. The order to apply these adjustments is randomly selected for each patch.</p><p>? Color dropping: RGB pixel values of an image are converted to grayscale according to 0.2989r + 0.5870g + 0.1140b.</p><p>? Gaussian blurring: We use a 23 ? 23 kernel to blur the 224 ? 224 image, with a standard deviation uniformly sampled over [0.1, 2.0].</p><p>? Solarization: This is a color transformation x = x ? 1 {x&lt;0.5} + (1 ? x) ? 1 {x?0.5} for pixels with values in [0, 1] (we convert all pixel values into floats between [0, 1]).</p><p>As described in Sec. 2, we use augmentation functions t and t to transform an image into two views. t and t are compositions of the above image augmentations in the listed order, each applied with a predefined probability. The image augmentation parameters to generate t and t are listed in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>During evaluation, we perform center cropping, as done in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Images are resized to 256 pixels along the shorter side, after which a 224 ? 224 center crop is applied. During both training and evaluation, we normalize image RGB values by subtracting the average color and dividing by the standard deviation, computed on ImageNet, after applying the augmentations.</p><p>Differences from the original SimCLR <ref type="bibr" target="#b11">[12]</ref>. Since the image augmentation parameters that BYOL <ref type="bibr" target="#b29">[30]</ref> uses are different from the original SimCLR, we list the original SimCLR parameters in the last column of <ref type="table" target="#tab_8">Table 6</ref>, which are the same for t and t , to clarify the differences. Additionally, the original SimCLR samples the aspect ratio of cropped patches uniformly, instead of logarithmically, between [3/4, 4/3].  <ref type="figure" target="#fig_1">Figure 1</ref> and <ref type="figure" target="#fig_8">Figure 2</ref>). We vary the ResNet width <ref type="bibr" target="#b56">[75]</ref> (and thus the number of parameters) from 1? to 2?. In Sec. D, we additionally report C-BYOL results with different ResNet depth, from 50 to 152. The representations h x , h y in SimCLR and h, h t , h t in BYOL correspond to the 2048-dimensional (for ResNet-50 1?) final average pooling layer output. These representations are projected to a smaller space by an MLP (called "projection" in <ref type="figure" target="#fig_1">Figure 1</ref> and <ref type="figure" target="#fig_8">Figure 2</ref>). As in <ref type="bibr" target="#b29">[30]</ref>, this MLP consists of a linear layer with output size 4096 followed by batch normalization [41], ReLU, and a final linear layer with output dimension 256. q(?) in BYOL/C-BYOL <ref type="figure" target="#fig_8">(Fig. 2)</ref> is called the predictor. The predictor q(?) is also a two-layer MLP which shares the same architecture with the projection MLP <ref type="bibr" target="#b29">[30]</ref>.</p><p>Differences from the original SimCLR <ref type="bibr" target="#b11">[12]</ref>. The original SimCLR <ref type="bibr" target="#b11">[12]</ref> uses a 2048-d hidden layer and a 128-d output layer for the projection MLP, after which an additional batch normalization is applied to the 128-d output. Both BYOL <ref type="bibr" target="#b29">[30]</ref> and our work do not have this batch normalization on the last layer. We did not observe significant change in performance for the uncompressed models and found it harmful to the compressed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 von Mises-Fisher Distributions</head><p>We use the vMF implementation in public Tensorflow Probability (TFP) library <ref type="bibr" target="#b16">[17]</ref>, specifically the current TFP version 0.13. We have found that sampling and computing log probabilities in high dimensions with the current TFP version has become sufficiently stable and fast to train all of the models in our paper. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Optimization</head><p>We follow BYOL <ref type="bibr" target="#b29">[30]</ref> for our optimization settings. During self-supervised training, we use the LARS optimizer <ref type="bibr" target="#b55">[74]</ref> with a cosine decay learning rate schedule [49] over 1000 epochs and a linear warm-up period at the beginning of training. The linear warm-up period is 10 epochs in most cases. We increase it to 20 epochs for BYOL and C-BYOL with larger ResNets (ResNet-50 2x, ResNet-101, ResNet-152) as we found it helpful to prevent mode collapse and improve performance. In most cases, we set the base learning rate to 0.2 and scale it linearly by batch size (LearningRate = 0.2 ? BatchSize/256). For C-BYOL, we increase the base learning rate to 0.26 for better performance. For careful comparison, we extensively searched base learning rate for BYOL but did not find a configuration better than 0.2 as used in the original work <ref type="bibr" target="#b29">[30]</ref>. We use a weight decay of 1.5 ? 10 ?6 . For the BYOL/C-BYOL target network, the exponential moving average update rate ? starts from where k is the current training step and K is the total number of training steps.</p><p>For 300-epoch models used in ablations, we set the base learning rate to 0.3 in most cases, and increase it to 0.35 for C-BYOL. We use a weight decay of 10 ?6 . For BYOL and C-BYOL, the base exponential moving average update rate ? base is set to 0.99.</p><p>We note that there is a small chance that both BYOL and C-BYOL can end up with collapsed solutions, but it mostly happens in early phase of training and is easy to observe with the learning objective spiking or reaching NaN.</p><p>Differences from the original SimCLR <ref type="bibr" target="#b11">[12]</ref>. Optimization settings of the original SimCLR are very similar but, for 1000-epoch training, they use a base learning rate of 0.3 and weight decay of 10 ?6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 SimCLR and C-SimCLR details</head><p>As described in Section A.1, Section A.2, Section A.4, we made minor modifications to the original SimCLR to align with BYOL on the choices of image augmentations, network architecture, and optimization settings. With these modifications, our SimCLR baseline reproduction outperforms the original (top-1 accuracy 70.6% versus 69.3%).</p><p>For C-SimCLR, we use ? e = 1024 for the true encoder e(?|x) and ? b = 10 for the backward encoder, where e(?|x) and b(?|y) are von Mises-Fisher distributions. The compression factor ? that we use for C-SimCLR is 1.0. Note that the original SimCLR has temperature ? = 0.1 which is equivalent to having ? b = 10, since ? b = 1/? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 BYOL and C-BYOL details</head><p>As shown in <ref type="table">Table 7</ref> and <ref type="table">Table 8</ref>, our BYOL implementation stably reproduces results comparable to <ref type="bibr" target="#b29">[30]</ref> with 300 and 1000 epochs of training. An interesting behavior we observed is that, for shorter training with 300 epochs, scaling the BYOL regression loss can improve performance. Specifically we add a weight multiplier w byol = ? d /2 to the BYOL loss Eq. <ref type="bibr" target="#b12">(13)</ref>.</p><p>L byol = w byol ||? e ? y || 2 2 (23) <ref type="table">Table 7</ref> shows that multiplying the loss by five increases the linear evaluation accuracy from 72.5% to 72.8%. This improvement is consistent across multiple runs. Therefore, we choose w byol = 5 for 300-epoch BYOL/C-BYOL. However, we do not see the same improvement for 1000-epoch models. <ref type="table">Table 8</ref> shows that w byol makes little difference for 1000-epoch BYOL models. We still choose w byol = 2 for all 1000-epoch BYOL and C-BYOL models since it tends to work better than w byol = 1 for the compressed models and models with larger ResNets. Furthermore, <ref type="table">Table 7</ref> verifies that the additional linear layer with l 2 -normalization that we added for C-BYOL and z sampling (both were described in Section 2.3) do not result in a difference in performance. The improvement happens only when CEB compression is used.</p><p>We set ? e = 16384.0, ? b = 10.0, and the compression factor beta = 1.0 for C-BYOL if not specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Linear evaluation protocol on ImageNet</head><p>As common in self-supervised learning literature <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b13">14]</ref>, we assess the performance of our representations learned on the ImageNet training set (without labels) by training a linear classifier on top of the frozen representations using the labeled data. For training this linear classifier, we only apply the random cropping and flipping image augmentations. We optimize the cross-entropy loss using SGD with Nesterov momentum over 40 epochs. We use a batch size of 1024 and momentum of 0.9 without weight decay, and sweep the base learning rate over {0.4, 0.3, 0.2, 0.1, 0.05} to choose the best learning rate on a validation set, following <ref type="bibr" target="#b29">[30]</ref>. We perform center cropping during evaluation, as done in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Images are resized to 256 pixels along the shorter side, after which the 224 ? 224 center crop is selected. During both training and evaluation, we normalize image RGB values by subtracting the average color and dividing by the standard deviation, computed on ImageNet, after applying the augmentations.</p><p>Learning with a few labels In Section 3.2 we described learning the linear classifier on 1% and 10% of the ImageNet training set with labels. We performed this experiment with the same 1% and 10% splits from <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Transfer to other classification tasks</head><p>We analyze the effect of compression on transfer to other classification tasks in <ref type="table" target="#tab_10">Table 9</ref>. This allows us to assess whether the compressive representations learned by our method are generic and transfer across image domains.</p><p>Datasets. We perform the transfer experiments on the Food-101 dataset <ref type="bibr" target="#b6">[7]</ref>, CIFAR-10 and CIFAR-100 [46], Birdsnap <ref type="bibr" target="#b5">[6]</ref>, SUN397 <ref type="bibr" target="#b53">[72]</ref>, Stanford Cars [45], FGVC Aircraft [51], the Describable Texture Dataset (DTD) <ref type="bibr" target="#b15">[16]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b35">[55]</ref>, Caltech-101 <ref type="bibr" target="#b24">[25]</ref>, and Oxford 102 Flowers [52]. We carefully follow their evaluation protocol, i.e. we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100, Birdsnap, SUN397, Stanford Cars, anad DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101, and Oxford 102 Flowers. These datasets are also used by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">44]</ref>. More exhaustive details about train, validation, and test splits of these datasets can be found in Section D of <ref type="bibr" target="#b29">[30]</ref> (arXiv v3).</p><p>Transfer via linear classifier. To demonstrate the effectiveness of compressed representations, we compare SimCLR and C-SimCLR representations as an example. We freeze the representations of our model and train an 2 -regularized multinomial logitstic regression classifier on top of these frozen representations. We minimize the cross-entropy objective using the L-BFGS optimizer. As in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12]</ref>, we selected the 2 regularization parameter from a range of 45 logarithmically spaced values between [10 ?6 , 10 5 ].</p><p>We observe in <ref type="table" target="#tab_10">Table 9</ref> that our Compressed SimCLR model consistently outperforms the uncompressed SimCLR baseline on each of the 11 datasets we tested. We note absolute improvements in accuracy ranging from 0.5% (CIFAR-10, SUN397) to 3% (Stanford Cars). These experiments </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extra C-BYOL results with Deeper ResNets</head><p>In <ref type="table" target="#tab_11">Table 10</ref>, we additionally report results of C-BYOL and BYOL retrained for 1000 epochs with ResNet-101 and ResNet-152, as it could be of interest to demonstrate improvements over the stateof-the-art BYOL on these deeper ResNet models. It can be observed that C-BYOL gives significant gains across ResNets with different depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Ablations</head><p>The hyperparameter and architecture choices of SimCLR and BYOL have been investigated in the original works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Here we focus on analysing hyperparameters specific to C-SimCLR and C-BYOL. <ref type="table">Table 11</ref>, <ref type="table" target="#tab_2">Table 12</ref> and <ref type="table" target="#tab_3">Table 13</ref> show how changing ? e and ? b affect the results, respectively. We also investigate the effect of the compression factor ? for C-BYOL <ref type="table" target="#tab_4">(Table 14</ref>) in addition to the compression analysis for SimCLR in Sec. 3.4. Similar to C-SimCLR, as compression strength (?) increases, the linear evaluation result improves, with ? = 1.0 obtaining the best results, but overly strong compression leads to a drop in performance.</p><p>Finally, we conduct a preliminary exploration on the interplay between CEB compression and image augmentations, using cropping area ratio as an example in <ref type="table" target="#tab_5">Table 15</ref>. As described in Section A.1, we follow <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12]</ref> to randomly crop an image to an area between 8% and 100% of the original image. We refer to this 8% as the "area lower bound", which is the most aggressive cropping area ratio that can happen. As the area lower bound decreases, we are reducing the amount of information that can be shared between the two representations, because there is less and less mutual information between the two images: I(X; X ) gets smaller the more we reduce the area lower bound <ref type="bibr" target="#b46">[65]</ref>. Thus, smaller area lower bounds should force the model to be more compressed. What we see in <ref type="table" target="#tab_5">Table 15</ref> is that the SimCLR models are much more sensitive to the changes in the area lower bound than the C-SimCLR models are. We speculate that this is because the compression done by the C-SimCLR objective overlaps to some extent with the compression given by varying the area lower bound. Regardless, the compression due to the area lower bound hyperparameter appears to be insufficient to adequately compress away irrelevant information in the SimCLR model, which is why the C-SimCLR models continue to outperform the SimCLR models at all area lower bound values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Robustness benchmark details</head><p>In this section, we provide some additional details on each of the datasets used in our robustness evaluations. Note that we use the public robustness benchmark evaluation code of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>   <ref type="table" target="#tab_4">Table 14</ref>: The effect of ? on C-BYOL. Note that <ref type="table" target="#tab_5">Table 5</ref> in the main paper studied this effect on C-SimCLR. The final column is the uncompressed BYOL baseline.  <ref type="table" target="#tab_5">Table 15</ref>: The effect of varying the area range lower rounds for SimCLR and Compressed SimCLR. We report the ImageNet Top-1 accuracy from linear evaluation. Note how the baseline SimCLR model is much more sensitive to this data-augmentation hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>8% 16% 25% 50% ImageNet-v2 <ref type="bibr" target="#b38">[58]</ref>: This is a new test set for ImageNet, and was collected following the same protocol as the original ImageNet dataset. The authors posit that the collected images are more "difficult", and observed consistent accuracy drops across a wide range of models trained on the original ImageNet.</p><p>ObjectNet <ref type="bibr" target="#b4">[5]</ref>: This is a more challenging test set for ImageNet, where the authors control for different viewpoints, backgrounds and rotations. Note that ObjectNet has a vocabulary 313 object classes, of which 113 are common with ImageNet. Following <ref type="bibr" target="#b17">[18]</ref>, we evaluate on only the images in the dataset which have one of the 113 ImageNet labels. Our network is still able to predict any one of the 1000 ImageNet classes though.</p><p>ImageNet-Vid and YouTube-BB <ref type="bibr" target="#b43">[62]</ref> evaluate the robustness of image classifiers to natural perturbations arising in video. This dataset was created by <ref type="bibr" target="#b43">[62]</ref> by augmenting the ImageNet-Vid <ref type="bibr" target="#b40">[60]</ref> and YouTube-BB <ref type="bibr" target="#b37">[57]</ref> datasets with additional annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Analysis of Lipschitz Continuity and Compression</head><p>In this section, we provide a more detailed explanation of the relation between Lipschitz continuity and SimCLR with CEB compression, introduced in Section 2.4. Lipschitz Continuity provides a way of measuring how smooth a function is. For some function f and a distance measure D(f (x 1 ), f (x 2 )),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lipschitz continuity defines an upper bound on how quickly f can change as x changes:</head><p>L||?x|| ? D(f (x), f (x + ?x)) <ref type="bibr" target="#b23">(24)</ref> where L is the Lipschitz constant, ?x is the vector change in x, and ||?x|| &gt; 0.</p><p>Frequently, the choice of D is the absolute difference function: |f (x 1 ) ? f (x 2 )|. However, we can use a multiplicative distance rather than an additive distance by considering the absolute difference of the logs of the functions:</p><formula xml:id="formula_20">D(f (x 1 ), f (x 2 )) ? | log f (x 1 ) ? log f (x 2 )|<label>(25)</label></formula><p>It is trivial to see that Equation <ref type="bibr" target="#b24">(25)</ref> obeys the triangle inequality, which can be written:</p><formula xml:id="formula_21">|a ? b| ? |a| ? |b|<label>(26)</label></formula><p>Equation <ref type="formula" target="#formula_3">(26)</ref> is true for any scalars a and b. Setting a = log f (x 1 ) and b = log f (x 2 ) is sufficient, given that f (?) is a positive, scalar-valued function. For D(?) to be a valid distance metric, f (x) must also satisfy the identity of indiscernibles requirement: f (</p><formula xml:id="formula_22">x 1 ) = f (x 2 ) ? x 1 = x 2 .</formula><p>If that requirement is violated, then D(?) becomes a pseudometric, which is inconsistent with Lipschitz continuity.</p><p>Noting that |a ? b| ? max(a ? b, b ? a), we will simplify the analysis by considering the two arguments to the implicit max in Equation <ref type="formula" target="#formula_2">(25)</ref> one at a time, starting with:</p><formula xml:id="formula_23">L ? 1 ||?x|| (log f (x) ? log f (x + ?x))<label>(27)</label></formula><p>If we define f (x) to be our encoder distribution, e(z|x), we get a function of z of Lipschitz value: 7</p><formula xml:id="formula_24">L(z) ? 1 ||?x||</formula><p>(log e(z|x) ? log e(z|x + ?x))</p><p>Note that the encoder distribution must not violate the identity of indiscernibles property: ?z : e(z|x 1 ) = e(z|x 2 ) ? x 1 = x 2 . This is not the case in general, but for reasonable distribution families, the sets of z that violate this property for any (x 1 , x 2 ) pair will have measure zero. In the case that e(z|?) is parameterized by some function f (?), such as a neural network, f must also not violate the identity of indiscernibles property. This argues in favor of using invertible networks for f , or at least not using activation functions like relu that are likely to cause f to map multiple x values to some constant. We note that in practice, it doesn't seem to matter, as shown empirically in Section 3.</p><p>As e(z|x) is parameterized by the output of our model, Equation <ref type="formula" target="#formula_5">(28)</ref> captures the semantically relevant smoothness of the model. For example, if our encoder distribution is a Gaussian with learned mean and variance, the impact of the model parameters on the means is semantically distinct from the impact of the model parameters on the variance. In that setting, using the parameter vectors themselves naively in a Lipschitz formulation like L naive ||?x|| ? ||f ? (x) ? f ? (x + ?x)||, where f ? outputs concatenated mean and variance parameters, would clearly fail to correctly capture the model's smoothness. Our formulation using the encoder distribution directly does not have this flaw, and thus generalizes to capture a notion of smoothness for any choice of distribution parameterization. Note that this notion of smoothness of the distribution still depends directly on the the smoothness of the underlying function that generates the distribution's parameters, while also capturing the smoothness of the distribution itself.</p><p>We can remove the dependence on z of Equation (28) by taking the expectation over z with respect to e(z|x). This gives us an expected Lipschitz value:</p><formula xml:id="formula_26">E z?e(z|x) L(z) ? 1 ||?x|| E z?e(z|x) log e(z|x) ? log e(z|x + ?x)<label>(29)</label></formula><formula xml:id="formula_27">= 1 ||?x|| KL[e(z|x)||e(z|x + ?x)]<label>(30)</label></formula><p>It is important to note that Equation <ref type="bibr" target="#b29">(30)</ref> no longer obeys the triangle inequality, due to the KL divergence, since it is easy to find three distributions p, q, r such that KL[p||q] &gt; KL[p||r] + KL <ref type="bibr">[q||r]</ref>.</p><p>We could also have computed the expectation over z ? e(z|x + ?x), yielding:</p><formula xml:id="formula_28">E z?e(z|x+?x) L(z) ? ? 1 ||?x|| KL[e(z|x + ?x)||e(z|x)]<label>(31)</label></formula><p>But this is trivially true due to L(z) being non-negative and the negative KL term being non-positive, so we can ignore this term here. However, when we consider the second argument to the implicit max in Equation <ref type="formula" target="#formula_2">(25)</ref>, the negative and positive KL terms are swapped, and we are left with:</p><formula xml:id="formula_29">E z?e(z|x+?x) L(z) ? 1 ||?x|| KL[e(z|x + ?x)||e(z|x)]<label>(32)</label></formula><p>When we take the expectations over z, the resulting KL divergences have an underlying quadratic growth in ||?x||: as ||?x|| increases linearly, the KL divergences increase quadratically. <ref type="bibr" target="#b7">8</ref> This is why the KL divergence violates the triangle inequality, and also why it is problematic for measuring Lipschitz continuity: in general, L will be unbounded when measured by the KL even when the underlying function f (x) parameterizing the distributions has a bounded Lipschitz constant, since the KL will always grow faster then ||?x||. We can address this by instead considering the squared Lipschitz constant:</p><formula xml:id="formula_30">L 2 ||?x|| 2 ? KL[e(z|x)||e(z|x + ?x)] and L 2 ||?x|| 2 ? KL[e(z|x + ?x)||e(z|x)]<label>(33)</label></formula><p>which is equivalent to:</p><formula xml:id="formula_31">L 2 ? 1 ||?x|| E z?e(z|x) L(z) and L 2 ? 1 ||?x|| E z?e(z|x+?x) L(z)<label>(34)</label></formula><p>Finally, we note the following relationship: </p><formula xml:id="formula_32">L 2 = max x,</formula><p>In words, the true squared Lipschitz constant of the encoder is equal to the least smooth (x, ?x) pair, as measured by the greater of the two KL divergences at that pair.</p><p>Putting all of this together, we observe that the following two KL divergences together give a lower bound on the encoder's Lipschitz constant: Example: the von Mises-Fisher distribution. An exponential family distribution has the form:</p><formula xml:id="formula_34">L 2 ? 1 ||?x|| 2 max</formula><formula xml:id="formula_35">h(z) exp(? T T (z) ? A(?))<label>(37)</label></formula><p>where T (z) is the sufficient statistic, ? is the canonical parameter, and A(?) is the cumulant. For the von Mises-Fisher distribution, which has the form:</p><formula xml:id="formula_36">C n (?) exp(?? T z)<label>(38)</label></formula><p>we have h(z) = 1, T (z) = z and A(?) is the negative log of the normalizing constant C n (?). Instead of a general parameter vector ?, the standard von Mises-Fisher distribution uses a unit vector ? = ?/||?|| and a scale or concentration parameter ? = ||?||. If e(z|x) is parameterized by a deterministic neural network for the von Mises-Fisher canonical parameter denoted e(x), then we have:</p><formula xml:id="formula_37">e(z|x) = C n (||e(x)||) exp(e(x) T z)<label>(39)</label></formula><p>and KL[e(z|x)||e(z|y)] (define y = x + ?x) is:</p><formula xml:id="formula_38">(e(x) ? e(y)) T z(x) + log C n (||e(x)||) ? log C n (||e(y)||)<label>(40)</label></formula><p>where z(x) is the mean direction function of the distribution (e(x) = ||e(x)||z(x)). The symmetric KL-divergence (KL[e(z|x)||e(z|y)] + KL[e(z|y)||e(z|x)]) is then:</p><formula xml:id="formula_39">(e(x) ? e(y)) T (z(x) ? z(y))<label>(41)</label></formula><p>which is closely related to the L 2 2 norm of the vector e(x) ? e(y). Furthermore, we can choose ? = ||e(x)|| as a hyperparameter and just parameterize e(z|x)'s unit length mean direction z(x). Apart from choosing different ? hyperparameters, this is exactly what we do in the C-SimCLR setting described in Section 2.2.</p><p>Specifically, in Section 2.2, minimizing the residual information term I(X; Z|Y ) correspond to minimizing KL[e(z|x)||b(z|y)] instead of KL[e(z|x)||e(z|y)], where b and e have the same mean direction parameterization but different ? hyperparameters, say ? e and ? b . We can show that the two KLs are actually consistent as learning objectives. With ? e , ? b as hyperparameters, KL[e(z|x)||e(z|y)] (Equation <ref type="formula" target="#formula_38">(40)</ref>) can be written as</p><formula xml:id="formula_40">? e (z(x) ? z(y)) T z(x) + log C n (? e ) ? log C n (? e ),<label>(42)</label></formula><p>and KL[e(z|x)||b(z|y)] can be written as</p><formula xml:id="formula_41">(? e z(x) ? ? b z(y)) T z(x) + log C n (? e ) ? log C n (? b ) (43) = (? e ? ? b ) + ? b (z(x) ? z(y)) T z(x) + log C n (? e ) ? log C n (? b ).<label>(44)</label></formula><p>It is not difficult to see that the two KLs are only different in scale and by a constant, and thus are consistent as learning objectives. As we claimed in Section 2.4 (after Equation <ref type="formula" target="#formula_19">(22)</ref>), the use of different constant hyperparameters ? e and ? b in the encoders of x and y only changes the minimum achievable KL divergences. We can reach the same conclusion for the residual information in another direction I(Y ; Z|X). Thus, whether or not ? e and ? b are the same, we are still minimizing the Lipschitz constant of our encoder function at each observed (x, y) pair when we minimize the residual information terms in the bidirectional CEB objective (Equation <ref type="formula" target="#formula_0">(12)</ref>).</p><p>Estimating the local Lipschitz constant. We can evaluate Equation <ref type="formula" target="#formula_0">(19)</ref> (also Equation <ref type="formula" target="#formula_5">(28)</ref>  the training or the validation set (using only a center crop in both cases), and x + ?x is generated by either increasing or decreasing exactly one of: brightness, contrast, saturation, or hue. The absolute changes are the maximum adjustment strength in our image defined in <ref type="table" target="#tab_8">Table 6</ref> (e.g. for increasing brightness, we increase by 0.4). In <ref type="figure" target="#fig_6">Figures 3 and 4</ref>, we compare the histograms of Equation <ref type="formula" target="#formula_0">(19)</ref> on the SimCLR ResNet-50 model and the corresponding C-SimCLR ResNet-50 model. On both datasets and all eight augmentations, the C-SimCLR models have substantially more mass in the lower values of the local Lipschitz estimates for those image pairs, and have lower mean values computed over the dataset. Additionally, the mean C-SimCLR results on the validation set are almost all lower or equal to the mean SimCLR results on the training set, so the smoothness improvements from adding compression to SimCLR appear to be substantial. The only exceptions are for 'brightness+' (SimCLR training mean: 0.088, C-SimCLR validation mean: 0.089) and 'brightness-' (SimCLR training mean: 0.147, C-SimCLR validation mean: 0.162). In practice, we follow SimCLR to apply this loss in a bidirectional manner as loss = simclr_ceb_loss(x, y) + simclr_ceb_loss(y, x). We use the same notation as the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Pseudocode</head><p>Args:</p><p>x The notation corresponds to Section 2.3 and <ref type="figure" target="#fig_8">Figure 2</ref> of the paper. This code presents an updated version of C-BYOL as described in the general response.</p><p>In practice, we follow BYOL to apply this loss in a bidirectional manner as loss = byol_ceb_loss(x, x_prime, ...) + byol_ceb_loss(x_prime, x, ...). We use the same notation as the main paper.</p><p>Args:</p><p>x </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Returns:</head><p>A tensor loss . The loss is per-sample. """ r = f_enc(x) mu_e = tf.math.l2_normalize(q_net(r), -1) e_zx = tfd.VonMisesFisher(mu_e, kappa_e) z = e_zx.sample() y_pred = tf.math.l2_normalize(l_net(z), -1) r_t = tf.math.l2_normalize(f_enc_target(x), -1) y = tf.stop_gradient(r_t) mu_b = tf.math.l2_normalize(m_net(y), -1) b_zy = tfd.VonMisesFisher(mu_b, kappa_b) r_t_prime = tf.math.l2_normalize(f_enc_target(x_prime), -1) y_prime = tf.stop_gradient(r_t_prime) # byol_loss corresponds to -log d(y|z) as described in Section 2.3 byol_loss = tf.reduce_sum(tf.math.square(y_pred -y_prime), axis=-1) log_e_zx = e_zx.log_prob(z) log_b_zy = b_zy.log_prob(z) i_xzy = log_e_zx -log_b_zy loss = byol_loss_weight * byol_loss + beta * i_xzy return loss Listing 2: Tensorflow pseudocode of C-BYOL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Z</head><label></label><figDesc>?(H(Z) ? H(Z|X) ? H(Z) + H(Z|Y )) ? H(Y ) + H(Y |Z) H(Z|X) + H(Z|Y )) + H(Y |Z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>C-SimCLR explicitly defines encoder distributions e(?|x) and b(?|y) where x and y are two augmented views of an image. y is also referred as x . The upper and lower encoder outputs are used to specify mean directions of e and b, and the two encoders share parameters. r x , r y are l 2 -normalized. Our modifications to SimCLR are highlighted in blue. No new parameters are added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>inf e(z|?) KL[e(z|x)||e(z|x + ?x)] + KL[e(z|x + ?x)||e(z|x)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>. 6 ImageNet</head><label>6</label><figDesc>-A [38]: This dataset of "Natural adversarial examples" consists of images of ImageNet classes which a ResNet-50 classifier failed on. The dataset authors performed manual, humanverification to ensure that the predictions of the ResNet-50 model were indeed incorrect and egregious [38].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[37]: This dataset adds 15 corruptions to ImageNet images, each at 5 levels of severity. We report the average accuracy over all the corruptions and severity levels.ImageNet-R [36]:This dataset, which has the full name "Imagenet Rendition", captures naturally occuring distribution changes in image style, camera operation and geographic location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Histograms of Equation<ref type="bibr" target="#b18">(19)</ref> (also Equation<ref type="bibr" target="#b27">(28)</ref> in this section) on 10,000 training images. Each local estimate is Equation(28)with a (x, x + ?x) pair. Here x is the original image and x + ?x is the augmented image. SimCLR is in orange. C-SimCLR is in blue. Higher y-axis values for lower x-axis values are better. We also report the mean (?) values. C-SimCLR consistently outperforms SimCLR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>on any (x, x + ?x) pairs to estimate how smooth our model is at that point, and to compare the relative smoothness of different models. Here, we consider (x, x + ?x) pairs where x is taken either from The same asFigure 3, but on 50,000 validation images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Listings 1 and 2 show</head><label>2</label><figDesc>Tensorflow pseudocode for C-SimCLR and C-BYOL respectively. Compute a Contrastive version of CEB loss for C-SimCLR model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Listing 1 :</head><label>1</label><figDesc>: An augmented image view. The expected shape is [B, H, W, C]. y: An augmented image view. The expected shape is [B, H, W, C]. f_enc: An image encoder (Conv + Projection in Fig. 1). kappa_e: A float. Concentration parameter of distribution e. kappa_b: A float. Concentration parameter of distribution b. beta: CEB beta for controlling compression strength (Equation 1).Returns:A tensor loss . The loss is per-sample.""" # Obtain unit-length mean direction vectors with expected shape [B, r_dim]. r_x = tf.math.l2_normalize(f_enc(x), -1) r_y = tf.math.l2_normalize(f_enc(y), -1) batch_size = tf.shape(r_x)[0] labels_idx = tf.range(batch_size) # Labels are pseudo-labels which mark corresponding positives in a batch labels = tf.one_hot(labels_idx, batch_size) mi_upper_bound = tf.math.log(tf.cast(batch_size, tf.float32)) e_zx = tfd.VonMisesFisher(r_x, kappa_e) b_zy = tfd.VonMisesFisher(r_y, kappa_b) z = e_zx.sample() log_e_zx = e_zx.log_prob(z) log_b_zy = b_zy.log_prob(z) i_xzy = log_e_zx -log_b_zy # residual information I(X;Z|Y) logits_ab = b_zy.log_prob(z[:, None, :]) # broadcast # The following categorical corresponds to c(y|z) and d(x|z) in Equation 12: cat_dist_ab = tfd.Categorical(logits=logits_ab) h_yz = -cat_dist_ab.log_prob(labels_idx) i_yz = mi_upper_bound -h_yz loss = beta * i_xzy -i_yz return loss Tensorflow pseudocode of C-SimCLR. Compute loss for C-BYOL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>: An augmented image view. The expected shape is [B, H, W, C]. x_prime: An augmented image view. The expected shape is [B, H, W, C]. f_enc: An image encoder (Conv + Projection in Fig. 2). f_enc_target: The target image encoder. A slow moving-average of f_enc. q_net: The BYOL predictor, which is a two-layer MLP. l_net: A transformation function. We choose a linear layer in this work. m_net: A transformation function. We choose a two-layer MLP in this work. kappa_e: A float. Concentration parameter of distribution e. kappa_b: A float. Concentration parameter of distribution b. beta: CEB beta for controlling compression strength (Equation 1). byol_loss_weight: BYOL loss weight. byol_loss_weight = kappa_d/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Uncompressed 69.1?0.089 89.1?0.034 42.1?1.06 72.8?0.155 91.0?0.072 37.3?0.089 Compressed 70.1?0.177 89.6?0.099 41.0?0.107 73.6?0.039 91.5?0.080 36.5?0.045 ResNet-50, 1000 epochs Uncompressed 70.7?0.094 90.1?0.081 40.0?0.123 74.2?0.139 91.7?0.041 35.7?0.114 Compressed 71.6?0.084 90.5?0.067 39.7?0.876 75.6?0.151 92.7?0.076 34.0?0.127 ResNet-50 2x, 1000 epochs Uncompressed 74.5?0.014 92.1?0.031 35.2?0.038 77.2?0.057 93.5?0.036 31.8?0.073 Compressed 75.0?0.082 92.4?0.086 34.7?0.129 78.8?0.088 94.5?0.016 29.8?0.028</figDesc><table><row><cell></cell><cell></cell><cell>SimCLR</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell></row><row><cell>Method</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Brier</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Brier</cell></row><row><cell cols="2">ResNet-50, 300 epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell></row><row><cell>Method</cell><cell>1%</cell><cell>10%</cell><cell>1%</cell><cell>10%</cell></row><row><cell cols="5">Supervised [77] 25.4 56.4 48.4 80.4</cell></row><row><cell>SimCLR</cell><cell cols="4">49.3 63.3 75.8 85.9</cell></row><row><cell>C-SimCLR</cell><cell cols="4">51.1 64.5 77.2 86.5</cell></row><row><cell>BYOL</cell><cell cols="4">55.5 68.2 79.7 88.4</cell></row><row><cell>C-BYOL</cell><cell cols="4">60.6 70.5 83.4 90.0</cell></row></table><note>ImageNet accuracy when training linear classifiers with 1% and 10% of the labeled Ima- geNet data, on top of frozen, self-supervised repre- sentations learned on the entire ImageNet dataset without labels. For the supervised baseline, the whole ResNet-50 network is trained from random initialisation. We report mean results of 3 trials.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison to other methods on ImageNet linear evaluation and supervised baselines. *: trained for 800 epochs. Other methods are 1000 epochs. on what the original authors reported. C-BYOL achieves the best results compared to other state-of-the-art methods. Moreover, we can improve C-BYOL with ResNet-50 even further to 76.0 Top-1 accuracy when we train it for 1500 epochs.</figDesc><table><row><cell></cell><cell cols="2">ResNet-50</cell></row><row><cell>Method</cell><cell>1x</cell><cell>2x</cell></row><row><cell>SimCLR [12]</cell><cell cols="2">69.3 74.2</cell></row><row><cell cols="2">SwAV* (2 crop) [11, 15] 71.8</cell><cell>-</cell></row><row><cell>InfoMin Aug* [65]</cell><cell>73.0</cell><cell>-</cell></row><row><cell>Barlow Twins [76]</cell><cell>73.2</cell><cell>-</cell></row><row><cell>BYOL [30]</cell><cell cols="2">74.3 77.4</cell></row><row><cell>C-SimCLR (ours)</cell><cell cols="2">71.6 75.0</cell></row><row><cell>C-BYOL (ours)</cell><cell cols="2">75.6 78.8</cell></row><row><cell>Supervised [12, 30]</cell><cell cols="2">76.5 77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of robustness and generalization of self-supervised models, using a ResNet-50 backbone trained on ImageNet for 1000 epochs. We report the mean Top-1 accuracy over 3 trials on a range of benchmarks (detailed in Sec. 3.3 and the appendix) which measure an ImageNet-trained model's generalization to different domains and distributions.</figDesc><table><row><cell>Method</cell><cell>ImageNet-A</cell><cell>ImageNet-C</cell><cell>ImageNet-R</cell><cell>ImageNet-v2</cell><cell>ImageNet-Vid</cell><cell>YouTube-BB</cell><cell>ObjectNet</cell></row><row><cell>SimCLR</cell><cell>1.3</cell><cell>35.0</cell><cell>18.3</cell><cell>57.7</cell><cell>63.8</cell><cell>57.3</cell><cell>18.7</cell></row><row><cell>C-SimCLR</cell><cell>1.4</cell><cell>36.8</cell><cell>19.6</cell><cell>58.7</cell><cell>64.7</cell><cell>59.5</cell><cell>20.8</cell></row><row><cell>BYOL</cell><cell>1.6</cell><cell>42.7</cell><cell>24.4</cell><cell>62.1</cell><cell>67.9</cell><cell>60.7</cell><cell>23.4</cell></row><row><cell>C-BYOL</cell><cell>2.3</cell><cell>45.1</cell><cell>25.8</cell><cell>63.9</cell><cell>70.8</cell><cell>63.6</cell><cell>25.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of CEB compression using C-SimCLR models trained for 300 epochs with a ResNet-50 backbone. ? controls the level of CEB compression. We evaluate linear-evaluation on ImageNet, and model robustness on the remaining datasets as described in Sec. 3.3.</figDesc><table><row><cell>Method</cell><cell>ImageNet</cell><cell>ImageNet-A</cell><cell>ImageNet-C</cell><cell>ImageNet-R</cell><cell>ImageNet-v2</cell><cell>ImageNet-Vid</cell><cell>YouTube-BB</cell><cell>ObjectNet</cell></row><row><cell>SimCLR</cell><cell>69.0</cell><cell>1.2</cell><cell>32.9</cell><cell>17.8</cell><cell>56.0</cell><cell>61.1</cell><cell>58.3</cell><cell>17.6</cell></row><row><cell>? = 0</cell><cell>69.7</cell><cell>1.1</cell><cell>35.8</cell><cell>17.6</cell><cell>56.8</cell><cell>62.5</cell><cell>58.4</cell><cell>18.5</cell></row><row><cell>? = 0.01</cell><cell>69.7</cell><cell>1.2</cell><cell>36.2</cell><cell>17.5</cell><cell>57.2</cell><cell>61.2</cell><cell>58.5</cell><cell>18.7</cell></row><row><cell>? = 0.1</cell><cell>70.1</cell><cell>1.1</cell><cell>36.1</cell><cell>17.6</cell><cell>56.9</cell><cell>62.4</cell><cell>58.6</cell><cell>18.4</cell></row><row><cell>? = 1</cell><cell>70.2</cell><cell>1.1</cell><cell>36.7</cell><cell>18.2</cell><cell>57.5</cell><cell>62.6</cell><cell>60.4</cell><cell>19.2</cell></row><row><cell>? = 1.5</cell><cell>69.7</cell><cell>1.1</cell><cell>36.4</cell><cell>18.3</cell><cell>57.3</cell><cell>62.0</cell><cell>57.9</cell><cell>18.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>consider "natural adversarial examples" with ImageNet-A [37] which consists of difficult images which a ResNet-50 classifier failed on. ImageNet-C [37] adds synthetic corruptions to the ImageNet validation set, ImageNet-R [36] considers other naturally occuring distribution changes in image style while ObjectNet [5] presents a more difficult test set for ImageNet where the authors control for different parameters such as viewpoint and background. ImageNet-Vid and YouTube-BB<ref type="bibr" target="#b43">[62]</ref> evaluate the robustness of image classifiers to natural perturbations arising in video. Finally, ImageNet-v2<ref type="bibr" target="#b38">[58]</ref> is a new validation set for ImageNet where the authors attempted to replicate the original data collection process. Further details of these robustness benchmarks are in Section F.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [35] Olivier Henaff. Data-efficient image recognition with contrastive predictive coding. In International Conference on Machine Learning, pages 4182-4192. PMLR, 2020. [36] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.[37] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. ICLR, 2019.</figDesc><table><row><cell>arXiv</cell></row><row><cell>preprint arXiv:1608.03983, 2016.</cell></row><row><cell>[50] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.</cell></row><row><cell>Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,</cell></row><row><cell>2017.</cell></row></table><note>[38] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. CVPR, 2021.[39] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 2006.[40] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.[41] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. PMLR, 2015.[42] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.[43] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1920-1929, 2019.[44] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2661-2671, 2019.[45] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-grained cars. In The 2nd Fine-Grained Visual Categorization Workshop, 2013.[46] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.[47] Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio Guadarrama. Predictive information accelerates learning in RL. Advances in Neural Information Processing Systems, 33:11890-11901, 2020.[48] Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2021.[49] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts.[51] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine- grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.[52] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing, pages 722-729. IEEE, 2008.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Image augmentation parameters. We use the hyperparameter values from BYOL<ref type="bibr" target="#b29">[30]</ref>, and include the values from the original SimCLR<ref type="bibr" target="#b11">[12]</ref> as reference.</figDesc><table><row><cell>Parameter</cell><cell>t</cell><cell>t</cell><cell>Orig. SimCLR [12]</cell></row><row><cell>Random crop probability</cell><cell cols="2">1.0 1.0</cell><cell>1.0</cell></row><row><cell>Flip probability</cell><cell cols="2">0.5 0.5</cell><cell>0.5</cell></row><row><cell>Color jittering probability</cell><cell cols="2">0.8 0.8</cell><cell>0.8</cell></row><row><cell cols="3">Brightness adjustment max strength 0.4 0.4</cell><cell>0.8</cell></row><row><cell>Contrast adjustment max strength</cell><cell cols="2">0.4 0.4</cell><cell>0.8</cell></row><row><cell cols="3">Saturation adjustment max strength 0.2 0.2</cell><cell>0.8</cell></row><row><cell>Hue adjustment max strength</cell><cell cols="2">0.1 0.1</cell><cell>0.2</cell></row><row><cell>Color dropping probability</cell><cell cols="2">0.2 0.2</cell><cell>0.2</cell></row><row><cell>Gaussian blurring probability</cell><cell cols="2">1.0 0.1</cell><cell>1.0</cell></row><row><cell>Solarization probability</cell><cell cols="2">0.0 0.2</cell><cell>0.0</cell></row><row><cell>A.2 Network architecture</cell><cell></cell><cell></cell><cell></cell></row></table><note>Following [12, 30], we use ResNet-50 [34] as our backbone convolutional encoder (the "Conv" part in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :Table 8 :</head><label>78</label><figDesc>Ablation study on BYOL models trained for 300 epochs. Top-1 denotes the linear evaluation Top-1 accuracy on ImageNet. The effect of loss weights on BYOL models trained for 1000 epochs. Top-1 denotes the linear evaluation Top-1 accuracy on ImageNet.</figDesc><table><row><cell>Method</cell></row></table><note>? base = 0.996 and ramps up to 1 with a cosine schedule, ? 1 ? (1 ? ? base )(cos(?k/K) + 1)/2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Transfer to other classification tasks, by performing linear evaluation. The backbone network is ResNet-50, pretrained in a self-supervised fashion for 1000 epochs.</figDesc><table><row><cell>Method</cell><cell>Food101</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell>Flowers</cell><cell>Pet</cell><cell>Cars</cell><cell>Caltech-101</cell><cell>DTD</cell><cell>SUN397</cell><cell>Aircraft</cell><cell>Birdsnap</cell></row><row><cell>SimCLR</cell><cell>72.5</cell><cell>91.1</cell><cell>74.4</cell><cell>88.4</cell><cell>83.5</cell><cell>49.7</cell><cell>89.5</cell><cell>72.5</cell><cell>61.8</cell><cell>51.6</cell><cell>35.4</cell></row><row><cell>C-SimCLR</cell><cell>73.0</cell><cell>91.6</cell><cell>75.2</cell><cell>89.0</cell><cell>84.0</cell><cell>52.7</cell><cell>91.2</cell><cell>73.0</cell><cell>62.3</cell><cell>53.5</cell><cell>38.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>C-BYOL and BYOL trained for 1000 epochs with different ResNet depth. We report ImageNet Top-1 accuracy from linear evaluation, averaged over 3 trials.</figDesc><table><row><cell>C-BYOL</cell><cell>BYOL [30]</cell></row><row><cell cols="2">Architecture Top-1 Top-5 Top-1 Top-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :Table 12 :</head><label>1112</label><figDesc>The effect of varying ? e for C-SimCLR models. We report ImageNet Top-1 accuracy from linear evaluation. ? e 256 512 1024 2048 4096 8192 ImageNet Top-1 accuracy 69.8 69.8 70.2 69.8 69.6 69.6 The effect of varying ? e for C-BYOL models. We report ImageNet Top-1 accuracy from linear evaluation.</figDesc><table><row><cell></cell><cell cols="5">? e 4096 8192 16384 32768</cell></row><row><cell cols="4">ImageNet Top-1 accuracy 73.0 73.3</cell><cell>73.6</cell><cell>73.2</cell></row><row><cell cols="6">Table 13: The effect of varying ? b for C-SimCLR and C-BYOL models. We report ImageNet Top-1</cell></row><row><cell>accuracy from linear evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>? b =1</cell><cell>3</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>C-SimCLR</cell><cell>65.0</cell><cell cols="4">68.5 70.2 69.1 68.6</cell></row><row><cell>C-BYOL</cell><cell>73.1</cell><cell cols="4">73.3 73.6 73.4 73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Thus, taking the pointwise maximum across pairs of inputs in any dataset gives a valid estimate of the maximum lower bound of the encoder's Lipschitz constant. Equation (36) can be evaluated directly on any pair of valid inputs (x, x + ?x). Equation (36) is the same as Equation (20) used in Section 2.4.</figDesc><table><row><cell>KL[e(z|x)||e(z|x + ?x)], KL[e(z|x + ?x)||e(z|x)]</cell><cell>(36)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, e(z|x) could not generally be a mixture distribution, as sampling the mixture distribution has a discrete component, and we cannot easily take gradients through discrete samples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that by taking an expectation we get a KL divergence, which violates the triangle inequality, even though we started from a valid distance metric. Squaring the Lipschitz constant addresses this in the common case where the KL divergence grows quadratically in ||?x||, as detailed in Section G.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We note that comparing supervised and self-supervised methods is difficult, as it can only be system-wise. Various complementary techniques can be used to further improve evaluation results in both settings. For example, the appendix of<ref type="bibr" target="#b29">[30]</ref> reports that various techniques improve supervised model accuracies, whilst<ref type="bibr" target="#b29">[30,</ref> 43]  report various techniques to improve evaluation accuracy of self-supervised representations. We omit these in order to follow the common supervised baselines and standard evaluation protocols used in prior work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Previous versions of TFP were unstable for sampling from vMF distributions with higher than 5 dimensions, and at the time of writing, the authors of the library have not updated the documentation to indicate that this is no longer the case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/google-research/robustness_metrics</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that if we choose an encoder distribution where the density ever goes to 0 or ?, Equation (28) will have a maximum value of ?. Of course, it's generally easy to avoid this situation by choosing "well-behaved" distributions like the Gaussian or von Mises-Fisher distributions, whose densities are non-zero on the entire domain, and to parameterize them with variance or concentration parameters that don't go to 0 or ?, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">This is easiest to see with Gaussian distributions whose means are parameterized by an identity map of x and x + ?x: the KL divergence is quadratic in difference of the means, which is ||?x||.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Toby Boyd for his help in making our implementation open source.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information dropout: Learning optimal representations through noisy computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2897" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9453" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Joshua V Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Langmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10604</idno>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tensorflow distributions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frances</forename><surname>Hubis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/robustness_metrics" />
	</analytic>
	<monogr>
		<title level="j">Robustness Metrics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08558</idno>
		<title level="m">On robustness and transferability of convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bloem-Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10800</idno>
		<title level="m">Lossy compression for lossless prediction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning optimal representations with the decodable information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedantam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18674" to="18690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient and accurate estimation of lipschitz constants for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Fazlyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Robey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George J</forename><surname>Pappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07017</idno>
		<title level="m">Learning robust representations via multi-view information bottleneck</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The conditional entropy bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ceb improves model robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selfsupervised adversarial robustness for the low-label, high-data regime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bootstrap latent-predictive representations for multitask reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3875" to="3886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">St?phane Gentric, and Liming Chen. von misesfisher mixture model-based deep learning: Application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Bohn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Milgram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Youtubeboundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<title level="m">Do ImageNet classifiers generalize to ImageNet? ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and variational inference in deep latent gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning and generalization with the information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<editor>Yoav Freund, L?szl? Gy?rfi, Gy?rgy Tur?n, and Thomas Zeugmann</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02168</idno>
		<title level="m">Do image classifiers generalize across time? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An information theoretic framework for multi-view learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference on Learning Theory</title>
		<meeting>the 21st Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="403" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. arXiv preprint physics/0004057</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat?as</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">Rey</forename><surname>Vega</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05355</idno>
		<title level="m">The role of information complexity and randomization in representation learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluating the robustness of neural networks: An extreme value theory approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Tsui-Wei Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A closer look at accuracy vs. robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Barlow twins: Selfsupervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">S4L: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
	<note>ResNet-50</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">suggest that the representations learned by compressed model are generic, and transfer beyond the ImageNet domain which they were learned on</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
