<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
							<email>wandt@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hannover Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
							<email>rudolph@tnt.uni-hannover.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Leibniz University Hannover Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrissa</forename><surname>Zell</surname></persName>
							<email>zell@tnt.uni-hannover.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Leibniz University Hannover Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<email>helge.rhodin@ubc.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">UBC Vancouver Vancouver</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<email>rosenhahn@tnt.uni-hannover.de</email>
							<affiliation key="aff4">
								<orgName type="institution">Leibniz University Hannover Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation from single images is a challenging problem in computer vision that requires large amounts of labeled training data to be solved accurately. Unfortunately, for many human activities (e.g. outdoor sports) such training data does not exist and is hard or even impossible to acquire with traditional motion capture systems. We propose a self-supervised approach that learns a single image 3D pose estimator from unlabeled multi-view data. To this end, we exploit multi-view consistency constraints to disentangle the observed 2D pose into the underlying 3D pose and camera rotation. In contrast to most existing methods, we do not require calibrated cameras and can therefore learn from moving cameras. Nevertheless, in the case of a static camera setup, we present an optional extension to include constant relative camera rotations over multiple views into our framework. Key to the success are new, unbiased reconstruction objectives that mix information across views and training samples. The proposed approach is evaluated on two benchmark datasets (Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation from single images is an ongoing research topic in computer vision. There exist a large amount of supervised deep learning solutions in the literature. These approaches achieve remarkable results in a supervised setting, i.e. having 2D to 3D annotations, but heavily rely on a vast amount of available training data. However, there are many activities a person can perform which are not present in common datasets. For instance, human motions performed during outdoor and/or sports activities, e.g. as shown in <ref type="figure">Fig. 1</ref>, are hard or even impossible to cap- <ref type="bibr">Figure 1</ref>. CanonPose learns a monocular 3D human pose estimator from multi-view self-supervision. By estimating 3D poses from different views in a canonical form together with the respective camera rotations we exploit multi-view consistency in the training data. Even for challenging outdoor datasets with moving cameras we achieve convincing 3D pose estimates from single images after training. ture with a commercial motion capture systems. Therefore, the acquisition of training data is a major practical challenge. To this end, we propose a novel self-supervised training procedure that does not require any 2D or 3D annotations in the multi-view training dataset and works with uncalibrated cameras. To acquire 2D joint predictions from images we use a 2D human joint estimator <ref type="bibr" target="#b6">[7]</ref> that is pretrained on a different dataset with only 2D joint annotations. The only requirements for our method are at least two temporally synchronised cameras that observe the person of interest from different angles. No further knowledge about the scene, camera calibration and intrinsics is required. Several related works consider a sparse set of 3D annotations <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref>, unpaired 3D data <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16]</ref>, or known camera positions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref> to solve this problem. However, such data rarely exists for outdoor settings with moving cameras. To the best of our knowledge, there are only three competing methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11]</ref> that apply to our setting. They either require additional knowledge about the scene or observed person, such as scene geometry <ref type="bibr" target="#b1">[2]</ref> and bone lengths constraints <ref type="bibr" target="#b10">[11]</ref>, or sophisticated traditional computer vision algorithms that produce a pseudo ground truth pose <ref type="bibr" target="#b13">[14]</ref>.</p><p>We propose a self-supervised training method which mixes outputs of multiple weight-sharing neural networks. <ref type="figure">Fig. 2</ref> shows our training pipeline when using two cameras. Each individual network takes a single image as input and outputs a 3D pose in a canonical rotation, which gives our method its name CanonPose. This representation allows for the projection of all estimated 3D poses to any camera of the setup. Our approach splits into two stages. The first stage predicts the 2D human pose from an image using a neural network pretrained on the MPII dataset <ref type="bibr" target="#b23">[24]</ref>, in our case AlphaPose <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. The second stage lifts these 2D detections to a 3D pose represented in a learned canonical coordinate system. In a separate path it predicts the camera orientation to rotate the predicted 3D pose back into the camera coordinate system. Combining the 3D pose from a first view with the rotation predicted from a second view, results in a rotated pose in the second camera coordinate system. In other words, both 3D poses in the pose coordinate system should be equal and the predicted rotations project it back into the respective camera coordinate systems. This enables the definition of a reprojection loss for each original and newly combined reprojection. For static camera setups we propose an optional reprojection loss that is computed by mixing relative camera rotations between samples in a training batch. Additionally, in contrast to existing self-supervised approaches, we also make use of the confidences that are typically provided by a 2D pose estimator for each predicted 2D joint by including them into the 2D input vector as well as into the reprojection loss formulation.</p><p>We evaluate our approach on the two benchmark datasets Human3.6M <ref type="bibr" target="#b9">[10]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b23">[24]</ref> and set the new state of the art in several metrics for self-supervised 3D pose estimation. Notably, this is without assuming any camera calibration or static cameras. Our results are competitive to the fully supervised approach of Martinez et al. <ref type="bibr" target="#b22">[23]</ref> which sets the baseline for single image pose estimation from 2D detections. Additionally, we show results for the SkiPose <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35]</ref> dataset. This dataset represents all challenges that arise when activities are captured that cannot be performed in the restricted setting of a standard motion capture system. It consists of outdoor scenes captured on a ski slope and includes fast motions, a large capture volume and pan-tiltzoom cameras.</p><p>Summarizing, our contributions are:</p><p>? We present CanonPose: a self-supervised approach to train a single image 3D pose estimator from unlabeled multi-view images by mixing poses across views.</p><p>? Our approach requires no prior knowledge about the scene, 3D skeleton or camera calibration.</p><p>? The proposed method directly employs multi-view images without any laborious pre-processing, such as camera calibration or multi-view geometry estimation.</p><p>? We integrate the confidences from the 2D joint estimator into the training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we discuss recent 3D human pose estimation approaches by different types of supervision.</p><p>Full Supervision Recent supervised approaches rely on large datasets that contain millions of images with corresponding 3D pose annotations. Li et al. <ref type="bibr" target="#b17">[18]</ref> were the first to learn CNNs to directly regress a 3D pose from image input. By integrating a structured learning framework into CNNs they later improved their work <ref type="bibr" target="#b19">[20]</ref>. Several others followed this end-to-end approach <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b12">13]</ref>. Typically, these end-to-end approaches perform exceptionally well on similar image data. However, their ability to generalize to other scenes is limited. Many works tackle this problem by cross dataset training or data augmentation.</p><p>There are other approaches that do not consider the image data directly but use a pretrained 2D joint detector <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25]</ref>. They benefit from training on large datasets that contain 2D annotations for many human activities in various scenes and are therefore agnostic to the image data. Martinez et al. <ref type="bibr" target="#b22">[23]</ref> directly train a neural network on 2D detections and 3D ground truth. Due to its simplicity it can be trained quickly for many epochs leading to high accuracy and serves as a baseline for many following works. The approach of <ref type="bibr" target="#b22">[23]</ref> was extended by Hossain et al. <ref type="bibr" target="#b31">[32]</ref> by employing a recurrent neural network for sequences of human poses. While effective, the major downside of all supervised approaches is that they do not generalize well to unseen poses. Therefore, their application to in-the-wild scenes is limited.</p><p>Weak Supervision Weakly supervised approaches only require a small set or even no annotated 2D to 3D correspondences. An example for a commonly applied evaluation protocol for the Human3.6M dataset assumes that 3D annotations for one of the subjects of the training set are available. A transfer learning approach is introduced by Mehta et al. <ref type="bibr" target="#b23">[24]</ref> to allow for in-the-wild pose estimation of datasets where no training data is available. This framework was later extended by Mehta et al. <ref type="bibr" target="#b25">[26]</ref> to achieve real-time performance. Rhodin et al. <ref type="bibr" target="#b34">[35]</ref> use multi-view images and known camera positions to learn a 3D pose embedding in an unsupervised fashion. The embedding facilitates the training with only a sparse set of 3D annotations. This idea was adopted in other works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28]</ref>. Another approach is to employ unpaired 2D and 3D poses <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. Since these method learn distributions of plausible 3D poses and their properties they generalize better to unseen poses. Although they are able to reconstruct out-of-distribution poses to a limited degree they struggle with completely unseen poses.</p><p>Self-supervised and Unsupervised Learning without 3D Ground Truth Recently, the interest in multi-view selfsupervised and unsupervised 3D pose estimation is growing. Our work also falls into this category. Drover et al. <ref type="bibr" target="#b3">[4]</ref> propose an unsupervised approach to monocular human pose estimation. They randomly project an estimated 3D pose back to 2D. This 2D projection is then evaluated by a discriminator following adversarial training approaches. Chen et al. <ref type="bibr" target="#b1">[2]</ref> extended <ref type="bibr" target="#b3">[4]</ref> with a cycle consistency loss that is computed by lifting the randomly projected 2D pose to 3D and inversing the previously defined random projection. Although these two approaches are unsupervised, they integrate knowledge about the scene by constraining the camera rotation axis that is used for the random projection. Rochette et al. <ref type="bibr" target="#b35">[36]</ref> use a large amount of cameras from different viewing angles to achieve on par performance with a comparable fully supervised approach. However, due to the restriction to the camera setup the practical applicability is limited. Kocabas et al. <ref type="bibr" target="#b13">[14]</ref> propose a multi-view self-supervised approach which does not require any 3D supervision. They apply traditional computer vision methods, namely epipolar geometry, to 2D pose predictions from multiple views to compute a pseudo ground truth which is then used to train the 3D lifting network. Although this simple and effective straight-forward approach gives promising results, the laborious preprocessing step is very parameter sensitive and therefore does not generalize well. Moreover, mistakes due to wrongly estimated joints in the 2D prediction step result in a wrong pseudo ground truth. Iqbal et al. <ref type="bibr" target="#b10">[11]</ref> tackle this problem by training an end-to-end network that refines the pre-trained 2D pose estimator during the self-supervised training. Unfortunately, such approaches tend to easily overfit to a specific dataset. For example, it could learn a background image for the training dataset which leads to exceptional performance on the specific dataset but does not generalize to other backgrounds. This even happens in self-supervised settings. Furthermore, Iqbal et al. <ref type="bibr" target="#b10">[11]</ref> employ a loss on normalized 3D bone lengths which is computed from the ground truth 3D poses of the Human3.6M training set.</p><p>In contrast, our approach does not require knowledge about the scene and camera position or any anthropometric constraints. Additionally, it is modular such that any 2D <ref type="figure">Figure 2</ref>. Network structure to learn single image 3D pose estimation from multi-view self-supervision. Each lifting network predicts a 3D pose and a camera rotation which is used to project the 3D pose back to 2D. Both networks observe the same 3D pose from different angles. We exploit this fact by applying the camera rotation to the respective other pose. This projects a predicted 3D pose into the other camera and gives an additional reprojection error. At inference time only one view (gray box) is applied. pose estimator can be used which makes it agnostic to the image data. Even though our approach relaxes many constraints of the comparable works it still outperforms them in most experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our approach consists of two steps: first applying an offthe-shelf 2D joint detector to the input images, and second lifting these detections and the respective confidences for each joint to 3D. The core idea of our approach is that 2D detections from one view can be projected to another view via a canonical pose space. <ref type="figure">Fig. 2</ref> shows our pipeline using two cameras. For simplicity the network structure is shown for only two cameras. If more cameras are available it is straight-forwardly extended. A single neural network, the 3D lifting network, predicts the 3D pose X ? R 3?j with j joints and a rotation R ? R 3?3 to rotate the pose to the camera coordinate system. The pose is represented in a canonical pose coordinate system which is automatically learned during training. Subsequently, the predicted 3D pose is rotated from the pose coordinate system to the camera coordinate system by the predicted rotation. This separation into canonical human pose and camera rotation enables us to formulate various reprojection losses for selfsupervision across views and samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reprojection</head><p>Before a 2D pose is lifted to 3D it is normalized by centering it to the root joint and scaled by dividing it by its Frobenius norm. This sidesteps the scale-depth ambiguity in monocular reconstruction. In particular, the root center-ing gives a common rotation point for all 3D predictions. For each view the predicted 3D pose is rotated into the camera coordinate system by RX. R ? R 3?3 is a rotational matrix such that RR T = I 3 with I 3 as the 3 ? 3 identity matrix and det(R) = 1. Since we assume weak perspective cameras, the projection to the camera plane is simply done by removing the depth coordinate, which is expressed as</p><formula xml:id="formula_0">W rep = 1 0 0 0 1 0 RX,<label>(1)</label></formula><p>where W rep ? R 2?j is called the reprojected 2D pose.</p><p>With W as the input 2D pose we define the scaleindependent reprojection loss as</p><formula xml:id="formula_1">L rep = W ? W rep W rep F 1 ,<label>(2)</label></formula><p>where ? 1 denotes the L 1 norm. Since the global scale of the 3D pose is unknown and we consider weak perspective projections, scaling the reprojection W rep is essential. Note that the input 2D pose W is already divided by its Frobenius norm in the preprocessing. That means both, the input pose and the predicted pose, have the same scale.</p><p>To ensure that the network predicts a proper rotation, the matrix R is not predicted directly, but in axis-angle representation. Let (?) be a rotational angle and ? = (? 1 , ? 2 , ? 3 ) denote a rotation axis. With</p><formula xml:id="formula_2">A = ? ? 0 ?? 3 ? 2 ? 3 0 ?? 1 ?? 2 ? 1 0 ? ? (3)</formula><p>Rodrigues' formula is applied to obtain the rotation matrix</p><formula xml:id="formula_3">R = I 3 + (sin ?)A + (1 ? cos ?)A 2 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">View-consistency</head><p>A straight-forward way of ensuring view consistency would be to enforce a loss, such as L 2 , between the canonical poses predicted by two views. In theory, that loss should be zero for the correct solution because the same person seen from two different views should have the same canonical pose. In practice, however, this leads to the lifting network learning 3D poses that are view invariant but no longer in close correspondence to the input pose, preventing the network to converge to plausible solutions in our preliminary experiments.</p><p>The key insight to the proposed method is that rotations and poses from different views can be mixed to enforce the view consistency as a variant of the previously introduced reprojection objective. We mix the predicted camera and pose of two views, say view-1 and view-2, by rotating the predicted canonical 3D pose from the source view-1 to the target view-2 by using the rotation from view-2. For two cameras as in <ref type="figure">Fig. 2</ref> there exist four possible combinations of rotations and poses. The same approach is easily extended to m cameras which gives m 2 combinations. During training time all possible combinations are reprojected to the respective cameras. With this training scheme we enforce multi-view consistency without bias towards trivial solutions. Note that the lifting network is only applied to a single frame at inference stage and does not need any other inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Confidences</head><p>The output of most pretrained 2D joint estimators are 2D heatmaps where each entry indicates the confidence for the presence of the corresponding joint at the associated position in the image. Commonly, the argmax or soft-argmax is computed and given as input to the following lifting network. However, this gives an exact joint position independent of the confidence of the 2D detection. That means uncertain predictions are processed in the same way as certain ones. We circumvent this problem by two simple modifications. First, we concatenate the maximum value of each heatmap, which is a surrogate to its confidence, to the 2D pose input vector to our lifting network. Second, we modify the reprojection error in Eq. 2 such that each difference between input and reprojected 2D is linearly weighted with its confidence by</p><formula xml:id="formula_4">L rep,c = W ? W rep W rep F C 1 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">C = c 1 c 2 . . . c j c 1 c 2 . . . c j<label>(6)</label></formula><p>with c i as the maximum value of the heatmap for joint i and as the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Camera-consistency</head><p>A reasonable assumption for many practical motion capture setups is that cameras are static during recording a sequence, i.e. they do not change their position or orientation. This is the case for the Human3.6M 1 and 3DHP dataset. However, this assumption is not mandatory for our proposed method, but an enhancement for scenes with static cameras. We will show the effect of this optional improvement in the experiments as well as the performance of our approach without it on the SkiPose dataset that contains moving cameras.</p><p>For a static camera setup all relative rotations between the cameras are equal. An intuitive approach to enforce static cameras is to calculate an L 2 -loss between the relative rotations over one batch of training samples. However, a batch-wise loss leads to degraded solutions or had no effect if its weight was set to a low value. This observation is similar to the findings regarding the canonical pose equality in Sec. 3.2. For this reason we propose a similar mixing approach as in Sec. 3.2, now over estimates from different samples in one batch. A relative rotation R 1,2 using the rotation matrices R 1 and R 2 from view-1 to view-2 respectively, is defined by</p><formula xml:id="formula_6">R 1,2 = R 2 R T 1 .<label>(7)</label></formula><p>Let R (s) 1,2 be the predicted relative rotation between view-1 and view-2 of sample s. We then randomly permute these relative rotations in the batch and use them to reproject the canonical poses similar to Eq. 1</p><formula xml:id="formula_7">W rep = 1 0 0 0 1 0 R (s) 1,2 R (s ) 1 X (s ) ,<label>(8)</label></formula><p>where R (s ) 1 and X (s ) are the rotation and estimated 3D pose in the current frame and R (s) 1,2 is the randomly assigned relative rotation from another sample in the batch 2 . The loss is calculated in the same way as the reprojection loss in Eq. 2. Similar to Sec. 3.2 this is easily extended to multiple cameras. Again, we emphasize that this loss is optional to improve the results for the case of static cameras. However, our method works without it. <ref type="figure">Fig. 3</ref> shows the architecture of our lifting network. The input 2D pose vector is concatenated with a vector containing the confidences for each joint. It is upscaled to 1024 neurons by a one fully connected layer. It is followed by a residual block consisting of fully connected layers with dimension 1024. Similar to <ref type="bibr" target="#b43">[44]</ref> the output is fed into two paths, each containing two consecutive residual blocks with identical architecture to the first block. The 3D pose path directly outputs the 3D coordinates of the predicted pose in the pose coordinate system. The camera path outputs a three-dimensional vector ?? which is the axis angle representation. The rotation matrix is computed using Rodrigues' formula as described in Sec. 3.1. The activation functions after each layer, except the two output layers, are leaky ReLU's with a negative slope of 0.01. We train the network for 100 epochs using the Adam optimizer with an initial learning rate of 0.0001 and weight decay at epochs 30, 60 and 90, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform experiments on the well-known benchmark datasets Human3.6M <ref type="bibr" target="#b9">[10]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b23">[24]</ref>. Additionally, we evaluate on the SkiPose dataset <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35]</ref> to test <ref type="bibr" target="#b1">2</ref> For the Human3.6M dataset we ensure that relative rotations are only changed in between subjects since camera positions vary between them. the generalizability of our method to real world scenarios. To conform with our setting of training a single image pose estimator with unlabeled images for a specific set of activities, we train one network for each dataset without using additional datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metrics</head><p>For the evaluation on Human3.6M there exist two standard protocols. Both protocols calculate the mean per joint position error (MPJPE), i.e. the mean euclidean distance between the reconstructed and the ground truth joint coordinates. Since a multi-view self-supervised setting does not contain metric data, we adjust the scale of our predictions before calculating the MPJPE. For a fair comparison with other works we compare to their scale adjusted predictions if they are available. Protocol-I computes the MPJPE directly whereas Protocol-II first employs a rigid alignment between the poses. Additional to the MPJPE one protocol for 3DHP calculates the Percentage of Correct Keypoints (PCK). As the name suggests it is the percentage of predicted joints that are within a distance of 150mm or lower to their corresponding ground truth joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Poses Score (CPS)</head><p>For practical applications, such as motion analysis and prediction, the evaluation of the whole pose is a crucial prerequisite. Even if a single joint of a pose is incorrect it can change downstream tasks significantly. The formerly introduced metrics evaluate the quality of the prediction joint by joint. However, they ignore the assignment of joints to poses and instead average over all joints in the test set. <ref type="figure" target="#fig_2">Fig. 5</ref> compares 3D pose estimates with their respective ground truths. Each column shows two different reconstructions from the same pose. The reconstructions in the top row have a lower PMPJPE compared to the bottom row. However, the overall 3D poses appear better reconstructed in the bottom row. In this section we present a simple yet powerful metric to evaluate such cases, the Correct Poses Score (CPS). A pose W is considered correct if for all joints i the Euclidean distance is below a threshold value ?. Given a pose with joint positions w i and predicted joint positions? i after rigid alignment, a correct pose is defined by</p><formula xml:id="formula_8">CP ? = 1 w i ?? i 2 &lt; ? ?i ? {1, ..., j} 0 else .<label>(9)</label></formula><p>Additional to the PMPJPE <ref type="figure" target="#fig_2">Fig. 5</ref> shows the CP@180mm, which classifies the reconstructed poses into correct and incorrect. The percentage of correct poses is calculated for the test dataset. To be independent of the threshold, we calculate the area under curve for ? ? [0mm, 300mm] which defines the CPS. <ref type="figure">Figure 3</ref>. Network structure of the lifting network. The 2D input vector contains the xand y-coordinates of the 2D pose and the confidence given by the 2D joint detector. It is upscaled using a fully connected layer with 1024 neurons which then goes to a residual block. After that the network splits into two paths that predict the 3D pose in the canonical space and the camera rotation, respectively. Each of the paths has two consecutive residual blocks followed by a fully connected layer that downscales the features to the required size. The Rodrigues block implements Rodrigues formula (Eq. 4) and has no trainable parameters.  Each column compares to different predicted 3D reconstructions with the same ground truth. While PMPJPE averages out high individual joint errors which are located in the right arm in the visualized case, CP indicates them. In this way, the correctness of the overall pose is evaluated. Note that for the calculation of the CPS we vary the threshold, which in these examples is 180mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Skeleton Morphing</head><p>We deploy an off-the-shelf detector AlphaPose <ref type="bibr" target="#b6">[7]</ref> for retrieving the 2D human pose estimation required as input to our method. The keypoint locations in the datasets used to train AlphaPose and other 2D pose estimation methods differ from the 3D skeleton of the test benchmarks. For example, the root joint position is not in the middle of the hip joints and the relative position of the neck to the shoulders is different. We circumvent this problem by training a 2D skeleton morphing network that predicts the offset between the 2D pose from AlphaPose to the ground truth 2D pose in the dataset. We train the morphing network on subject 1 of each dataset with the given ground truth poses. To not include these ground truth poses into our training, subject 1 is excluded in all experiments. Thereby our data used for the self supervised training does not contain any 2D ground truth data, mimicking real application scenarios. Note that the morphing network never sees any images and therefore is not able to learn domain specific image features. In an experimental setting where the skeletal structure does not need to match a different skeleton this step is obsolete. This is the case for most practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation on Human3.6M and 3DHP</head><p>For the Human3.6M dataset, to keep it consistent with previous approaches, we follow standard protocols and evaluate only on every 64th frame. However, with a sufficiently fast 2D pose estimator, which is the performance bottle neck of our complete pipeline, we can achieve realtime performance. <ref type="table" target="#tab_0">Table 1</ref> shows the results of the proposed method compared to other state-of-the-art approaches. We outperform every other comparable approach in terms of PMPJPE. Note that we even achieve comparable performance to the fully supervised method of Martinez et al. <ref type="bibr" target="#b22">[23]</ref> which has a lifting network with similar structure to ours. Only one other self-supervised approach attains a lower MPJPE, however, by using additional information. Our analysis revealed that although our pose structure is very accurate (which results in a low PMPJPE) the largest part of the error originates from a slight offset in the rotation. For example, comparing frame 1 from subject 9 of the Human3.6M dataset to itself rotated by only 15 ? around the longitudinal axis already results in an MPJPE of 67.7mm. Iqbal et al. <ref type="bibr" target="#b10">[11]</ref> still set the state of the art in terms of MPJPE. However, they need bone length constraints which they directly compute from the ground truth 3D data of the training set. Our approach does not require any predefined priors on the skeletal structure. Using our static camera constraint (Ours+C) improves the MPJPE significantly. <ref type="figure" target="#fig_2">Fig. 5</ref> shows the CPS for our method compared to Epipo-larPose <ref type="bibr" target="#b13">[14]</ref>, which is the only comparable approach with  <ref type="bibr" target="#b33">[34]</ref> 122.6 98.2 3D interpreter <ref type="bibr" target="#b45">[46]</ref> 98.4 88.6 AIGN <ref type="bibr" target="#b42">[43]</ref> 97.2 79.0 RepNet <ref type="bibr" target="#b43">[44]</ref> 89.9 65.1 HMR <ref type="bibr" target="#b11">[12]</ref> -66.5 Wang <ref type="bibr" target="#b44">[45]</ref> 86.4 62.8 <ref type="bibr">Kolotouros [15]</ref> -62.0 Kundu <ref type="bibr" target="#b15">[16]</ref> 85.8 self</p><p>Chen <ref type="bibr" target="#b1">[2]</ref> -68.0 EpipolarPose <ref type="bibr" target="#b13">[14]</ref> 76.6 67.5 Iqbal <ref type="bibr" target="#b10">[11]</ref> 69.1 55.9 Ours 81.9 53.0 Ours + C 74.3 53.0 publicly available code, and the 3D pose estimation baseline of Martinez et al. <ref type="bibr" target="#b22">[23]</ref>. On this metric, we outperform EpipolarPose by a large margin. Note the high threshold of over 80mm that is required by <ref type="bibr" target="#b13">[14]</ref> to achieve a CP above 1% compared to our threshold slightly below 50mm. As for the CPS metric, we are on par with the fully supervised approach of <ref type="bibr" target="#b22">[23]</ref>. Since their originally trained model is not publicly available anymore we retrained their model with their provided code to report the new CPS metric. The retrained model achieved a PMPJPE of 53.5mm, which is slightly lower compared to their original number. The new model is used only for reporting CPS. <ref type="figure" target="#fig_3">Fig. 6</ref> shows qualitative results for the Human3.6M data set in the first row.</p><p>We also evaluate our approach on the 3DHP dataset <ref type="bibr" target="#b23">[24]</ref> following the standard test protocols and metrics. <ref type="table" target="#tab_1">Table 2</ref> shows the results. We outperform every other selfsupervised approach. In contrast to other approaches the proposed method does not require calibrated cameras 3 or anthropometric constraints. For the CPS metric we achieve a score of 134.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Moving cameras</head><p>Our main motivation is to enable 3D human pose estimation in the wild by using a multi-view camera system with temporally synchronised cameras. Moreover, the performed activity should be very challenging to capture and hard to simulate in a traditional motion capture studio. That means a straight-forward activity domain transfer, e.g. pretraining or combined training with a different dataset, is not reasonable. The SkiPose dataset <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35]</ref> comprises all challenges of this motivation. It features competitive alpine skiers performing giant slalom runs. To record this dataset huge effort was taken to setup and calibrate the cameras  and keep them in place after calibration. Additionally, the cameras are rotating and zooming to keep the alpine skier in the field of view. The proposed method can deal with all these difficulties since it does not require a calibrated or static setup and works with multiple synchronised cameras.</p><p>Since the camera setup is not static we cannot apply the relative rotation constraint here. <ref type="table" target="#tab_2">Table 3</ref> shows our results in comparison to Rhodin et al. <ref type="bibr" target="#b34">[35]</ref>. Since they consider a (sparse-)supervised setting and known camera positions a direct comparison is not possible and only serves as a baseline. <ref type="figure" target="#fig_3">Fig. 6</ref> shows qualitative results for the SkiPose dataset in the second row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>To analyze our approach we perform a number of ablation studies. First, to simulate a practical setting with limited resources, we reduced the number of cameras to train the model. <ref type="table" target="#tab_3">Table. 4</ref> shows the results for the training with only the first two or three cameras. While the performance  expectedly slightly drops due to the lower number of training samples and views our approach still produces good results which underlines its applicability in real world scenarios. In a second experiment we show the impact of using the confidences from the 2D joint estimator as inputs to the network and for the calculation of the reprojection error. They significantly impact the performance of our model and produce a gain of 19.4mm in MPJPE and 11.2mm in PM-PJPE. To prove that the proposed mixing of rotations and poses to achieve view-and camera-consistency is superior to simple equality constraints, we performed experiments with such equality constraints. The results show that indeed our mixing approach is an essential part to make it work. We also trained with ground truth 2D annotations to compute a lower bound for the proposed method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Are We Learning a Canonical Pose Basis?</head><p>Finally, we evaluate the claim that we learn a canonical pose basis. To visualize the disentanglement for different 3D poses <ref type="figure" target="#fig_4">Fig. 7</ref> shows a visualization of reconstructed 3D poses in the canonical basis obtained from 4 views on the left and in the middle. The right image shows 10 randomly picked reconstructions in the canonical space. Although the similarity of the poses is not enforced directly as described in Sec. 3.2 the poses are similarly oriented in the canonical space. In particular, the hip joints are aligned which leads to a similar alignment of the upper body. The standard deviation for the hip joints of the canonical poses from the test set of Human3.6M are 7.9mm and 7.7mm for the right and left hip, respectively. This underlines that pose and rotation are disentangled plausibly by our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present CanonPose, a neural network trained for single image 3D human pose estimation from multi-view data without 2D or 3D annotations. Given a pretrained 2D human pose estimator we exploit multi-view consistency to automatically decompose a 2D observation into a canonical 3D pose and a camera rotation that is used to reproject it back to the observation after mixing. Since our approach does not require either 2D nor 3D annotations for the multi-view data it is practically applicable to many in-thewild scenarios, including outdoor scenes with moving cameras. We not only achieve state-of-the-art results on benchmark datasets with less prerequisites compared to other approaches, but also show promising results on challenging outdoor scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of PMPJPE and our CP-metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of CPS curves for distances from 1mm to 300mm with corresponding AUC for the Human3.6M dataset. A higher value means a better result, i.e. the leftmost curve achieves the best result in terms of CP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results for the Human3.6M dataset (top) and for the challenging SkiPose dataset (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the canonical pose space from the Hu-man3.6M dataset. Left and middle: Canonical poses for the same 3D pose predicted from 4 different views. Right: 10 randomly sampled canonical poses. Our network automatically learns a disentanglement of a 2D pose into 3D and a camera rotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results for the Human3.6M dataset in mm. The bottom section, labeled with self, shows methods that can solve our setting. Best results are marked in bold and second best in italic.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell cols="2">MPJPE? PMPJPE?</cell></row><row><cell>full</cell><cell>Martinez [23]</cell><cell>67.5</cell><cell>52.5</cell></row><row><cell>weak</cell><cell>Rhodin [35]</cell><cell>80.1</cell><cell>65.1</cell></row><row><cell></cell><cell>Rhodin</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results for the 3DHP dataset. The bottom section, labeled with self, shows methods that can solve our setting. Best results are marked in bold and second best in italic. MPJPE and PMPJPE are given in mm, PCK is in %.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell cols="3">MPJPE? PMPJPE? PCK?</cell></row><row><cell>weak</cell><cell>Rhodin [35]</cell><cell>121.8</cell><cell>-</cell><cell>72.7</cell></row><row><cell></cell><cell>HMR [12]</cell><cell>169.5</cell><cell>-</cell><cell>59.6</cell></row><row><cell></cell><cell>Habibie [9]</cell><cell>-</cell><cell>-</cell><cell>70.4</cell></row><row><cell></cell><cell>Kolotouros [15]</cell><cell>124.8</cell><cell>-</cell><cell>66.8</cell></row><row><cell></cell><cell>Li [21]</cell><cell>-</cell><cell>-</cell><cell>74.1</cell></row><row><cell></cell><cell>Kundu [16]</cell><cell>103.8</cell><cell>-</cell><cell>82.1</cell></row><row><cell>self</cell><cell>Chen [2]</cell><cell></cell><cell>71.1</cell><cell></cell></row><row><cell></cell><cell>EpipolarPose [14]</cell><cell>125.7</cell><cell>-</cell><cell>64.7</cell></row><row><cell></cell><cell>Iqbal [11]</cell><cell>110.1</cell><cell>-</cell><cell>76.5</cell></row><row><cell></cell><cell>Ours</cell><cell>119.2</cell><cell>68.7</cell><cell>69.0</cell></row><row><cell></cell><cell>Ours + C</cell><cell>104.0</cell><cell>70.3</cell><cell>77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for the SkiPose dataset. The result for<ref type="bibr" target="#b34">[35]</ref> was estimated from a bar plot in the paper. Since<ref type="bibr" target="#b34">[35]</ref> considers a (sparse-)supervised setting and known camera position it is only shown as a baseline. MPJPE, PMPJPE and CPS are given in mm, PCK is in %.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell cols="4">MPJPE? PMPJPE? PCK? CPS?</cell></row><row><cell>weak</cell><cell>Rhodin [35]</cell><cell>85</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>self</cell><cell>Ours</cell><cell>128.1</cell><cell>89.6</cell><cell cols="2">67.1 108.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies on the Human3.6M dataset. All values are given in mm.</figDesc><table><row><cell></cell><cell cols="2">MPJPE? PMPJPE?</cell><cell>CPS?</cell></row><row><cell>2 cams</cell><cell>82.7</cell><cell>61.2</cell><cell>148.5</cell></row><row><cell>3 cams</cell><cell>82.0</cell><cell>62.2</cell><cell>145.6</cell></row><row><cell>w/o confidences</cell><cell>95.6</cell><cell>65.0</cell><cell>142.5</cell></row><row><cell>ground truth 2D</cell><cell>65.9</cell><cell>51.4</cell><cell>187.1</cell></row><row><cell>direct pose equality</cell><cell>554.3</cell><cell>360.8</cell><cell>0.0</cell></row><row><cell>direct camera equality</cell><cell>617.9</cell><cell>374.5</cell><cell>0.0</cell></row><row><cell>full (4 cams)</cell><cell>81.9</cell><cell>53.0</cell><cell>167.6</cell></row><row><cell>full+C (4 cams)</cell><cell>74.3</cell><cell>53.0</cell><cell>167.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In fact, camera angles change between subjects but not during a capture session with one subject.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The configuration Ours+C only assumes that cameras are static during the sequence, which is a much weaker constraint.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d human pose es-timation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Garnet: Graph attention residual networks based on adversarial learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Graphics</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="276" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops (ECCV)</title>
		<meeting>the European Conference on Computer Vision Workshops (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marker-less 3D human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feilin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6821" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning 3d global human motion estimation from unpaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Habekost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>disjoint datasets. 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="2380" to="7504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">9004</biblScope>
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximummargin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometry-driven selfsupervised method for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzhentao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard Yi Da</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11442" to="11449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">XNect: Real-time multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<editor>Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, and Christian Theobalt</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised 3d human pose representation with viewpoint and pose disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural scene decomposition for multi-person motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d pose estimation from a single image using multi-view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1216" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasearch dedicated to sports injury preventionthe&apos;sequence of prevention&apos;on the example of alpine ski racing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Habilitation with Venia Docendi in Biomechanics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">F</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distill knowledge from nrsfm for weakly supervised 3d pose learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<editor>Andrea Vedaldi</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="465" to="481" />
		</imprint>
	</monogr>
	<note type="report_type">editors</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
