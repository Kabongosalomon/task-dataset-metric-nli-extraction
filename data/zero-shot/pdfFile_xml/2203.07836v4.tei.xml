<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Pre-training for AMR Parsing and Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">??</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Pre-training for AMR Parsing and Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>meaning representation (AMR) highlights the core semantic information of text in a graph structure. Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively. However, PLMs are typically pretrained on textual data, thus are sub-optimal for modeling structural knowledge. To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs. In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training. We further design a unified framework to bridge the gap between pretraining and fine-tuning tasks. Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model. To our knowledge, we are the first to consider pretraining on semantic graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract meaning representation (AMR; <ref type="bibr" target="#b7">Banarescu et al. (2013)</ref>) is a semantic structure formalism. It represents the meaning of a text in a rooted directed graph, where nodes represent basic semantic units such as entities and predicates, and edges represent their semantic relations, respectively. One example is shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, with the corresponding sentence in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Serving as a structural representation, AMR has been shown useful for NLP tasks such as text summarization <ref type="bibr" target="#b30">(Liu et al., 2015;</ref><ref type="bibr" target="#b29">Liao et al., 2018;</ref>, machine translation <ref type="bibr" target="#b43">(Song et al., 2019)</ref>, information extraction <ref type="bibr" target="#b24">(Huang et al., 2016;</ref><ref type="bibr" target="#b53">Zhang and Ji, 2021)</ref> and dialogue systems <ref type="bibr" target="#b4">(Bai et al., 2021)</ref>.</p><p>There are two fundamental NLP tasks concerning AMR, namely AMR parsing <ref type="bibr" target="#b18">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b27">Konstas et al., 2017;</ref><ref type="bibr" target="#b34">Lyu and Titov, 2018;</ref><ref type="bibr" target="#b19">Guo and Lu, 2018;</ref><ref type="bibr" target="#b50">Zhang et al., 2019a;</ref><ref type="bibr" target="#b11">Cai and Lam, 2020;</ref><ref type="bibr" target="#b10">Bevilacqua et al., 2021)</ref> and AMR-totext generation <ref type="bibr" target="#b27">(Konstas et al., 2017;</ref>  2018; <ref type="bibr" target="#b57">Zhu et al., 2019;</ref><ref type="bibr" target="#b54">Zhao et al., 2020;</ref><ref type="bibr" target="#b5">Bai et al., 2020;</ref><ref type="bibr">Ribeiro et al., 2021a)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the former transforms a textual input (e.g., a sentence) into a corresponding AMR structure, and the latter transforms an AMR input into a fluent and grammatical sentence that conveys the same meaning. A common challenge to both tasks is that AMR exists in the form of a graph structure, which is difficult for neural models to learn with limited human-curated data.</p><p>Recently, large-scale pre-trained sequence-tosequence (seq2seq) language models <ref type="bibr" target="#b28">(Lewis et al., 2020;</ref><ref type="bibr" target="#b40">Raffel et al., 2020)</ref> have been shown useful for both tasks above. The basic idea is to linearize AMR structures into a sequence form, so that both AMR parsing and AMR-to-text generation can be solved as standard seq2seq tasks, using a pre-trained language model fine-tuned on taskspecific data. In this way, semantic knowledge learned in self-supervised text-to-text (t2t) pretraining can benefit both text-to-graph (t2g) and graph-to-text (g2t) transformation.</p><p>Intuitively, structural knowledge from AMR can be a useful complement to semantic knowledge from text. A natural question is whether similar self-supervision strategy can be useful for AMR graphs, so that graph-to-graph (g2g) denoise autoencoder training can serve as effective addition to t2t pre-training, before a model is fine-tuned on t2g and g2t tasks. We investigate this problem in this paper. In particular, there are three questions of interest. First, as mentioned before, is g2g pre-training complementary to t2t pre-training? Second, what is the most effective way to combine t2t and g2g training? Third, is silver data useful for AMR self-supervision training, and what is the most effective way of making use of such data?</p><p>Taking BART <ref type="bibr" target="#b28">(Lewis et al., 2020)</ref> as the seqto-seq model, we introduce two strategies for g2g pre-training and propose four tasks to combine t2t and g2g training. To reduce the gap among different pre-training tasks and between pre-training and fine-tuing, we unify all pre-training tasks and fine-tuning tasks in a general framework. Experimental results on standard benchmarks show that: 1) graph pre-training achieves significant improvements over the state-of-the-art systems; 2) silver data are useful for our pre-training framework; 3) our pre-training framework is a better way than finetuning to make use of silver data and; 4) our model is more robust than existing systems in unseen domains. Our final models give the best reported results on both AMR parsing and AMR-to-text generation tasks, with a large margin of improvement over the previous best results. To our knowledge, we are the first to consider graph-to-graph selfsupervised training on semantic graphs. We release code at https://github.com/muyeby/AMRBART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>AMR Parsing. Early AMR parsing systems use statistical methods <ref type="bibr" target="#b18">(Flanigan et al., 2014</ref><ref type="bibr" target="#b17">(Flanigan et al., , 2016</ref><ref type="bibr">Wang et al., 2015a,b)</ref>. With the advance in deep learning, various neural models are developed for AMR parsing. Those models can be categorized into: 1) neural transition-based parsers <ref type="bibr" target="#b6">(Ballesteros and Al-Onaizan, 2017;</ref><ref type="bibr" target="#b55">Zhou et al., 2021)</ref>; 2) sequence-to-graph parsers <ref type="bibr" target="#b50">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b33">Lyu et al., 2020;</ref><ref type="bibr" target="#b11">Cai and Lam, 2020)</ref> and; 3) sequence-to-sequence parsers <ref type="bibr" target="#b27">(Konstas et al., 2017;</ref><ref type="bibr" target="#b38">Peng et al., 2017</ref><ref type="bibr" target="#b37">Peng et al., , 2018</ref><ref type="bibr" target="#b51">Zhang et al., 2019b;</ref><ref type="bibr" target="#b49">Xu et al., 2020;</ref><ref type="bibr" target="#b10">Bevilacqua et al., 2021)</ref>. Recently, pretraining techniques have significantly boosted the performance of AMR parsing. For example, <ref type="bibr" target="#b34">Lyu and Titov (2018)</ref>, <ref type="bibr">Zhang et al. (2019a,b)</ref> and <ref type="bibr" target="#b11">Cai and Lam (2020)</ref> use BERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> for sentence encoding; <ref type="bibr" target="#b10">Bevilacqua et al. (2021)</ref> finetune BART for sequence-to-AMR generation. <ref type="bibr" target="#b49">Xu et al. (2020)</ref> pre-train a model on relevant seq2seq learning tasks (e.g., machine translation <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref>, syntactic parsing <ref type="bibr" target="#b58">(Zhu et al., 2013)</ref>) before fine-tuning on AMR parsing. Similar to those methods, we consider using pre-trained models to improve the model capacity. However, previous studies focus on fine-tuning language models trained on text data for AMR parsing task, in contract, we focus on integrating structural information into the pre-training. In addition, our method does not require information from auxiliary tasks. AMR-to-Text Generation. On a coarse-grained level, we categorize existing AMR-to-text generation approaches into two main classes: Graphto-sequence models that adopt a graph encoder to process an AMR graph and use a sequence decoder for generation <ref type="bibr" target="#b9">(Beck et al., 2018;</ref><ref type="bibr" target="#b14">Damonte and Cohen, 2019;</ref><ref type="bibr" target="#b57">Zhu et al., 2019)</ref>, and sequence-tosequence models that linearize an AMR graph into a sequence and solve it as a seq2seq problem using randomly initialized <ref type="bibr" target="#b27">(Konstas et al., 2017)</ref> or pretrained models <ref type="bibr" target="#b35">(Mager et al., 2020;</ref><ref type="bibr">Ribeiro et al., 2021a;</ref><ref type="bibr" target="#b10">Bevilacqua et al., 2021)</ref>. This work follows a seq2seq manner, but we use an encoder that integrates AMR and text information. The closest to our work, <ref type="bibr" target="#b42">Ribeiro et al. (2021b)</ref> integrate AMR structures into pre-trained T5 <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref> using adapters <ref type="bibr" target="#b21">(Houlsby et al., 2019)</ref> for AMR-totext generation. However, they do not pre-train on AMR graphs, and their method cannot solve both AMR parsing and AMR-to-text generation tasks as they require the full AMR structure as the input. Graph Self-supervised Learning.</p><p>Kipf and Welling (2016) introduce a variational graph autoencoder to allow self-supervised learning on graph data. <ref type="bibr">Hu et al. (2020a,b)</ref> propose local and global learning strategies to pre-train a graph neural network on large-scale protein ego-networks, academic graphs and recommendation data. <ref type="bibr" target="#b32">Lu et al. (2021)</ref> enhance the graph learning strategies of <ref type="bibr" target="#b23">Hu et al. (2020b)</ref> with dual adaptations. While existing work considers graph neural networks, we pre-train a seq2seq model on AMR graphs. In addition, we jointly pre-train on graphs and text for graph-text correlation modeling. In contrast, existing work pre-trains models on graphs and in isolation with text pre-training. To our knowledge, we are the first to consider AMR as a graph pre-training target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We take BART <ref type="bibr" target="#b28">(Lewis et al., 2020)</ref> as the basic seq2seq model (Section 3.1), and introduce graph pre-training strategies (Section 3.2) and an unified pre-training framework (Section 3.3) for both AMR parsing and AMR-to-text generation.  <ref type="figure">Figure 2</ref>: Illustration of two graph pre-training strategies: 1) node/edge level denoising (a? b); 2) sub-graph level denoising (c? b). Two transformations can be composed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BART</head><p>BART <ref type="bibr" target="#b28">(Lewis et al., 2020</ref>) is a pre-trained denoising auto-encoder, which is implemented as a seq2seq model based on standard Transformer <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref>  Text is divided into segments and then shuffled; 5) Document Rotation. A document is rotated to start with a random token. In the fine-tuning, BART takes a complete text as input and maps it into a task-specific output sequence. We linearize an AMR graph into a sequence, so that both AMR parsing and AMR-to-text generation can be performed using a seq2seq model. In addition, it allows pre-training on AMR structures using BART. Following <ref type="bibr" target="#b27">Konstas et al. (2017)</ref>, we adopt the depth-first search (DFS) algorithm, which is closely related to the linearized natural language syntactic trees <ref type="bibr" target="#b10">(Bevilacqua et al., 2021)</ref>. For instance, the AMR graph in <ref type="figure" target="#fig_0">Figure 1</ref> is linearized into: ( &lt;Z0&gt; possible :domain ( &lt;Z1&gt; go :arg0 ( &lt;Z2&gt; boy ) ) :polarity ( &lt;Z3&gt; negative ) ) , where &lt;Z0&gt;, &lt;Z1&gt; and &lt;Z2&gt; are special tokens to handle co-referring nodes. To deal with such AMR symbols, we follow previous work <ref type="bibr" target="#b10">(Bevilacqua et al., 2021)</ref> and expand the vocabulary by adding all relations and frames. In addition, to distinguish between texts and AMR graphs, we add two special tokens, &lt;g&gt; and &lt;/g&gt;, to mark the beginning and end of AMR graphs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training on AMR graphs</head><p>We introduce two self-supervised training strategies to further pre-train a BART model on AMR graphs. As shown in <ref type="figure">Figure 2</ref>(a), the node/edge level denoising strategy encourages the model to capture local knowledge about nodes and edges. The graph level denoising strategy <ref type="figure">(Figure 2</ref>(c)) enforces the model to predict a sub-graph, thus facilitating the graph-level learning. 1) Node/edge level denoising. We apply a noise function on AMR nodes/edges to construct a noisy input graph. In particular, the noise function is implemented by masking 15% nodes and 15% edges in each graph. As shown in <ref type="figure">Figure 2</ref>(a), the node [go-01] and edge [:arg0] are replaced with two [mask] tokens.</p><p>2) Sub-graph level denoising. This task aims to recover the complete graph when given part of the graph. We randomly remove a sub-graph 1 from the graph and replace it with a [mask] token (cf. <ref type="figure">Figure 2</ref>(c)). The masking probability is 0.35.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unified Pre-training Framework</head><p>The above standard pre-training and fine-tuning strategies are shown in Table 1(a), by using &lt;s&gt; and &lt;g&gt; for differentiating text and graphic information, respectively. However, the model does not fully learn the interaction between textual and AMR information during pre-training. To further address this issue, we consider a unified pretraining framework, which combines text and AMR sequences as input to the denoise auto-encoder. In this way, dynamic masking can be carried out on the text, AMR or both ends, so that the model can learn to make use of one source of information for inferring the other. This can benefit both a parser and a generation model by enforcing the learning of correspondence between text and AMR structures.</p><p>In addition, as shown in <ref type="table" target="#tab_2">Table 1</ref>, there is a gap between standard pre-training and fine-tuning for AMR from/to text transduction. Specifically, the input and output formats are same in the pre-training (i.e.,t2t and?2g) but different in the fine-tuning  (i.e., t2g and g2t). This gap restrains models to make the best use of pre-trained knowledge in the fine-tuning phase. The unified pre-training framework can also benefit task-specific fine-tuning by eliminating the difference of input and output formats between pre-training and fine-tuning. Formally, denoting the text and linearized graph sequence as t and g, where t = {x 1 , x 2 , ..., x n } and g = {g 1 , g 2 , ..., g n }.t and? represent the noisy text and graph, respectively, and t and g refer to the empty text and graph, respectively. As shown in <ref type="table" target="#tab_2">Table 1</ref>(b), we unify the input format for both pre-training and fine-tuning to tg. For consistency, all input sequences start with a text sequence and end with a graph sequence. Joint Text and Graph Pre-training. We introduce 4 auxiliary pre-training tasks to encourage information exchanging between graphs and text. As shown in Table 1(b), the auxiliary tasks are: 1) Graph augmented text denoising (tg2t), where an AMR graph is taken as additional input to help masked text reconstruction;</p><p>2) Text augmented graph denoising (t?2g), where text helps masked graph reconstruction;</p><p>3) Noisy graph augmented text denoising (t?2t), where the target text is generated based on a pair of masked text and masked graph; 4) Noisy text augmented graph denoising (t?2g), where a target graph is generated based on a pair of masked text and masked graph. Dynamic masking rate. Different from standard masking <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> that uses a static masking rate, we adopt a dynamic masking rate p for tasktg2t and t?2g. Formally, at step t, we calculate the masking probability p as:</p><formula xml:id="formula_0">p = 0.1 + 0.75 * t/T,<label>(1)</label></formula><p>where 0.1 is the initial masking rate, T denotes the total training step. p increases as t grows, as t approaches to T , the pre-training taskstg2t and t?2g are closer to fine-tuning tasks.</p><p>Unified Pre-training and Fine-tuning. In our unified framework, fine-tuning tasks can be viewed as having an empty text/graph in the original input, resulting in an input format of tg2t for AMR-totext generation and tg2g for AMR parsing. In this way, pre-training and fine-tuning tasks share the same input format, thus facilitating knowledge transfer from pre-training to fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>To pre-train our model, we optimize the total loss (L total ) which is calculated as:</p><formula xml:id="formula_1">Lt 2t = ? log P (t|t, g), L? 2g = ? log P (g|t,?), Lt g2t = ? log P (t|t, g), L t?2g = ? log P (g|t,?), Lt? 2t = ? log P (t|t,?), Lt? 2g = ? log P (g|t,?), L total = Lt 2t + L? 2g + Lt g2t + L t?2g + Lt? 2t + Lt? 2g ,<label>(2)</label></formula><p>where Lt 2t and L? 2g are standard pre-training loss on text (Section 3.1) and graph (Section 3.2), respectively. Lt g2t , L t?2g , Lt? 2t and Lt? 2g denote joint pre-training losses (Section 3.3), respectively. For fine-tuning, the training objectives are:</p><formula xml:id="formula_2">L amr2text = ? log P (t|t, g), L text2amr = ? log P (g|t, g),<label>(3)</label></formula><p>where L amr2text and L text2amr are training loss of AMR-to-text generation and AMR parsing, respectively.</p><p>Datasets AMR2.0 AMR3.0 New3 TLP <ref type="table" target="#tab_2">Bio   Train  36521  55635  ---Valid  1368  1722  ---Test  1371  1898  527  1562 500   Table 2</ref>: Benchmark AMR datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of our model on five benchmarks and compare the results with state-ofthe-art models on AMR parsing and AMR-to-text generation, respectively. In addition to standard supervised training settings, we evaluate the robustness of our model in a zero-shot domain adaptation setting. <ref type="table">Table 2</ref> shows the statistics of datasets. Following <ref type="bibr" target="#b10">Bevilacqua et al. (2021)</ref>, we use the AMR2.0 (LDC2017T10) and AMR3.0 (LDC2020T02). We also evaluate the model performance on New3, The Little Prince (TLP) and Bio AMR (Bio) corpora. For pre-training, we additionally use 200k silver data parsed by SPRING <ref type="bibr" target="#b10">(Bevilacqua et al., 2021)</ref>. These data are randomly selected from Gigaword (LDC2011T07) corpus, which shares the same textual source with AMR data. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>We follow <ref type="bibr" target="#b10">Bevilacqua et al. (2021)</ref> in preprocessing and post-processing AMR graphs, except for omitting the recategorization step which does not consistently improve model performance in our preliminary experiments. Our model is built based on a vanilla BART 3 . The best model and hyper-parameters are selected by performance on the validation set. The detailed hyper-parameters are given in Appendix A.</p><p>Metrics. Following <ref type="bibr" target="#b10">Bevilacqua et al. (2021)</ref>, we evaluate on the AMR parsing benchmarks by using Smatch  and other finegrained metrics. 4 Regarding AMR-to-text, we use three common Natural Language Generation measures, including BLEU <ref type="bibr" target="#b36">(Papineni et al., 2002)</ref>, CHRF++ <ref type="bibr" target="#b39">(Popovi?, 2017)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Models</head><p>For AMR parsing, we consider following systems for comparison: 1) Lyu and Titov <ref type="formula">(</ref>    <ref type="table" target="#tab_4">Table 3</ref> shows results on the validation set of AMR2.0 under different model settings, where we take a fine-tuned BART-based model <ref type="bibr" target="#b10">(Bevilacqua et al., 2021)</ref> as our baseline. We first study the effectiveness of pre-training only on text and graphs. As shown in <ref type="table" target="#tab_4">Table 3</ref>, both pre-training on the text (tg2t) and graph (t?2g) leads to better results, and combining them can give better results on both tasks. Also, adding joint pre-training tasks improves the performance. In particular, t?2g gives a Smatch improvement of 0.7 for AMR paring, andtg2t reaches a BLEU of 45.3 for AMR-to-text generation, which is 2.8 points higher than baseline. Addingt?2g gives a Smatch of 83.2 for AMR parsing, andt?2t improves the baseline by 1.7 BLEU points for generation. By combining t?2g andtg2t, the performance increase by 0.6 and 2.5 points on AMR parsing and AMR-to-text generation, respectively. Similar trend can be observed by combiningt?2g andt?2t. Finally, using all 6 pre-training tasks, our model reach a result of 83.6 Smatch and 45.6 BLEU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Development Experiments</head><p>We also study the impact of two graph selfsupervised training strategies. In particular, we evaluate the performance after removing the node/edge or the sub-graph masking task independently. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the performance decreases on both AMR parsing and AMR-to-text generation tasks without the node/edge level masking strategy. The performance drop is larger when removing the sub-graph masking task, with a margin of 0.5 Smatch and 0.9 BLEU, respectively. <ref type="figure" target="#fig_3">Figure 3</ref>(a) compares the performance of standard pre-training (t2t,?2g) and fine-tuning (t2g, g2t) with our unified framework. The unified framework gives better results than standard versions on both tasks. This confirms our assumption that our unified framework is helpful for reducing the gap between pre-training and fine-tuning. Besides, we find that by unifying pre-training and finetuning formats, our model converges faster than the baseline during fine-tuning (cf. Appendix C.1). Figure 3(b) shows the model performance regarding different scales of silver data. Even without silver data, the performance of our model is better than the baseline, indicating that graph pretraining is beneficial for downstream tasks when using various auxiliary tasks. When silver data are available, the performance of both AMR parsing and AMR-to-text generation tasks increases as the scale of silver data increases, with a margin of 2 BLEU score. We also fine-tune a BART model on silver data under our unified framework (i.e., tg2t and tg2g), and find that our dual graph and text denoising tasks are more useful (cf. Appendix C.2 for more analysis and discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Main Results</head><p>AMR parsing. <ref type="table" target="#tab_8">Table 5</ref> lists the result of different models on AMR2.0 and AMR3.0. Among previous works, Bevilacqua+ (2021, large) achieves the best results, consistently outperforming other systems. Compared with the system of <ref type="bibr" target="#b10">Bevilacqua et al. (2021)</ref>, our model obtains significantly (p&lt;0.01) better Smatch scores in both base and large settings on both datasets. In particular, our base model outperforms the Bevilacqua+ (2021, base) by 0.9 Smatch point on AMR2.0, and our large model obtains a Smatch of 85.4 and 84.2 on AMR2.0 and AMR3.0, respectively. To our knowledge, these are the best-reported results, showing the effectiveness of our method.</p><p>Besides, Bevilacqua+ (2021, large) s uses silver data for fine-tuning, yet does not lead to consistent improvement over Bevilacqua+ <ref type="bibr">(2021, large)</ref>. In contrast, our large model gives 1.1 and 1.2 higher Smatch than Bevilacqua+ (2021, large) s on AMR2.0 and AMR3.0, respectively. This indicates that our pre-training framework is a better way than fine-tuning to make use of silver data.  reason is that our models are pre-trained using a denoising auto-encoding manner, which is less sensitive to silver (or noisy) data than fine-tuning. We also find that further fine-tuning our models on silver data (same with pre-training) cannot bring improvement (cf. Appendix C.3). AMR-to-text generation. We report the results of different systems on AMR2.0 and AMR3.0 in <ref type="table" target="#tab_10">Table 6</ref>, respectively. With the help of BART, Ribeiro+ (2021) and Bevilacqua+ (2021, large) obtain significantly better results than previous graphto-sequence and GPT-based models. Compared with Bevilacqua+ (2021), our models (base and large) give significantly (p&lt;0.001) better results in terms of all evaluation metrics. In particular, our base model achieves comparable or better performance than Bevilacqua+ (2021, large). Compared with Bevilacqua+ (2021, large) s , our large model improves the performance by 3.9 and 2.7 points on AMR2.0 and AMR3.0, respectively. Similar with AMR parsing, we observe that when fine-tuning our models on silver data cannot bring improvement for AMR-to-text generation task <ref type="table" target="#tab_10">(Table 6</ref> and Appendix C.3).</p><p>Out-of-distribution evaluation We use the model trained on AMR2.0 to get predictions on out-ofdomain test sets. <ref type="table">Table 7</ref> shows the results on AMR parsing and AMR-to-text generation tasks. Similar to in-domain experiments, our models achieve better results than existing methods. In particular, our base model can give comparable performance than Bevilacqua+ (2021, large), and our large model obtains the best-reported results. This indicates that   <ref type="table">Table 7</ref>: Out of distribution performance on AMR parsing (Smatch) and AMR-to-text (BLEU).</p><p>New3 (both tasks) and TLP (only AMR-to-text generation). In contrast, our model gives consistent improvements on all 3 domains. This can be because fine-tuning leads to catastrophic forgetting of distributional knowledge <ref type="bibr" target="#b26">(Kirkpatrick et al., 2017)</ref>. <ref type="table" target="#tab_12">Table 8</ref> shows the effects of the graph size, graph diameter and reentrancies on the performance. We split the test set of AMR2.0 into different groups and report the performance improvement over the baseline model <ref type="bibr" target="#b10">(Bevilacqua et al., 2021)</ref>. All models are trained on AMR2.0. We first consider graph size, which records the number of nodes in an AMR graph. Our model consistently outperforms the baseline model on both tasks, with the performance gap growing on larger graphs. This indicates that our system is more powerful in dealing with larger graphs. The main reason is that our joint text and graph pre-training mechanism enhances the model with the ability to capture word or span level correlation between text and graph, which is helpful for dealing with long sequence and large graphs. The graph depth is defined as the longest distance between the AMR node and root node. A graph with deeper depth has more long-range dependencies. For AMR parsing, our model gives a better Smatch than the baseline model on the first two groups of graphs, and a comparable score on graphs with a depth bigger than 6. For AMR-to-text generation, our model consistently improves over the baseline model on all graphs, and the improvements are bigger on deeper graphs. This shows that our model is better for learning more complex graphs. It can be that our graph masking strategies train the model to learn the relationships between a sub-graph and the remaining graph context, making it easier to understand deep graphs.  Reentrancy is the number of nodes that has multiple parents. Reentrancies pose difficulties to both AMR parsing and AMR-to-text tasks <ref type="bibr" target="#b14">(Damonte and Cohen, 2019;</ref><ref type="bibr" target="#b45">Szubert et al., 2020)</ref>. The more reentrancies, the harder the graph is to be understood. Our method gives significantly (p&lt;0.01) better results on both tasks when the input graphs have less than 4 reentrancies. For graphs with more than 3 reentrancies, the proposed model is 0.4 better on AMR-to-text generation task and comparable than the baseline model on AMR parsing task. This means that our system has an overall better ability on learning reentrancies. <ref type="table">Table 9</ref> presents two cases of AMR parsing, with the model outputs generated by our model and the baseline model, and the gold output given the same input sentence. As shown in the first case, the baseline model omits the semantic unit "hard", thus generates an incomplete AMR graph of a different meaning compared with the input sentence. In contrast, our system preserves the concept "hard" and transfers the semantic relations correctly, thanks to the modeling of correspondence between text and graph during pre-training. In the second case, the baseline output includes a cyclic sub-graph (i.e., ( z1 harm-01 :ARG1 z1 )), which is contrary to the grammar that AMRs should be acyclic. Our system gives a valid AMR graph which is semantically similar with gold graph. <ref type="table" target="#tab_2">Table 10</ref> lists two AMR graphs and model outputs of our AMR-to-text model and the baseline model. In the first case, although the baseline generates a fluent sentence, it ignores the concept "havepurpose-91", resulting in that the generated sentence is of a different meaning compared with the input graph. In the second AMR graph, "before" modifies the phrase "won many championships". However, in the baseline output, "before" is used to <ref type="table">Table 9</ref>: Two AMR parsing cases. Given a text input, we present the gold AMR graph and two model outputs, parsed by the baseline and our model, respectively. modify the phrase "participating in international competitions". Compared with the baseline, our AMR#1: (h / have-purpose-91 :ARG1 (t / thing :ARG1-of (e / expend-01 :ARG2 (t2 / transport-01))) :ARG2 (a / amr-unknown)) Gold: What is the purpose of transportation-related expenditures? Baseline: What are the transportation expenses? Ours: What is the purpose of transportation expenses?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMR#2:</head><p>(w / win-01 :ARG0 (p2 / person :wiki -:name (n / name :op1 "Fengzhu" :op2 "Xu")) :ARG1 (c / championship-02 :ARG0 p2 :quant (m / many)) :time (b / before) :part-of (c2 / compete-01 :mod (i / international)))</p><p>Gold: Fengzhu Xu has won many championships in international competitions before.</p><p>Baseline: Fengzhu Xu won many championships before participating in international competitions.</p><p>Ours: Fengzhu Xu has won many championships in international competitions before. <ref type="table" target="#tab_2">Table 10</ref>: Two AMR-to-text generation cases. Given an AMR graph, we present the gold text and two generated outputs, given by baseline and our model, respectively. system recovers all concepts and maps the modification relationship from the AMR graph to text correctly. This indicates that our model generates more faithful sentences than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We investigated graph pre-training as a complement to text pre-training for AMR parsing and AMR-totext generation tasks, using a novel unified framework with dual graph and text denoising. We find that graph pre-training is highly effective for both AMR parsing and AMR -to-text generation, and is a more effective way of making use of silver data compared with fine-tuning. Our methods give the best results on multiple benchmarks for both tasks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Hyper-Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Fine-grained Evaluation Metric for AMR Parsing</head><p>The Smatch score  measures the degree of overlap between the gold and the prediction AMR graphs. It can be further broken into different sub-metrics, including:</p><p>? Unlabeled (Unlab.): Smatch score after re-   <ref type="figure">Figure 4</ref>: The learning curve of standard framework (std) and our unified framework (ours) on AMR-to-text generation task.</p><p>moving edge-labels ? NoWSD: Smatch score after ignoring Propbank senses (e.g., go-01 vs go-02)</p><p>? Concepts (Con.): F -score on the concept identification task</p><p>? Wikification (Wiki.): F -score on the wikification (:wiki roles)</p><p>? Named Entity Recognition (NER): F -score on the named entities (:name roles).</p><p>? Reentrancy (Reen.): Smatch score on reentrant edges.</p><p>? Negation (Neg.): F -score on the negation detection (:polarity roles).</p><p>? Semantic Role Labeling (SRL): Smatch score computed on :ARG-i roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Experimental Results</head><p>C.1 Effect of Unified Framework <ref type="figure">Figure 4</ref> compares the learning curve between our unified framework and standard framework (finetuning from vanilla BART, i.e., Bevilacqua+) on  <ref type="table" target="#tab_2">Table 13</ref>: Model performance on AMR2.0 and 3.0 datasets for AMR parsing and AMR-to-text. For AMR parsing, we report Smatch score here, and for AMR-totext, we report BLEU-4 score here. +silver denotes to that further fine-tuning the model on silver data.</p><p>AMR2.0 validation set 5 . It can be observed that our system has a initial BLEU score of 36.0, which is significantly (p&lt; 0.001) better than the baseline. This confirm that our unified framework can reduce the gap between pre-training and fine-tuning. In addition, the training curve of the proposed model converges faster while the BLEU score is better than the baseline. This indicates that our model has a larger capacity than baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Impact of denoising Tasks</head><p>To distinguish the contribution of de-nosing tasks and silver data, an ablation study is present where we 1) "fine-tune" a vanilla BART on silver data following our unified framework (i.e., tg2t and tg2g); 2) continue pre-train a BART on silver data according to proposed de-nosing tasks (in <ref type="table" target="#tab_2">Table 1</ref>). As shown in <ref type="table" target="#tab_2">Table 12</ref>, we observe that using sliver data for fine-tuning leads to a 0.1 Smatch decrease in AMR parsing and 2.4 BLEU increase in AMRto-text. This observation is consistent with previous works <ref type="bibr" target="#b27">(Konstas et al., 2017;</ref><ref type="bibr" target="#b10">Bevilacqua et al., 2021)</ref>. In addition, using silver data for pre-training gives further improvements on both tasks, with 1.0 Smatch for AMR pasring and 0.7 BLEU for AMR-to-text generation. This indicates that our de-nosing tasks can help model to better understand silver data.</p><p>C.3 Are Silver Data Still Helpful for Finetuning after Being Used for Pre-training?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of AMR tasks: (a) an AMR graph; (b) a corresponding sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2018;LyuT), a neural parser trained by jointly modeling alignments, concepts and relations; 2)<ref type="bibr" target="#b51">Zhang et al. (2019b;</ref>, a seq2seq approach that incrementally builds up an AMR via predicting semantic relations; 3), an alignerfree parser enhanced by explicit dependency and latent structures; 4) Cai and Lam (2020a; CaiL), a graph-based parser that enhances incremental sequence-to-graph model with a graph-sequence iterative inference mechanism; 5)<ref type="bibr" target="#b10">Bevilacqua et al. (2021;</ref><ref type="bibr" target="#b10">Bevilacqua+)</ref>, a fine-tuned BART model that predicts a linearized AMR graph.For AMR-to-text generation, the compared models are: 1)<ref type="bibr" target="#b57">Zhu et al. (2019;</ref>, a Transformer-based model that enhances selfattention with graph relations; 2), a graph-to-sequence model which uses a dynamic graph convolutional networks for better graph modeling. 3)<ref type="bibr" target="#b5">Bai et al. (2020;</ref>, a graph encoder<ref type="bibr" target="#b57">(Zhu et al., 2019)</ref> with a structural decoder that jointly predicts the target text and the input structure; 4)<ref type="bibr" target="#b35">Mager et al. (2020;</ref><ref type="bibr" target="#b35">Mager+)</ref>, a fine-tuned GPT that predicts text based on a PEN-MAN linearized AMR graph; 5)<ref type="bibr" target="#b10">Bevilacqua et al. (2021;</ref><ref type="bibr" target="#b10">Bevilacqua+)</ref>, a fine-tuned BART that predicts text based on a DFS linearized AMR graph; 6) Ribeiro et al.(2021; Ribeiro+), a fine-tuned BART based on a PENMAN linearized AMR graph. For a fair comparison, we leave out models based on T5(Ribeiro et al., 2021a,b), which has about two times more parameters than BART.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Development results: (a) comparison of standard pre-training and fine-tuning phase (std) and our unified frameworks; (b) impact of silver data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Std. P.T.t 2t &lt;s&gt; x1, ..[mask].., xn &lt;/s&gt; &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; g2g &lt;g&gt; g1, ..[mask].., gm &lt;/g&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; Std. F.T. g2t &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; t2g &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; g2t &lt;s&gt; x1, ..[mask].., xn &lt;/s&gt; &lt;g&gt; [mask] &lt;/g&gt; &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; t?2g &lt;s&gt; [mask] &lt;/s&gt;&lt;g&gt; g1, ..[mask].., gm &lt;/g&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; tg2t &lt;s&gt; x1, ..[mask].., xn &lt;/s&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; t?2g &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; &lt;g&gt; g1, ..[mask].., gm &lt;/g&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; t?2t &lt;s&gt; x1, ..[mask].., xn &lt;/s&gt; &lt;g&gt; g1, ..[mask].., gm &lt;/g&gt; &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; t?2g &lt;s&gt; x1, ..[mask].., xn &lt;/s&gt; &lt;g&gt; g1, ..[mask].., gm &lt;/g&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; Unified F.T. tg2t &lt;s&gt; [mask] &lt;/s&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt; &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; tg2g &lt;s&gt; x1, x2, ..., xn &lt;/s&gt; &lt;g&gt; [mask] &lt;/g&gt; &lt;g&gt; g1, g2, ..., gm &lt;/g&gt;</figDesc><table><row><cell>Phase</cell><cell>Task</cell><cell>Input</cell><cell>Output</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Unified P.T.t</cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Different pre-training and fine-tuning strategies. P.T. = pre-training, F.T. = fine-tuning. t/g denotes the original text/graph.t/? represents a noisy text/graph. t/g means an empty text/graph.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: AMP parsing (Smatch) and AMR-to-text gen-</cell></row><row><cell>eration (BLEU) performance on valid set of AMR2.0.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Impact of two masking strategies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The main Model Smatch Unlab. NoWSD Con. Wiki. NER Reent. Neg. SRL</figDesc><table><row><cell>AMR2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LyuT (2018)</cell><cell>74.4</cell><cell>77.1</cell><cell>75.5</cell><cell>85.9</cell><cell>75.7</cell><cell>86.0</cell><cell>52.3</cell><cell>58.4</cell><cell>69.8</cell></row><row><cell>Zhang+ (2019b)  ?</cell><cell>77.0</cell><cell>80.0</cell><cell>78.0</cell><cell>86.0</cell><cell>86.0</cell><cell>79.0</cell><cell>61.0</cell><cell>77.0</cell><cell>71.0</cell></row><row><cell>Zhou+ (2020)  ?</cell><cell>77.5</cell><cell>80.4</cell><cell>78.2</cell><cell>85.9</cell><cell>86.5</cell><cell>78.8</cell><cell>61.1</cell><cell>76.1</cell><cell>71.0</cell></row><row><cell>CaiL (2020a)  ?</cell><cell>80.2</cell><cell>82.8</cell><cell>80.0</cell><cell>88.1</cell><cell>86.3</cell><cell>81.1</cell><cell>64.6</cell><cell>78.9</cell><cell>74.2</cell></row><row><cell>Xu+ (2020)  ?</cell><cell>80.2</cell><cell>83.7</cell><cell>80.8</cell><cell>87.4</cell><cell>75.1</cell><cell>85.4</cell><cell>66.5</cell><cell>71.5</cell><cell>78.9</cell></row><row><cell>Bevilacqua+ (2021, base)  ?</cell><cell>82.7</cell><cell>85.1</cell><cell>83.3</cell><cell>89.7</cell><cell>82.2</cell><cell>90.0</cell><cell>70.8</cell><cell>72.0</cell><cell>79.1</cell></row><row><cell>Bevilacqua+ (2021, large)  ?</cell><cell>84.5</cell><cell>86.7</cell><cell>84.9</cell><cell>89.6</cell><cell>87.3</cell><cell>83.7</cell><cell>72.3</cell><cell>79.9</cell><cell>79.7</cell></row><row><cell>Bevilacqua+ (2021, large)  ?s</cell><cell>84.3</cell><cell>86.7</cell><cell>84.8</cell><cell>90.8</cell><cell>83.1</cell><cell>90.5</cell><cell>72.4</cell><cell>73.6</cell><cell>80.5</cell></row><row><cell>Ours (base)  ?</cell><cell>83.6</cell><cell>86.7</cell><cell>84.0</cell><cell>90.2</cell><cell>78.6</cell><cell>90.0</cell><cell>71.3</cell><cell>73.7</cell><cell>79.5</cell></row><row><cell>Ours (large)  ?</cell><cell>85.4</cell><cell>88.3</cell><cell>85.8</cell><cell>91.2</cell><cell>81.4</cell><cell>91.5</cell><cell>73.5</cell><cell>74.0</cell><cell>81.5</cell></row><row><cell>AMR3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bevilacqua+ (2021, large)  ?</cell><cell>83.0</cell><cell>85.4</cell><cell>83.5</cell><cell>89.8</cell><cell>82.7</cell><cell>87.2</cell><cell>70.4</cell><cell>73.0</cell><cell>78.9</cell></row><row><cell>Bevilacqua+ (2021, large)  ?s</cell><cell>83.0</cell><cell>85.4</cell><cell>83.5</cell><cell>89.5</cell><cell>81.2</cell><cell>87.1</cell><cell>71.3</cell><cell>71.7</cell><cell>79.1</cell></row><row><cell>Ours (base)  ?</cell><cell>82.5</cell><cell>85.7</cell><cell>82.9</cell><cell>89.4</cell><cell>76.1</cell><cell>86.8</cell><cell>69.9</cell><cell>70.3</cell><cell>78.2</cell></row><row><cell>Ours (large)  ?</cell><cell>84.2</cell><cell>87.1</cell><cell>84.6</cell><cell>90.2</cell><cell>78.9</cell><cell>88.5</cell><cell>72.4</cell><cell>72.1</cell><cell>80.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>AMR parsing results on AMR2.0 and AMR3.0. s means the model uses 200k silver data for fine-tuning.</figDesc><table /><note>? means the model is based on pre-trained models. The best result within each row block is shown in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>AMR-to-text results on AMR2.0 and AMR3.0. CH.=CHRF++. MET.=METEOR. s means the model uses 200k silver data for fine-tuning. Models marked with ? are based on PLMs. The best result within each row block is shown in bold. ? For fair comparison, we report results of tokenized output of Ribeiro+ (2021).</figDesc><table><row><cell>our model is more robust to new domains, thanks to</cell></row><row><cell>joint graph and text pre-training. Regarding differ-</cell></row><row><cell>ent domains, our method achieves bigger improve-</cell></row><row><cell>ments on New3 than the other two domains. This</cell></row><row><cell>is intuitive, as pre-training strengthens the model</cell></row><row><cell>representation power on the domain of graph pre-</cell></row><row><cell>training data, and New3 is closer to it than other</cell></row><row><cell>two datasets.</cell></row><row><cell>In addition, Bevilacqua+ (2021, large) s gives</cell></row><row><cell>lower results than Bevilacqua+ (2021, large) in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Performance improvements on AMR parsing</cell></row><row><cell>(Smatch) and AMR-to-text (BLEU).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Hyper-parameters of our models on Pretraining and Fine-tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11</head><label>11</label><figDesc>lists all model hyper-parameters used for our experiments. We implement our model based on Pytorch and Huggingface Transformers. The pre-processed data, source code and pretrained models are released at https://github.</figDesc><table /><note>com/muyeby/AMRBART.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Ablation study on silver data and denoising tasks.</figDesc><table><row><cell>9DOLG%/(8</cell></row><row><cell>2XUV</cell></row><row><cell>6WG</cell></row><row><cell>7UDLQLQJ(SRFK</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We define a sub-graph has at least one edge and one node.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yue Zhang is the corresponding author. We would like to thank anonymous reviewers for their insightful comments. This work is supported by the National Natural Science Foundation of China under grant No.61976180 and the Tencent AI Lab Rhino-Bird Focused Research Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As discussed in Section 4.5, we find that graph pre-training is a better way to make use of silver data compared with fine-tuning. We further investigate whether fine-tuning our model on silver data can still bring improvement. As shown in Ta- <ref type="bibr">5</ref> We use the same learning rate and optimizer. ble 13, our models achieve the best performance on all tasks and datasets, indicating that further fine-tuning our models on silver data decreases the performance. This can be that silver data are already presented in the pre-training phase and thus further fine-tuning can bring no improvement. In addition, fine-tuning can be more sensitive to data quality than pre-training. When training data contain noise (silver data), fine-tuning on such data can in turn damage the model performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>g / get-03 :ARG1 (a / and :op1 (k / keep-02 :ARG1 (s / strong-02)) :op2 (k2 / keep-02 :ARG1 (c / carry-on-02 :ARG1</idno>
		<title level="m">Text#1: It&apos;s getting hard to keep strong and keep carrying on with life</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno>Gold: (c / contrast-01 :ARG1 (a / addictive-02 :ARG0 (h / harm-01 :ARG1</idno>
		<title level="m">Text#2: Self harming is addictive, but you can overcome it</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<idno>ARG2 (p / possible-01 :ARG1 (o / overcome-01 :ARG0</idno>
		<title level="m">ARG1 h)))</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic representation for dialogue modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.342</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4430" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online back-parsing for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.92</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1206" to="1219" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AMR parsing using stack-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1130</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1269" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One spring to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12564" to="12573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AMR parsing via graphsequence iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DialogSum: A real-life scenario dialogue summarization dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.449</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5062" to="5074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structural neural encoders for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1366</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3649" to="3658" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transition-based parsing with stacktransformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.89</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1001" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1202" to="1206" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1134</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Better transitionbased AMR parsing with a refined search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1198</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GPT-GNN: generative pre-training of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3394486.3403237</idno>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Liberal event extraction and event schema induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="258" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1611835114</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Overcoming catastrophic forgetting in neural networks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1178" to="1190" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An AMR aligner tuned by transition-based parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2422" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to pre-train graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="4276" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A differentiable relaxation of graph segmentation and alignment for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno>abs/2010.12676</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">AMR parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GPT-too: A language-model-first approach for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arafat</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Sultan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1846" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models for cache transition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1842" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">chrF++: words helping character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4770</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hinrich Sch?tze, and Iryna Gurevych. 2021a. Investigating pretrained language models for graph-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 3rd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<biblScope unit="page" from="211" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structural adapters in pretrained language models for AMR-to-Text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.351</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4269" to="4282" />
		</imprint>
	</monogr>
	<note>Online and Punta Cana, Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic neural machine translation using AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00252</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMRto-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The role of reentrancies in Abstract Meaning Representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ida</forename><surname>Szubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.199</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2198" to="2207" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Boosting transition-based AMR parsing with refined actions and auxiliary analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1040</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving AMR parsing with sequence-to-sequence pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.196</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2501" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AMR parsing as sequence-tograph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Broad-coverage semantic parsing as transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3786" to="3798" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lightweight, dynamic graph convolutional networks for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.169</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2162" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation guided graph encoding and decoding for joint information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="39" to="49" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Line graph enhanced AMR-to-text generation with mix-order graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.67</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">AMR parsing with action-pointer transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.443</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5585" to="5598" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">AMR parsing with latent structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.397</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4306" to="4319" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modeling graph structure in transformer for better AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1548</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5459" to="5468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
