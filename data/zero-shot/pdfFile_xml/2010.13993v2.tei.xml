<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COMBINING LABEL PROPAGATION AND SIMPLE MOD- ELS OUT-PERFORMS GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook ?</orgName>
								<orgName type="department" key="dep2">Facebook AI ?</orgName>
								<orgName type="institution">Cornell University ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook ?</orgName>
								<orgName type="department" key="dep2">Facebook AI ?</orgName>
								<orgName type="institution">Cornell University ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook ?</orgName>
								<orgName type="department" key="dep2">Facebook AI ?</orgName>
								<orgName type="institution">Cornell University ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook ?</orgName>
								<orgName type="department" key="dep2">Facebook AI ?</orgName>
								<orgName type="institution">Cornell University ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook ?</orgName>
								<orgName type="department" key="dep2">Facebook AI ?</orgName>
								<orgName type="institution">Cornell University ?</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COMBINING LABEL PROPAGATION AND SIMPLE MOD- ELS OUT-PERFORMS GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&amp;S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/CUAI/CorrectAndSmooth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Following the success of neural networks in computer vision and natural language processing, there are now a wide range of graph neural networks (GNNs) for making predictions involving relational data <ref type="bibr" target="#b0">(Battaglia et al., 2018;</ref><ref type="bibr" target="#b47">Wu et al., 2020)</ref>. These models have had much success and sit atop leaderboards such as the Open Graph Benchmark <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. Often, the methodological developments for GNNs revolve around creating strictly more expressive architectures than basic variants such as the Graph Convolutional Network (GCN) <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2017)</ref> or GraphSAGE <ref type="bibr" target="#b14">(Hamilton et al., 2017a)</ref>; examples include Graph Attention Networks <ref type="bibr" target="#b43">(Veli?kovi? et al., 2018)</ref>, Graph Isomorphism Networks <ref type="bibr" target="#b48">(Xu et al., 2018)</ref>, and various deep models <ref type="bibr" target="#b26">(Li et al., 2019;</ref><ref type="bibr" target="#b37">Rong et al., 2019;</ref>. Many ideas for new GNN architectures are adapted from new architectures in models for language (e.g., attention) or vision (e.g., deep CNNs) with the hopes that success will translate to graphs. However, as these models become more complex, understanding their performance gains is a major challenge, and scaling them to large datasets is difficult.</p><p>Here, we see how far we can get by combining much simpler models, with an emphasis on understanding where there are easy opportunities for performance improvements in graph learning, particularly transductive node classification. We propose a simple pipeline with three main parts ( <ref type="figure">Figure 1)</ref>: (i) a base prediction made with node features that ignores the graph structure (e.g., an MLP or linear model); (ii) a correction step, which propagates uncertainties from the training data across the graph to correct the base prediction; and (iii) a smoothing of the predictions over the <ref type="figure">Figure 1</ref>: Overview of our GNN-free model, Correct and Smooth, with a toy example. The left cluster belongs to orange and the right cluster belongs to blue. We use MLPs for base predictions, ignoring the graph structure, which we assume gives the same prediction on all nodes in this example. After, base predictions are corrected by propagating errors from the training data. Finally, corrected predictions are smoothed with label propagation.</p><p>graph. Steps (ii) and (iii) are just post-processing and use classical methods for graph-based semisupervised learning, namely, label propagation <ref type="bibr" target="#b55">(Zhu, 2005)</ref>. 1 With a few modifications and new deployment of these classic ideas, we achieve state-of-the-art performance on several node classification tasks, outperforming big GNN models. In our framework, the graph structure is not used to learn parameters but instead as a post-processing mechanism. This simplicity leads to models with orders of magnitude fewer parameters that take orders of magnitude less time to train and can easily scale to large graphs. We can also combine our ideas with state-of-the-art GNNs and see modest performance gains.</p><p>A major source of our performance improvements is directly using labels for predictions. This idea is not new -early diffusion-based semi-supervised learning algorithms on graphs such as the spectral graph transducer <ref type="bibr" target="#b21">(Joachims, 2003)</ref>, Gaussian random field models <ref type="bibr" target="#b54">(Zhu et al., 2003)</ref>, and and label spreading <ref type="bibr" target="#b53">(Zhou et al., 2004)</ref> all use this idea. However, the motivation for these methods was semi-supervised learning on point cloud data, so the features were used to construct the graph. Since then, these techniques have been used for learning on relational data from just the labels (i.e., no features) <ref type="bibr" target="#b24">(Koutra et al., 2011;</ref><ref type="bibr" target="#b12">Gleich &amp; Mahoney, 2015;</ref><ref type="bibr" target="#b34">Peel, 2017;</ref><ref type="bibr" target="#b5">Chin et al., 2019)</ref> but have largely been ignored in GNNs. That being said, we find that even simple label propagation (which ignores features) does surprisingly well on a number of benchmarks. This provides motivation for combining two orthogonal sources of prediction power -one coming from the node features (ignoring graph structure) and one coming from using the known labels directly in predictions.</p><p>Recent research connects GNNs to label propagation <ref type="bibr" target="#b45">(Wang &amp; Leskovec, 2020;</ref><ref type="bibr" target="#b20">Jia &amp; Benson, 2020)</ref> as well as Markov Random fields <ref type="bibr" target="#b35">(Qu et al., 2019;</ref><ref type="bibr" target="#b9">Gao et al., 2019)</ref>, and some techniques use ad hoc incorporation of label information in the features <ref type="bibr" target="#b40">(Shi et al., 2020)</ref>. However, these approaches are still expensive to train, while we use label propagation in two understandable and low-cost ways. We start with a cheap "base prediction" from a model that ignores graph structure (apart from perhaps a cheap pre-processing feature augmentation step like a spectral embedding). After, we use label propagation for error correction and then to smooth final predictions. These post-processing steps are based on the fact that errors and labels on connected nodes are positively correlated. Assuming similarity between connected nodes is at the center of much network analysis and corresponds to homophily or assortative mixing <ref type="bibr" target="#b28">(McPherson et al., 2001;</ref><ref type="bibr" target="#b31">Newman, 2003;</ref><ref type="bibr" target="#b6">Easley &amp; Kleinberg, 2010)</ref>. In the semi-supervised learning literature, the analog is the smoothness or cluster assumption <ref type="bibr" target="#b2">(Chapelle et al., 2003;</ref><ref type="bibr" target="#b55">Zhu, 2005)</ref>. The good performance of label propagation that we see across a wide variety of datasets suggests that these correlations hold on common benchmarks.</p><p>Overall, our methodology demonstrates that combining several simple ideas yields excellent performance in transductive node classification at a fraction of the cost, in terms of both model size (i.e., number of parameters) and training time. For example, on the OGB-Products benchmark, we out-perform the current best-known GNN with more than two orders of magnitude fewer parameters and more than two orders of magnitude less training time. However, our goal is not to say that current graph learning methods are poor or inappropriate. Instead, we aim to highlight easier ways in which to improve prediction performance in graph learning and to better understand the source of performance gains. Our main finding is that more direct incorporation of labels into the learning algorithms is key. And by combining our ideas with existing GNNs, we also see improvements, although they are minor. We hope that our approach spurs new ideas that can help in other graph learning tasks, such as inductive node classification, link prediction, and graph prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">ADDITIONAL RELATED WORK</head><p>The Approximate Personalized Propagation of Neural Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type="bibr" target="#b23">(Klicpera et al., 2018)</ref>. However, they focus on integrating this smoothing into the training process so that their model can be trained end to end. Not only is this significantly more computationally expensive, it also prevents APPNP from incorporating label information at inference. Compared to APPNP, our framework produces more accurate predictions, is faster to train, and more easily scales to large datasets. Our framework also complements the Simplified Graph Convolution <ref type="bibr" target="#b46">(Wu et al., 2019)</ref>, as well as algorithms designed to increase scalability <ref type="bibr" target="#b1">(Bojchevski et al., 2020;</ref><ref type="bibr" target="#b51">Zeng et al., 2019;</ref><ref type="bibr" target="#b38">Rossi et al., 2020)</ref>. The primary focus of our approach, however, is using labels directly, and scalability is a byproduct. There is also prior work connecting GCNs and label propagation. <ref type="bibr" target="#b45">Wang &amp; Leskovec (2020)</ref> use label propagation as a pre-processing step to weight edges for GNNs, whereas we use label propagation as a post-processing step and avoid GNNs. <ref type="bibr" target="#b20">Jia &amp; Benson (2020)</ref> use label propagation with GNNs for regression tasks, and our error correction step adapts some of their ideas for the case of classification. Finally, there are several recent approaches that incorporate nonlinearity into label propagation methods to compete with GNNs and achieve scalability <ref type="bibr" target="#b7">(Eliav &amp; Cohen, 2018;</ref><ref type="bibr" target="#b19">Ibrahim &amp; Gleich, 2019;</ref><ref type="bibr" target="#b42">Tudisco et al., 2020)</ref>, but these methods focus on settings of low label rates and don't incorporate feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CORRECT AND SMOOTH MODEL</head><p>We start with some notation. We assume that we have an undirected graph G = (V, E), where there are n = |V | nodes with features on each node represented by a matrix X ? R n?p . Let A be the adjacency matrix of the graph, D be the diagonal degree matrix, and S be the normalized adjacency matrix D ?1/2 AD ?1/2 . For the prediction problem, the node set V is split into a disjoint set of unlabeled nodes U and labeled nodes L, which are subsets of the indices {1, . . . , n}. We represent the labels by a one-hot-encoding matrix Y ? R n?c , where c is the number of classes (i.e., Y ij = 1 if i ? L is in class j, and 0 otherwise), and we further split the labeled nodes into a training set L t and validation set L v . Our problem is transductive node classification: assign each node j ? U a label in {1, . . . , C}, given G, X, and Y .</p><p>Our approach starts with a simple base predictor on node features, which does not rely on any learning over the graph. After, we perform two types of label propagation (LP): one that corrects the base predictions by modeling correlated error and one that smooths the final prediction. We call the combination of these two methods Correct and Smooth (C&amp;S; <ref type="figure">Figure 1</ref>). The LPs are only post-processing steps -our pipeline is not trained end-to-end. Furthermore, the graph is only used in these post-processing steps and in a pre-processing step to augment the features X, but not for the base predictions. This makes training fast and scalable compared to standard GNN models. Moreover, we take advantage of both LP (which tends to perform fairly well on its own without features) and the node features. We will see that combining these complementary signals yields excellent predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SIMPLE BASE PREDICTOR</head><p>To start, we use a simple base predictor that does not rely on the graph structure. More specifically, we train a model f to minimize i?Lt (f (x i ), y i ), where x i is the ith row of X, y i is the ith row of Y , and is a loss function. For this paper, f is either a linear model or a shallow multilayer perceptron (MLP) followed by a softmax, and is the cross-entropy loss. The validation set L v is used to tune hyperparameters such as learning rates and the hidden layer dimensions for the MLP. From f , we get a base prediction Z ? R n?c , where each row of Z is a probability distribution resulting from the softmax. Omitting the graph structure for these base predictions avoids the scalability issues with GNNs. In principle, though, we can use any base predictor for Z, including those based on GNNs, and we explore this in Section 3. However, for our pipeline to be simple and scalable, we just use linear classifiers or MLPs with subsequent post-processing, which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CORRECTING FOR ERROR IN BASE PREDICTIONS WITH RESIDUAL PROPAGATION</head><p>Next, we improve the accuracy of the base prediction Z by incorporating labels to correlate errors.</p><p>The key idea is that we expect errors in the base prediction to be positively correlated along edges in the graph. In other words, an error at node i increases the chance of a similar error at neighboring nodes of i. We should "spread" such uncertainty over the graph. Our approach here is inspired in part by residual propagation <ref type="bibr" target="#b20">(Jia &amp; Benson, 2020)</ref>, where a similar concept is used for node regression tasks, as well as generalized least squares and correlated error models more broadly <ref type="bibr" target="#b39">(Shalizi, 2013)</ref>.</p><p>To this end, we first define an error matrix E ? R n?c , where error is the residual on the training data and zero elsewhere:</p><formula xml:id="formula_0">E Lt = Z Lt ? Y Lt , E Lv = 0, E U = 0.</formula><p>(1)</p><p>The residuals in rows of E corresponding to training nodes are zero only when the base predictor makes a perfect predictions. We smooth the error using the label spreading technique of <ref type="bibr" target="#b53">Zhou et al. (2004)</ref>, optimizing the objectiv?</p><formula xml:id="formula_1">E = arg min W ?R n?c trace(W T (I ? S)W ) + ? W ? E 2 F .<label>(2)</label></formula><p>The first term encourages smoothness of the error estimation over the graph, and is equal to c j=1 w T j (I ? S)w j , where w j is the jth column of W . The second term keeps the solution close to the initial guess E of the error. As in <ref type="bibr" target="#b53">Zhou et al. (2004)</ref>, the solution can be obtained via the iteration E (t+1) = (1 ? ?)E + ?SE (t) , where ? = 1/(1 + ?) and E (0) = E, which converges rapidly to?. This iteration is a diffusion, propagation, or spreading of the error, and we add the smoothed errors to the base prediction to get corrected predictions Z (r) = Z +?. We emphasize that this is a post-processing technique and there is no coupled training with the base predictions.</p><p>This type of propagation is provably the right approach under a Gaussian assumption in regression problems <ref type="bibr" target="#b20">(Jia &amp; Benson, 2020)</ref>; however, for the classification problems we consider, the smoothed errors? might not be at the right scale. We know that in general,</p><formula xml:id="formula_2">E (t+1) 2 ? (1 ? ?) E + ? S 2 E (t) 2 = (1 ? ?) E 2 + ? E (t) 2 .<label>(3)</label></formula><p>When E (0) = E, we then have that E (t) 2 ? E 2 . Thus, the propagation cannot completely correct the errors on all nodes in the graph, as it does not have enough "total mass," and we find that adjusting the scale of the residual can help substantially in practice. To do this, we propose two variations of scaling the residual.</p><p>Autoscale. Intuitively, we want to scale the size of errors in? to be approximately the size of the errors in E. We only know the true errors at labeled nodes, so we approximate the scale with the average error over the training nodes. Formally, let e j ? R c correspond to the jth row of E, and define ? = 1 |Lt| j?Lt e j 1 . Then the corrected predictions on an unlabeled node i is given by Z (r) i,: = Z i,: + ?? :,i / ? T :,i 1 for i ? U . Scaled Fixed Diffusion (FDiff-scale). Alternatively, we can use a diffusion like the one from <ref type="bibr" target="#b54">Zhu et al. (2003)</ref>, which keeps the known errors at training nodes fixed. More specifically, we</p><formula xml:id="formula_3">iterate E (t+1) U = [D ?1 AE (t) ] U and keep fixed E (t) L = E L until convergence to?, starting with E (0) = E.</formula><p>Intuitively, this fixes error values where we know the error (on the labeled nodes L), while other nodes keep averaging over the values of their neighbors until convergence. With this type of propagation, the maximum and minimum values of entries in E (t) do not go beyond those in E L . We still find it effective to learn a scaling hyperparameter s to produce Z (r) = Z + s?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SMOOTHING FINAL PREDICTIONS WITH PREDICTION CORRELATION</head><p>At this point, we have a score vector Z (r) , obtained from correcting the base predictor Z with a model for the correlated error?. To make a final prediction, we further smooth the corrected predictions. The motivation is that adjacent nodes in the graph are likely to have similar labels, which is expected given homophily or assortative properties of a network. Thus, we can encourage smoothness over the distribution over labels by another label propagation. First, we start with our best guess G ? R n?c of the labels:</p><formula xml:id="formula_4">G Lt = Y Lt , G Lv,U = Z (r) Lv,U .<label>(4)</label></formula><p>Here, we set the training nodes back to their true labels and use the corrected predictions for the validation and unlabeled nodes (we can also use the true validation labels, which we discuss later in the experiments). We then iterate G (t+1) = (1 ? ?)G + ?SG (t) with G (0) = G until convergence to give the final prediction? . The classification for a node i ? U is arg max j?{1,...,c}?ij .</p><p>As with error correlation, the smoothing here is a post-processing step, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type="bibr" target="#b23">(Klicpera et al., 2018)</ref>, which we compare against later. However, APPNP is trained end-to-end, propagates on final-layer representations instead of softmaxes, does not use labels, and is motivated differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">SUMMARY AND ADDITIONAL CONSIDERATIONS</head><p>To review our pipeline, we start with a cheap base prediction Z, using only node features but not the graph structure. After, we estimate errors? by propagating known errors on the training data, resulting in error-corrected predictions Z (r) = Z +?. Finally, we treat these as score vectors on unlabeled nodes, and combine them with the known labels through another LP step to produce smoothed final predictions. We refer to this general pipeline as Correct and Smooth (C&amp;S).</p><p>Before showing that this pipeline achieves state-of-the-art performance on transductive node classification, we briefly describe another simple way of improving performance: feature augmentation. The hallmark of deep learning is that we can learn features instead of engineering them. However, GNNs still rely on informative input features to make predictions. There are numerous ways to get useful features from just the graph topology to augment the raw node features <ref type="bibr" target="#b16">(Henderson et al., 2011;</ref><ref type="bibr" target="#b62">2012;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type="bibr" target="#b3">(Chaudhuri et al., 2012;</ref><ref type="bibr" target="#b52">Zhang &amp; Rohe, 2018)</ref> coming from the leading k eigenvectors of the matrix D ?1/2 ? (A + ? n 11 T )D ?1/2 ? , where 1 is a vector of all ones, ? is a regularization parameter set to the average degree, and D ? is diagonal with ith diagonal entry equal to D ii + ? . The underlying matrix is dense, but we can apply matrix-vector products in time linear in the number of edges and use iterative eigensolvers to compute the embeddings quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS ON TRANSDUCTIVE NODE CLASSIFICATION</head><p>To demonstrate the effectiveness of our methods, we use nine datasets <ref type="table" target="#tab_0">(Table 1)</ref>. The Arxiv and Products datasets are from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>; the Cora, Citeseer, and Pubmed are three classic citation network benchmarks <ref type="bibr" target="#b10">Getoor, 2005;</ref>; and wikiCS is a web graph <ref type="bibr" target="#b29">(Mernyei &amp; Cangea, 2020)</ref>. In these datasets, classes are categories of papers, products, or pages, and features are derived from text. We also use a Facebook social network of Rice University, where classes are dorm residences and features are attributes such as gender, major, and class year, amongst others , as well as a geographic dataset of US counties where classes are 2016 election outcomes and features are demographic <ref type="bibr" target="#b20">(Jia &amp; Benson, 2020)</ref>. Finally, we use an email dataset of a European research institute, where classes are department membership and there are no features .  <ref type="formula" target="#formula_1">(2020)</ref>. For the Rice, US counties, and email data, we use 40%/10%/50% random splits, and for the smaller citation networks, we use 60%/20%/20% random splits, as in <ref type="bibr" target="#b45">Wang &amp; Leskovec (2020)</ref> (in contrast to lower label rate settings <ref type="bibr" target="#b49">(Yang et al., 2016)</ref>) to ameliorate sensitivity to hyperparameters. In all of our experiments, the standard deviations in prediction accuracy over splits is typically less than 1% and does not change our qualitative comparisons.</p><p>Base predictors and other models. We use Linear and MLP models as simple base predictors, where the input features are the raw node features and the spectral embedding. We also use a Plain Linear model that only uses the raw features for comparison and Label Propagation (LP; specifically, the <ref type="bibr" target="#b53">Zhou et al. (2004)</ref> version), which only uses labels. For comparable GNN models to our framework (in terms of simplicity or style), we use GCN, SGC, and APPNP. For the GCN models, we added extra residual connections from the input to every layer and from every layer to the output, which produced better results. The number of layers and hidden channels for the GCNs are the same as the MLPs. Thus, GCNs here represent a class of GCN-type models and not the original model <ref type="bibr" target="#b22">Kipf &amp; Welling (2017)</ref>.</p><p>Finally, we include several "state-of-the-art" (SOTA) baselines. For Arxiv and Products, this is UniMP <ref type="bibr" target="#b40">(Shi et al., 2020</ref>) (top of OGB leaderboard, as of October 1, 2020). For Cora, Citeseer and Pubmed, we reuse the top performance scores from . For Email and US County, we use GCNII . For Rice31, we use GCN with spectral and node2vec <ref type="bibr" target="#b13">(Grover &amp; Leskovec, 2016)</ref> embeddings (this is the best GNN-based model that we found). For wikiCS, we use APPNP as reported by <ref type="bibr" target="#b29">Mernyei &amp; Cangea (2020)</ref>. We select a set of fixed hyperparameters using the validation set. See the appendix for additional model architecture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FIRST RESULTS ON NODE CLASSIFICATION</head><p>In our first set of results, we only use the training labels in our C&amp;S framework, as these are what GNNs typically use to train models. For the results discussed here, this is generous to our baselines. The ability to include validation labels is an advantage of our approach (and label propagation in general), and this improves performance of our framework even further <ref type="table" target="#tab_0">(Table 1)</ref>. We discuss this in the next section. <ref type="table" target="#tab_1">Table 2</ref> reports the results, and we highlight a few important findings. First, within our model, there are substantial gains from the LP post-processing steps (for instance, on Products, the MLP base prediction goes from 63% to 84%). Second, even the Plain Linear model with C&amp;S is sufficient to outperform plain GCNs in many cases, and LP (a method with no learnable parameters) is often fairly competitive with GCNs. This is striking given that the main motivation for GCNs was to address the fact that connected nodes may not have similar labels <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2017)</ref>. Our results suggest that directly incorporating correlation in the graph with simple use of the features is often a better idea. Third, our model variants can out-perform SOTA on Products, Cora, Email, Rice31, and US County (often substantially so). On the other datasets, there is not much difference between our best-performing model and the SOTA. To get a sense of how much using ground truth labels directly helps, we also experiment with a version of C&amp;S without labels. Instead of running our LP steps, we just smooth the output of the base predictors using the approach of <ref type="bibr" target="#b53">Zhou et al. (2004)</ref> and call this the Basic Model. We see that the linear and MLP base predictor can often exceed the performance of a GCN ( <ref type="table" target="#tab_2">Table 3</ref>). Again, these results suggest that smoothed outputs are important, and that the original motivations for GCNs are misleading. Instead, we hypothesize that GCNs gain performance by having smoothed outputs over the graph, a similar observation made by <ref type="bibr" target="#b46">Wu et al. (2019)</ref>. However, there are still gaps in performance between our models here and those in <ref type="table" target="#tab_1">Table 2</ref> that directly use labels. Next, we see how to improve performance of C&amp;S even further by using more labels. We improve the C&amp;S performance by using both training and validation labels in Equation (4) instead of just the training labels. Importantly, we do not use validation labels to update the base prediction model -they are just used to select hyperparameters. Using validation labels boosts performance even further: <ref type="table" target="#tab_3">Table 4</ref> shows results, <ref type="table" target="#tab_0">Table 1</ref> shows gains over SOTA, and the appendix has more details. The ability to incorporate labels is a benefit of our approach. On the other hand, GNNs do not have this advantage, as they   often rely on early stopping to prevent overfitting, may not always benefit from more data (e.g., under distributional shift), and do not directly use labels. Thus, our comparisons in <ref type="table" target="#tab_1">Table 2</ref> are more generous than needed. With validation labels, our best model out-performs SOTA in seven of nine datasets, often by substantial margins <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>The evaluation procedure for GNN benchmarks differ from those for LP. For GNNs, a sizable validation set is often used (and needed) for substantial hyperparameter tuning, as well as early stopping. With LP, one can use the entire set of labeled nodes L with cross-validation to select the single hyperparameter ?. Given the setup of transductive node classification, however, there is no reason not to use validation labels at inference if they are helpful (e.g., via LP in our case). The results in <ref type="table" target="#tab_0">Tables 1 and 4</ref> show the true performance of our model and is the proper point of comparison.</p><p>Overall, our results highlight two important findings. First, big and expensive-to-train GNN models are not actually necessary for good performance for transductive node classification on many datasets. Second, combining classical label propagation ideas with simple base predictors outperforms graph neural networks on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FASTER TRAINING AND IMPROVING EXISTING GNNS</head><p>Our C&amp;S framework often requires significantly fewer parameters compared to GNNs or other SOTA solutions. As an example, we plot parameters vs. performance for Products in <ref type="figure" target="#fig_0">Figure 2</ref>. While having fewer parameters can be useful, the real gain is in faster training time, and our models are typically orders of magnitude faster to train than models with comparable accuracy because we do not use the graph structure for our base predictions. As one example, although our MLP + C&amp;S model based for the Arxiv dataset has a similar number of parameters compared to the GCN + labels method on the OGB leaderboards, our model runs 7 times faster per epoch and converges much faster. In addition, compared to the SOTA for the Products dataset, our framework with a linear base predictor has higher accuracy, trains over 100 times faster, and has 137 times fewer parameters.</p><p>We also evaluated our methods on an even larger dataset, the papers100M benchmarks <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. Here, we obtain 65.33% using C&amp;S with the Linear model as the base predictor, which outperforms the state-of-the-art on October 1, 2020 (63.29%). Due to computational limits, we could not run exhaustive benchmarks of other GNN models on this dataset.</p><p>Our pipeline can also be used to improve the performance of GNNs in general. We applied our error correction and final prediction smoothing to more complex base predictors such as GCNII or GAT. This improves our results on some datasets, including beating SOTA on ogbn-arxiv <ref type="table" target="#tab_4">(Table 5)</ref>. However, the performance improvements are sometimes only minor, suggesting that big models might be capturing the same signal as our simple C&amp;S framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">PERFORMANCE VISUALIZATION</head><p>To aid in understanding the performance of our C&amp;S framework, we visualize the predictions on the US County dataset <ref type="figure" target="#fig_1">(Figure 3</ref>). As expected, the residual error correlation tends to correct nodes where neighboring counties provide relevant information. For example, we see that many errors in the base predictions are corrected by the residual correlation <ref type="figure" target="#fig_1">(Figure 3b</ref>, left and right panels) In these cases, which correspond to parts of Texas and Hawaii, the demographic features of the counties are outliers compared to the rest of the country, leading both the linear model and GCN astray. The residual correlation from neighboring counties is able to fix the predictions. We also see that the final prediction correlation will smooth the prediction, as shown in the center panel of <ref type="figure" target="#fig_1">Figure 3b</ref> so that the errors can be fixed based on the correct classification of the neighbors. We observe similar behavior on the Rice31 dataset (see the appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>GNN models are becoming more expressive, more parameterized, and more expensive to train.</p><p>Our results suggest that we should explore other techniques for improving performance, such as label propagation and feature augmentation. In particular, label propagation and its variants are longstanding, powerful ideas. More directly incorporating them into graph learning models has major benefits, and we have shown that these can lead to both better predictions and faster training.</p><p>B PERFORMANCE RESULTS WITH ONLY RESIDUAL CORRELATION <ref type="table" target="#tab_5">Table 6</ref> shows results when using residual correlation but not smoothing in the final predictions, i.e., just the "C" step of our C&amp;S framework. The results indicate both the label propagation steps matter significantly for the final improvements.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy and model size on Products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) US County visualizations, where the embedding is given by GraphViz (roughly, a compressed rotated version of the latitude and longitude coordinates). Colors correspond to class labels. (b) Panels corresponding to parts of (a) that show at which stage C&amp;S made a correct prediction. (c) The same panels showing GCN predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>US County and Rice31 dataset are shown in Figures 4 to 9. The Rice31 visualization is generated by projecting the 128-dimensional spectral embedding used in the main text down to two dimensions with UMAP (McInnes et al., 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>US County ground truth class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>US County Linear Base Prediction within C&amp;S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>US County GCN (includes spectral embedding features).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Rice31 ground truth class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Rice31 Linear Base Predictor within C&amp;S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Rice31 GCN (includes spectral embedding features).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary statistics of datasets. We include (i) the reduction in the number of parameters, (ii) the change in accuracy of our best C&amp;S model compared to the state-of-the-art GNN method, and (iii) the training time. By avoiding expensive GNNs, our methods require fewer parameters and are faster to train. Our methods are typically more accurate (seealso Tables 2 and 4).</figDesc><table><row><cell>Datasets</cell><cell>Classes</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Parameter ? Accuracy ?</cell><cell>Time</cell></row><row><cell>Arxiv</cell><cell>40</cell><cell>169,343</cell><cell>1,166,243</cell><cell>-84.9%</cell><cell>+ 0.97</cell><cell>9.89 s</cell></row><row><cell>Products</cell><cell>47</cell><cell cols="2">2,449,029 61,859,140</cell><cell>-93.47%</cell><cell>+1.53</cell><cell>170.6 s</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2,708</cell><cell>5,429</cell><cell>-98.37%</cell><cell>+ 1.09</cell><cell>0.5 s</cell></row><row><cell>Citeseer</cell><cell>6</cell><cell>3,327</cell><cell>4,732</cell><cell>-89.68%</cell><cell>-0.69</cell><cell>0.48 s</cell></row><row><cell>Pubmed</cell><cell>3</cell><cell>19,717</cell><cell>44,338</cell><cell>-96.00%</cell><cell>-0.30</cell><cell>0.85 s</cell></row><row><cell>Email</cell><cell>42</cell><cell>1,005</cell><cell>25,571</cell><cell>-97.89%</cell><cell>+ 4.26</cell><cell>42.83 s</cell></row><row><cell>Rice31</cell><cell>10</cell><cell>4,087</cell><cell>184,828</cell><cell>-99.02%</cell><cell>+ 1.39</cell><cell>39.33 s</cell></row><row><cell cols="2">US County 2</cell><cell>3,234</cell><cell>12,717</cell><cell>-74.56%</cell><cell>+ 1.77</cell><cell>39.05 s</cell></row><row><cell>wikiCS</cell><cell>10</cell><cell>11,701</cell><cell>216,123</cell><cell>-84.88%</cell><cell>+ 2.03</cell><cell>7.09 s</cell></row></table><note>Data splits. The training/validation/test splits for Arxiv and Products are given by the benchmark, and the splits for wikiCS come from Mernyei &amp; Cangea</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of our C&amp;S framework, using only the ground truth training labels in Equation (4). Further improvements can be made by including ground truth validation labels(Table 4).</figDesc><table><row><cell>Methods</cell><cell cols="3">Base Model Arxiv Products</cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell></row><row><cell>LP</cell><cell>-</cell><cell>68.5</cell><cell>74.76</cell><cell>86.50</cell><cell>70.64</cell><cell>83.74</cell></row><row><cell>Plain GCN</cell><cell>-</cell><cell>71.74</cell><cell>75.64</cell><cell>85.77</cell><cell>73.68</cell><cell>88.13</cell></row><row><cell>SGC</cell><cell cols="2">Plain Linear 69.39</cell><cell>68.83</cell><cell>86.81</cell><cell>72.04</cell><cell>84.04</cell></row><row><cell>APPNP</cell><cell cols="2">Plain Linear 66.38</cell><cell>OOM</cell><cell>87.87</cell><cell>76.53</cell><cell>89.40</cell></row><row><cell>SOTA</cell><cell>-</cell><cell>73.79</cell><cell>82.56</cell><cell>88.49</cell><cell>77.99</cell><cell>90.30</cell></row><row><cell></cell><cell cols="2">Plain Linear 52.32</cell><cell>47.73</cell><cell>73.85</cell><cell>70.27</cell><cell>87.10</cell></row><row><cell>Base</cell><cell>Linear</cell><cell>70.08</cell><cell>50.05</cell><cell>74.75</cell><cell>70.51</cell><cell>87.19</cell></row><row><cell>Prediction</cell><cell>MLP</cell><cell>71.51</cell><cell>63.41</cell><cell>74.06</cell><cell>68.10</cell><cell>86.85</cell></row><row><cell></cell><cell cols="2">Plain Linear 71.11</cell><cell>80.24</cell><cell>88.62</cell><cell>76.31</cell><cell>89.99</cell></row><row><cell>Autoscale</cell><cell>Linear</cell><cell>72.07</cell><cell>80.25</cell><cell>88.73</cell><cell>76.75</cell><cell>89.93</cell></row><row><cell></cell><cell>MLP</cell><cell>72.62</cell><cell>78.60</cell><cell>87.39</cell><cell>76.31</cell><cell>89.33</cell></row><row><cell></cell><cell cols="2">Plain Linear 70.60</cell><cell>82.54</cell><cell>89.05</cell><cell>76.22</cell><cell>89.74</cell></row><row><cell>FDiff-</cell><cell>Linear</cell><cell>71.57</cell><cell>83.01</cell><cell>88.66</cell><cell>77.06</cell><cell>89.51</cell></row><row><cell>scale</cell><cell>MLP</cell><cell>72.43</cell><cell>84.18</cell><cell>87.39</cell><cell>76.42</cell><cell>89.23</cell></row><row><cell>Methods</cell><cell cols="2">Base Model Email</cell><cell>Rice31</cell><cell cols="2">US County wikiCS</cell><cell></cell></row><row><cell>LP</cell><cell>-</cell><cell>70.69</cell><cell>82.19</cell><cell>87.90</cell><cell>76.72</cell><cell></cell></row><row><cell>Plain GCN</cell><cell>-</cell><cell>-</cell><cell>15.45</cell><cell>84.13</cell><cell>78.61</cell><cell></cell></row><row><cell>SGC</cell><cell>Plain Linear</cell><cell>-</cell><cell>16.59</cell><cell>83.92</cell><cell>72.86</cell><cell></cell></row><row><cell>APPNP</cell><cell cols="2">Plain Linear 70.28</cell><cell>11.34</cell><cell>84.14</cell><cell>69.83</cell><cell></cell></row><row><cell>SOTA</cell><cell>-</cell><cell>71.96</cell><cell>86.50</cell><cell>88.08</cell><cell>79.84</cell><cell></cell></row><row><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>9.84</cell><cell>75.74</cell><cell>72.45</cell><cell></cell></row><row><cell>Base</cell><cell>Linear</cell><cell>66.24</cell><cell>70.26</cell><cell>84.07</cell><cell>74.29</cell><cell></cell></row><row><cell>Prediction</cell><cell>MLP</cell><cell>69.13</cell><cell>17.16</cell><cell>87.70</cell><cell>73.07</cell><cell></cell></row><row><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>75.99</cell><cell>85.25</cell><cell>79.57</cell><cell></cell></row><row><cell>Autoscale</cell><cell>Linear</cell><cell>72.50</cell><cell>86.42</cell><cell>86.15</cell><cell>79.53</cell><cell></cell></row><row><cell></cell><cell>MLP</cell><cell>74.55</cell><cell>85.50</cell><cell>89.64</cell><cell>78.10</cell><cell></cell></row><row><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>73.66</cell><cell>87.38</cell><cell>79.54</cell><cell></cell></row><row><cell>FDiff-</cell><cell>Linear</cell><cell>72.53</cell><cell>87.55</cell><cell>88.11</cell><cell>79.25</cell><cell></cell></row><row><cell>scale</cell><cell>MLP</cell><cell>75.74</cell><cell>85.74</cell><cell>89.85</cell><cell>78.24</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Performance of our Basic</cell></row><row><cell cols="3">Model, which only uses labels for</cell></row><row><cell cols="2">base predictions.</cell><cell></cell></row><row><cell cols="3">Base Model Arxiv Products</cell></row><row><cell cols="2">Plain Linear 63.30</cell><cell>66.27</cell></row><row><cell>Linear</cell><cell>71.42</cell><cell>78.73</cell></row><row><cell>MLP</cell><cell>72.48</cell><cell>80.34</cell></row><row><cell>Plain GCN</cell><cell>71.74</cell><cell>75.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of our Correct and Smooth (C&amp;S) model with both train and validation labels ground truth labels used in Equation (4).</figDesc><table><row><cell></cell><cell cols="2">Methods</cell><cell cols="3">Base Model Arxiv Products</cell><cell>Cora</cell><cell>Citeseer Pubmed</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Plain Linear 72.71</cell><cell>80.55</cell><cell>89.54</cell><cell>76.83</cell><cell>90.01</cell></row><row><cell></cell><cell cols="3">Autoscale Linear</cell><cell>73.78</cell><cell>80.56</cell><cell>89.77</cell><cell>77.11</cell><cell>89.98</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MLP</cell><cell>74.02</cell><cell>79.29</cell><cell>88.55</cell><cell>76.36</cell><cell>89.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Plain Linear 72.42</cell><cell>82.89</cell><cell>89.47</cell><cell>77.08</cell><cell>89.74</cell></row><row><cell></cell><cell cols="2">FDiff-</cell><cell>Linear</cell><cell>72.93</cell><cell>83.27</cell><cell>89.53</cell><cell>77.29</cell><cell>89.57</cell></row><row><cell></cell><cell>scale</cell><cell></cell><cell>MLP</cell><cell>73.46</cell><cell>84.55</cell><cell>88.18</cell><cell>76.41</cell><cell>89.38</cell></row><row><cell></cell><cell cols="2">SOTA</cell><cell>-</cell><cell>73.65</cell><cell>82.56</cell><cell>88.49</cell><cell>77.99</cell><cell>90.30</cell></row><row><cell></cell><cell cols="2">Methods</cell><cell cols="2">Base Model Email</cell><cell>Rice31</cell><cell cols="2">US County wikiCS</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>76.59</cell><cell>85.22</cell><cell>81.87</cell></row><row><cell></cell><cell cols="3">Autoscale Linear</cell><cell>73.33</cell><cell>87.25</cell><cell>86.38</cell><cell>81.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MLP</cell><cell>73.45</cell><cell>86.13</cell><cell>89.71</cell><cell>80.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>75.31</cell><cell>88.16</cell><cell>81.18</cell></row><row><cell></cell><cell cols="2">FDiff-</cell><cell>Linear</cell><cell>72.57</cell><cell>87.89</cell><cell>88.06</cell><cell>81.06</cell></row><row><cell></cell><cell>scale</cell><cell></cell><cell>MLP</cell><cell>76.22</cell><cell>86.26</cell><cell>90.05</cell><cell>80.83</cell></row><row><cell></cell><cell cols="2">SOTA</cell><cell>-</cell><cell>71.96</cell><cell>86.50</cell><cell>88.08</cell><cell>79.84</cell></row><row><cell></cell><cell>84</cell><cell></cell><cell>ogbn-products</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>78 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>76</cell><cell cols="2">OGB Leaderboard C&amp;S (Ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">10 4</cell><cell>10 5 # Parameters</cell><cell>10 6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>C&amp;S with GNN base predictors.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Performance</cell></row><row><cell></cell><cell>GAT</cell><cell>73.56</cell></row><row><cell cols="2">ogbn-arxiv GAT + C&amp;S</cell><cell>73.86</cell></row><row><cell></cell><cell>SOTA</cell><cell>73.79</cell></row><row><cell cols="2">US County GCNII (SOTA)</cell><cell>88.08</cell></row><row><cell></cell><cell>GCNII + C&amp;S</cell><cell>89.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance of our C&amp;S framework with the error correction step but not the final prediction smoothing, using only the ground truth training labels in Equation(4).</figDesc><table><row><cell>Methods</cell><cell cols="3">Base Model Arxiv Products</cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell></row><row><cell></cell><cell cols="2">Plain Linear 66.89</cell><cell>74.63</cell><cell>79.56</cell><cell>72.56</cell><cell>88.56</cell></row><row><cell>Autoscale</cell><cell>Linear</cell><cell>71.52</cell><cell>70.93</cell><cell>79.08</cell><cell>70.77</cell><cell>88.84</cell></row><row><cell></cell><cell>MLP</cell><cell>71.97</cell><cell>69.85</cell><cell>74.11</cell><cell>71.78</cell><cell>87.35</cell></row><row><cell></cell><cell cols="2">Plain Linear 65.62</cell><cell>80.97</cell><cell>76.48</cell><cell>70.48</cell><cell>87.52</cell></row><row><cell>FDiff-</cell><cell>Linear</cell><cell>70.26</cell><cell>73.89</cell><cell>79.32</cell><cell>70.53</cell><cell>84.47</cell></row><row><cell>scale</cell><cell>MLP</cell><cell>71.55</cell><cell>72.72</cell><cell>74.36</cell><cell>71.45</cell><cell>86.97</cell></row><row><cell>Methods</cell><cell cols="2">Base Model Email</cell><cell>Rice31</cell><cell cols="2">US County wikiCS</cell><cell></cell></row><row><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>43.97</cell><cell>82.60</cell><cell>77.49</cell><cell></cell></row><row><cell>Autoscale</cell><cell>Linear</cell><cell>73.39</cell><cell>86.19</cell><cell>84.08</cell><cell>74.06</cell><cell></cell></row><row><cell></cell><cell>MLP</cell><cell>71.64</cell><cell>84.61</cell><cell>88.83</cell><cell>78.72</cell><cell></cell></row><row><cell></cell><cell>Plain Linear</cell><cell>-</cell><cell>72.44</cell><cell>87.16</cell><cell>75.98</cell><cell></cell></row><row><cell>FDiff-</cell><cell>Linear</cell><cell>71.31</cell><cell>85.22</cell><cell>88.27</cell><cell>73.86</cell><cell></cell></row><row><cell>scale</cell><cell>MLP</cell><cell>72.59</cell><cell>85.42</cell><cell>89.62</cell><cell>78.40</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">One of the main methods that we use<ref type="bibr" target="#b53">(Zhou et al., 2004)</ref> is often called label spreading. The term "label propagation" is used in a variety of contexts<ref type="bibr" target="#b55">(Zhu, 2005;</ref><ref type="bibr" target="#b44">Wang &amp; Zhang, 2007;</ref><ref type="bibr" target="#b36">Raghavan et al., 2007;</ref><ref type="bibr" target="#b12">Gleich &amp; Mahoney, 2015)</ref>. The salient point for this paper is that we assume positive correlations on neighboring nodes and that the algorithms work by "propagating" information from one node to another.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was supported by Facebook AI, NSF Award DMS-1830274, ARO Award W911NF19-1-0057, ARO MURI, and JP Morgan Chase &amp; Co.</p><p>We would also like to thank the rest of Cornell University Artificial Intelligence for their support and discussion. In addition, we'd like to thank Matthias Fey and Marc Brockschmidt for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><forename type="middle">R</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Relational inductive biases, deep learning, and graph networks. ArXiv, abs/1806.01261</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cluster kernels for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering of graphs with general degrees in the extended planted partition model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tsiatas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="35" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupled smoothing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><forename type="middle">M</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Networks, crowds, and markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bootstrapped graph diffusions: Exposing the power of nonlinearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buchnik</forename><surname>Eliav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Measurement and Analysis of Computing Systems</title>
		<meeting>the ACM on Measurement and Analysis of Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional random field enhanced graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Methods for Knowledge Discovery from Complex Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="189" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning probabilistic models of relational structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using local spectral methods to robustify graph-based learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">It&apos;s who you know: graph mining using recursive structural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="663" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RolX: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1231" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear diffusion for community detection and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rania</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="739" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Residual correlation in graph neural network regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unifying guilt-by-association approaches: Theorems and fast algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-You</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duen Horng Polo</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsing-Kuo Kenneth</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="245" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph evolution: Densification and shrinking diameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UMAP: Uniform Manifold Approximation and Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gro?berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wiki-cs: A wikipedia-based benchmark for graph neural networks. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?t?lina</forename><surname>P&amp;apos;eter Mernyei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cangea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mixing patterns in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26126</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised learning for relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leto</forename><surname>Peel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="435" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GMNN: Graph markov neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Near linear time algorithm to detect community structures in large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usha</forename><surname>Nandini Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soundar</forename><surname>Kumara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">SIGN: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Advanced data analysis from an elementary point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosma</forename><surname>Shalizi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified massage passing model for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Social structure of facebook networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason A</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Nonlinear higher-order label spreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Tudisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prokopchik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04762</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Local higher-order graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph-SAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Understanding regularized spectral clustering via graph conductance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Rohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10631" to="10640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A MODEL DETAILS Here we provide some more details on the models that we use. In all cases we use the Adam optimizer and tune the learning rate. We follow the models and hyperparameters provided in OGB (Hu et al., 2020) and wikiCS (Mernyei &amp; Cangea, 2020) and manually tune some hyperparameters on the validation data for the potential better performance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">For our MLPs, every linear layer is followed by batch normalization, ReLU activation, and 0.5 dropout. The other parameters depend on the dataset as follows</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">3 layers and 256 hidden channels with learning rate equal to 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Ogb</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Getoor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Pubmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">layers and 64 hidden channels with learning rate = 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Email</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">? wikiCS: 3 layers and 256 hidden channels with learning rate equal to 0</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">5 layers and 256 hidden channels with learning rate equal to 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Us County ; Traud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">We determined SOTA for Email, US County, and Rice31 based on all other models used in the paper. The best performing SOTA baselines were as follows. For Email, GCNII with 5 layers, 256 hidden channels, learning rate equal to 0.01. For US County, GCNII with 8 layers, 256 hidden channels, learning rate equal to 0.03. For Rice31, we reused our base GCN architecture and trained it over spectral and node2vec embeddings</title>
	</analytic>
	<monogr>
		<title level="m">Most of the &quot;State-of-the-Art&quot; models are taken from benchmark datasets</title>
		<imprint/>
	</monogr>
	<note>This significantly outperformed the other GNN variants</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">All models were implemented with PyTorch(Paszke et al., 2019) and PyTorch Geometric</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Fey &amp; Lenssen</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
