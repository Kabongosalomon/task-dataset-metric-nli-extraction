<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Tencent</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtu</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open-vocabulary object detection (OVD) aims to scale up vocabulary size to detect objects of novel categories beyond the training vocabulary. Recent work resorts to the rich knowledge in pre-trained vision-language models. However, existing methods are ineffective in proposal-level vision-language alignment. Meanwhile, the models usually suffer from confidence bias toward base categories and perform worse on novel ones. To overcome the challenges, we present MEDet, a novel and effective OVD framework with proposal mining and prediction equalization. First, we design an online proposal mining to refine the inherited vision-semantic knowledge from coarse to fine, allowing for proposal-level detection-oriented feature alignment. Second, based on causal inference theory, we introduce a class-wise backdoor adjustment to reinforce the predictions on novel categories to improve the overall OVD performance. Extensive experiments on COCO and LVIS benchmarks verify the superiority of MEDet over the competing approaches in detecting objects of novel categories, e.g., 32.6% AP50 on COCO and 22.4% mask mAP on LVIS. Code is available at the https://github.com/Pealing/MEDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection has achieved great success in localizing objects. Yet, supervised object detection algorithms <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref> typically require costly manual annotations of bounding boxes, which limits their scalability in category size. Recently, open-vocabulary object detection (OVD) <ref type="bibr" target="#b48">[49]</ref> has attracted increasing attention due to its ability to expand the detection vocabulary with the help of pre-trained vision-language models (VLMs) <ref type="bibr" target="#b31">[32]</ref>. Typical OVD methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> first learn an unbounded vocabulary of concepts from image-caption pairs, and then transfer the general vision-language knowledge to facilitate OVD with detection annotations of base categories alone.</p><p>Despite their promising progress, existing OVD methods have two main problems. First, acquiring proposal-level vision-language knowledge suitable for object detection is challenging. Although pre-trained VLMs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">10]</ref> have covered a wide range of concepts, the learned knowledge is restricted to the image level, which is insufficient for object detection. Furthermore, since the text representations of each category are learned implicitly from image captions, they typically yield suboptimal decision boundaries for OVD tasks. As shown in the left image of <ref type="figure" target="#fig_7">Fig. 1a</ref>, CLIP <ref type="bibr" target="#b31">[32]</ref> has large ambiguity to match correct proposals to "person" and "skis" concepts, and the proposal of "person" only encloses part of the person. Therefore, simply transferring the coarse vision-language knowledge to OVD models <ref type="bibr" target="#b48">[49]</ref> or exploiting the noisy proposal-concept pairs for fine-grained detection model with distillation <ref type="bibr" target="#b13">[14]</ref> may interfere with accurate object localization and degrade the learning process of proposal-level vision-language alignment.  <ref type="figure" target="#fig_11">Figure 1</ref>: (a): The left image illustrates that the pre-trained CLIP model <ref type="bibr" target="#b31">[32]</ref> provides noisy proposal-concept pairs when selecting proposals based on the highest confidence value of concepts. The right one shows the refined pairs from our Online Proposal Mining (OPM). (b): The histograms of confidence scores of four categories on COCO dataset <ref type="bibr">[1]</ref>. It visualizes the under confidence of OVD detectors on novel classes.</p><p>Second, the optimized OVD models produce under-confident predictions for novel classes. As illustrated in <ref type="figure" target="#fig_9">Fig. 1b</ref>, the objects of novel categories are more likely to be assigned with relatively low confidence or mis-classified as base categories. It results in suboptimal predictions on novel categories. The reasons for the bias are two-fold. First, the OVD training schemes lead to biased models. Previous works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51]</ref> point out that the detectors fine-tuned on base categories easily forget the general vision-language knowledge from pre-trained VLMs, especially when the dataset only contains a small number of base categories. Second, the OVD training datasets are naturally class imbalanced, especially for the base and novel categories. The class imbalance causes the insufficient learning of proposal-level vision-language knowledge for novel categories. As a result, the optimized detectors are biased towards base categories in inference. This issue has been largely overlooked and rarely addressed by almost all existing OVD methods.</p><p>In this paper, we present MEDet (Proposal Mining and Prediction Equalization for Open Vocabulary Object Detection), a novel framework to overcome the two problems for better OVD performance. The overall framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> in Sec. <ref type="bibr">3</ref>. For the first problem, we conduct Online Proposals Mining (OPM) in the end-to-end OVD network at the training stage to fully explore proposal-level vision-language knowledge from the image-caption information. Specifically, we first leverage a caption parser and an RPN to obtain noun concepts in the caption and object proposals in the image, respectively. For high-accuracy proposal mining, we first augment text embeddings of noun concepts by online feature interaction with the corresponding image features. Then, noisy proposal-concept pairs are removed by analyzing their semantic similarity distribution and spatial relationship. Finally, a recurrent attention memory-based contrastive learning module is used to discover the full latent proposal-level vision-language alignments. Overall, OPM enables more accurate proposal-concept pairs compared with common pre-trained VLMs, as shown in the right image of <ref type="figure" target="#fig_7">Fig. 1a</ref>.</p><p>In addition, to mitigate the under-confidence issue on novel classes, we devise a simple yet effective debiasing scheme. By reconstructing the issue from the causal intervention <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b26">27]</ref>, we design a class-wise backdoor adjustment (CBA) module to produce equalized predictions. Specifically, at the inference stage, we estimate the class-specific confounder embedding via density clustering on the training dataset, and leverage it to adjust the output for less biased prediction. Experiments on COCO <ref type="bibr">[1]</ref> and LVIS <ref type="bibr" target="#b14">[15]</ref> benchmarks demonstrate that our method outperforms other cutting-edge OVD methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51]</ref>. For example, MEDet reaches 32.6% AP50 for novel categories on the COCO dataset, which considerably suppresses the state-of-the-art approaches by 3.5%.</p><p>Our major contributions can be summarized as follows:</p><p>? We propose a novel online proposal mining method to fully exploit the caption information for learning proposal-level vision-language knowledge toward open vocabulary object detection. ? We devise an effective backdoor adjustment scheme to alleviate the under-confidence issue and promote the detection results on novel classes, achieving better predictions.</p><p>? Extensive experiments show that our proposed framework achieves new state-of-the-art performance on the novel categories when using the same caption dataset and base categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Open-Vocabulary Object Detection (OVD). Typically researchers scale up the vocabulary size for object detection by exploiting rich knowledge within pre-trained VLMs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">10]</ref>. OVR-CNN <ref type="bibr" target="#b48">[49]</ref> first learns a projection layer behind the backbone of Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> to align the visual space with the textual space <ref type="bibr" target="#b17">[18]</ref>, and then fine-tunes the detector with only the detection data of base categories. Distillation-based approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45]</ref> align the visual extractor of a detector with both the image and text encoder of CLIP <ref type="bibr" target="#b31">[32]</ref>. Recently, Zhong et al. <ref type="bibr" target="#b50">[51]</ref> tackle the domain shift problem when adopting VLMs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18]</ref> and propose RegionCLIP <ref type="bibr" target="#b50">[51]</ref>: it first leverages a CLIP model to match image regions with template captions, and pre-trains the model to align the region-concept pairs in the feature space, and finally transfers the pre-trained model to object detection. However, its training pipeline is complex, and the "pseudo" region-concept pairs are noisy. In this paper, we propose online proposal mining (OPM) within an end-to-end OVD network and simplify the training pipeline. Moreover, OPM provides more reliable region-concept pairs for acquiring rich proposal-level vision-language knowledge. Recently, Zhou et al. <ref type="bibr" target="#b52">[53]</ref> apply additional image-level supervision (e.g., predefined classification categories) to the largest proposal and does not supervise other outputs for image-labeled data. A limitation is that it requires image-level annotations within a predefined hand-crafted taxonomy and only learns the classes within the taxonomy. In contrast, we leverage a larger vocabulary of concepts within captions that can generalize well to target classes.</p><p>Debiasing Strategies in Object Detection. Real-world detection datasets <ref type="bibr">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> typically exhibit class-imbalance label distributions, making it difficult to learn generalized representation across classes. Existing debiasing approaches can be roughly categorized into four mainstreams: (1) reweighting <ref type="bibr">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref> to sample more instances of rare classes; (2) cost-sensitive learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref> to spare more attention on hard examples; (3) knowledge transfer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref> to learn generalized representation progressively; and (4) post calibration <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42</ref>] to refine the model's output during the inference. To apply the above methods, we need the information of the rare target classes (e.g., class frequency) in advance. Unfortunately, in the setting of OVD <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref>, we do not have access to the definition of novel categories during training, let alone class frequencies. The scarcity of information makes it difficult and ineffective to leverage the popular imbalance debiasing schemes. Besides, the prediction bias also results from the training strategies of OVD models. Existing methods adopt model freezing <ref type="bibr" target="#b48">[49]</ref> and focal scaling <ref type="bibr" target="#b50">[51]</ref> to alleviate the catastrophic forgetting of the general knowledge in the pre-trained VLMs when fine-tuning the detectors on the base categories. In this paper, building on the theory of causal intervention <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46]</ref>, we present a novel and effective backdoor adjustment scheme in the inference to eschew the bias for equalized predictions and enhance overall results on both base classes and novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>The detection dataset D det contains N det samples and the i-th sample is</p><formula xml:id="formula_0">(I i , {(b i k , c i k )}), where I i is the image and (b i k , c i k )</formula><p>is the k-th object's bounding box and class in the image. D det contains two sets of classes: the base categories C B and the novel categories C N . C B and C N are disjoint, i.e., C N ? C B = ?. The image-caption dataset D cap contains N cap samples and the i-th sample is denoted by (I i , {s i j }, {w i k }), where s i j is the j-th caption's sentence and w i k is the k-th word concepts for the image I i . D cap is also correlated with a vocabulary C cap . Following the common setting of OVD, MEDet is trained on base categories C B from D det and the vocabulary C cap from D cap .</p><p>The training stage of MEDet is illustrated in the blue and orange parts of <ref type="figure" target="#fig_1">Fig. 2</ref>. With a mini-batch of samples from two datasets, MEDet learns both object localization and proposal-level vision-language alignment simultaneously in an end-to-end manner. Specifically, for samples from D det (shown in blue), MEDet follows the standard two-stage detection training <ref type="bibr" target="#b34">[35]</ref>. Note for OVD, the classifier's weights are replaced by the text embeddings of base class names, so that the novel class embeddings can be used during testing, without changing the semantic space <ref type="bibr" target="#b48">[49]</ref>. To augment the discrimination A seabird flies over a dark rocky coastline.</p><p>A bird is flying about the sea shore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lcap</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Augmentation</head><p>Caption data During inference (green part), we propose class-wise backdoor adjustment to handle the confidence bias between the base and novel categories. The * means a frozen network. The means contributions of this paper.</p><p>of text embeddings, we propose a Concept Augmentation (CAug), which augments text embeddings by online feature interaction with corresponding image features. For samples from D cap (shown in orange), the region proposal network (RPN) first provides proposals with high objectness scores, and concept parsing is conducted as <ref type="bibr" target="#b36">[37]</ref>. Then, MEDet adopts Online Proposal Mining (OPM), which considers the semantic similarity distribution and spatial relationship between proposals and concepts, to refine proposal-level vision-language knowledge. The denoised proposal-concept pairs are further supervised by a recurrent attention memory based contrastive learning loss for latent set alignments. The inference stage is shown in green part. MEDet is tested on both C B and C N of D det . We design a Class-wise Backdoor Adjustment (CBA) to handle the confidence bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online Proposal Mining</head><p>We propose OPM to progressively learn the rich and fine-grained proposal-level vision-language knowledge within D cap . As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, it contains three steps: (a) concept augmentation for classification discrimination; (b) noisy proposal-concept pair removal by analyzing the semantic similarity distribution; (c) fragment proposals merging based on the spatial distribution.</p><p>Concept Augmentation. Existing OVD approaches <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref> generate text embeddings by feeding a prompt template with the category's name (e.g., It is a photo of [category].) to the text encoder of a pre-trained vision-language model. The resulting category embeddings are then used to supervise the training of detectors. However, such embeddings are typically less effective in discriminating image features, because the text is highly semantic and information-dense, while image data are much more diverse <ref type="bibr" target="#b16">[17]</ref>. Even when the embeddings can be boosted via prompt ensemble <ref type="bibr" target="#b13">[14]</ref> and properly designed prompts <ref type="bibr">[9]</ref>, they cannot eliminate the difference in information density. To mitigate this problem, we enhance the discrimination of text embeddings by leveraging the semantic relationship between different concepts and the image embedding, where we adopt a cross-attention module to adapt text embeddings to objects in each image.</p><p>Specifically, during training, we first compose a mini-batch using images from D det and D cap . The text embeddings of concepts from both base categories C B and caption parsing {w i k } K k=1 are denoted as T . Denote the image embedding output by the image encoder as V img . Then, the augmented text embeddings T aug is obtained by a cross-attention based transformer module <ref type="bibr">[8,</ref><ref type="bibr" target="#b12">13]</ref>:</p><formula xml:id="formula_1">T aug = T aug + F F N (T aug ), T aug = T + CA(W q ? T, W k ? V img , W o ? V img ),<label>(1)</label></formula><p>where F F N is the feed-forward module of two linear layers, and W { * } are the projection matrices. CA is the cross-attention module, in which D is the dimension of the text embedding. The augmented text embeddings are then exploited for the proposal-concept alignment and final detection.</p><p>Noisy Pair Removal via Similarity Distribution. MEDet aims to learn rich fine-grained proposallevel vision-language knowledge. It is challenging because there are no ground-truth bounding-box annotations in the image-caption data and the pseudo-labeled proposals with much noise are not conducive for learning. The semantic similarity between image proposals and concept texts is the most useful reference factor for screening positive vision-language pairs. However, straight-forwardly using the maximum vision-language similarity values to identify positive pairs has two problems. First, as shown in <ref type="figure" target="#fig_7">Fig. 1a</ref>, highly overlapped proposals may be assigned to different categories because of the semantic confusion, especially at the boundary of the interacting objects, where the similarity distribution is flat. Second, it is hard to choose an appropriate similarity threshold to remove noise for both base concepts and novel concepts because of their learning bias. Thus, we propose to refine the proposal-concept pairs by analyzing the semantic similarity distribution instead. Specifically, we remove noisy pairs based on a simple yet effective similarity entropy calculation.</p><formula xml:id="formula_2">Formally, given D cap = {I i , {s i j }, {w i k }} |D cap | i , we obtain M initial proposals {p m } M m=1</formula><p>with the highest objectness score from the RPN, where p m ? R 4 is the coordinates of the m-th proposal. We then extract the visual features {V m } M m=1 of these proposals from the image encoder. In addition, we extract W augmented text embeddings {T w } W w=1 ? T aug of the base class concepts C B and the concepts {w i k } K k=1 of the caption corresponding to the image I i . We first calculate the semantic similarity SC m,w between the visual feature of each proposal and the augmented text embedding of each concept based on cosine similarity and objectness score:</p><formula xml:id="formula_3">SC m,w = (V m ? T w )/( V m T w ) ? Obj m ,<label>(2)</label></formula><p>where V m is the visual feature of the m-th proposal and T w is the text embedding of the w-th concept and Obj m denotes the objectness score of the m-th proposal computed by RPN. The semantic similarity relationship SC m,w is then normalized by the softmax function to obtain the similarity entropy for the proposal P m , which can be formulated as</p><formula xml:id="formula_4">E m = ? W w=1 SC m,w ? log(SC m,w ).<label>(3)</label></formula><p>We then adopt a threshold ? se to filter out confusing (i.e., high-entropy) proposals. The remaining proposals, whose features are {V r } R r=1 , thus have similarity entropy lower than the threshold. Next, we match each concept with proposals of high confidence to form positive proposal-level visionlanguage pairs. For each concept w i k , proposals with top-3 scores SC r,k are selected. Then, the vision-language pairs are constrained by a threshold ? sc k : if scores of all proposals paired with concept w k are below the ? sc k , it means the concept is unable to match the proposal. Delete these concepts in this iteration, and {{V j } 3 j=1 , T q } Q q=1 pairs are filtered (Q is the number of retained concepts). Merge Fragment Proposals by Spatial Relationship. The above scheme can remove a large number of negative proposal-concept pairs, but some fragment proposals that merely contain part of an object still exist. To further eliminate them, we propose proposal merging on spatial relationships: for each concept T q , we compute the IoU (intersection over union) between two matched proposals P i and P j to evaluate their spatial similarity as IoU i,j . If IoU i,j is larger than ? iou , two proposals are merged into the larger one; otherwise we keep them and repeat these steps until no proposals are merged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning MEDet</head><p>Baseline. For the detection data D det (base categories only), we apply it in a traditional manner L det = L rpn + L cls + L reg . L rpn denotes the constraints (i.e., objectness loss and localization loss) for RPN. After obtaining the features of all proposals, the detector classifies them into base categories with L cls and constrains them into ground truth bounding boxes with the regression loss L reg .</p><p>For the caption data D cap , we apply the binary cross-entropy (BCE) loss to learn a coarse visionlanguage space in the baseline. Given the embeddings of a caption T cap by a text encoder and the image features V img by the image encoder of the detector, we have</p><formula xml:id="formula_5">L cap = i BCE(V img i , T cap i ).</formula><p>Reinforced Vision-Language Contrastive Learning. Once obtaining proposal-level visionlanguage pairs of high confidence via OPM, i.e., {{V j } J j=1 , T q } Q q=1 , J ? 3, a common way to align these pairs is through the point-to-point contrastive loss <ref type="bibr" target="#b50">[51]</ref>. However, directly adopting this loss could hurt the performance, because there are potential mismatched pairs, and it may force the similarity of error pairs to be higher than any other pairs. As such, we regard the {{V j } J j=1 , T q } Q q=1 as visual feature sets V and text feature sets T, where we can naturally adopt the recurrent attention memory based contrastive learning (RAMCL) <ref type="bibr">[5]</ref> to mitigate the negative impact from mismatched point-to-point pairs. Specifically, within each batch, RAMCL takes V b and T b from the same image to form the positive set pair, and regards the feature sets (V b , T b ) from different images (b = b ) as the negative set pairs. After that, RAMCL augments feature sets with recurrent cross-modal attention and evaluates similarities of set pairs in each recurrent step. The final loss is formulated as</p><formula xml:id="formula_6">L ramcl = B b=1 [? ? F (V b , T b ) + F (V b , T b * )] + + B b=1 [? ? F (V b , T b ) + F (V b * , T b )] + ,<label>(4)</label></formula><p>where B is the batch size and ? is the margin. F (?, ?) calculates the vision-language similarity of a set pair, which considers all the proposal-concept matching situations in the set pair. The hard negative pair that has the largest similarity value among negative pairs is denoted by b * .  <ref type="figure" target="#fig_11">Figure 3</ref>: The causal intervention for proposallevel vision-language predictions in OVD task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Class</head><p>Given the biased data distribution of base and novel categories in D det and D cap , the optimization can be inevitably biased toward C B . But we cannot apply existing debiasing methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b20">21]</ref>, under the setting of OVD, due to the lack of important information of C N (e.g., class name or frequency). Inspired by causal learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50]</ref>, we rethink the challenge from the view of causal intervention <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> and present a simple yet effective debiasing method, namely class-wise backdoor adjustment (CBA). <ref type="figure">Fig. 3</ref> shows our causal graph and the intervened graph for OVD task. X is vision-language data (e.g., concepts and images). M is the confounder factor behind X (e.g., the distribution of vision-language pairs) and tells us how positive is the relation between one visual instance and one textual concept. R denotes the proposal-level feature from M and X. Y is the prediction on X. M ? X indicates that X is the causality instance of M . M ? R ? X means that the region-level feature R is generated given X and M . R ? Y ? X implies that the output Y depends on X and R. With the causal graph, it's arguable that the underconfidence issue in X ? Y has its root in the biased distribution M . Even if vision-language pairs for novel categories in X have nothing to do with base categories, the backdoor path X ? M ? R ? Y can still increase the prediction confidences of base classes and lead to outputs with under-confidence for novel classes vice versa. Thereby, for more unbiased predictions, we should block the backdoor path X ? M ? R ? Y . For further details, please refer to the appendix. To this end, we implement a backdoor adjustment <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12]</ref> as follows:</p><formula xml:id="formula_7">P (Y |X = x) ? P (Y |do(X = x 0 )) = P (Y |X = x) ? P (Y |M ) = W T ? ?(x) ? ? ? ?,<label>(5)</label></formula><p>where P (Y |do(X = x 0 )) ? R |C| (C is the class set) is the bias in M and x 0 means a null input. W is the weight of the text-based classifier. ?(?) is the vision feature for each proposal. ? ? R |C| is the class-wise confounder embedding ( ? |C| = 1 for back-ground). ? is a hyper-parameter. Intuitively, for the class with higher distribution, the model has more training data and the optimized model suffers more from over-confidence. Thus, we need stronger debiasing adjustment correspondingly.</p><p>As for the confounder factor M , it's hard to obtain the exact distribution P (Y |M ). Motivated by Zhang et al. <ref type="bibr" target="#b49">[50]</ref>, we resort to the density of class prototype to estimate the confounder factor and obtain the class-specific confounder embedding ?. First, given the names of C B and C N in the inference, we calculate text embeddings of class names as the weights of the text-based classifier W for C B and C N . Then we conduct inference on the training dataset. Next, we extract visual features of high-confidence proposals. Eventually, we conduct a density-based clustering <ref type="bibr" target="#b35">[36]</ref> to obtain class-wise prototypes for both base and novel categories, on which we estimate the class-wise density for ?. Note that we do not conduct any optimization, so our CBA is flexible for any C N .  <ref type="bibr" target="#b21">[22]</ref> and LVIS <ref type="bibr" target="#b14">[15]</ref>. COCO Caption <ref type="bibr">[6]</ref> and Conceptual Caption (CC) <ref type="bibr" target="#b37">[38]</ref> are used respectively for OVD on COCO and LVIS to learn large vocabulary of concepts D cap . COCO Caption has the same images and train/test split as the COCO Object dataset, which has 118, 287 images and 5? captions. We parse the captions by Scene-Graph-Parser <ref type="bibr" target="#b36">[37]</ref> and get 62, 628 noun concepts for COCO Caption and 49, 010 noun concepts for CC. On COCO, we follow the data split of <ref type="bibr" target="#b48">[49]</ref> with 48 base categories C B and 17 novel categories C N , which are subsets of 80 COCO object classes. The rest 15 categories C R are also evaluated in our experiments. On LVIS, following <ref type="bibr" target="#b13">[14]</ref>, we use the training/validation images and adopt the category split with 866 base categories (common and frequent objects) and 337 novel categories (rare objects). We adopt the standard object detection metrics: mean Average Precision (mAP) and AP50 (AP at an intersection over union of 0.5). On COCO, we report AP50 and use text embeddings of C B and C N as classifier weights to mimic the generalized setting <ref type="bibr" target="#b33">[34]</ref>. On LVIS, we use a standard class-agnostic mask head <ref type="bibr" target="#b15">[16]</ref> to produce segmentation masks for boxes. Following <ref type="bibr" target="#b52">[53]</ref>, the mask mAPs for novel categories and all categories are used for evaluation.</p><p>Implementation Details. We use the CLIP text-encoder <ref type="bibr" target="#b31">[32]</ref> based on the ViT-B-32 <ref type="bibr">[7]</ref> to convert concepts to text embeddings. For COCO, we use Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> with the RN50-C4 configuration and train 40, 000 iteration of batch size 4 for the detection data and batch size 16 for the caption data on eight V100 GPUs. The learning rate is initially 0.02 and multiplied by 0.1 at the 25, 000 and 35, 000 steps. For LVIS, following <ref type="bibr" target="#b52">[53]</ref>, we use centerNet2 <ref type="bibr" target="#b51">[52]</ref> with the RN50-FPN architecture and the same data augmentation and learning schedules. All models are pre-trained for 30, 000 iteration on the detection dataset and the caption dataset using only detection loss L det and caption loss L cap .</p><p>In OPM, we choose the image-size proposal as a benchmark. An image-size proposal contain all objects, therefore we set the ? se as the similarity entropy of the image-size proposal. The same, the similarity between the image-size proposal and the k-th concept is set as the threshold ? sc k . For merging fragmented proposals, we set ? iou to 0.6. And set the margin ? of L ramcl to 0.2. In CBA, we set ? as 0.4. For the empirical results of various ? and the optimal one, please refer to Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>OVD on COCO. The comparisons of OVD results on the COCO dataset are shown in Tab. 1. 'Base-only' means training Faster R-CNN only on the detection data of base categories, and using text embeddings of class names from CLIP to replace the classifier's weights. Compared with weakly supervised methods such as WSDDN <ref type="bibr">[2]</ref> and Cap2Det <ref type="bibr" target="#b46">[47]</ref>, and zero-shot methods PL <ref type="bibr" target="#b32">[33]</ref>, our MEDet obtains a significant improvement on all metrics. Compared with the OVD method OVR-CNN <ref type="bibr" target="#b48">[49]</ref>, our MEDet also demonstrates superiority (e.g., 32.6% vs. 22.8% on C N ).</p><p>As for the performance on novel categories C N , the core in OVD task: even using a weaker configuration, the proposed MEDet outperforms other methods. ViLD <ref type="bibr" target="#b13">[14]</ref> adopts advanced training strategies (e.g., model distillation, model ensemble, and data augmentation), yet MEDet is substantially better on C N (32.6% vs. 27.6%) and still competitive on C B with a weaker backbone (RN50-C4 vs. RN50-FPN) and a simple training strategy. RegionCLIP <ref type="bibr" target="#b50">[51]</ref> uses a stronger RPN pre-trained on the large box-supervision dataset LVIS for proposal-concept pairs and initialize the detector, yet our MEDet is higher on C N by 5.8%. Detic <ref type="bibr" target="#b52">[53]</ref> labels the caption data with all the 80 categories in COCO, even with the annotated data of novel class in the training, yet the proposed method achieves higher results on all metrics (e.g., 32.6% vs. 29.1% on C N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Retain Novel All  OVD Generalization of MEDet. Besides C B and C N , we also analyze the performance on the retained 15 classes C R in COCO dataset <ref type="bibr">[1]</ref> to further investigate the generalization ability of OVD models. Tab. 2 shows the results. Note that, when using COCO Caption, Detic <ref type="bibr" target="#b52">[53]</ref> requires a handcrafted taxonomy to map each concept to one class in a set C T . 'Detic-80' uses C B ? C N ? C R (80 classes) as C T and works well on C N . But when C T is C B ? C N (65 classes), 'Detic-65' performs worse in novel categories. In contrast, our MEDet works better on both C N and C R . The results ensure that the proposed method utilizes the caption dataset more effectively.  OVD on LVIS. To further check the competitiveness of our method, we conduct experiments on the LVIS benchmark <ref type="bibr" target="#b14">[15]</ref>. Tab. 3 shows the comparison of OVD task. As we observe that our MEDet obtains significant improvements on the 337 novel categories C N (22.4% vs. 16.8%, 17.1%, and 19.8%) using only 1.1M samples from the CC dataset, which is only half the amount of data used by other methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr">9]</ref>. Besides, our method also outperforms Detic (22.4% vs. 21.0%) without requiring one hand-crafted taxonomy or additional labels. The comparisons verify the effectiveness and flexibility of the proposed MEDet in OVD scenario.  <ref type="table" target="#tab_9">Table 4</ref>: (a) Ablation studies of MEDet. The Line2-4 show the increase in accuracy after the addition of our proposed OPM, CAug, and CBA modules. For 'MEDet w/o L ramcl ' we use a cross-modal attention used in OVR <ref type="bibr" target="#b48">[49]</ref> to learn the vision-language knowledge. (b) Comparison of different strategies in OPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies and Investigations</head><p>Ablation Studies of MEDet. Tab. 4a shows the ablation studies of MEDet. The pre-trained model only uses constraints L det and L cap , and is trained 30, 000 steps without Online Proposal Mining (OPM) and Class-wise Backdoor Adjustment (CBA). When we apply OPM (w/o CAug), the AP50 on novel categories reaches 31.0% (+13.4%). The performance is also higher than other OVD methods in Tab. 1. It means that OPM explores proposal-level vision-language knowledge more effectively. We add the Concept Augmentation (CAug) into OPM to effectively consider the semantic relationship among different concepts and the image embedding to adapt the text embeddings online. Then, the result on novel categories is increased by 1.0%. In the fourth row, the CBA handles the confidence bias and thus boosts the performance on novel categories by 0.6%. Lastly, we replace the vision-language contrastive learning method <ref type="bibr">[5]</ref> (i.e., Eq. 4) with a common grounding loss used by <ref type="bibr" target="#b48">[49]</ref> to verify the rationality of L ramcl . Although the performance on novel categories drops by 1.8% (32.6% vs. 30.8%), it is also competitive compared with the other methods in Tab. 1. Analysis of Online Proposal Mining. In Tab. 4b, we remove CAug and CBA to verify the rationality of OPM. Rows 1-3 show the results of different strategies for selecting proposals. For each concept:  'TOP-1 Sim' selects the proposal with the highest cosine similarity score; 'TOP-1 SC' selects the proposal with the highest SC score (i.e., Eq. 2); and 'TOP-100 Obj' uses proposals of the top-100 objectness scores to align with concepts. The results show that our proposed OPM can select more effective proposal-concept pairs. Rows 4-6 show how different steps in OPM affect the performance on novel categories. Without using the similarity entropy to remove some noisy pairs, the AP50 of novel categories decreases by 0.8% (30.2% vs. 31.0%). Disabling merging fragmented proposals may lose some large proposals, thus the performance decreases by 0.6% (30.4% vs. 31.0%).</p><p>Qualitative Results of OPM. <ref type="figure" target="#fig_4">Fig. 4b</ref> shows the result after proposal filtering via similarity entropy. The gray proposals are obtained according to the top-100 objectness scores from RPN. The green proposals are retained after removing incorrect proposals. <ref type="figure" target="#fig_4">Fig. 4c</ref> shows proposals of the TOP-3 SC scores for every concept. Actually, we can select proposals for objects of different sizes and types. <ref type="figure" target="#fig_4">Fig. 4d</ref> shows the refined proposals after merging fragmented proposals. In a word, our OPM removes the most noise and obtains more reliable proposal-level vision-language knowledge.  Different ? in CBA. To investigate the optimal ? of Eq. 5 in CBA, we analyze the OVD performance with varied ?. The AP50 results on COCO dataset <ref type="bibr">[1]</ref> are listed in Tab. 5. The observations show the optimal ? and indicate that the positive effect of CBA is relatively insensitive to ?.</p><p>class id of the 65 categories beta in CBA Visualization of ? in CBA. Recall that in Eq. 5, a higher confounder value goes with a stronger debiasing effect. And we should conduct lower adjustment on the classes, where the model generates under-confident outputs, to boost the result. <ref type="figure" target="#fig_6">Fig. 5</ref> provides supportive cues. For the classes that the model achieves sub-optimal performance (e.g., umbrella (id 21, 9.0% AP50, ? = 9.01) and scissors (id 63, 4.5% AP50, ? = 4.93)), the confounder values are relatively lower and vice versa (e.g., person (id 0, 76.8% AP50, ? = 23.9)). Despite that the estimated ? contains some statistical error, it does not influence the overall observation.</p><p>Limitations. We make two assumptions about the underlying models: (1) the pre-trained VLMs have obtained sufficient knowledge to facilitate OVD tasks; (2) the RPN optimized with D det is effective in generating proposals for novel categories. But, the image-level pre-training may not be effective in acquiring the necessary proposal-level knowledge for detection, and the RPN trained for C B may not always produce high-quality proposals for C N . We leave the intriguing challenges to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present MEDet, a novel and effective framework to solve the two observable challenges for better OVD results. To sharpen the vision-language knowledge of caption datasets from image-level to proposal-level and learn an unbounded vocabulary of concepts, we devise an online proposal mining scheme in an end-to-end network to fully explore the knowledge of caption data. Moreover, we propose a class-wise backdoor adjustment to generate accurate predictions on both base and novel categories with less bias. Experiments on popular COCO and LVIS benchmarks verify that our MEDet outperforms the counterpart approaches in detecting objects of novel categories. For future work, we can design powerful detector or optimize a stronger RPN to generate higher quality for vision-language pairs to further improve the generalization of new categories.</p><p>In the appendix, we provide additional information for the readers to better understand the proposed MEDet and its effectiveness in OVD scenario as follows:</p><p>? In Sec. A, we introduce additional explanations about class-wise backdoor adjustment (CBA) for better understanding on the causal graph and the estimation procedure of ?. ? In Sec. B, we show further results: the experiment of tuning ? iou ; the qualitative results of SC m,w and CBA; and the visualization of textual embeddings with the proposed concept augmentation (CAug); and the qualitative results of the overall comparisons between our method and the state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Description of MEDet</head><p>We give further explanations of the causal graph and the estimation procedure for an approximate ?. The causal graph. Following the practice in causal graph <ref type="bibr">[6,</ref><ref type="bibr">4,</ref><ref type="bibr">9]</ref>, the direct links in <ref type="figure" target="#fig_7">Fig. A</ref> indicate the causalities between cause and effect. The prior distribution M is the causality of the vision-language data X and the region-level feature R; the output Y is the effect given X and R.</p><p>The causal intervention. Given the biased distribution in M and the causality link M ? X, the inference process X ? Y can be also biased toward base or frequent classes and results in sub-optimal predictions. Considering a causal graph knowledge (X) ? age (M ) ? cancer (Y ), the elder people generally have more knowledge about the world and they are also more vulnerable to get cancer, then the confounder age creates a spurious correlation that more knowledge will increase the chance of getting cancer. In our case, there is also spurious correlation between the input data X and the output Y by the prior class-wise distribution M : for each class, higher prior distribution indicates that the model has more training instances, and then the optimized model tends to produce predictions with higher confidence, even if the input X has nothing to do with the class. To obtain less biased outputs Y , we block the backdoor path <ref type="bibr">[4,</ref><ref type="bibr">2]</ref> </p><formula xml:id="formula_8">X ? M ? R ? Y : P (Y |X = x) ? P (Y |M ).</formula><p>The estimation procedure of ?. In order to implement the CBA, we should figure out how to estimate P (Y |M ), i.e., the class prior distribution. In this paper, we gauge an approximate value of P (Y |M ) based on a density-based clustering method <ref type="bibr">[5]</ref>. Basically, the class prior distribution is positively correlated with the density of class-wise clustering structure in feature space. Thus, we resort to the density on class-specific prototypes to gauge P (Y |M ).  Qualitative Results of SC m,w . We leverage both the cosine similarity and the objectness score to measure the correlation of proposals and concepts. At the early stage of OVD training, the cosine  similarity of low-quality proposals may be even higher than high-quality ones because of the lack of proposal-level knowledge in the pre-trained model, as shown in <ref type="figure" target="#fig_7">Fig. Ba</ref>. Thus, we jointly use the objectness score as references, since proposals are more likely to be correct and effective when their similarities with concept and objectness scores are high at the same time, as shown in <ref type="figure" target="#fig_9">Fig. Bb.</ref> (a) (b) <ref type="figure" target="#fig_11">Figure C</ref>: The AP, AR, and ? for each category on COCO dataset <ref type="bibr">[1]</ref> when (a) we don't apply CAB or (b) we apply CBA. The results indicate the positive contribution from CBA to the OVD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Class-wise Backdoor Adjustment</head><p>To further understand the rationale and effectiveness of the proposed CBA, we analysis the corresponding AR, AP, and estimated ? for each class. The results are shown in <ref type="figure">Fig. C</ref>. As we observe that: for the class with lower ? (i.e., prior distribution), such as umbrella and scissors, the optimized model generates under-confident predictions, and generally has sub-optimal AP or AR performance, thus we should conduct less debiasing adjustment on it. On the other hand, for the class with higher ?, such as person, the model produces over-confidence outputs, and thus has higher AR performance but inferior AP performance. So we need to conduct more debiasing adjustment on it. And with the help of our CBA, we handle the confidence bias well and promote the overall OVD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Concept Augmentation</head><p>To gain deeper insight about our concept augmentation (CAug), we depict the augmented text embedding space of both base and novel categories in <ref type="figure">Fig. D</ref> by using t-SNE <ref type="bibr">[7]</ref>. Specifically, in <ref type="figure" target="#fig_7">Fig. Da</ref>, we visualize the overall text embeddings of 65 COCO categories generated by CAug on the whole validation dataset and the fixed CLIP text encoder, respectively. The augmented text   MEDet can capture some small objects, such as the "sports ball" in first figure. Our methods also can recognize more novel categories, for example the "cat" and "cup" in second image, the "dog" in third and fourth images and the "skateboard" in last image. It means that our method explores the knowledge of caption dataset fully and learns a more abundant vocabulary of concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>proposali), (loaf, proposalj), ?? The framework of our MEDet. With a mini-batch of data from both the detection dataset and the image-caption dataset, MEDet jointly trains object detection (blue part) and learns rich and complete proposallevel vision-language knowledge by conducting online proposal mining (orange part) in an end-to-end manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of each step in OPM. (b) Filter top-100 proposals from RPN by similarity entropy, where green and gray boxes are preserved and removed proposals, respectively. (c) Select proposals with top-3 SC scores for every concept. (d) Merge proposals to eliminate fragmented proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The estimated ? for CBA. The results are gauged on the training dataset of COCO[1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A :</head><label>A</label><figDesc>The causal intervention for proposallevel vision-language predictions in OVD task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )</head><label>a</label><figDesc>Cosine similarity only. (b) Cosine similarity and objectness score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure B :</head><label>B</label><figDesc>Qualitative results of SCm,w.The gray boxes are the RPN top-10. The red/yellow/green boxes denote the top-3 proposals, respectively. The numbers are the cosine similarity between proposals and concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) The overall distributions of text embeddings of 65 COCO categories generated by the concept augmentation on the whole validation dataset and the fixed CLIP text encoder, respectively. (b) -(d) The adaptation of the text embeddings generated by the concept augmentation in three validation images according to the respective vision contents. Points with black edges denote CLIP text embeddings. Points without black edges are our augmented text embeddings. embeddings expand around the initial CLIP embeddings, maintaining the discrimination capability. And meanwhile, they dynamically adapt to each image to fit the visual appearance diversity[3]  of each category. Additionally, fromFig. Db to Fig. Dd, we compare the distributions of text embeddings generated by CAug and the CLIP text encoder respectively in a single image. CAug smoothly pulls the embedding points of categories apart from each other according to the image content.B.4 OVD Qualitative Results(a) Qualitative Results of OVR-CNN [8].(b) Qualitative Results of Detic [10]. (c) Qualitative Results of MEDet (Ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure E :</head><label>E</label><figDesc>Qualitative results of OVD methods. Our MEDet can detect smaller object and more novel categories, such as sports ball, cup, cat, dog and skateboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. E shows</head><label></label><figDesc>the qualitative results of different OVD methods. Compared with other methods, Our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>?0.4 | 29.1 52.0 ?0.4 | 52.4 45.9 ?0.3 | 46.2 ?0.4 | 32.6 53.3 ?0.2 | 53.5 47.8 ?0.2 | 48.0 Results (mean ? variation | best) of OVD on COCO dataset [1]. MEDet outperforms the others on the Novel categories. The results of MEDet are the average of five experiments.</figDesc><table><row><cell>Method</cell><cell>Detector Training Backbone RPN pretrained</cell><cell>Novel</cell><cell>COCO Generalized (48+17) Base</cell><cell>All</cell></row><row><cell cols="2">Base-only (CLIP) WSDDN [2] Cap2Det [47] PL [33] OVR-CNN [49] HierKD [25] ViLD [14] RegionCLIP [51] Detic [53] (reproduced) RN50-C4 COCO Base (48) 28.7 MEDet (Ours) ------RN50-FPN COCO Base (48) RN50-C4 COCO Base (48) RN50-C4 COCO Base (48) RN50-FPN COCO Base (48) RN50-C4 LVIS (1203) RN50-C4 COCO Base (48) 32.2 4 Experiments</cell><cell>1.3 20.5 20.3 4.12 22.8 20.3 27.6 26.8</cell><cell>48.7 23.4 20.1 35.9 46.0 51.3 59.5 54.8</cell><cell>39.3 24.6 20.1 27.9 39.9 43.2 51.3 47.5</cell></row><row><cell>4.1 Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Datasets &amp; Metric. We evaluate our method on two standard open-vocabulary detection benchmarks modified from COCO</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Generalization on novel class.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results on LVIS [15] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The results of various ? in CBA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A :</head><label>A</label><figDesc>The results of various ? iou in OPM Different ? iou in OPM. To investigate the ? iou in OPM, we analyze the AP50 results on COCO dataset with varied ?</figDesc><table /><note>iou in Tab. A. The observations show that a small threshold (? iou = 0.2) cause too many proposals to be merged and the accuracy is reduced. And a high threshold (? iou = 0.8) makes only a few (or no) proposal be merged.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image-level or object-level? a tale of two resampling strategies for long-tailed detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to prompt for openvocabulary object detection with vision-language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14095</idno>
		<title level="m">Pyramidclip: Hierarchical feature alignment for vision-language model pretraining</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolox</surname></persName>
		</author>
		<title level="m">Exceeding yolo series in 2021. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A cause-effect look at language bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On model calibration for long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Models, reasoning and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>CambridgeUniversityPress</publisher>
			<biblScope unit="volume">19</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Clustering by fast search and find of density peaks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Laio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">344</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth workshop on vision and language</title>
		<meeting>the fourth workshop on vision and language</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Equalization loss v2: A new gradient balance approach for long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Zsd-yolo: Zero-shot yolo detection using vision-language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12066</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deconfounded image captioning: A causal retrospect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cap2det: Learning to amplify weak caption supervision for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Berent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Counterfactual zero-shot and open-set visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Xian-Sheng Hua, and Qianru Sun. Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09106</idno>
		<title level="m">Region-based language-image pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Detecting twentythousand classes using image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02605</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.03340</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Prompt distribution learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Clustering by fast search and find of density peaks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Laio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">344</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Xian-Sheng Hua, and Qianru Sun. Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Detecting twentythousand classes using image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02605</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
