<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
							<email>balazs.hidasi@gravityrd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
							<email>lbaltrunas@netflix.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
							<email>domonkos.tikk@gravityrd.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Gravity R&amp;D Inc. Budapest</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Telefonica Research Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Netflix Los Gatos</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Gravity R&amp;D Inc. Budapest</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNNbased approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches. * The author spent 3 months at Telefonica Research during the research of this topic. ? This work was done while the author was a member</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Session-based recommendation is a relatively unappreciated problem in the machine learning and recommender systems community. Many e-commerce recommender systems (particularly those of small retailers) and most of news and media sites do not typically track the user-id's of the users that visit their sites over a long period of time. While cookies and browser fingerprinting can provide some level of user recognizability, those technologies are often not reliable enough and moreover raise privacy concerns. Even if tracking is possible, lots of users have only one or two sessions on a smaller e-commerce site, and in certain domains (e.g. classified sites) the behavior of users often shows session-based traits. Thus subsequent sessions of the same user should be handled independently. Consequently, most session-based recommendation systems deployed for e-commerce are based on relatively simple methods that do not make use of a user profile e.g. itemto-item similarity, co-occurrence, or transition probabilities. While effective, those methods often take only the last click or selection of the user into account ignoring the information of past clicks.</p><p>The most common methods used in recommender systems are factor models <ref type="bibr" target="#b8">(Koren et al., 2009;</ref><ref type="bibr" target="#b20">Weimer et al., 2007;</ref><ref type="bibr" target="#b4">Hidasi &amp; Tikk, 2012)</ref> and neighborhood methods <ref type="bibr" target="#b14">(Sarwar et al., 2001;</ref><ref type="bibr" target="#b7">Koren, 2008)</ref>. Factor models work by decomposing the sparse user-item interactions matrix to a set of d dimensional vectors one for each item and user in the dataset. The recommendation problem is then treated as a matrix completion/reconstruction problem whereby the latent factor vectors are then used to fill the missing entries by e.g. taking the dot product of the corresponding user-item latent factors. Factor models are hard to apply in session-based recommendation due to the absence Published as a conference paper at ICLR 2016 of a user profile. On the other hand, neighborhood methods, which rely on computing similarities between items (or users) are based on co-occurrences of items in sessions (or user profiles). Neighborhood methods have been used extensively in session-based recommendations.</p><p>The past few years have seen the tremendous success of deep neural networks in a number of tasks such as image and speech recognition <ref type="bibr" target="#b12">(Russakovsky et al., 2014;</ref><ref type="bibr" target="#b6">Hinton et al., 2012)</ref> where unstructured data is processed through several convolutional and standard layers of (usually rectified linear) units. Sequential data modeling has recently also attracted a lot of attention with various flavors of RNNs being the model of choice for this type of data. Applications of sequence modeling range from test-translation to conversation modeling to image captioning.</p><p>While RNNs have been applied to the aforementioned domains with remarkable success little attention, has been paid to the area of recommender systems. In this work we argue that RNNs can be applied to session-based recommendation with remarkable results, we deal with the issues that arise when modeling such sparse sequential data and also adapt the RNN models to the recommender setting by introducing a new ranking loss function suited to the task of training these models. The session-based recommendation problem shares some similarities with some NLP-related problems in terms of modeling as long as they both deals with sequences. In the session-based recommendation we can consider the first item a user clicks when entering a web-site as the initial input of the RNN, we then would like to query the model based on this initial input for a recommendation. Each consecutive click of the user will then produce an output (a recommendation) that depends on all the previous clicks. Typically the item-set to choose from in recommenders systems can be in the tens of thousands or even hundreds of thousands. Apart from the large size of the item set, another challenge is that click-stream datasets are typically quite large thus training time and scalability are really important. As in most information retrieval and recommendation settings, we are interested in focusing the modeling power on the top-items that the user might be interested in, to this end we use ranking loss function to train the RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SESSION-BASED RECOMMENDATION</head><p>Much of the work in the area of recommender systems has focused on models that work when a user identifier is available and a clear user profile can be built. In this setting, matrix factorization methods and neighborhood models have dominated the literature and are also employed on-line. One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user profile is the item-to-item recommendation approach <ref type="bibr" target="#b14">(Sarwar et al., 2001;</ref><ref type="bibr" target="#b9">Linden et al., 2003)</ref> in this setting an item to item similarity matrix is precomputed from the available session data, that is items that are often clicked together in sessions are deemed to be similar. This similarity matrix is then simply used during the session to recommend the most similar items to the one the user has currently clicked. While simple, this method has been proven to be effective and is widely employed. While effective, these methods are only taking into account the last click of the user, in effect ignoring the information of the past clicks.</p><p>A somewhat different approach to session-based recommendation are Markov Decision Processes (MDPs) <ref type="bibr">(Shani et al., 2002)</ref>. MDPs are models of sequential stochastic decision problems. An MDP is defined as a four-tuple S, A, Rwd, tr where S is the set of states, A is a set of actions Rwd is a reward function and tr is the state-transition function. In recommender systems actions can be equated with recommendations and the simplest MPDs are essentially first order Markov chains where the next recommendation can be simply computed on the basis of the transition probability between items. The main issue with applying Markov chains in session-based recommendation is that the state space quickly becomes unmanageable when trying to include all possible sequences of user selections.</p><p>The extended version of the General Factorization Framework (GFF) <ref type="bibr" target="#b5">(Hidasi &amp; Tikk, 2015)</ref> is capable of using session data for recommendations. It models a session by the sum of its events. It uses two kinds of latent representations for items, one represents the item itself, the other is for representing the item as part of a session. The session is then represented as the average of the feature vectors of part-of-a-session item representation. However, this approach does not consider any ordering within the session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DEEP LEARNING IN RECOMMENDERS</head><p>One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines (RBM) for Collaborative Filtering <ref type="bibr" target="#b13">(Salakhutdinov et al., 2007)</ref>. In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In <ref type="bibr" target="#b18">Van den Oord et al. (2013)</ref> a convolutional deep network is used to extract feature from music files that are then used in a factor model. More recently <ref type="bibr" target="#b19">Wang et al. (2015)</ref> introduced a more generic approach whereby a deep network is used to extract generic content-features from any types of items, these features are then incorporated in a standard collaborative filtering model to enhance the recommendation performance. This approach seems to be particularly useful in settings where there is not sufficient user-item interaction information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RECOMMENDATIONS WITH RNNS</head><p>Recurrent Neural Networks have been devised to model variable-length sequence data. The main difference between RNNs and conventional feedforward deep models is the existence of an internal hidden state in the units that compose the network. Standard RNNs update their hidden state h using the following update function:</p><formula xml:id="formula_0">h t = g(W x t + U h t?1 )</formula><p>(1) Where g is a smooth and bounded function such as a logistic sigmoid function x t is the input of the unit at time t. An RNN outputs a probability distribution over the next element of the sequence, given its current state h t .</p><p>A Gated Recurrent Unit (GRU) <ref type="bibr" target="#b0">(Cho et al., 2014)</ref> is a more elaborate model of an RNN unit that aims at dealing with the vanishing gradient problem. GRU gates essentially learn when and by how much to update the hidden state of the unit. The activation of the GRU is a linear interpolation between the previous activation and the candidate activation? t :</p><formula xml:id="formula_1">h t = (1 ? z t )h t?1 + z t?t (2)</formula><p>where the update gate is given by:</p><formula xml:id="formula_2">z t = ?(W z x t + U z h t?1 )<label>(3)</label></formula><p>while the candidate activation function? t is computed in a similar manner:</p><formula xml:id="formula_3">h t = tanh (W x t + U (r t h t?1 ))<label>(4)</label></formula><p>and finaly the reset gate r t is given by:</p><formula xml:id="formula_4">r t = ?(W r x t + U r h t?1 )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CUSTOMIZING THE GRU MODEL</head><p>We used the GRU-based RNN in our models for session-based recommendations. The input of the network is the actual state of the session while the output is the item of the next event in the session. The state of the session can either be the item of the actual event or the events in the session so far. In the former case 1-of-N encoding is used, i.e. the input vector's length equals to the number of items and only the coordinate corresponding to the active item is one, the others are zeros. The latter setting uses a weighted sum of these representations, in which events are discounted if they have occurred earlier. For the stake of stability, the input vector is then normalized. We expect this to help because it reinforces the memory effect: the reinforcement of very local ordering constraints which are not well captured by the longer memory of RNN. We also experimented with adding an additional embedding layer, but the 1-of-N encoding always performed better.</p><p>The core of the network is the GRU layer(s) and additional feedforward layers can be added between the last layer and the output. The output is the predicted preference of the items, i.e. the likelihood of being the next in the session for each item. When multiple GRU layers are used, the hidden state of the previous layer is the input of the next one. The input can also be optionally connected to GRU layers deeper in the network, as we found that this improves performance. See the whole architecture on <ref type="figure" target="#fig_0">Figure 1</ref>, which depicts the representation of a single event within a time series of events.</p><p>Since recommender systems are not the primary application area of recurrent neural networks, we modified the base network to better suit the task. We also considered practical points so that our solution could be possibly applied in a live environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">SESSION-PARALLEL MINI-BATCHES</head><p>RNNs for natural language processing tasks usually use in-sequence mini-batches. For example it is common to use a sliding window over the words of sentences and put these windowed fragments next to each other to form mini-batches. This does not fit our task, because (1) the length of sessions can be very different, even more so than that of sentences: some sessions consist of only 2 events, while others may range over a few hundreds;</p><p>(2) our goal is to capture how a session evolves over time, so breaking down into fragments would make no sense. Therefore we use session-parallel mini-batches. First, we create an order for the sessions. Then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs. See <ref type="figure" target="#fig_1">Figure 2</ref> for more details.</p><p>, , Recommender systems are especially useful when the number of items is large. Even for a mediumsized webshop this is in the range of tens of thousands, but on larger sites it is not rare to have hundreds of thousands of items or even a few millions. Calculating a score for each item in each step would make the algorithm scale with the product of the number of items and the number of events. This would be unusable in practice. Therefore we have to sample the output and only compute the score for a small subset of the items. This also entails that only some of the weights will be updated. Besides the desired output, we need to compute scores for some negative examples and modify the weights so that the desired output is highly ranked.</p><formula xml:id="formula_5">Session4 Session5 ? ,<label>, , , , , , , , , , , , , , , , Session1 Session2 Session3</label></formula><p>The natural interpretation of an arbitrary missing event is that the user did not know about the existence of the item and thus there was no interaction. However there is a low probability that the user did know about the item and chose not to interact, because she disliked the item. The more popular the item, the more probable it is that the user knows about it, thus it is more likely that a missing event expresses dislike. Therefore we should sample items in proportion of their popularity. Instead of generating separate samples for each training example, we use the items from the other training examples of the mini-batch as negative examples. The benefit of this approach is that we can further reduce computational times by skipping the sampling. Additionally, there are also benefits on the implementation side from making the code less complex to faster matrix operations. Meanwhile, this approach is also a popularity-based sampling, because the likelihood of an item being in the other training examples of the mini-batch is proportional to its popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">RANKING LOSS</head><p>The core of recommender systems is the relevance-based ranking of items. Although the task can also be interpreted as a classification task, learning-to-rank approaches <ref type="bibr" target="#b11">(Rendle et al., 2009;</ref><ref type="bibr" target="#b16">Shi et al., 2012;</ref><ref type="bibr" target="#b17">Steck, 2015)</ref> generally outperform other approaches. Ranking can be pointwise, pairwise or listwise. Pointwise ranking estimates the score or the rank of items independently of each other and the loss is defined in a way so that the rank of relevant items should be low. Pairwise ranking compares the score or the rank of pairs of a positive and a negative item and the loss enforces that the rank of the positive item should be lower than that of the negative one. Listwise ranking uses the scores and ranks of all items and compares them to the perfect ordering. As it includes sorting, it is usually computationally more expensive and thus not used often. Also, if there is only one relevant item -as in our case -listwise ranking can be solved via pairwise ranking.</p><p>We included several pointwise and pairwise ranking losses into our solution. We found that pointwise ranking was unstable with this network (see Section 4 for more comments). Pairwise ranking losses on the other hand performed well. We use the following two.</p><p>? BPR: Bayesian Personalized Ranking <ref type="bibr" target="#b11">(Rendle et al., 2009</ref>) is a matrix factorization method that uses pairwise ranking loss. It compares the score of a positive and a sampled negative item. Here we compare the score of the positive item with several sampled items and use their average as the loss. The loss at a given point in one session is defined as:</p><formula xml:id="formula_7">L s = ? 1 N S ? N S<label>j=1</label></formula><p>log (? (r s,i ?r s,j )), where N S is the sample size,r s,k is the score on item k at the given point of the session, i is the desired item (next item in the session) and j are the negative samples.</p><p>? TOP1: This ranking loss was devised by us for this task. It is the regularized approximation of the relative rank of the relevant item. The relative rank of the relevant item is given by</p><formula xml:id="formula_8">1 N S ? N S</formula><p>j=1 I{r s,j &gt;r s,i }. We approximate I{?} with a sigmoid. Optimizing for this would modify parameters so that the score for i would be high. However this is unstable as certain positive items also act as negative examples and thus scores tend to become increasingly higher. To avoid this, we want to force the scores of the negative examples to be around zero. This is a natural expectation towards the scores of negative items. Thus we added a regularization term to the loss. It is important that this term is in the same range as the relative rank and acts similarly to it. The final loss function is as follows:</p><formula xml:id="formula_9">L s = 1 N S ? N S j=1 ? (r s,j ?r s,i ) + ? r 2 s,j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the proposed recursive neural network against popular baselines on two datasets.</p><p>The first dataset is that of RecSys Challenge 2015 1 . This dataset contains click-streams of an ecommerce site that sometimes end in purchase events. We work with the training set of the challenge and keep only the click events. We filter out sessions of length 1. The network is trained on ? 6 months of data, containing 7,966,257 sessions of 31,637,239 clicks on 37,483 items. We use the sessions of the subsequent day for testing. Each session is assigned to either the training or the test set, we do not split the data mid-session. Because of the nature of collaborative filtering methods, we filter out clicks from the test set where the item clicked is not in the train set. Sessions of length one are also removed from the test set. After the preprocessing we are left with 15,324 sessions of 71,222 events for the test set. This dataset will be referred to as RSC15.</p><p>The second dataset is collected from a Youtube-like OTT video service platform. Events of watching a video for at least a certain amount of time were collected. Only certain regions were subject to this collection that lasted for somewhat shorter than 2 months. During this time item-to-item recommendations were provided after each video at the left side of the screen. These were provided by a selection of different algorithms and influenced the behavior of the users. Preprocessing steps are similar to that of the other dataset with the addition of filtering out very long sessions as they were probably generated by bots. The training data consists of all but the last day of the aforementioned period and has ? 3 million sessions of ? 13 million watch events on 330 thousand videos. The test set contains the sessions of the last day of the collection period and has ? 37 thousand sessions with ? 180 thousand watch events. This dataset will be referred to as VIDEO.</p><p>The evaluation is done by providing the events of a session one-by-one and checking the rank of the item of the next event. The hidden state of the GRU is reset to zero after a session finishes. Items are ordered in descending order by their score and their position in this list is their rank. With RSC15, all of the 37,483 items of the train set were ranked. However, this would have been impractical with VIDEO, due to the large number of items. There we ranked the desired item against the most popular 30,000 items. This has negligible effect on the evaluations as rarely visited items often get low scores. Also, popularity based pre-filtering is common in practical recommender systems.</p><p>As recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the first few items of the list. Therefore, our primary evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items in all test cases. Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR) <ref type="bibr" target="#b10">(Liu et al., 2012;</ref><ref type="bibr" target="#b4">Hidasi &amp; Tikk, 2012)</ref>. The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g. the lower ranked items are only visible after scrolling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BASELINES</head><p>We compare the proposed network to a set of commonly used baselines.</p><p>? POP: Popularity predictor that always recommends the most popular items of the training set. Despite its simplicity it is often a strong baseline in certain domains. ? S-POP: This baseline recommends the most popular items of the current session. The recommendation list changes during the session as items gain more events. Ties are broken up using global popularity values. This baseline is strong in domains with high repetitiveness. ? Item-KNN: Items similar to the actual item are recommended by this baseline and similarity is defined as the cosine similarity between the vector of their sessions, i.e. it is the number of co-occurrences of two items in sessions divided by the square root of the product of the numbers of sessions in which the individual items are occurred. Regularization is also included to avoid coincidental high similarities of rarely visited items. This baseline is one of the most common item-to-item solutions in practical systems, that provides recommendations in the "others who viewed this item also viewed these ones" setting. Despite of its simplicity it is usually a strong baseline <ref type="bibr" target="#b9">(Linden et al., 2003;</ref><ref type="bibr" target="#b2">Davidson et al., 2010)</ref>.  ? BPR-MF: BPR-MF <ref type="bibr" target="#b11">(Rendle et al., 2009</ref>) is one of the commonly used matrix factorization methods. It optimizes for a pairwise ranking objective function (see Section 3) via SGD. Matrix factorization cannot be applied directly to session-based recommendations, because the new sessions do not have feature vectors precomputed. However we can overcome this by using the average of item feature vectors of the items that had occurred in the session so far as the user feature vector. In other words we average the similarities of the feature vectors between a recommendable item and the items of the session so far. <ref type="table" target="#tab_0">Table 1</ref> shows the results for the baselines. The item-KNN approach clearly dominates the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PARAMETER &amp; STRUCTURE OPTIMIZATION</head><p>We optimized the hyperparameters by running 100 experiments at randomly selected points of the parameter space for each dataset and loss function. The best parametrization was further tuned by individually optimizing each parameter. The number of hidden units was set to 100 in all cases. The best performing parameters were then used with hidden layers of different sizes. The optimization was done on a separate validation set. Then the networks were retrained on the training plus the validation set and evaluated on the final test set.</p><p>The best performing parametrizations are summarized in table 2. Weight matrices were initialized by random numbers drawn uniformly from <ref type="bibr">[?x, x]</ref> where x depends on the number of rows and columns of the matrix. We experimented with both rmsprop <ref type="bibr" target="#b1">(Dauphin et al., 2015)</ref> and adagrad <ref type="bibr" target="#b3">(Duchi et al., 2011)</ref>. We found adagrad to give better results.</p><p>We briefly experimented with other units than GRU. We found both the classic RNN unit and LSTM to perform worse.</p><p>We tried out several loss functions. Pointwise ranking based losses, such as cross-entropy and MRR optimization (as in Steck <ref type="formula" target="#formula_4">(2015)</ref>) were usually unstable, even with regularization. For example cross-entropy yielded only 10 and 6 numerically stable networks of the 100 random runs for RSC15 and VIDEO respectively. We assume that this is due to independently trying to achieve high scores for the desired items and the negative push is small for the negative samples. On the other hand pairwise ranking-based losses performed well. We found the ones introduced in Section 3 (BPR and TOP1) to perform the best.</p><p>Several architectures were examined and a single layer of GRU units was found to be the best performer. Adding addition layers always resulted in worst performance w.r.t. both training loss and recall and MRR measured on the test set. We assume that this is due to the generally short Using embedding of the items gave slightly worse results, therefore we kept the 1-of-N encoding. Also, putting all previous events of the session on the input instead of the preceding one did not result in additional accuracy gain; which is not surprising as GRU -like LSTM -has both long and short term memory. Adding additional feed-forward layers after the GRU layer did not help either. However increasing the size of the GRU layer improved the performance. We also found that it is beneficial to use tanh as the activation function of the output layer. <ref type="table" target="#tab_2">Table 3</ref> shows the results of the best performing networks. Cross-entropy for the VIDEO data with 1000 hidden units was numerically unstable and thus we present no results for that scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS</head><p>The results are compared to the best baseline (item-KNN). We show results with 100 and 1000 hidden units. The running time depends on the parameters and the dataset. Generally speaking the difference in runtime between the smaller and the larger variant is not too high on a GeForce GTX Titan X GPU and the training of the network can be done in a few hours 2 . On CPU, the smaller network can be trained in a practically acceptable timeframe. Frequent retraining is often desirable for recommender systems, because new users and items are introduced frequently.</p><p>The GRU-based approach has substantial gain over the item-KNN in both evaluation metrics on both datasets, even if the number of units is 100 3 . Increasing the number of units further improves the results for pairwise losses, but the accuracy decreases for cross-entropy. Even though cross-entropy gives better results with 100 hidden units, the pairwise loss variants surpass these results as the number of units increase. Although, increasing the number of units increases the training times, we found that it was not too expensive to move from 100 units to 1000 on GPU. Also, the cross-entropy based loss was found to be numerically unstable as the result of the network individually trying to increase the score for the target items, while the negative push is relatively small for the other items. Therefore we suggest using any of the two pairwise losses. The TOP1 loss performs slightly better on these two datasets, resulting in ? 20 ? 30% accuracy gain over the best performing baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION &amp; FUTURE WORK</head><p>In this paper we applied a kind of modern recurrent neural network (GRU) to new application domain: recommender systems. We chose the task of session based recommendations, because it is a practically important area, but not well researched. We modified the basic GRU in order to fit the task better by introducing session-parallel mini-batches, mini-batch based output sampling and ranking loss function. We showed that our method can significantly outperform popular baselines that are used for this task. We think that our work can be the basis of both deep learning applications in recommender systems and session based recommendations in general.</p><p>Our immediate future work will focus on the more thorough examination of the proposed network. We also plan to train the network on automatically extracted item representation that is built on content of the item itself (e.g. thumbnail, video, text) instead of the current input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>General architecture of the network. Processing of one event of the event stream at once.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Session-parallel mini-batch creation 3.1.2 SAMPLING ON THE OUTPUT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Recall@20 and MRR@20 using the baseline methods</figDesc><table><row><cell>Baseline</cell><cell cols="4">RSC15 Recall@20 MRR@20 Recall@20 MRR@20 VIDEO</cell></row><row><cell>POP</cell><cell>0.0050</cell><cell>0.0012</cell><cell>0.0499</cell><cell>0.0117</cell></row><row><cell>S-POP</cell><cell>0.2672</cell><cell>0.1775</cell><cell>0.1301</cell><cell>0.0863</cell></row><row><cell>Item-KNN</cell><cell>0.5065</cell><cell>0.2048</cell><cell>0.5508</cell><cell>0.3381</cell></row><row><cell>BPR-MF</cell><cell>0.2574</cell><cell>0.0618</cell><cell>0.0692</cell><cell>0.0374</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Best parametrizations for datasets/loss functions</figDesc><table><row><cell cols="2">Dataset Loss</cell><cell cols="4">Mini-batch Dropout Learning rate Momentum</cell></row><row><cell>RSC15</cell><cell>TOP1</cell><cell>50</cell><cell>0.5</cell><cell>0.01</cell><cell>0</cell></row><row><cell>RSC15</cell><cell>BPR</cell><cell>50</cell><cell>0.2</cell><cell>0.05</cell><cell>0.2</cell></row><row><cell>RSC15</cell><cell>Cross-entropy</cell><cell>500</cell><cell>0</cell><cell>0.01</cell><cell>0</cell></row><row><cell cols="2">VIDEO TOP1</cell><cell>50</cell><cell>0.4</cell><cell>0.05</cell><cell>0</cell></row><row><cell cols="2">VIDEO BPR</cell><cell>50</cell><cell>0.3</cell><cell>0.1</cell><cell>0</cell></row><row><cell cols="2">VIDEO Cross-entropy</cell><cell>200</cell><cell>0.1</cell><cell>0.05</cell><cell>0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Recall@20 and MRR@20 for different types of a single layer of GRU, compared to the best baseline (item-KNN). Best results per dataset are highlighted. sessions not requiring multiple time scales of different resolutions to be properly represented. However the exact reason of this is unknown as of yet and requires further research.</figDesc><table><row><cell>Loss / #Units</cell><cell>Recall@20</cell><cell cols="2">RSC15</cell><cell>MRR@20</cell><cell>Recall@20</cell><cell cols="2">VIDEO</cell><cell>MRR@20</cell></row><row><cell>TOP1 100</cell><cell cols="2">0.5853 (+15.55%)</cell><cell cols="2">0.2305 (+12.58%)</cell><cell cols="2">0.6141 (+11.50%)</cell><cell>0.3511 (+3.84%)</cell></row><row><cell>BPR 100</cell><cell cols="2">0.6069 (+19.82%)</cell><cell cols="2">0.2407 (+17.54%)</cell><cell cols="2">0.5999 (+8.92%)</cell><cell>0.3260 (-3.56%)</cell></row><row><cell>Cross-entropy 100</cell><cell cols="2">0.6074 (+19.91%)</cell><cell cols="2">0.2430 (+18.65%)</cell><cell cols="2">0.6372 (+15.69%)</cell><cell>0.3720 (+10.04%)</cell></row><row><cell>TOP1 1000</cell><cell cols="7">0.6206 (+22.53%) 0.2693 (+31.49%) 0.6624 (+20.27%) 0.3891 (+15.08%)</cell></row><row><cell>BPR 1000</cell><cell cols="4">0.6322 (+24.82%) 0.2467 (+20.47%)</cell><cell cols="2">0.6311 (+14.58%)</cell><cell>0.3136 (-7.23%)</cell></row><row><cell cols="3">Cross-entropy 1000 0.5777 (+14.06%)</cell><cell cols="2">0.2153 (+5.16%)</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>lifespan of the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://2015.recsyschallenge.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Using Theano with fixes for the subtensor operators on GPU. 3 Except for using the BPR loss on the VIDEO data and evaluating for MRR.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007(FP7/ -2013 under CrowdRec Grant Agreement n ? 610594.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04390</idno>
		<title level="m">Rmsprop and equilibrated adaptive learning rates for non-convex optimization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The YouTube video recommendation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liebald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junning</surname></persName>
		</author>
		<idno>978-1- 60558-906-0</idno>
	</analytic>
	<monogr>
		<title level="m">Recsys&apos;10: ACM Conf. on Recommender Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD&apos;12, Part II, number 7524 in LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">General factorization framework for context-aware recommendations. Data Mining and Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-015-0417-y</idno>
		<idno>1384-5810. doi: 10.1007/ s10618-015-0417-y</idno>
		<ptr target="http://dx.doi.org/10.1007/s10618-015-0417-y" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;08: ACM Int. Conf. on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>York</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Baidu&apos;s recommender system for the biggest Chinese Q&amp;A website</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tianjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enlister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="285" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno>978-0-9749039-5-8</idno>
	</analytic>
	<monogr>
		<title level="m">UAI&apos;09: 25 th Conf. on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0575" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Badrul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An mdp-based recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eighteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to maximize reciprocal rank with collaborative less-is-more filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Climf</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2365952.2365981</idno>
		<ptr target="http://doi.acm.org/10.1145/2365952.2365981" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Conference on Recommender Systems, RecSys &apos;12</title>
		<meeting>the Sixth ACM Conference on Recommender Systems, RecSys &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gaussian ranking by matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2792838.2800185</idno>
		<ptr target="http://doi.acm.org/10.1145/2792838.2800185" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems, RecSys &apos;15</title>
		<meeting>the 9th ACM Conference on Recommender Systems, RecSys &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dit-Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Maximum margin matrix factorization for collaborative ranking. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
