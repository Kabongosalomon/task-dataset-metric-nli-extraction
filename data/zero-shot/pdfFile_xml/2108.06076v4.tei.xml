<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PVT: Point-Voxel Transformer for Point Cloud Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Wu</surname></persName>
							<email>wuzizhao@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PVT: Point-Voxel Transformer for Point Cloud Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Point Cloud Learning</term>
					<term>Transformer</term>
					<term>Neural Network</term>
					<term>3D Vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently developed pure Transformer architectures have attained promising accuracy on point cloud learning benchmarks compared to convolutional neural networks. However, existing point cloud Transformers are computationally expensive because they waste a significant amount of time on structuring irregular data. To solve this shortcoming, we present the Sparse Window Attention (SWA) module to gather coarse-grained local features from nonempty voxels. The module not only bypasses the expensive irregular data structuring and invalid empty voxel computation, but also obtains linear computational complexity with respect to voxel resolution. Meanwhile, we leverage two different self-attention variants to gather fine-grained features about the global shape according to different scale of point clouds. Finally, we construct our neural architecture called Point-Voxel Transformer (PVT), which integrates these modules into a joint framework for point cloud learning. Compared with previous Transformer-based and attention-based models, our method attains a top accuracy of 94.1% on the classification benchmark and 10? inference speedup on average. Extensive experiments also validate the effectiveness of PVT on semantic segmentation benchmarks. Our code and pretrained model are avaliable at https://github.com/HaochengWan/PVT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud learning has been receiving increasing attention from both industry and academia due to its wide applications including autonomous driving, robotics, AR/VR, etc. In these settings, sensors like LIDAR produce irregular and unordered sets of points that correspond to object surfaces. However, how to capture semantics directly and * These authors contributed equally. ? Corresponding author. quickly from these data remains a challenge for point cloud learning.</p><p>Most existing point cloud learning methods can be classified into two categories in terms of data representations: voxel-based models and point-based models. The Voxelbased models generally rasterize point clouds onto regular grids and apply 3D convolution for feature learning <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>. These models are computationally efficient due to their excellent memory locality, but the inevitable information loss degrades the fine-grained localization accuracy <ref type="bibr" target="#b34">[35]</ref>. By contrast, point-based models naturally preserve the accuracy of point location, but are generally computationally intensive <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Recently, Transformer architecture has drawn great attention in natural language processing and 2D vision because of its superior capability in capturing long-range dependencies. Powered by Transformer <ref type="bibr" target="#b39">[40]</ref> and its variants <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref>, point-based models have applied self-attention (the core unit of Transformer) to extract features from point clouds and improve performance significantly <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b52">53]</ref>. However, most of them suffer from the timeconsuming process of sampling and aggregating features from irregular points, which becomes the efficiency bottleneck <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this paper, we study how to design an efficient and high-performance Transformer architecture while avoiding the shortcoming of previous point cloud Transformers. We observe that voxel-based models have regular data locality and can efficiently encode coarse-grained features, while point-based networks preserve the accuracy of location information with the flexible fields and can effectively aggregate fine-grained features. Inspired by this, we propose a novel point cloud learning architecture, namely, Point-Voxel Transformer (PVT), which combines the ideas of voxel-based and point-based models. Our network focuses on how to fully exploit the potential of the two models mentioned above in Transformer architecture, capturing useful discriminative features from 3D data.</p><p>To investigate this, we conduct self-attention (SA) computation in voxels to obtain efficient learning pattern while performing SA in points to preserve the accuracy of location information with the flexible fields. However, directly performing SA computation to voxels is infeasible, mainly owing to two facts. First, non-empty voxels are sparsely distributed in the voxel domain and only account for a small proportion of total voxels <ref type="bibr" target="#b53">[54]</ref>. Second, the original SA computation leads to quadratic complexity with respect to the number of voxels, making it unsuitable for various 3D vision tasks, particularly for the dense prediction tasks of object detection and semantic segmentation. To tackle these issues, we design a sparse window attention (SWA) module which only has linear computational complexity by computing SA locally within the non-overlapping 3D window. A key design element of SWA lies in its sparse attention computing, in which a GPU-based Rule Book stores the nonempty voxel indexes. On the basis of this strategy, we can bypass the invalid computation of empty voxels and retain the original 3D shape structure. For cross-window information interaction, inspired by Swin Transformer <ref type="bibr" target="#b25">[26]</ref>, we propose to apply shifted window to enlarge the receptive field. In addition, we leverage two different SA variants to capture the global information according to different scales of point clouds. For small-scale point clouds, we propose relative-attention (RA), which extends the SA mechanism to consider representations of the relative position, or distances between points. The advantage of RA is that the absolute coordinates of the same object can be completely different with rigid transformations; thus, injecting relative position representations (RPR) in our structure is generally more robust. We observe that our network obtains significant improvements over not using this RPR term without adding extra training parameters. Nevertheless, with tens of thousands of points (e.g., SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>) as inputs, directly applying the RA module in points incurs unacceptable O(N 2 ) memory consumption, where N is the input point number. Thus, for large-scale point clouds, we perform External Attention (EA), a linear attention variant to avoid the O(N 2 ) computational complexity of RA module.</p><p>Based on these modules, We propose a two-branch PVT that mainly consists of the voxel branch and the point branch ( <ref type="figure" target="#fig_1">Figure 1</ref>). As illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, by behaving SWA in the voxel branch and performing RA (or EA) in the point branch, our method disentangles the coarse-grained local feature aggregation and the fine-grained global context transformation so that each branch can be implemented efficiently and accurately.</p><p>By using the PVT, we construct PVT networks for various 3D tasks. These networks can capably serve as a general-purpose backbone for point cloud learning. In particular, we conduct the classification experiment on the ModelNet40 <ref type="bibr" target="#b46">[47]</ref> dataset and obtain the state-of-the-art accuracy of 94.1% (no voting), while being on average 10? faster than its Transformer baselines. On ShapeNet Part <ref type="bibr" target="#b23">[24]</ref>, S3DIS <ref type="bibr" target="#b0">[1]</ref>, and SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> datasets, our model also obtains strong performance (86.6%, 69.2%, and 64.9% mIoU, respectively).</p><p>The main contributions are summarized as following:</p><p>? We propose PVT, the first Transformer-based approach, to deeply incorporate the advantages from both point-based and voxel-based networks to our knowledge.</p><p>? We propose an efficient local attention module, named SWA, which achieves linear computational complexity with respect to the input voxel size and bypasses the invalid empty voxel computation.</p><p>? Extensive experiments show that our PVT model attains competitive results on general point cloud learning tasks with 10? latency speed-up than its baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Voxel-based models on points</head><p>To leverage the success of convolutional neural networks on 2D images, many researchers have endeavored to project 3D point clouds directly onto voxels and perform 3D convolution for information aggregation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>. However, the memory and computational consumption of such fullvoxel-based models increases cubically with respect to the voxel's resolution. To tackle this shortcoming, O-CNN <ref type="bibr" target="#b42">[43]</ref>, OctNet <ref type="bibr" target="#b32">[33]</ref>, and kd-Net <ref type="bibr" target="#b16">[17]</ref> constructed tree structures for 3D voxels to bypass the invalid computation of the empty voxels. Although such methods are efficient in data structuring, geometric details are lost during the projection because multiple bucketing points into the same voxel leads to indistinguishable features for learning.</p><p>Compared with most voxel-based 3D models, our model is more efficient and effective for the following reasons: 1) Instead of using 3D convolutions, we propose SWA to handle the sparsity characteristic of voxels that have a linear computational complexity with respect to voxel resolution and bypasses the invalid computation of empty voxels. 2) As we employ RA in the point domain, an inherent advantage is that we can avoid feature loss during voxelization, maintaining the geometric details of a point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Point-based models for point cloud learning</head><p>Instead of voxelization, it is possible to make a neutral network that consumes directly on point clouds. <ref type="bibr" target="#b28">[29]</ref> proposed PointNet, the pioneering work that learns directly on sparse and unstructured point clouds. Inspired by PointNet, previous works introduced sophisticated neural modules to learn per-point features. These models can be generally classified as: 1) neighbouring feature pooling <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b15">16]</ref>, 2) graph message passing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b41">42]</ref>, and 3) attention-based or Transformer-based models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b8">9]</ref>.</p><formula xml:id="formula_0">S-SWA RS-SWA {R=30,W=4} {R=30,W=4}</formula><p>{R=15,W=4} <ref type="figure" target="#fig_1">Figure 1</ref>. Model Architecture: The model for part segmentation take as input N points and feeds them into 3 stacked PVT blocks to learn a semantically rich and discriminative representation for each point, followed by a MLP layer to generate the output feature. After that, we leverage the max-pooling and repeating operators to extract an effective global feature representing the entire point cloud. Note that shortcut connections are used to extract multi-scale features and one MLP layer (1280) to aggregate multi-scale features, where we concatenate features from previous layers to get a 64+64+128+1024=1280-dimensional point cloud. Finally, we predict the final point-wise segmentation scores for the input point cloud and the part label of a point is also determined as the one with maximal score. PVT block (grey box): The PVT block is composed of two branches. The upper branch is voxel-based for aggregating local features and the lower is point-based for capturing global features. We can effectively fuse two branches because they are providing complementary information. R and w denote the voxel resolution size and 3d window size, respectively. : addition, : concatenation.</p><p>Owing to the sparsity and irregularity of point clouds, the methods that directly consume points have achieved stateof-the-art performance. However, the cost of data structuring of such methods has become the computation burden, especially on the large-scale point clouds dataset <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51]</ref>. In this study, we handle this shortcoming by using the SWA module. <ref type="bibr" target="#b1">[2]</ref> proposed a neural machine translation method with an attention mechanism, in which attention weight is computed through the hidden state of an RNN. Then <ref type="bibr" target="#b24">[25]</ref> further proposed self-attention to visualize and interpret sentence embeddings. Subsequent works employed self-attention layers to replace some or all the spatial convolution layers, such as Transformer for machine translation <ref type="bibr" target="#b39">[40]</ref>. <ref type="bibr" target="#b6">[7]</ref> proposed the bidirectional transformers (BERT), which is one of the most powerful models in the NLP field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-attention and Transformer</head><p>Given the success of self-attention and Transformer architectures in NLP, researchers have applied them to vision tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b61">62]</ref>. For instance, <ref type="bibr" target="#b7">[8]</ref> proposed an image recognition network, ViT, which directly applied a Transformer architecture on image patches and achieved better performance than the traditional convolutional neural networks. <ref type="bibr" target="#b25">[26]</ref> recently introduced Swin Transformer to incorporate inductive bias for spatial locality, hierarchy and translation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Transformers on point cloud</head><p>Recently, plenty of researchers have attempted to explore Transformer-based architectures for point cloud learning. <ref type="bibr" target="#b8">[9]</ref> proposed PT 1 to extract global features by introducing the dot-product SA mechanism. <ref type="bibr" target="#b13">[14]</ref> proposed offsetattention to calculate the offset difference between the SA features and the input features by element-wise subtraction. Moreover, <ref type="bibr" target="#b18">[19]</ref> proposed PT 2 to build local vector attention in neighborhood point sets and achieved significant progress. However, they wasted a high percentage of the total time on structuring the irregular data, which becomes the efficiency bottleneck. In this work, we study how to solve this shortcoming, so as to design an efficient Transformerbased architecture for point cloud analysis.</p><p>There also exists some Transformer-based architecture for various point cloud processing tasks. For instance, <ref type="bibr" target="#b10">[11]</ref> proposed the SE(3)-Transformer, a variant of the selfattention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations. Late, <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b48">[49]</ref> have attempted to explore Transformer-based architectures for point cloud completion and achieved significant performance on all completion tasks. Recently, <ref type="bibr" target="#b57">[58]</ref> proposed Point-BERT to unleash the scalability and generalization of Transformers for 3D point cloud representation learning. Extensive experiments also demonstrate that Point-BERT significantly improves the performance of standard point cloud Transformers. Different from these Transformers, in this paper, we study how to design an efficient and high-performance Transformer archi-tecture for point cloud learning, making it suitable for edge devices with limited computational resources or real-time autonomous driving scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method Overview</head><p>In contrast to the previous point cloud Transformers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b64">65]</ref>, we design the network as efficient as possible with high accuracy so that it can be widely used on various 3D tasks.</p><p>We introduce an efficient neural architecture for point cloud learning, namely, PVT. Formally, given a point cloud embedding F ? R N ?D , our PVT is designed to map the input features F to a new set of point feature F ? R N ?D . As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, the PVT consists of two main branches: a voxel branch and a point branch. We leverage the voxel branch to map the inputs F to F local ? R N ?D , which aggregates local features in the voxel domain. However, full voxel method will inevitably encounter information loss during voxelization. Thus, we utilize the point branch to map the inputs F to F global ? R N ?D , which can directly extract global features for each individual point. With both local features and aggregated global context, we can efficiently fuse two branches into an addition layer as both provide complementary information.</p><p>Below we detail the two branches in Sections 3.1 and 3.2. Section 3.3 details our feature fusion module, and Section 3.4 discusses the relationship between the proposed model and the prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Voxel branch</head><p>This branch aims to effectively capture local information, which can bypass expensive sampling and neighbor points querying. Specifically, it contains three steps: voxelization, feature aggregation, and devoxelization.</p><p>The voxelized and devoxelized methods map the input point cloud P to a new set of voxel features V and transform the voxel-wise features back to the point-wise features F local (Readers can refer to <ref type="bibr" target="#b26">[27]</ref> for more details). In this work, we apply the standard Transformer architecture <ref type="bibr" target="#b39">[40]</ref> to perform feature aggregation on regular 3D voxels, which can improve accuracy significantly. However, the standard Transformer architecture conducts global-SA which leads to quadratic complexity with respect to the number of voxels, making it unsuitable for many 3D vision problems requiring dense prediction, such as semantic segmentation.</p><p>Window Attention: To obtain efficient modeling power, we propose to compute SA within local 3D windows, which are arranged to evenly partition the voxel space in a nonoverlapping manner. Assume that each local 3D window contains W ? W ? W voxels and R denotes the voxel resolution. The computational complexity of a global-SA and   <ref type="figure">Figure 2</ref>. Sparse Window Attention: This is also a 2D example and can be easily extended to 3D cases. SWA is a GPU-based method and can efficiently index the non-empty voxels in the attention field.</p><p>a window-based one on R 3 voxels are:</p><formula xml:id="formula_1">?(Global-SA) = 4R 3 D 2 + 2(R 3 ) 2 D (1) ?(W indow-SA) = 4R 3 D 2 + 2B 3 R 3 D<label>(2)</label></formula><p>where the former is quadratic to the number of voxels R 3 , the latter is linear when W is fixed, D is the dimension of features. In summary, global-SA computation is generally unaffordable for a large voxel resolution, whereas the local window-SA is scalable.</p><p>Sparse Window Attention: Different from 2D pixels densely placed on an image plane, non-empty voxels only account for a small proportion of total voxels. Inspired by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b53">54]</ref>, we design Sparse Window Attention to handle the sparsity characteristic of voxels. As shown in <ref type="figure">Figure  2</ref>, for each querying index v i , we first use Window Attention to determine all neighboring voxel indices in the attention field, and then adopt a Hash table to get the hashed integer neighboring voxel indices. Last, a GPU-based Rule Book stores the hashed voxel indices as keys, and the corresponding indices for the non-empty voxel array as values. Finally, we can perform Window Attention to gather the coarse-grained local features. Note that all the steps can be conducted in parallel on GPUs by assigning each querying voxel v i a separate CUDA thread.</p><p>The SWA lacks information interaction across windows, which may limit the representation power of our model. Thus, we extend the shifted 2D window mechanism of Swin Transformer <ref type="bibr" target="#b25">[26]</ref> to 3D windows for the purpose of introducing cross-window information interaction while maintaining the efficient computation of non-overlapping SWA. Readers can refer to Swin Transformer for more details As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, with the cyclic shifted window partitioning approach, the step of feature aggregation of this branch can be described as follows: Output: F local , an array with shape [B, N, D]</p><formula xml:id="formula_2">F 1 = voxelize(P, F) #shape = [B, R 3 , D] F 1 = cyclic shift(F 1) F 2 = SWA(layernorm(F 1)) + F 1 F 2 = MLP(layernorm(F 2)) + F 2 F 2 = reverse cyclic shift (F 2) F 3 = SWA(layernorm(F 2)) + F 2 F local = MLP(layernorm(F 3)) + F 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Point branch</head><p>The voxel branch gathers the neighborhood information with low resolution. However, in order to capture longrange dependencies, low-resolution voxel branch alone is limited. To this end, we attempt to employ the following self-attention on the entire point cloud for global context aggregation, which is computed as:</p><formula xml:id="formula_3">F sa = sof tmax(QK T )V, F sa ? R N ?D (3) F global = M LP (F sa ) + F, F global ? R N ?D<label>(4)</label></formula><p>where (Q, K, V) ? R N ?D is generated by shared linear transformations and the input features F and are all ordered independent. Moreover, softmax and weighted sum are both permutation-independent operators. Thus, the self-attention computation is permutation-invariant, making it well-suited to handle the irregular, disordered 3D points.</p><p>Relative Attention: Self-attention mentioned above fails to incorporate relative position representations in its structure, whereas such ability is very important to 3D visual tasks. For example, the absolute coordinates of the same object can be completely different with rigid transformations. Therefore, injecting relative position representations are generally more robust. In this study, we design our relative-attention to embed relative position representations, which are not well studied in prior point cloud learning works.</p><p>First of all, by embedding RPR into the scaled dotproduct self-attention module, Eq 3 can be re-formulated as:</p><formula xml:id="formula_4">F ra = sof tmax(QK T + B)V<label>(5)</label></formula><p>where B ? R N ?N is the relative representations bias. Suppose that the original point cloud with N points is denoted by P = {p i } N i=1 ? R 3 . We compute the relative position B as follows:</p><formula xml:id="formula_5">B pi,pj ,m = p i,m ? p j,m , m ? {x, y, z}.<label>(6)</label></formula><p>To map relative coordinates to the corresponding position encoding, we maintain three learnable look-up tables t x , t y , t z ? R L?N ?N corresponding to the x, y and z axis, respectively. As the relative coordinates are continuous floating-point numbers, we uniformly quantize the range of B pi,pj ,m into L discrete parts and map the relative coordinates B pi,pj ,m to the indices of the tables as:</p><formula xml:id="formula_6">idx i,j,m = B pi,pj ,m + s max s quad<label>(7)</label></formula><p>where s max is the maximum size of point cloud coordinates and s quad = 2?smax L is the quantization size, and ? denotes floor rounding.</p><p>We look up the tables to retrieve corresponding embedding with the index and sum them up to obtain the position encoding of</p><formula xml:id="formula_7">B pi,pj = 3 m=1 t m [idx i,j,m ],<label>(8)</label></formula><p>where t[idx] indicates the idx-th entry of the learnable look-up table t, and B pi,pj means the relative position encoding between p i and p j . External Attention: Although the RA module has demonstrated significant performance on small-scale point clouds, it is unsuitable for large-scale point clouds (e.g., Se-manticKITTI <ref type="bibr" target="#b2">[3]</ref>) due to its unacceptable O(N 2 ) memory consumption. Therefore, in this work, we perform External Attention computation on large-scale point clouds. External Attention, is a novel attention mechanism based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers. It has linear complexity and implicitly considers the correlations between all data samples, making it suitable for large-scale point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature fusion</head><p>We effectively fuse the outputs of two branches with an addition as they are providing complementary information:</p><formula xml:id="formula_8">F = F local + F global , F ? R N ?D<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relationship to prior works</head><p>The proposed PVT is related to several prior works which includes PVCNN <ref type="bibr" target="#b26">[27]</ref>, PCT <ref type="bibr" target="#b13">[14]</ref>, PT 1 [9], PT 2 <ref type="bibr" target="#b64">[65]</ref>,.</p><p>Although we are inspired by the idea of PVCNN <ref type="bibr" target="#b26">[27]</ref>, our PVT is different in several ways: 1) in the voxel branch, PVCNN uses a 3D convolution to gather local information while we employ SWA computation within each local window, which is highly efficient. Per-layer complexity for different layer types are shown in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>. Per-layer complexity for different layer types. R is the resolution of voxels, v = W ? W ? W is the number of voxels in a same 3D window, r denotes the proportion of non-empty voxels in a local 3D window, N is the number of points, D is the representation dimension, and k is the kernel size of convolutions.</p><formula xml:id="formula_9">(k ? R 3 ? D 2 ) voxel-Window-attention O(v ? R 3 ? D) branch SWA O(r 2 ? v ? R 3 ? D) The- 1D Convolutions O(k ? N ? D 2 ) point- Relative-attention O(N 2 ? D) branch External-attention O(N ? D)</formula><p>(r = 0.1) <ref type="bibr" target="#b50">[51]</ref> which means that our SWA can save significant computation power compared with window-attention.</p><p>2) in the point branch, PVCNN uses 1D convolution, which is computation efficient but lacks the global context modeling capability. By performing RA (or EA) computation on the entire points, our method gathers the global modeling power. Unlike prior Transformer-based 3D models that need to gather the downsampled points and find the corresponding neighbors by using expensive FPS and k-NN in point domain, our approach does not require explicitly identify which point is the farthest and what are in the neighboring set. Instead, the voxel branch observes regular data and learns to capture local features using SWA. Additionally, the point branch only needs to perform RA (or EA) on the entire point cloud, which also does not require to find the neighboring points. Thus, our approach obtains highly efficiency than PCT, PT 1 and PT 2 (see <ref type="table" target="#tab_3">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now discuss the PVT that can be constructed from PVT blocks for different point cloud learning tasks: shape classification, and object and semantic segmentation. The performance is quantitatively evaluated with four metrics, including overall accuracy (OA), average precision (AP), the intersection over union (IoU), and mean IoU (mIoU). For the sake of fair comparison with baselines, we report the measured latency on a GTX 2080 GPU to reflect the efficiency but evaluate other indicators on a GTX 3090 GPU.</p><p>Model. The architecture used for the segmentation task is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. In our settings, dropout with keep probability of 0.5 is used in the last two linear layers. All layers include ReLU and batch normalization. In addition, for other point cloud learning tasks, we use the similar architecture as in segmentation. Readers can refer to our source code for more derails.</p><p>Baselines. We select four models as the comparison baselines, including the strong attention-based model PointASNL and the three powerful Transformer-based net- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shape Classification</head><p>Data. We conduct the classification experiment on the ModelNet40 <ref type="bibr" target="#b46">[47]</ref> dataset. ModelNet40 includes 12,311 CAD models from 40 different object classes, in which 9843 models are used for training and the rest for testing. We follow the experimental configuration of PointNet, i.e., for each model, we uniformly sample 1024 points with 3 channels (or 6) of spatial location (and normal) as input; the point cloud is re-scaled to fit the unit sphere.</p><p>Setting.</p><p>We utilize random translation, random anisotropic scaling and random input dropout strategies to augment the input points data during training. During testing, no data augmentation or voting methods were used. For classification on ModelNet40, the SGD optimizer was used for 200 epochs with the batch size 32. We set the initial learning rate to 0.01 and adopt a cosine annealing schedule to adjust the learning rate at every epoch.</p><p>Results. The results are shown in Tables 2 and 3, we  can see that our PVT outperforms most previous models. Compared with its baselines, such as PCT, PT 1 and PT 2 , PointASNL, our PVT not only achieves state-of-the-art accuracy of 94.1%, but also has the best speed-accuracy tradeoff (10? faster on average). <ref type="figure" target="#fig_2">Figure 3</ref> also provides an accuracy plot under equal-epoch setting. As can be seen, our method outperforms all Transformer-based methods, being the fastest and most accurate towards convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of computational requirements</head><p>Now, we analyze the computational requirements of PVT and several other baselines by comparing the floating point operations required (FLOPs) and number of parameters (Params) in <ref type="table" target="#tab_3">Table 4</ref>. PVT has the lowest memory requirements with only 2.45M parameters, and also puts a low load on the processor of only 1.62G FLOPs, yet delivers highly accurate results of 94.1%. These characteristics make it suitable for deployment on a mobile device. In addition, PT 2 has attained top accuracy on various point cloud learning benchmarks, but its shortcomings of slow inference time and high computing cost are also obvious. In the summary in <ref type="table" target="#tab_3">Table 4</ref>, PVT only spends 6.3% of the total runtime on structuring the irregular data, which is much lower than previous Transformer-based models. Overall, PVT has the best accuracy and the lowest computational and memory requirements compared with its baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object part segmentation</head><p>Data. The model is also evaluated on ShapeNet Parts <ref type="bibr" target="#b23">[24]</ref>. It is composed of a total of 16,880 models (14,006 models are used for training, the rest for testing), each of which has 2 to 6 parts and the whole dataset is labeled in 50 different parts. A total 2048 points are sampled from each model as input and only few point sets have six labeled parts. We directly adopt the same train-test split as PointNet in our experiment.</p><p>Setting. The same training setting as in our classification task was adopted. For this task on ShapeNet Part, we train our model using the SGD optimizer for 200 epochs with the batch size 16. The initial learning rate was set to 0.1, with a cosine annealing schedule to adjust the learning rate at every epoch.</p><p>Results. <ref type="table" target="#tab_4">Table 5</ref> lists the class-wise segmentation results. Part-average IoU is used to evaluate our model, which is given both overall and for each object category. The results show that our PVT makes an improvement of 2.9% over PointNet. Compared with all baselines, PVT attains the top pIoU with 86.6%.</p><p>Visualization. In <ref type="figure" target="#fig_4">Figure 4</ref>, we illustrate output features from the point and voxel branches respectively, where a warmer color represents larger magnitude. As we can see, the voxel branch captures large, continuous parts while the point-based counterpart captures global shape details (e.g., table legs, airplane wings, and tail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Indoor Scene Segmentation</head><p>Data. To further assess our network, we conduct semantic segmentation task on S3DIS dataset <ref type="bibr" target="#b0">[1]</ref>, which includes 273 million points from six indoor areas of three different buildings. Each point is annotated with a semantic label from 13 classes-e.g., beam, bookcase, chair, column, and window. We follow <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b28">[29]</ref> and uniformly sampled points from blocks of area size 1m ? 1m, where each point is represented by a 9D vector (XYZ, RGB, and normalized spatial coordinates).</p><p>Setting.</p><p>During the training process, we generate training data by randomly sampling 4,096 points from each block on-the-fly. Following <ref type="bibr" target="#b44">[45]</ref>, we utilize Area-5 as the test scene and all the other areas for training. Note that the position and RGB information of points are used to as the input features. The same training setting as in our classification task is adopted, but requires 50 epochs with the batch size 8 to fulfil the experiment.</p><p>Results. The results are presented in <ref type="table" target="#tab_5">Tables 6 and Table  8</ref>. From Tables 6, we can see that our PVT attains mIoU of 67.3%, which outperforms MLPs-based frameworks such as PointNet <ref type="bibr" target="#b28">[29]</ref> and PointNet++ <ref type="bibr" target="#b30">[31]</ref>, graph-based methods such as DGCNN <ref type="bibr" target="#b44">[45]</ref>, attention-based models such as PointASNL <ref type="bibr" target="#b52">[53]</ref>. Moreover, PVT attains mIoU of 69.2%  . We demonstrate the output features extracted from two branches using Open3D <ref type="bibr" target="#b65">[66]</ref>. The voxel branch focuses on the large, continuous parts, while the point-based captures the global shape details. under 6-fold cross-validation, substantially outperforming most prior models.</p><p>Visualization. <ref type="figure" target="#fig_5">Fig.5</ref> shows the visualization of PVT's predictions. The predictions of our network are very close to the ground truth. PVT captures detailed semantic structure in complex 3D scenes, which is important in our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Outdoor semantic Segmentation</head><p>Data. To further evaluate our network, we conduct outdoor semantic segmentation task on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> dataset, which includes 43,552 densely labeled LIDAR scans belonging to 21 sequences. Each scan is a largescale point cloud with ? 10 5 points and spanning up to 160?160?20 m in 3D space. Officially, the sequences 00 ? 07 and 09 ? 10 (19,130 scans) are used for training, the sequence 08 (4071 scans) for validation, and the sequences 11 ? 21 (20,351 scans) for online testing. The raw 3D points only have 3D coordinates without color information.</p><p>Setting. Based on UNet, we build our backbone network for large-scale point clouds segmentation with a stem, four down-sampling and four up-sampling stages, and the dimensions of these nine stages are <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">256,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b63">64</ref>, and 64, respectively. As for the voxel branch, the voxel resolution is 0.05 m for segmentation experiments. We train PVT for 100 epochs on a single GeForce RTX 3090 GPU with the batch size 8. In addition, the Adam optimizer is employed to minimize the overall loss; the learning rate starts from 0.01 and decays with a rate of 0.5 after every 10 epochs.</p><p>Results. As shown in <ref type="table">Table 7</ref>, PVT outperforms the previous state-of-the-art point-based model BAAF by 5.0% in mIoU with 48? measured speedup. Compared with strong attention-based PointASNL and point-based KPConv, our PVT achieves +18.1% and +6.1% mIoU improvements, with 51? and 25? measured speedup respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head><p>In this section, we conduct extensive ablation study to analyze the effectiveness of different components of PVT block. The results of the ablation study are summarized in <ref type="table">Tables 9 and 10</ref>.</p><p>Impact of the voxel-based and point-based branches.  bination of two branches, which provide richer information about the points. Effect of the cyclic shifted windows scheme. Ablation of the shifted window method on classification is reported in <ref type="table">Table 9</ref> (Model C). The network with the shifted window partitioning (PVT f ull ) outperforms the Model C without shifting at each layer by +1.0% OA on ModelNet40.</p><formula xml:id="formula_10">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? PVT(</formula><p>The results indicate the effectiveness and efficiency of using cyclic shifted window to build cross-window information interaction in the preceding layers.</p><p>Impact of the number of PVT blocks. we validate the impact of the PVT block by controlling the number of PVT   <ref type="table">Table 10</ref>. Ablation study on the relative-attention and relative position bias on two benchmarks. MLP: replace relative-attention with MLP layer in our architecture. EdgeConv: replace relativeattention with EdgeConv layer in our architecture. no rel. pos: the relative attention without an additional relative position bias term (see Eq 5).</p><p>blocks and report the results in <ref type="table">Table 9</ref>. From this table, we can conclude the following: on one hand, reducing the number of PVT blocks can save latency, for example, compared with PVT f ull , Model D saves 25% latency but incurs a loss on accuracy; on the other hand, increasing the number of PVT blocks from PVT f ull can hardly support Model E accuracy benefit but leads to an increase on latency. To balance between speed and accuracy, we adopt 3 PVT blocks as our full model. Effect of Relative-attention. We validate the impact of RA module used in our network. The results are shown in <ref type="table">Table 10</ref>. We set two baselines. "MLP" is a no-attention baseline that replaces RA with a MLP layer. "EdgeConv" is a more advanced no-attention baseline that replaces RA with a EdgeConv layer. EdgeConv performs feature aggregating at each point and enables each point to exchange information with its neighboring points, but does not leverage attention mechanisms. We can see that the RA mod-ule achieves better results than the no-attention baselines. The performance gap between RA and MLP baselines is significant: 94.1% vs. 92.6% and 86.6% vs. 85.3%, an respectivel improvement of 1.5 and 1.3 absolute percentage points. Compared with EdgeConv baseline, our RA module also achieves improvements of 0.9 and 0.9 absolute percentage points, respectively.</p><p>Effect of relative position representations. Finally, we also investigate the impact of RPR used in the RA module. As demonstrated in <ref type="table">Table 10</ref>, our PVT with RPR yields +0.8% OA/+0.4% mIoU on ModelNet40 and ShapeNet in relation to those without this term respectively, indicating the effectiveness of the RPR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this work, we present PVT for efficient point cloud learning. PVT is found to surpass previous Transformerbased and attention-based models in efficiency. It achieves this by deeply combining the advantages from both voxelbased and point-based networks. To reduce the computation cost, we design a GPU-based SWA computing method that has linear computational complexity with respect to voxel resolution and bypasses the invalid empty voxel computations. In addition, we study two different self-attention variants to gather fine-grained features about the global shape according to different scales of point clouds.</p><p>In the future, we expect to promote the primary structure for other research areas, such as point cloud pretraining, generation, and completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Pseudo-code for the Voxel Branch Input: P , an array with shape [B, N, C]. F , an array with shape [B, N, D].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Accuracy of different Transformer-based methods over time and epochs on ModelNet40. Our method performs the best in both equal-time and equal-epoch comparisons. The accuracy of 90% can be achieved not only after 8 epochs but also in the shortest training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Top row: features aggregated from the voxel branch.(b) Bottom row: features extracted from the point branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4. We demonstrate the output features extracted from two branches using Open3D [66]. The voxel branch focuses on the large, continuous parts, while the point-based captures the global shape details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of semantic segmentation results on the S3DIS dataset. The input is in the top row, PVT predictions on the middle, the ground truth on the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table Rule</head><label>Rule</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>voxel index</cell><cell></cell></row><row><cell>key</cell><cell>value</cell><cell>1021</cell><cell>59</cell><cell></cell></row><row><cell>(0, 1) (0, 3) (2, 2) (3, 0)</cell><cell>1021 1156 1689 2324</cell><cell>? 1156 ? 1689 ?</cell><cell>? 64 ? 99 ?</cell><cell>Performing attention</cell></row><row><cell></cell><cell></cell><cell>2324</cell><cell>103</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>book</cell><cell>Non-empty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>voxel array</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The speed comparison of different models when the number of input points varies. works PT 1 , PT 2 , and PCT.</figDesc><table><row><cell>Model</cell><cell>Input</cell><cell cols="2">Points</cell><cell cols="2">OA(%) Latency</cell></row><row><cell></cell><cell cols="2">OA&lt;92.5</cell><cell></cell><cell></cell></row><row><cell>PointNet</cell><cell>xyz</cell><cell cols="2">16?1024</cell><cell>89.2</cell><cell>13.6ms</cell></row><row><cell cols="4">PointNet++ [31] xyz,nor 16?1024</cell><cell>91.9</cell><cell>35.3ms</cell></row><row><cell>SO-Net [22]</cell><cell cols="3">xyz,nor 8?2048</cell><cell>90.9</cell><cell>?</cell></row><row><cell>PointGrid [21]</cell><cell cols="3">xyz,nor 16?1021</cell><cell>92.0</cell><cell>?</cell></row><row><cell cols="4">SpiderCNN [52] xyz,nor 8?1024</cell><cell>92.4</cell><cell>82.6ms</cell></row><row><cell>PointCNN [23]</cell><cell>xyz</cell><cell cols="2">16?1024</cell><cell>92.2</cell><cell>221.2ms</cell></row><row><cell>PointWeb [64]</cell><cell cols="3">xyz,nor 16?1024</cell><cell>92.3</cell><cell>?</cell></row><row><cell>PVCNN [27]</cell><cell cols="3">xyz,nor 16?1024</cell><cell>92.4</cell><cell>24.2ms</cell></row><row><cell></cell><cell cols="2">OA&gt;92.5</cell><cell></cell><cell></cell></row><row><cell>KPConv [39]</cell><cell>xyz</cell><cell cols="2">16?6500</cell><cell>92.9</cell><cell>120.5ms</cell></row><row><cell>DGCNN [45]</cell><cell>xyz</cell><cell cols="2">16?1024</cell><cell>92.9</cell><cell>85.8ms</cell></row><row><cell>LDGCNN [59]</cell><cell>xyz</cell><cell cols="2">16?1024</cell><cell>92.7</cell><cell>?</cell></row><row><cell cols="4">PointASNL [53] xyz,nor 16?1024</cell><cell>93.2</cell><cell>923.6ms</cell></row><row><cell>PT 1 [9]</cell><cell cols="3">xyz,nor 16?1024</cell><cell>92.8</cell><cell>320.6ms</cell></row><row><cell>PT 2 [65]</cell><cell cols="3">xyz,nor 8?1024</cell><cell>93.7</cell><cell>530.2ms</cell></row><row><cell>PCT [14]</cell><cell>xyz</cell><cell cols="2">16?1024</cell><cell>93.2</cell><cell>92.4ms</cell></row><row><cell>PVT (Ours)</cell><cell>xyz</cell><cell cols="2">16?1024</cell><cell>93.7</cell><cell>45.5ms</cell></row><row><cell>PVT (Ours)</cell><cell cols="3">xyz,nor 16?1024</cell><cell>94.1</cell><cell>48.6ms</cell></row><row><cell cols="6">Table 2. Results on ModelNet40 [48]. Compared with previous</cell></row><row><cell cols="6">Transformer-based models, our PVT achieves the state-of-the-art</cell></row><row><cell cols="5">accuracy with 10? measured speed-up on average.</cell></row><row><cell>Input</cell><cell>PT 1</cell><cell>PT 2</cell><cell cols="2">PCT Ours</cell></row><row><cell cols="4">16?512 247.5 430.7 76.4</cell><cell>31.5</cell></row><row><cell cols="4">16?1024 320.6 530.2 92.4</cell><cell>45.5</cell></row><row><cell cols="5">8?2048 460.5 720.4 145.4 71.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Computational resource requirements. SDA means the rate of total runtime on structuring the sparse data.</figDesc><table><row><cell>Model</cell><cell cols="4">Params FLOPs SDA(%) OA(%)</cell></row><row><cell>PointNet</cell><cell>3.47M</cell><cell>0.45G</cell><cell>0.0</cell><cell>89.2</cell></row><row><cell>PointNet++(SSG)</cell><cell>1.48M</cell><cell>1.68G</cell><cell>43.5</cell><cell>90.7</cell></row><row><cell cols="2">PointNet++(MSG) 1.74M</cell><cell>4.09G</cell><cell>47.6</cell><cell>91.9</cell></row><row><cell>DGCNN</cell><cell>1.81M</cell><cell>2.43G</cell><cell>57.2</cell><cell>92.9</cell></row><row><cell>PointASNL</cell><cell>3.98M</cell><cell>5.92G</cell><cell>39.8</cell><cell>93.1</cell></row><row><cell>PT 1</cell><cell>21.1M</cell><cell>5.05G</cell><cell>32.5</cell><cell>92.8</cell></row><row><cell>PT 2</cell><cell>9.14M</cell><cell>17.1G</cell><cell>65.4</cell><cell>93.7</cell></row><row><cell>PCT</cell><cell>2.88M</cell><cell>2.17G</cell><cell>24.6</cell><cell>93.2</cell></row><row><cell>PVT(Ours)</cell><cell cols="2">2.76M 1.93G</cell><cell>9.1</cell><cell>94.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>82.1 88.7 82.1 92.4 75.5 91.0 88.9 85.6 95.4 76.2 94.7 84.2 65.0 75.3 81.7 Results of part segmentation on ShapeNet. pIoU means part-average Intersection-over-Union.</figDesc><table><row><cell>Model</cell><cell cols="2">pIoU Areo Bag</cell><cell>Cap</cell><cell>Car</cell><cell cols="2">Chair Ear</cell><cell cols="9">Guitar Knife Lamp Laptop Motor Mug Pistol Rocket Skate</cell><cell>Table</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Phone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Board</cell><cell></cell></row><row><cell># Shapes</cell><cell cols="2">2690 76</cell><cell>55</cell><cell>898</cell><cell cols="2">3758 69</cell><cell>787</cell><cell>392</cell><cell cols="2">1547 451</cell><cell>202</cell><cell>184</cell><cell>283</cell><cell>66</cell><cell>152</cell><cell>5271</cell></row><row><cell>PointNet</cell><cell cols="16">83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6</cell></row><row><cell>P2Sequence</cell><cell cols="16">85.1 82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8</cell></row><row><cell>PointASNL</cell><cell cols="16">86.1 84.1 84.7 87.9 79.7 92.2 73.7 91.0 87.2 84.2 95.8 74.4 95.2 81.0 63.0 76.3 83.2</cell></row><row><cell>RS-CNN</cell><cell cols="16">86.2 83.5 84.8 88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.1 73.7 94.1 83.4 60.5 77.7 83.6</cell></row><row><cell>PT 1</cell><cell>85.9 ?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>PCT</cell><cell cols="16">86.4 85.0 82.4 89.0 81.2 91.9 71.5 91.3 88.1 86.3 95.8 64.6 95.8 83.6 62.2 77.6 73.7</cell></row><row><cell>PVT (Ours)</cell><cell>86.6 85.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>10.76 52.61 58.93 40.28 5.85 26.38 33.22 PointNet++ 50.04 90.79 96.45 74.12 0.02 5.77 43.59 25.39 69.22 76.94 21.45 55.61 49.34 41.88 PointNet++-CE 51.56 92.28 96.87 74.77 0.02 7.04 46.78 25.42 69.13 79.18 26.67 53.39 54.61 44.03 DGCNN 47.08 92.42 97.46 76.03 0.37 12.00 51.59 27.01 64.85 68.58 7.67 43.76 29.44 40.83 PVCNN 56.12 91.23 97.54 77.13 0.29 13.02 51.72 26.74 68.52 75.48 Indoor scene segmentation results on the S3DIS dataset, evaluated on Area5. From this table, we can see that the proposed PVT outperforms most of previous 3D models in some categories significantly, 3000 40.9 83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5 PointASNL 7260 46.8 87.4 74.3 24.3 1.8 83.1 87.9 39.0 0.0 25.1 29.2 84.1 52.2 70.6 34.2 57.6 0.0 43.9 57.8 36.9 RandLA-Net * 880 53.9 94.2 26.0 25.8 40.1 38.9 49.2 48.2 7.2 90.7 60.3 73.7 20.4 86.9 56.3 81.4 61.3 66.8 49.2 47.7 KPConv * 3560 58.8 96.0 30.2 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4 BAAF 6880 59.9 90.9 74.4 62.2 23.6 89.8 95.4 48.7 31.8 35.5 46.7 82.7 63.4 67.9 49.5 55.7 53.0 60.8 53.7 52.0 SPVCNN</figDesc><table><row><cell>Model</cell><cell></cell><cell>mIoU</cell><cell>Ceiling</cell><cell></cell><cell>Floor</cell><cell>Wall</cell><cell cols="5">Bean Column Window</cell><cell>Door</cell><cell>Chair</cell><cell></cell><cell>Table</cell><cell cols="2">Bookcase</cell><cell>Sofa</cell><cell>Board</cell><cell></cell><cell>Clutter</cell></row><row><cell>PointNet</cell><cell></cell><cell cols="6">41.09 88.80 97.33 69.80 0.05</cell><cell></cell><cell>3.92</cell><cell cols="7">46.26 28.64</cell><cell></cell><cell cols="4">53.29 27.21 41.92</cell></row><row><cell>BPM [12]</cell><cell></cell><cell>61.43</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell></row><row><cell cols="10">PointASNL [12] 62.60 94.31 98.42 79.13 0.00 26.71</cell><cell cols="6">55.21 66.21 83.32 86.83</cell><cell>47.64</cell><cell></cell><cell cols="4">68.32 56.41 52.12</cell></row><row><cell>IAF-Net [50]</cell><cell></cell><cell cols="8">64.60 91.41 98.60 81.80 0.00 34.90</cell><cell cols="6">62.00 54.70 79.70 86.90</cell><cell>49.90</cell><cell></cell><cell cols="4">72.40 74.80 52.10</cell></row><row><cell>PVT(Ours)</cell><cell></cell><cell cols="8">68.21 91.18 98.76 86.23 0.31 34.21</cell><cell cols="6">49.90 61.45 81.62 89.85</cell><cell>48.20</cell><cell></cell><cell cols="4">79.96 76.45 54.67</cell></row><row><cell>Model</cell><cell cols="2">Latency mIoU</cell><cell>Car</cell><cell>Bicycle</cell><cell>MotorCycle</cell><cell>Truck</cell><cell>Other-vehicle</cell><cell>Person</cell><cell>Bicyclist</cell><cell>Motorcyclist</cell><cell>Road</cell><cell>Parking</cell><cell>Sidewalk</cell><cell>Other-ground</cell><cell>Building</cell><cell>Fence</cell><cell>vegetation</cell><cell>Trunk</cell><cell>Terrain</cell><cell>Pole</cell><cell>Traffic-sign</cell></row><row><cell>PointNet  *</cell><cell>500</cell><cell cols="3">14.6 46.3 1.3</cell><cell>0.3</cell><cell>0.1</cell><cell>0.8</cell><cell>0.2</cell><cell>0.2</cell><cell cols="11">0.0 61.6 15.8 35.7 1.4 41.4 12.9 31.0 4.6 17.6 2.4</cell><cell>3.7</cell></row><row><cell>PointNet++  *</cell><cell cols="4">5900 20.1 53.7 1.9</cell><cell>0.2</cell><cell>0.9</cell><cell>0.2</cell><cell>0.9</cell><cell>1.0</cell><cell cols="11">0.0 72.0 18.7 41.8 5.6 62.3 16.9 46.5 13.8 30.0 6.2</cell><cell>8.9</cell></row><row><cell>PVCNN  *</cell><cell>146</cell><cell>39.0</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>TangentConv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>** 110 63.7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Ours) 142 64.9 97.7 76.9 59.8 30.5 91.8 94.4 49.2 34.3 42.3 62.1 81.3 65.6 70.9 47.5 68.7 86.2 70.8 52.1 54.0Table 7. Results of outdoor scene segmentation on SemanticKITTI. * : results directly taken from<ref type="bibr" target="#b35">[36]</ref> Results of semantic segmentation of 3D indoor scenes on S3DIS, evaluated with 6-fold cross-validation. From this table, we can see that the proposed PVT outperforms most of previous 3D models.We set two baselines: A and B. Model A only encodes global context features by the point branch, and Model B only encodes local features by the voxel one. As reported inTable 9, the Baseline model A obtains a low accuracy of 92.8% on classification benchmarks, and model B gets 92.3%. When we combine local and global features (PVT f ull ), there is a notable improvement in both tasks. This means our network can take advantage of the com-Ablation study on ModelNet40. PB and VB mean the point-based branch and the voxel-based branch; NPB denotes the number of PVT blocks; shifting means all self-attention modules adopt the cyclic shifted box partitioning method.</figDesc><table><row><cell>Model</cell><cell cols="3">mIoU(%) OA(%) mAcc(%)</cell><cell>Model</cell><cell cols="3">PB VB shifting NPB OA(%) Latency</cell></row><row><cell>PointNet</cell><cell>47.6</cell><cell>78.5</cell><cell>66.2</cell><cell>A</cell><cell>3</cell><cell>92.8</cell><cell>33.5</cell></row><row><cell>G+RCU [10] TangentConv [37] RSNet [28] 3P-RNN [56]</cell><cell>49.7 52.8 56.5 56.3</cell><cell>81.1 ? ? 86.9</cell><cell>? ? 66.5 ?</cell><cell>B C D</cell><cell>3 3 2</cell><cell>92.3 93.0 93.1</cell><cell>25.2 47.9 36.4</cell></row><row><cell>SGPN [44]</cell><cell>50.4</cell><cell>?</cell><cell>?</cell><cell>E</cell><cell>4</cell><cell>93.6</cell><cell>72.5</cell></row><row><cell>SPGraph [20]</cell><cell>62.1</cell><cell>85.5</cell><cell>73.0</cell><cell>PVT f ull</cell><cell>3</cell><cell>94.1</cell><cell>48.6</cell></row><row><cell>HAPGN [5]</cell><cell>62.9</cell><cell>85.8</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVCNN</cell><cell>63.2</cell><cell>85.8</cell><cell>72.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ShellNet [61]</cell><cell>66.8</cell><cell>87.1</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVT(Ours)</cell><cell>69.2</cell><cell>88.3</cell><cell>76.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="9296" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clusternet: Deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hapgn: Hierarchical attentive pooling graph network for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2335" to="2346" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">4d spatio-temporal convnets: Minkowski convolutional neural networks. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. in ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Point transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Se(3)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boundary-aware geometric encoding for semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">A</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling local geometric structure of 3d point clouds using geo-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointgrid: A deep network for 3d shape understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno>6cd):210.1- 210.12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding. in ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rs-net: Regression-segmentation 3d cnn for synthesis of full resolution missing brain mri in the presence of tumours. International Workshop on Simulation and Synthesis in Medical Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Voxsegnet: Volumetric cnns for semantic part segmentation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3d shapenets for 2.5d object recognition and next-best-view prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<title level="m">3d shapenets: A deep representation for volumetric shapes. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Snowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5479" to="5489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Investigate indistinguishable points in semantic segmentation of 3d point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/2103.10339, 2021. 9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gridgcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="403" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12478" to="12487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pointbert: Pre-training 3d point cloud transformers with masked point modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/2111.14819, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Linked dynamic graph cnn: Learning on point cloud via linking hierarchical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10014</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pcan: 3d attention map learning using contextual information for point cloud based retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
