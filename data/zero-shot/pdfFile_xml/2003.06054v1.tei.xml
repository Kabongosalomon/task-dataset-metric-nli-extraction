<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Domain-Adversarial Image Generation for Domain Generalisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Domain-Adversarial Image Generation for Domain Generalisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on Deep Domain-Adversarial Image Generation (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Most existing deep learning models assume that the training (source) and testing (target) data come from the same domain/dataset and thus follow the same distribution. However, in practice this assumption is often invalid. For example, a module for recognising pedestrians and traffic signs in an autonomous driving car may be deployed anywhere in the world under any weather condition. Considering each city and weather combination as a domain, it is impossible to collect training data of every domain for model training. Other domain changes can correspond to the change of image style/modality such as those shown in <ref type="figure">Figure 1</ref> where a classifier trained on images of cartoon, photo and sketch is applied to art images. Unfortunately, it is well known that existing deep learning models are sensitive to domain changes/shifts <ref type="bibr" target="#b21">Shankar et al. 2018;</ref><ref type="bibr" target="#b0">Balaji, Sankaranarayanan, and Chellappa 2018)</ref> in that they tend to overfit the source domains, resulting in poor generalisation.</p><p>A straightforward way to deal with the domain gap between source and target domains is to acquire labelled tar-get data and perform supervised model fine-tuning. However, large-scale data collection and annotation for every new target domain is prohibitively expensive and timeconsuming, which make the fine-tuning strategy infeasible. A more economical solution is to use unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b5">Ganin and Lempitsky 2015;</ref><ref type="bibr" target="#b8">Hoffman et al. 2018;</ref>, which only use unlabelled target data. Although the data annotation step is avoided, UDA still requires a data collection step followed by a model adaptation step for each new domain, which hinders its applicability.</p><p>As a result, domain generalisation (DG) <ref type="bibr" target="#b18">(Muandet, Balduzzi, and Scholkopf 2013)</ref> has received an increasing interest lately. The goal of DG is to train a model using data from multiple source domains and deploy the model to an arbitrary unseen target domain without any adaption. Many existing DG methods adopt a core idea from the domain adaption (DA) research, which is to align source domain distributions at feature-level, assuming that a source domain invariant model can be learned <ref type="bibr" target="#b10">(Li et al. 2018b;</ref><ref type="bibr" target="#b11">Li et al. 2018c</ref>). However, without access to any target domain data, the model learned with domain alignment can still overfit the source domains. Alternatively, meta learning based methods have been recently employed to address DG where held-out source domains are used to simulate unseen target domains <ref type="bibr" target="#b9">(Li et al. 2018a;</ref><ref type="bibr" target="#b0">Balaji, Sankaranarayanan, and Chellappa 2018)</ref>. However, meta learning models still focus on narrowing domain gaps among source domains and thus offer no guarantee for generalisation to unseen domains.</p><p>In this paper, we tackle the DG problem by synthesising data from unseen domains. We assume that augmenting the original training data of source domains with synthetic data from unseen domains could make the task model intrinsically more domain-generalisable <ref type="bibr" target="#b22">(Tobin et al. 2017;</ref><ref type="bibr" target="#b26">Yue et al. 2019)</ref>. To this end, a novel framework based on Deep Domain-Adversarial Image Generation (DDAIG) is introduced, which is illustrated in <ref type="figure">Figure 1</ref>. There are three components in DDAIG, which are label classifier, domain classifier and domain transformation network (DoT-Net). Each component is a deep neural network. The label classifier and domain classifier are trained to predict the class labels and domain labels of the input data respectively. The functionality of DoTNet is to transform the input data in such a way that they can be recognised by the label classifier but fool the domain classifier. In particular, the transformation produced by DoTNet is designed to be perturbations with the same shape as the input. Therefore, the new data is generated by combining the perturbations with the original input. By doing so, we can efficiently generate additional training data that covers the (otherwise sparsely sampled) manifold of domains, which in turn allows a more domainagnostic label classifier to be learned. We further show that DoTNet can be easily extended to incorporate other types of transformations, e.g., geometric transformations by adding spatial transformer network (STN) <ref type="bibr" target="#b8">(Jaderberg et al. 2015)</ref>. In practice, the three networks are trained jointly in an endto-end manner. Unlike the domain alignment and meta learning methods, our DDAIG works directly at pixel-level, thus largely improving the interpretability of the model.</p><p>To evaluate DDAIG, we conduct extensive experiments on three DG benchmark datasets, namely PACS , <ref type="bibr">Office-Home (Venkateswara et al. 2017</ref>) and digit recognition among <ref type="bibr">MNIST (LeCun et al. 1998)</ref>, <ref type="bibr">MNIST-M (Ganin and Lempitsky 2015)</ref>, <ref type="bibr">SVHN (Netzer et al. 2011)</ref> and SYN <ref type="bibr" target="#b5">(Ganin and Lempitsky 2015)</ref>. These datasets cover a variety of visual recognition tasks and contain different types of domain variation (see <ref type="figure" target="#fig_2">Figure 4</ref>). We demonstrate that DDAIG outperforms current state-of-theart DG methods on all datasets. We also verify the effectiveness of DDAIG on a heterogeneous DG task, i.e., person re-identification where the source and target domains have different label spaces. Finally, we visualise the generated images and feature embeddings to provide insights on why our approach works. The code is available at https://github.com/KaiyangZhou/DG-research-pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Early kernel alignment <ref type="bibr" target="#b18">(Muandet, Balduzzi, and Scholkopf 2013;</ref><ref type="bibr" target="#b4">Gan, Yang, and Gong 2016)</ref> and examplar SVM based <ref type="bibr" target="#b19">Niu, Li, and Xu 2015)</ref> DG models have been followed mainly by deep neural network based ones. The current deep DG studies can be generally divided into three groups: (i) domain alignment, (ii) meta learning and (iii) data augmentation. Domain alignment has been extensively studied for domain adaptation (DA) problems, where some labelled or unlabelled data are accessible during training. These DA methods aim to either (i) minimise the distance (e.g., maximum mean discrepancy (MMD) <ref type="bibr" target="#b20">(Pan, Kwok, and Yang 2008)</ref>) between source and target distributions, or (ii) fool a domain classifier that tries to discriminate different domains <ref type="bibr" target="#b5">(Ganin and Lempitsky 2015)</ref>. As shown in <ref type="bibr" target="#b17">(Motiian et al. 2017)</ref>, domain alignment based DA models can be easily modified for DG by iteratively training on every pair of source domains. Among the recent alignment based deep DG models, <ref type="bibr" target="#b10">(Li et al. 2018b)</ref> proposed to minimise MMD of all possible pairs within source domains, meanwhile an adversarial autoencoder was used to ensure that the learned features follow the Laplace distribution. <ref type="bibr" target="#b11">(Li et al. 2018c</ref>) considered aligning the conditional distributions as well as the marginal ones via adversarial training. Though domain alignment is a sensible strategy for DA, the potential risk of applying it to DG is that the model might overfit all seen domains yet still generalise poorly to the unseen domains.</p><p>Meta learning in computer vision has been widely exploited for few-shot learning <ref type="bibr" target="#b3">(Finn, Abbeel, and Levine 2017)</ref>. Recently, meta learning has been adapted to address the DG setting <ref type="bibr" target="#b9">(Li et al. 2018a;</ref><ref type="bibr" target="#b0">Balaji, Sankaranarayanan, and Chellappa 2018)</ref>. Since the final objective of a DG model is to generalise to unseen domains, the key idea of using meta learning is to simulate domain shift during training to prepare models for domain shift during testing. Specifically, source domains are separated into two disjoint sets, namely meta-train and meta-validation, and a model is optimised on meta-train so as to boost the performance on metavalidation. One early work in this direction is MLDG <ref type="bibr" target="#b9">(Li et al. 2018a)</ref>, which is based on MAML <ref type="bibr" target="#b3">(Finn, Abbeel, and Levine 2017)</ref>. Recently, MetaReg (Balaji, Sankaranarayanan, and Chellappa 2018) proposed to learn a customised regulariser to improve DG. A meta learning based DG approach is appealing as it reduces the efforts in manual design. However, as a black-box approach it is hard to diagnose exactly how it improves the DG performance. Importantly, using only the original source domain data, it still has the risk of overfitting source domains.</p><p>Data augmentation is a common practice to train deep neural networks, e.g. flipping and rotation. However, conventional data augmentation methods only deal with simple geometric changes within the same dataset <ref type="bibr" target="#b23">(Volpi and Murino 2019)</ref>. When the domain gap is large such as those illustrated in <ref type="figure" target="#fig_2">Figure 4</ref> containing image style variations, learning-based augmentation strategies are required. Very recently, inspired by adversarial attacks <ref type="bibr" target="#b6">(Goodfellow, Shlens, and Szegedy 2015)</ref>, <ref type="bibr" target="#b21">(Shankar et al. 2018)</ref> introduced CrossGrad to generate a new samplex by adding to the original sample its gradient from a domain classifier h(?), i.e.,</p><p>x ? x + ?h(x) ?x . A similar approach with an additional regularisation term in h(?) was proposed in <ref type="bibr" target="#b24">(Volpi et al. 2018)</ref> for the single source domain case. The main drawback of these methods is their direct and simple dependence on the gradient, which only makes simple perturbations that cannot account for semantic changes like style or font shift (see <ref type="bibr">Figure 9)</ref>. Moreover, being based on adversarial attack models</p><formula xml:id="formula_0">x T ? f h ' xJ D J L @J L @? @J D @? x f J L f J L x h ' x @J L @ @J L @ J D @J D @'</formula><p>Training label classifier f Training domain classifier h ' Training domain transformation network T ? that are deliberately designed to make imperceptible modifications to an image, the perturbations are too subtle to be representative of real-world domain shift. In contrast, by learning a full CNN model (i.e. DoTNet) to generate the 'shift', we can produce more sophisticated and more overt perturbations to synthesise new data and the results are easier to interpret. We demonstrate clear advantages of our approach over CrossGrad both quantitatively and qualitatively. We also show that our transformation CNN can be easily extended to incorporate geometric transformations such as rotation (see <ref type="figure">Figure 5</ref>), which is impossible with gradientbased perturbation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Our idea to tackle domain generalisation (DG) is based on Deep Domain-Adversarial Image Generation (DDAIG), which aims to train a domain transformation network (DoT-Net) to synthesise data from unseen domains given some input and use both original and synthetic data to learn a domain-invariant classifier. To learn DoTNet, we simultaneously train a label classifier and a domain classifier, which are tasked to recognise the class labels and domains of the input data, respectively. The learning objective for DoTNet is to transform the input data in such a way that the synthetic data keeps the same labels as the input but fools the domain classifier. As the synthetic data has labels, it can be combined with the original data to train the label classifier using supervised learning. As a result, the label classifier can learn representations that are more invariant to domain shift than that trained with the original data only 1 . An overview of our DDAIG framework is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. The learning procedure for each component is detailed below. Domain transformation network. Let T ? be the DoTNet parameterised by ?, f ? the label classifier parameterised by ?, h ? the domain classifier parameterised by ?, y the class label of input x and d the domain label, the objective function for T ? is</p><formula xml:id="formula_1">min ?J L (f ? (T ? (x)), y) ?J D (h ? (T ? (x)), d),<label>(1)</label></formula><p>whereJ L andJ D are cross-entropy losses for label and domain classification, respectively. We use differentiable neural networks to construct T ? , f ? and h ? , thus the gradients can be back-propagated through f ? and h ? and all the way to T ? . The specific architecture design of T ? will be discussed later.</p><p>Label classifier. The label classifier f ? is fed with both original and synthetic data. The loss function is min</p><formula xml:id="formula_2">? (1 ? ?)J L (f ? (x), y) + ?J L (f ? (T ? (x)), y), (2)</formula><p>where ? is a balance weight, which is fixed to 0.5. Domain classifier. The domain classifier h ? is required to capture domain-discriminative features, thus its learning objective is to minimise the domain classification loss w.</p><formula xml:id="formula_3">r.t ?, min ? J D (h ? (x), d).<label>(3)</label></formula><p>Note that our domain classifier is analogous to the discriminator in the classic GAN framework <ref type="bibr" target="#b21">(Goodfellow et al. 2014)</ref> but differs in that we do multi-class classification (Odena, Olah, and Shlens 2017) (on source domains 2 ) while GAN's discriminator performs binary classification (real or fake). This difference ensures that maximising the domain classification loss does not simply force the synthetic data to fall into another single domain distribution. Suppose there are three source domains, given a synthetic instance from the first domain we maximise ? log e z 1 e z 1 +e z 2 +e z 3 (z i denotes logit), which essentially minimises z 1 whilst giving equal gradients to maximise z 2 and z 3 simultaneously. As such, neither one of the gradients to z 2 and z 3 is dominant. Architecture design. For the label classifier, any network architecture suitable for the given recognition problem can be adopted. Throughout this paper, the domain and label classifiers share the same architecture. To construct DoT-Net, we use fully convolutional network (FCN) <ref type="bibr" target="#b16">(Long, Shelhamer, and Darrell 2015)</ref>. Instead of directly generating data, which is often difficult because the data can be highdimensional such as RGB image, we use FCN to generate perturbations, which are added to the input, resulting i?  where ? is a positive weight typically set between 0.1 and 0.7. This is inspired by the residual feature learning <ref type="bibr" target="#b7">(He et al. 2016</ref>) but the residual connection links the input directly to the output. This design is also related to adversarial attack methods <ref type="bibr" target="#b21">(Szegedy et al. 2014;</ref><ref type="bibr" target="#b6">Goodfellow, Shlens, and Szegedy 2015)</ref>. However, different from adversarial perturbations, which are usually imperceptible, our DoTNet is allowed to produce visually perceptible perturbations, which can better represent the real-world domain shift (see <ref type="bibr">Figure 8)</ref>. Note that (4) will replace the T ? (x) in <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula">(2)</ref>. More concretely, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the architecture of DoTNet starts with a 3 ? 3 conv layer, followed by n 2-conv residual blocks <ref type="bibr" target="#b7">(He et al. 2016)</ref> to extract mid-level features. All residual blocks use 3 ? 3 kernels. The output is branched into an identity layer and a global average-pooling layer. The latter produces a context vector encapsulating the global information <ref type="bibr" target="#b14">(Liu, Rabinovich, and Berg 2016)</ref>. The context vector is expanded spatially and then concatenated back to the main stream, which is further processed by a 1?1 conv layer for feature fusion. Finally, a 1 ? 1 conv layer is used to generate the perturbation. All layers use ReLU as the non-linearity function except the last one which uses the Tanh function. Similar to <ref type="bibr" target="#b32">(Zhu et al. 2017)</ref>, we insert instance normalisation layer <ref type="bibr" target="#b23">(Ulyanov, Vedaldi, and Lempitsky 2017)</ref> after every conv layer excluding the last one. The stride is set to 1 for all layers. All 3 ? 3 kernels use the reflection padding of size 1.</p><formula xml:id="formula_4">x = x + ?T ? (x),<label>(4)</label></formula><p>In this paper, we mainly investigate this perturbation design for T . However, our framework is generic and does not impose any restriction on the architecture (as long as it is differentiable). The design of T thus mostly depends on the specific tasks at hand. For instance, T can take the form of <ref type="bibr">STN (Jaderberg et al. 2015)</ref> to deal with geometric transformations; or even a combination of STN and the perturbation architecture in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The full algorithm of DDAIG is presented in Algorithm 1. Note that the warm-up scheme mainly aims to make the data generated by DoTNet more reliable before being fed to the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Settings</head><p>We first evaluate our approach DDAIG on three conventional DG benchmark datasets, which cover a variety of recognition problems. <ref type="formula" target="#formula_1">(1)</ref>     shift mainly corresponds to image style changes as depicted in <ref type="figure" target="#fig_0">Figure 4 2nd row.</ref> (3) Office-Home (Venkateswara et al. 2017), originally introduced for domain adaptation, is getting popular in the DG community <ref type="bibr" target="#b0">Carlucci et al. 2019)</ref>. It contains four domains, which are Artistic, Clipart, Product and Real World. Each domain has 65 classes, which are related to office and home objects. There are around 15,500 images in total. The domain variations mainly take place in background, viewpoint and image style. See <ref type="figure" target="#fig_2">Figure 4</ref> (the third row) for example images.</p><p>For performance measure, we report top-1 classification accuracy (%) averaged over five runs and 95% confidence intervals. We compare our DDAIG with state-of-the-art DG methods with reported results on these datasets or codes.  <ref type="bibr" target="#b12">(Li et al. 2019a)</ref>. We also include a strong baseline called Vanilla, which directly combines data from all source domains for model training without any DGtargeting tricks.</p><p>Evaluation on Digits-DG Implementation. Images are resized to 32 ? 32 and converted to RGB by replicating channels. The classification network is constructed by four 3 ? 3 conv layers (64 kernels), each followed by ReLU and 2?2 max-pooling. A softmax classification layer is attached on the top, which takes the flattened vector as input. The networks are trained from scratch using SGD, initial learning rate of 0.05, batch size of 128 and weight decay of 5e-4 for 50 epochs. The learning rate is decayed by 0.1 every 20 epochs. Results. <ref type="table" target="#tab_3">Table 1</ref> shows that our DDAIG achieves the best overall performance (Avg.), outperforming the second best CrossGrad by a clear margin of 1.8% and all domain alignment methods (CCSA &amp; MMD-AAE) by more than 3%. On the most difficult target domains, namely MNIST-M and SVHN which contain complex backgrounds and cluttered digits respectively, DDAIG obtains large margins over the competitors, notably with +5.3% and +6.9% improvements compared with the Vanilla model. This demonstrates the effectiveness of the generated unseen domain data, which essentially increases the diversity of source domains.</p><p>Evaluation on PACS Implementation. Images are resized to 224 ? 224. <ref type="bibr">Following (Carlucci et al. 2019;</ref><ref type="bibr" target="#b12">Li et al. 2019a</ref>), we use the  ImageNet-pretrained ResNet18 <ref type="bibr" target="#b7">(He et al. 2016)</ref> as the classification network. The networks are trained with SGD, initial learning rate of 5e-4, batch size of 16 and weight decay of 5e-4 for 25 epochs. The learning rate is decayed by 0.1 at the 20th epoch. For data augmentation, we use random crop on images rescaled by a factor of 1.25 and random horizontal flip. During the first three epochs, the label classifier is only fed with real data. Results. We summarise our findings from <ref type="table" target="#tab_4">Table 2</ref> as follows. (1) Our DDAIG is clearly the best method, beating the second best methods MetaReg and Epi-FCR by around 1.5% (on Avg.). It is noted that MetaReg benefits from additional training data from the validation split. The recently proposed Epi-FCR uses episodic training to improve the Vanilla model. DDAIG outperforms Epi-FCR on all domains by a clear margin, suggesting that data augmentation with unseen domain data is much more effective. (2) Again, DDAIG achieves large improvements (3.7%+ on Avg.) against all domain alignment methods. (3) Compared with CrossGrad, DDAIG yields large improvements on all domains except Photo. In particular, the margins are 4.4% on Art, 1.3% on Cartoon and 4.5% on Sketch. This justifies that learning a dedicated CNN for perturbation generation is much more useful than gradient-based perturbations.</p><p>Evaluation on Office-Home Implementation. Following <ref type="bibr" target="#b0">Carlucci et al. 2019)</ref>, we randomly split the data into 90% for training and 10% for validation. The commonly used leave-one-domain-out protocol is adopted for evaluation. For fair comparison with published methods, we only use the training split of source domains for model training.  Other implementation details for network training are the same as those for PACS. Results. From <ref type="table" target="#tab_5">Table 3</ref>, we observe that the Vanilla model achieves very strong performance, largely outperforming most DG methods including MMD-AAE, D-SAM and Ji-Gen. This makes sense because the domain gap is much smaller compared to that in PACS, especially among Artistic, Product and Real World, where the variations mainly take place in background and viewpoint. Among all methods, only DDAIG achieves a clear margin against Vanilla.</p><p>In particular, it is worth noting that DDAIG achieves huge improvement (+2.9%) on Clipart, which contains the largest domain gap as opposed to the source domains (see <ref type="figure" target="#fig_2">Figure 4)</ref>. Therefore, the results strongly demonstrate the versatility of our DDAIG framework. Compared with CrossGrad, DDAIG achieves better performance on all domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Heterogeneous DG</head><p>In this section, we evaluate our approach on the cross-dataset person re-identification (re-ID) task, which is essentially a heterogeneous DG problem due to disjoint label spaces between training and test identities <ref type="bibr" target="#b13">(Li et al. 2019b)</ref>. Person re-ID aims to match people across non-overlapping camera views. In this task, we treat each camera view as a domain. Datasets. We use two commonly used re-ID datasets, namely Market1501 <ref type="bibr" target="#b27">(Zheng et al. 2015)</ref> and DukeMTMC-reID (Duke) <ref type="bibr" target="#b20">(Ristani et al. 2016;</ref><ref type="bibr" target="#b28">Zheng, Zheng, and Yang 2017)</ref>. Market1501 contains 32,668 images of 1,501 identities, which are captured by 6 cameras. Duke contains 36,411 images of 1,812 identities, which are captured by 8 cameras. Each dataset is split into training set, query set and gallery set based on the standard protocols <ref type="bibr" target="#b27">(Zheng et al. 2015;</ref><ref type="bibr" target="#b28">Zheng, Zheng, and Yang 2017)</ref>. For evaluation, we train models using one dataset and perform test on the other. Cumulative Matching Characteristics (CMC) ranks and mean Average Precision (mAP) are used as the performance measure. Implementation. Images are resized to 256?128. We adopt the state-of-the-art re-ID model OSNet <ref type="bibr" target="#b31">Zhou et al. 2019a</ref>) as the CNN backbone. Following <ref type="bibr" target="#b31">Zhou et al. 2019a)</ref>, we train the re-ID model using the standard classification pipeline where each person identity is regarded as a class. Therefore, the entire training . It is clear that only our DDAIG consistently improves upon the vanilla baseline on both settings, with noticeable margins. It is widely acknowledged that crossdomain re-ID is a challenging problem <ref type="bibr" target="#b29">(Zhong et al. 2019;</ref><ref type="bibr" target="#b31">Zhou et al. 2019a)</ref>. Without using target data, it is difficult to gain improvement over the vanilla model. Therefore, the results strongly demonstrate the versatility of our DDAIG: It is not only effective for the conventional DG tasks but also useful to heterogeneous DG problems such as person re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Analysis</head><p>Impact of ?. <ref type="table" target="#tab_7">Table 5</ref> shows the results of varying ? in Eq. 4 from 0.1 to 0.7. When the target domain is less dissimilar to the source domains such as MNIST and SYN, the result does not vary too much with different ?s. However, when the target domain has a larger domain gap than the sources, e.g. MNIST-M and SVHN, our model shows a moderate sensitivity to ?. It is important to note that all results are better than the Vanilla baseline.</p><p>Dealing with geometric transformations. Image perturbation is less useful in simulating geometric transformations such as rotation. To overcome this limitation, we extend the perturbation CNN ( <ref type="figure" target="#fig_1">Figure 3</ref>) by inserting STN (Jaderberg et al. 2015) before it. Therefore, the new DoTNet first geometrically transforms the input and then adds perturbation. Note that such a transformation is impossible with gradientbased perturbation methods <ref type="bibr" target="#b21">(Shankar et al. 2018)</ref>. We test this design on Digits-DG where the target domain is rotated MNIST and the source domains are MNIST-M, SVHN and SYN. Note that the 'rotation' shift is never observed among the sources. The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. First of all, we observe that all methods' performance drops as the rotation degree increases. This is expected because increasing the rotation degree essentially enlarges the domain gap with the source data, making the target domain more challenging.</p><p>Comparing DDAIG (without STN) with CrossGrad, the performance drop of the latter is much larger. This indicates that the CNN-learned perturbation does not contain solely the style changes but also sophisticated transformations, which   <ref type="figure">Figure 5</ref> for the visualisation of the transformations. Importance of combining source and novel data. <ref type="table" target="#tab_9">Table 7</ref> shows that training with the novel data only does not bring any gain at all. This is expected because the performance gain of DDAIG mainly comes from the aggregation of source domains and the generated novel domains. Visualisation. To better understand why DDAIG works for DG, we visualise the feature embeddings in the domain space using t-SNE (Maaten and Hinton 2008) (see <ref type="figure" target="#fig_4">Figure 6</ref>). It is clear that the new data distributions do not overlap with any of the existing source domains. Instead, they are distributed over the unfilled domain space, indicating exploration of unseen domains. Consequently, the generated unseen-domain data along with the source-domain data allows the model to learn more domain-generalisable representations, which explains why DDAIG achieves excellent performance on all DG benchmark datasets. <ref type="figure">Figure 7</ref> shows four example images from the first domain space where the new "art" image clearly differs from the other source images in terms of image style. <ref type="figure">Figure 8</ref> provides a clearer view of how images are transformed by the perturbations. Comparing the transformed images with the original images, we observe that the domainrelated information has been drastically changed while the category-specific properties are well maintained, which is consistent with the motivation of our method. The perturba- tions are instance-specific and can represent complex transformations such as colour and texture. For instance, the perturbation for the elephant image tends to add green and pink colours to the background and enhance the textures on the elephant body.</p><p>Comparison with adversarial perturbations. <ref type="figure">Figure 9</ref> compares the perturbations between DDAIG and Cross-Grad. It is obvious that CrossGrad's perturbation does not contain meaningful patterns and look like salt-andpepper noise, resembling those of adversarial attack methods . In contrast, our perturbation is instance-specific and has obvious effects on the transformed image, which is more representative of the real-world domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We presented DDAIG, a novel DG method to synthesise data from unseen domains for data augmentation. Unlike current data augmentation-based DG methods, DDAIG learns a full transformation CNN to model the domain shift. Extensive experiments on three DG datasets showed that our method can improve the generalisation of CNN models on unseen domains, outperforming current state-of-the-art DG methods. Results on the cross-domain person re-ID task further demonstrated the versatility of DDAIG beyond DG.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our framework. A domain transformation network T ? is trained by minimising the label classification lossJL while maximising the domain classification lossJD on the transformed datax. The label classifier f ? is learned by minimising the label classification loss given both original and transformed data. The domain classifier h? is trained to classify each instance into one of source domains. The red dashed arrows represent the gradient flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of domain transformation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example images from Digits-DG (1st row), PACS (2nd row) and Office-Home (3rd row) show that large domain gaps exist, challenging domain generalisation. M (Ganin and Lempitsky 2015), SVHN (Netzer et al. 2011) and SYN (Ganin and Lempitsky 2015), which differ drastically in font style and background (see Figure 4 1st row</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>These methods include CCSA (Motiian et al. 2017), MMD-AAE (Li et al. 2018b), CrossGrad (Shankar et al. 2018), MetaReg (Balaji, Sankaranarayanan, and Chellappa 2018), D-SAM (D'Innocente and Caputo 2018), JiGen (Carlucci et al. 2019) and Epi-FCR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>T-SNE visualisation in the domain space using PACS's validation set. The first three images compare transformed data (T) with original data for each source domain. The last image shows an overall comparison between original (grey) and transformed (pink) data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Examples of transformed images from PACS (1st row), Digits-DG (2nd row) and Office-Home (3rd row). x,x and T (x) denote original image, transformed image and transformation (perturbation), respectively. Comparison between perturbations generated by CNN (ours) and adversarial perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Deep Domain-Adversarial Image Generation 1: Input: source domains S, label classifier f ? , domain classifier h?, DoTNet T ? , learning rate ?, hyperparameter ?, loss balance weight ?, maximum iteration K, warmup iteration Km. 2: Output: label classifier f ? . 3: for k = 1 to K do</figDesc><table><row><cell>4:</cell><cell>(x, y, d) ? S</cell><cell></cell><cell cols="2">// Randomly sample a minibatch</cell></row><row><cell cols="3">5:x = x + ?T ? (x)</cell><cell cols="2">// Transform the minibatch</cell></row><row><cell>6:</cell><cell cols="2">? = ? ? ?? ? (JL ?JD)</cell><cell></cell><cell>// Update DoTNet</cell></row><row><cell>7:</cell><cell cols="2">if k &lt; Km then</cell><cell></cell></row><row><cell>8:</cell><cell cols="2">? = ? ? ?? ? JL</cell><cell cols="2">// Update label classifier</cell></row><row><cell>9:</cell><cell>else</cell><cell></cell><cell></cell></row><row><cell cols="3">10:x = x + ?T ? (x)</cell><cell cols="2">// Transform the minibatch using</cell></row><row><cell></cell><cell cols="2">updated DoTNet</cell><cell></cell></row><row><cell>11:</cell><cell cols="3">? = ? ? ?? ? ((1 ? ?)JL + ?JL)</cell><cell>// Update label</cell></row><row><cell></cell><cell cols="4">classifier using both original and synthetic data</cell></row><row><cell>12:</cell><cell>end if</cell><cell></cell><cell></cell></row><row><cell>13:</cell><cell cols="2">? = ? ? ???JD</cell><cell cols="2">// Update domain classifier</cell></row><row><cell cols="2">14: end for</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MNIST</cell><cell>MNIST-M</cell><cell>SVHN</cell><cell>SYN</cell></row><row><cell></cell><cell>Art Painting</cell><cell>Cartoon</cell><cell>Photo</cell><cell>Sketch</cell></row><row><cell></cell><cell>Artistic</cell><cell>Clip Art</cell><cell>Product</cell><cell>Real World</cell></row></table><note>We conduct leave-one-domain-out digit recognition on MNIST (LeCun et al. 1998), MNIST- Algorithm 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Leave-one-domain-out results on Digits-DG dataset (with 95% confidence intervals). 8?.3 58.8?.5 61.7?.5 78.6?.6 73.7 CCSA 95.2?.2 58.2?.6 65.5?.2 79.1?.8 74.5 MMD-AAE 96.5?.1 58.4?.1 65.0?.1 78.4?.2 74.6 CrossGrad 96.7?.1 61.1?.5 65.3?.5 80.2?.2 75.8 DDAIG (ours) 96.6?.2 64.1?.4 68.6?.6 81.0?.5 77.6</figDesc><table><row><cell>Method</cell><cell>MNIST MNIST-M SVHN</cell><cell>SYN Avg.</cell></row><row><cell>Vanilla</cell><cell>95.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Leave-one-domain-out results on PACS dataset (with 95% confidence intervals). ?: results are reported in their papers. ?: use train+val for training. 83.7?.1 77.2?.3 95.5?.2 70.3?.3 81.7 Vanilla 77.0?.6 75.9?.6 96.0?.1 69.2?.6 79.5 CCSA 80.5?.6 76.9?.6 93.6?.4 66.8?.9 79.4</figDesc><table><row><cell>Method</cell><cell>Art</cell><cell cols="2">Cartoon Photo</cell><cell cols="2">Sketch Avg.</cell></row><row><cell>MetaReg  ? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMD-AAE</cell><cell cols="5">75.2?.3 72.7?.3 96.0?.1 64.2?.2 77.0</cell></row><row><cell>CrossGrad</cell><cell cols="5">79.8?.8 76.8?.8 96.0?.2 70.2?.4 80.7</cell></row><row><cell>D-SAM  ?</cell><cell>77.3</cell><cell>72.4</cell><cell>95.3</cell><cell>77.8</cell><cell>80.7</cell></row><row><cell>JiGen  ?</cell><cell>79.4</cell><cell>75.3</cell><cell>96.0</cell><cell>71.6</cell><cell>80.5</cell></row><row><cell>Epi-FCR  ?</cell><cell>82.1</cell><cell>77.0</cell><cell>93.9</cell><cell>73.0</cell><cell>81.5</cell></row><row><cell cols="6">DDAIG (ours)) 84.2?.3 78.1?.6 95.3?.4 74.7?.8 83.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Leave-one-domain-out results on Office-Home dataset (with 95% confidence intervals). ?: results are reported in their papers.</figDesc><table><row><cell>Method</cell><cell cols="5">Artistic Clipart Product Real World Avg.</cell></row><row><cell>Vanilla</cell><cell cols="5">58.9?.3 49.4?.1 74.3?.1 76.2?.2 64.7</cell></row><row><cell>CCSA</cell><cell cols="5">59.9?.3 49.9?.4 74.1?.2 75.7?.2 64.9</cell></row><row><cell>MMD-AAE</cell><cell cols="5">56.5?.4 47.3?.3 72.1?.3 74.8?.2 62.7</cell></row><row><cell>CrossGrad</cell><cell cols="5">58.4?.7 49.4?.4 73.9?.2 75.8?.1 64.4</cell></row><row><cell>D-SAM  ?</cell><cell>58.0</cell><cell>44.4</cell><cell>69.2</cell><cell>71.5</cell><cell>60.8</cell></row><row><cell>JiGen  ?</cell><cell>53.0</cell><cell>47.5</cell><cell>71.5</cell><cell>72.8</cell><cell>61.2</cell></row><row><cell cols="6">DDAIG (ours)) 59.2?.1 52.3?.3 74.6?.3 76.0?.1 65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on cross-domain person re-ID datasets. DDAIG (ours) 50.6 65.2 70.3 28.6 60.9 77.1 83.2 29.0</figDesc><table><row><cell>Method</cell><cell>Market1501?Duke R1 R5 R10 mAP R1 R5 R10 mAP Duke?Market1501</cell></row><row><cell>Vanilla</cell><cell>48.5 62.3 67.4 26.7 57.7 73.7 80.0 26.1</cell></row><row><cell>CrossGrad</cell><cell>48.5 63.5 69.5 27.1 56.7 73.5 79.5 26.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results on Digits-DG using different ?'s.</figDesc><table><row><cell>Method</cell><cell cols="3">MNIST MNIST-M SVHN SYN Avg.</cell></row><row><cell>Baseline</cell><cell>95.8</cell><cell>58.8</cell><cell>61.7 78.6 73.7</cell></row><row><cell cols="2">DDAIG ? = 0.1 96.3</cell><cell>62.3</cell><cell>68.6 79.8 76.8</cell></row><row><cell cols="2">DDAIG ? = 0.3 96.4</cell><cell>61.9</cell><cell>68.0 81.0 76.8</cell></row><row><cell cols="2">DDAIG ? = 0.5 96.6</cell><cell>61.2</cell><cell>68.0 80.5 76.6</cell></row><row><cell cols="2">DDAIG ? = 0.7 96.4</cell><cell>64.1</cell><cell>65.9 80.8 76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Accuracy on rotated MNIST when MNIST-M, SVHN and SYN are used as the source domains.</figDesc><table><row><cell>Method</cell><cell></cell><cell>0 ?</cell><cell>20 ?</cell><cell>30 ?</cell><cell>40 ?</cell><cell>50 ?</cell></row><row><cell cols="2">CrossGrad</cell><cell>96.7</cell><cell>82.7</cell><cell>65.1</cell><cell>45.6</cell><cell>30.1</cell></row><row><cell cols="2">DDAIG w/o STN</cell><cell>96.6</cell><cell>87.7</cell><cell>72.1</cell><cell>54.1</cell><cell>40.3</cell></row><row><cell cols="2">DDAIG w/ STN</cell><cell>96.4</cell><cell>87.7</cell><cell>76.7</cell><cell>61.1</cell><cell>46.6</cell></row><row><cell>x</cell><cell cols="2">TSTN(x) TP(TSTN(x))</cell><cell>x</cell><cell cols="3">TSTN(x) TP(TSTN(x))</cell></row><row><cell cols="7">Figure 5: Transformations produced by an extended DoTNet.</cell></row><row><cell cols="6">STN: Spatial Transformer Network. P: Perturbation.</cell></row><row><cell cols="7">algorithm remains the same as before. At test time, the fea-</cell></row><row><cell cols="7">tures extracted from the re-ID model are used to compute</cell></row><row><cell cols="7">Euclidean distance for image matching. The code is based</cell></row><row><cell cols="5">on Torchreid (Zhou and Xiang 2019).</cell><cell></cell></row><row><cell cols="7">Results. We compare our method with CrossGrad and the</cell></row><row><cell cols="7">strong vanilla model. The overall results are shown in Ta-</cell></row><row><cell>ble 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison between models trained using source data, novel data and source+novel data. Transformed image vs. original source images. make the task model more robust to the geometric domain shift. With STN, the performance drop is significantly reduced (especially on 40 ? and 50 ? ) which demonstrates the flexibility of the DDAIG framework. See</figDesc><table><row><cell cols="3">Souce Novel MNIST MNIST-M SVHN SYN Avg.</cell></row><row><cell>95.8</cell><cell>58.8</cell><cell>61.7 78.6 73.7</cell></row><row><cell>95.6</cell><cell>58.3</cell><cell>57.9 79.9 72.9</cell></row><row><cell>96.6</cell><cell>64.1</cell><cell>68.6 81.0 77.6</cell></row><row><cell>Figure 7:</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To clarify, a classifier means the combination of a feature extraction backbone and a softmax classification layer, unless specified otherwise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In DG tasks, we often assume that we have access to multiple source domains.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaranarayanan</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>; D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>NeurIPS. [Carlucci et al. 2019</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Domain generalization by solving jigsaw puzzles</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; D&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbeel</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Levine ; Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlens</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>ICCV. [Li et al. 2018a] Li, D.</editor>
		<editor>Yang, Y.</editor>
		<editor>Song, Y.-Z.</editor>
		<editor>and Hospedales, T. M.</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Deeper, broader and artier domain generalization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-critic networks for heterogeneous domain generalization</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabinovich</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<editor>ICLR-W</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelhamer</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Visualizing data using t-sne. JMLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balduzzi</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-W</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok</forename><surname>Yang ; Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-W</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">triguing properties of neural networks. In ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedaldi</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure from latent domains for domain generalization</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">;</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Torchreid: A library for deep learning person re-identification in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10093</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning generalisable omni-scale representations for person re-identification</title>
		<idno type="arXiv">arXiv:1910.06827</idno>
		<idno>Zhou et al. 2019b</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Omni-scale feature learning for person re-identification</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
