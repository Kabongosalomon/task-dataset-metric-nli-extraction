<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Projection Network for Cross Dimension Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">SIAT</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
							<email>ttwong@cse.cuhk.edu.hkhengshuang.zhao@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">SIAT</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Projection Network for Cross Dimension Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2D image representations are in regular grids and can be processed efficiently, whereas 3D point clouds are unordered and scattered in 3D space. The information inside these two visual domains is well complementary, e.g., 2D images have fine-grained texture while 3D point clouds contain plentiful geometry information. However, most current visual recognition systems process them individually. In this paper, we present a bidirectional projection network (BPNet) for joint 2D and 3D reasoning in an end-to-end manner. It contains 2D and 3D sub-networks with symmetric architectures, that are connected by our proposed bidirectional projection module (BPM). Via the BPM, complementary 2D and 3D information can interact with each other in multiple architectural levels, such that advantages in these two visual domains can be combined for better scene recognition. Extensive quantitative and qualitative experimental evaluations show that joint reasoning over 2D and 3D visual domains can benefit both 2D and 3D scene understanding simultaneously. Our BPNet achieves top performance on the ScanNetV2 benchmark for both 2D and 3D semantic segmentation. Code is available at https://github.com/wbhu/BPNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding is a fundamental while challenging problem in computer vision. Multiple sensors are used to capture the scene information. The 2D camera is the most common sensor in our daily life. It projects the 3D space to image planes with plenty of fine-grained textures captured. 2D images with pixels densely arranged in regular grids can be processed efficiently with deep convolutional neural networks. We have witnessed remarkable improvements on 2D visual reasoning, e.g., image classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref> and semantic segmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b71">72]</ref>. 3D sensors, on the other hand, can provide important geometry information of the scene. 3D data is usually represented in points that * Equal contribution. ? Corresponding author. are unordered and irregularly scattered in 3D space. Conventional convolution that relies on ordered grids can not be directly adapted to 3D data. Hence, several tailor-made neural networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed for 3D scene recognition and understanding.</p><p>We observe that the information inside 2D and 3D data is well complementary. 2D images provide detailed texture and color information while 3D point clouds contain strong shape and geometry knowledge. Although the techniques for individual 2D and 3D reasoning are studied a lot in the literature, the exploration of combining both 2D and 3D data for recognition is very limited. Existing methods that utilize both 2D and 3D data for recognition mostly adopt the unidirectional scheme. For example, to incorporate 3D information for 2D scene understanding, some methods either encode depth into geocentric inputs <ref type="bibr" target="#b12">[13]</ref> or incorporate it into convolution operations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60]</ref>. But depth map only contains limited geometry information as it is view-dependent. The occluded part under the viewpoint together with the global context is missing. In the other aspect, 3DMV <ref type="bibr" target="#b7">[8]</ref> and MVPNet <ref type="bibr" target="#b23">[24]</ref> utilize 2D information to assist the 3D recognition by first extracting multi-view image features and then lifting them into 3D space for fusing with the 3D features. However, we argue that the unidirectional scheme cannot fully leverage the complementary information inside the 2D and 3D data, bidirectionally interacting and fusing 2D and 3D features can better combine the advantages of these two visual domains as evidenced by our experiments.</p><p>In this paper, we present a Bidirectional Projection Network (BPNet) to enable information inside the 2D and 3D domains to flow bidirectionally at the network architectural level. Such that the complementary information can be well combined for joint 2D and 3D scene understanding in an end-to-end manner. Our method adopts two similar U-Net structures to process 2D and 3D data and introduces a Bidirectional Projection Module (BPM) to bidirectionally fuse the multi-view 2D and 3D features. In this way, both 2D and 3D sides can benefit from each other. The overall framework is shown in <ref type="figure">Figure 1</ref>. To be mentioned, we employ BPM at multiple pyramid levels, such that the features from 2D and 3D domains can be aggregated in a coarse-to-fine manner and the BPNet can harvest both low-and high-level complementary information. At each level, BPM builds the projection link matrix between 2D and 3D, and then transfers the features bidirectionally according to the link matrix, i.e., 2D features are projected into 3D space for boosting the recognition of 3D and vice the verse. We evaluated our model on ScanNetV2 <ref type="bibr" target="#b6">[7]</ref> dataset for both 2D and 3D semantic segmentation tasks. BPNet achieves top performance on the benchmark in terms of mIoU and consistently outperforms the baseline with a single 2D/3D network. Also, the qualitative results show the effectiveness of combining 2D and 3D information. BPNet can distinguish objects without much shape difference (e.g., "wall" and "picture") in 3D segmentation. Meanwhile, 2D objects are better segmented with sharper boundaries thanks to the underlying geometric clues provided by 3D features. Besides, we evaluated the generalization ability of BPNet for 2.5D data on the typical RGB-D dataset, NYUv2 <ref type="bibr" target="#b37">[38]</ref>, and the results show BPNet performs favorably against the typical RGB-D and joint 2D-3D baselines. We believe the proposed BPM is also advantageous to other tasks where both 2D and 3D resources are available, e.g., classification, detection, and instance segmentation. Our contributions are summarized below.</p><p>? We argue that 2D and 3D information is complementary for the understanding of each other and joint optimization over both 2D and 3D scenes is applicable and proved to be beneficial.</p><p>? We propose a Bidirectional Projection Module (BPM) that enables information interacting between the 2D and 3D representations. And such bidirectional projection operation can be adopted at multiple levels in the decoder stage.</p><p>? We present a novel framework named Bidirectional Projection Network (BPNet) for jointly reasoning over 2D and 3D scenes. Our method achieves top performance on the challenging large-scale ScanNetv2 benchmark for both 2D and 3D semantic segmentation tasks, which demonstrates its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2D semantic segmentation. Semantic segmentation on 2D images has been significantly improved with deep neural networks. By replacing the last fully-connected layers in classification frameworks with convolution operations, FCN <ref type="bibr" target="#b34">[35]</ref> can produce the per-pixel predictions. Several encoder-decoder architectures <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b0">1]</ref> are proposed to utilize the information in low-level layers to help refine the segmentation outputs. The receptive field is vital for accurate scene understanding, thus, dilated convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b65">66]</ref> are introduced to enlarge the receptive field. Contextual information is also proved to be effective as adopted in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b64">65]</ref>. Meanwhile, attention models <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b22">23]</ref> are also utilized in semantic segmentation for their abilities to capture long-range contextual relationships. The capability of these 2D-driven approaches is limited by the lack of geometric information, and their performance would be further boosted with other domain features like 3D geometric representations.</p><p>3D semantic segmentation. 3D point clouds are irregularly scattered in 3D space, resulting in difficulties in 3d scene understanding, as convolution operation generally work on regular grids <ref type="bibr" target="#b69">[70]</ref>. Point based frameworks mainly adopt Multilayer Perception (MLP)-style networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref>, including local region-feature enhancement <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b19">20]</ref>, kernel-based parametric convolution <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b54">55]</ref> and graph reasoning <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. Apart from directly handling the irregular inputs, some methods transform the unordered point sets to ordered ones that can be processed by convolution operations, including voxelization, followed by 3D convolution approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b13">14]</ref> and the efficient generations <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>. Multi-view mechanisms <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> are also widely adopted where 3D point clouds are projected into 2D images via different camera views. Besides, convolution on meshes <ref type="bibr" target="#b46">[47]</ref>, fused point-voxel representation <ref type="bibr" target="#b66">[67]</ref>, occupancy-aware design <ref type="bibr" target="#b13">[14]</ref>, and multi-task learning <ref type="bibr" target="#b20">[21]</ref> are explored to improve the 3D scene understanding performance. However, the 3D source lacks detailed textures and color information, resulting in limited performance. And it would be improved with the help of 2D representations.</p><p>Recognition with combined 2D-3D data. There exist several approaches that unidirectionally fuse 2D and 3D information for improving the recognition performance in the 2D or 3D domain. To incorporate 3D information for 2D scene understanding, Gupta et al. <ref type="bibr" target="#b12">[13]</ref> encode depth as HHA images to provide the geocentric information, 3DGNN <ref type="bibr" target="#b43">[44]</ref> utilizes 3D graph neural networks for gradually refining the semantic representations and Depth-aware CNN <ref type="bibr" target="#b59">[60]</ref> presents a depth-aware convolution to cooperate depth information. On the other hand, some methods are dedicated to introducing 2D information into 3D scene understanding, e.g., back projecting multi-view 2D features to 3D volumes <ref type="bibr" target="#b7">[8]</ref>, or point clouds <ref type="bibr" target="#b23">[24]</ref>, then aggregating them with the original 3D features; and extracting features from texture patches <ref type="bibr" target="#b21">[22]</ref> for the 3D semantic segmentation using surface parameterization. Different from them, we conduct crossdimension reasoning which enables bidirectional 2D and 3D feature interaction, such that both 2D and 3D semantic segmentation can benefit from each other. Besides, AutoContext <ref type="bibr" target="#b10">[11]</ref> employs a decision tree to fuse up the hand-crafted features from images and point clouds for facade segmentation, xMUDA <ref type="bibr" target="#b24">[25]</ref> presents cross-modal unsupervised domain adaption to improve performance on the image or point cloud domain. SurfaceNet <ref type="bibr" target="#b25">[26]</ref> encodes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D UNet 3D MinkowskiUNet</head><p>Multi-view RGB images input Predicted 2D labels Predicted 3D labels 3D scene input <ref type="figure">Figure 1</ref>. Overview of the Bidirectional Projection Network (BPNet). It consists of two symmetric sub-networks, 2D UNet <ref type="bibr" target="#b45">[46]</ref> (in the left-hand side) and 3D MinkowskiUNet <ref type="bibr" target="#b5">[6]</ref> (in the right-hand side). Both the 2D and 3D sub-networks are the U-shaped network with the same number of pyramid levels, but they are constructed based on conventional 2D convolution and 3D sparse convolution <ref type="bibr" target="#b11">[12]</ref>, respectively. At multiple pyramid levels in the decoder stage, the features interact bidirectionally between 2D and 3D sub-networks via the proposed Bidirectional Projection Module (BPM). Both 2D and 3D semantic labels will be predicted simultaneously by our BPNet.</p><p>camera parameters and images in a 3D voxel representation and adopts 3D CNN on it for multiview stereopsis. Moreover, SPLATNet <ref type="bibr" target="#b49">[50]</ref> presents to represent point clouds in a high-dimensional lattice that enables fusing image and point features in the high-dimensional lattice for facade and 3D part segmentation. However, this 2D-3D fusion relies on the tailor-made bilateral convolution on the high-dimensional lattice. Different from them, our method can be directly applied to the conventional 2D and 3D CNNs for interacting 2D and 3D features bidirectionally in multi-levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The goal of our method is to jointly predict the 2D and 3D semantics on real-world data, given 3D scenes and 2D image sequences along with corresponding camera matrix. In this section, we first present our Bidirectional Projection Module (BPM) ( <ref type="figure">Figure 2</ref>) that enables cross-dimension feature interaction between 2D and 3D feature representations. Then we introduce the design details of our Bidirectional Projection Network (BPNet) ( <ref type="figure">Figure 1</ref>). Finally, we give our implementation details of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bidirectional Projection Module</head><p>The Bidirectional Projection Module (BPM), as shown in <ref type="figure">Figure 2</ref>, is designed to construct skip connections between 2D and 3D sub-networks at the same decoder level for bidirectionally interacting features in 2D and 3D domains, such that the strengths of these two visual domains can be integrated to boost both 2D and 3D scene understanding. Specifically, given the 3D scene, and 2D image together with camera matrix M, we first construct the link matrix L between voxels and pixels according to the perspective projection from 3D to 2D space. Then, at multiple levels in the decoding stage, we not only project the 3D features F 3D to 2D space but also back-project 2D features F 2D to 3D space according to the constructed link matrix. And finally, we concatenate the projected features with the original features followed by a 1 ? 1 convolution to fuse them. We then feed the fused features into the next levels. The detailed procedures in BPM are discussed in the following paragraphs. Link matrix construction. The camera imaging process can be seen as a perspective projection from 3D voxels to 2D pixels. Mathematically, we can formulate it as:</p><formula xml:id="formula_0">[u i , v i , 1] T = M[x i , y i , z i , 1] T , (1) where [x i , y i , z i , 1] T and [u i , v i , 1]</formula><p>T are the homogeneous 3D coordinates of the i th voxel and its projected 2D homogeneous coordinates, respectively; and M is the matrix that is the product of the intrinsic camera calibration matrix and the extrinsic camera pose matrix.</p><p>Based on it, we can create a link matrix L to store the corresponding 2D pixel coordinate [u, v] T for each voxel [x, y, z] T , as shown in part (a) of <ref type="figure">Figure 2</ref>. But some voxels may have no corresponding pixels, since the image under a certain viewpoint only captures part of the whole 3D scene, like the k th voxel in part (a) of <ref type="figure">Figure 2</ref>. Therefore, we further add a mask m with binary values to indicate whether this 3D voxel has a corresponding pixel. Due to the occlusion, another problem is the hidden surfaces (or voxels) that can be projected to certain pixels but have no relations with the pixels at all, like the j th voxel in part (a) of <ref type="figure">Figure 2</ref> </p><formula xml:id="formula_1">is 1?1 #$%&amp; view ! " ' !" ( !" ) !" ' !" ( !" ) !" * #$ (,, &amp;) * %$ (') 3D space / * #$ (,, &amp;) / * %$ (') 1?1 #$%&amp; (a) Link Matrix Construction (b) Bidirectional Projection Input to next level Link matrix ? 2D plane Camera matrix ? ! ! " ! 1 ! " " " 0 ! # " # 0 U V M Figure 2. Bidirectional Projection Module (BPM).</formula><p>The link matrix construction procedure between one view and the 3D scene is illustrated in (a), and the bidirectional projection procedure is shown in (b). M is the camera matrix; the i th , j th and k th voxels are typical examples of three kinds of voxels: voxels that have corresponding pixels under this view, voxels that are occluded by others, and voxels that are out of the view frustum. F2D and F3D are features in 2D and 3D spaces, respectively. F2D and F3D are the 2D and 3D bidirectionally projected features from the other domain, respectively.</p><p>occluded by the i th voxel. In computer graphics, Z-Buffer algorithm <ref type="bibr" target="#b2">[3]</ref> is commonly used to handle the hidden surface removal problem. It is also applicable to our case, but the depth map is available here, thus, we can efficiently determine whether a voxel is hidden or not by comparing the depth value and the z coordinate of the projected voxel. Compared with the Z-Buffer algorithm, doing so can speed up the computing, as we compare the projected z coordinate only once for each voxel. As a result, we construct link matrix L as an N ?3 matrix,</p><formula xml:id="formula_2">[U, V, M ],</formula><p>where N is the number of voxels in the 3D scene; U and V are the perspectively projected 2D coordinate according to Equation 1; and M is the binary mask to indicate whether this row of the link matrix is valid or not. Each row of the link matrix L can be represented as:</p><formula xml:id="formula_3">L i = ? ? ? ? ? ? ? ? ? ? ? if U min ? u i ? U max [u i , v i , 1], and V min ? v i ? V max and |d(u i , v i ) ? z i | ? ? [u i , v i , 0], otherwise<label>(2)</label></formula><p>where i ? [1, N ] is the index of the voxels; [u i , v i ] T is the 2D coordinate; 1 or 0 is the binary mask value; U min , U max , V min and V max are the boundaries of the view frustum; d(?) is the mapping from pixel coordinates to the depth; z i is the projected z coordinate of the voxel; and ? &gt; 0 is the threshold for depth matching (it is set to be the voxel size in our implementation.) Note that, this link matrix is bidirectional, it can be used both to transform 2D points to 3D points and vice versa.</p><p>Bidirectional projection. The link matrix is constructed for the original input 3D voxels and 2D images, while the 2D and 3D features have different spatial sizes compared with the original input data. To produce the link matrix for 2D and 3D features in certain decoder levels, we linearly remap the [u, v] coordinates according to the down-sampling ratio to fit for the spatial size of features in the link vectors.</p><p>Having the remapped links for each level of features, we can transform the features bidirectionally between 2D and 3D domains in multiple levels, as shown in part (b) of <ref type="figure">Figure 2</ref>. At one of the decoder levels, we have the 2D feature F 2D with the shape of H ? W ? C 2D , and the 3D feature F 3D with the shape of N ? C 3D , where N is the number of voxels, C 2D and C 3D are the channel numbers of 2D and 3D, respectively. Meanwhile, the constructed link matrix L has the shape of N ? 3, where "3" indicates the corresponding u, v coordinate of 2D feature and validity mask m. To project 3D features to 2D space, we can formulate it as:</p><formula xml:id="formula_4">F 2D (u i , v i ) = ? ? ? m i ? F 3D (i), if [u i , v i ] ? L uv 0, otherwise<label>(3)</label></formula><p>where F 2D is the projected 2D feature from 3D feature that has the shape of H ? W ? C 3D ; i ? [1, N ] is the index of voxels or link matrix entries; [u i , v i , m i ] is the i th entry of L; and L uv is the first two columns of the link matrix L. On the other hand, to back project 2D features to 3D space, we formulate it as:</p><formula xml:id="formula_5">F 3D (i) = m i ? F 2D (u i , v i ),<label>(4)</label></formula><p>where F 3D is the back-projected feature from 2D feature with the shape of N ?C 2D . These procedures can be implemented efficiently by the fanny indexing in PyTorch <ref type="bibr" target="#b39">[40]</ref> via our constructed link matrix. View fusion. The above discussion is based on a 3D scene with a single 2D view. For a 3D scene with multiple 2D views, we can directly project 3D features to each view with the corresponding link matrix. But to transform multi-view 2D features to 3D space, we need to fuse them after backprojecting them to 3D space. Different from 3DMV <ref type="bibr" target="#b7">[8]</ref> that simply aggregates multi-view features by max-pooling, we use two-layer sparse convolutions to learn the impact factors for each view at every point and then weighted sum them up by the learned impact factors as following:</p><formula xml:id="formula_6">F 3D = R r=1 w r ? F r 3D ,<label>(5)</label></formula><p>where R is the number of views and w r is a vector of weights that learned from all the back-projected features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bidirectional Projection Network</head><p>Based on the proposed BPM, we give our full bidirectional projection network (BPNet) as illustrated in <ref type="figure">Figure 1</ref>. The 2D and 3D sub-networks in our BPNet are both Ushaped networks with residual blocks <ref type="bibr" target="#b16">[17]</ref>, but based on conventional 2D convolution and sparse 3D convolution <ref type="bibr" target="#b11">[12]</ref>, respectively. During the training phase, we randomly sample n 2D views to maintain the diversity of 2D data, while during the testing phase, we divide the 2D frames into n groups and select one central view from each group to reduce the overlap among 2D views. In our implementation, n is set to three and the ablation is to be discussed in Section 4.</p><p>We firstly voxelize the 3D point clouds into volumes, and then the voxels represented in the sparse tensor format are fed into the 3D MinkowskiUNet, while the multi-view 2D images are simultaneously fed into the 2D UNet. During the decoder stages, we bidirectionally interact the features between 2D and 3D sub-networks in multiple pyramid levels, i.e., P2, P3, P4, and P5 levels, via the proposed BPM, thus harvesting both the low-and high-level features in these two domains. And finally, both 2D and 3D semantic labels are predicted from 2D and 3D parts in our BPNet, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>As our goal is simultaneously predicting the 2D and 3D semantic labels, the loss function L consists of two terms:</p><formula xml:id="formula_7">L = H 3d + ? ? H 2d ,<label>(6)</label></formula><p>where H 3d and H 2d are the cross entropy losses for 3D and 2D predictions, respectively; and ? is a weight to balance the 2D and 3D losses, which is empirically set to 0.1 in our experiments.</p><p>We implement our algorithm based on the PyTorch <ref type="bibr" target="#b39">[40]</ref> platform and MinkowskiEngine <ref type="bibr" target="#b5">[6]</ref> sparse convolution library. We train our BPNet on the ScanNetV2 dataset <ref type="bibr" target="#b6">[7]</ref> for 100 epochs. The 2D UNet part is initialized from the classification weights pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, while the 3D part is initialized from scratch. We use SGD solver <ref type="bibr" target="#b1">[2]</ref> with a base learning rate of 0.01 and a mini-batch size of 16. And we employ a poly learning rate scheduler with the power set to 0.9. Momentum and weight decay are set to 0.9 and 0.0001, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metric</head><p>ScanNetV2 <ref type="bibr" target="#b6">[7]</ref> contains indoor scenes like offices and living rooms, with 2.5 million RGB-D frames in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and semantic segmentation. It is officially split into 1201 training and 312 validation scans that were taken from 706 different scenes, which means each scene was captured around one to three times, and 100 scans test set with hidden ground truth, used for the benchmark. For the 3D input, as done in <ref type="bibr" target="#b5">[6]</ref>, we extract points cloud from the reconstructed surfaces, and voxelize them into 3D volumes, each voxel is associate with a 3D feature ( R, G, B), while for 2D input, we use the RGB images. Following Minkowsk-iNet <ref type="bibr" target="#b5">[6]</ref>, we set the voxel size to 2cm for the benchmark results, and set it to 5cm in the ablation study for efficient training. For the evaluation metrics, we use the mean of class-wise intersection over union (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison on the ScanNet Benchmark</head><p>3D semantic segmentation. To evaluate the effectiveness of our BPNet for 3D semantic segmentation, we compare our method with typical streams of methods on the test set of ScnaNetV2 in <ref type="table" target="#tab_0">Table 1</ref>, including point-based methods: PointNet++ <ref type="bibr" target="#b42">[43]</ref>, MCCNN <ref type="bibr" target="#b18">[19]</ref>, PointASNL <ref type="bibr" target="#b63">[64]</ref> and KP-FCNN <ref type="bibr" target="#b54">[55]</ref>; sparse convolution based method: Minkowsk-iNet <ref type="bibr" target="#b5">[6]</ref>; and joint 2D-3D-input based methods: 3DMV <ref type="bibr" target="#b7">[8]</ref>, MVPNet <ref type="bibr" target="#b23">[24]</ref> and SPLATNet <ref type="bibr" target="#b49">[50]</ref>. We can see that BP-Net outperforms the point-based methods, e.g., PointNet++, MCCNN, and KP-FCNN, by a large margin (? 6.5 mIoU), due to their limited receptive field and less effectiveness on feature extraction. For the strong sparse convolution based method, MinkowskiNet, although its network receptive field is very large and global context is rich, our method also outperforms it by 1.3 mIoU, since our method leverages the extra 2D features' information. And importantly, our method significantly outperforms other joint 2D-3D-input based methods (? 10.5 mIoU), since their methods adopt  the unidirectional scheme to combine the 2D and 3D information, that means they only regard the 2D CNN as a pre-feature-extractor without bidirectionally interacting features between 2D and 3D CNNs. Besides, other information may further be utilized to boost the performance, like the instance annotation <ref type="bibr" target="#b13">[14]</ref>.</p><p>To qualitatively evaluate the 3D semantic segmentation results, we compare our results with the strong 3D-only method, MinkowskiNet <ref type="bibr" target="#b5">[6]</ref>, and also the ground truth in "3D" rows part in <ref type="figure" target="#fig_1">Figure 3</ref>. As shown in the red boxes, MinkowskiNet cannot correctly distinguish the boundaries in "backpack/floor" and "picture/wall", while our BPNet performs well. This is because the textures of input 3D data are too coarse, that makes it challenging to correctly predict  <ref type="table">Table 2</ref>. Comparison with the top methods on ScanNetV2 2D Semantic label benchmark, including 2D-only, 3D predictions projection, and joint 2D-3D-input based methods (marked with ?). semantics only depend on the 3D input, while our method can additionally leverage the advantages in 2D data to integrate the high-quality texture in images for better predicting semantics. It is worth noting that, obtaining corresponding 2D input is not expensive as cameras are usually equipped with the 3D sensor in the common applications, e.g., indoor/outdoor robotics and autonomous driving. 2D semantic segmentation. We then evaluate the effectiveness of BPNet for 2D semantic segmentation. We compare BPNet with top methods on the ScanNet benchmark, including 2D-only methods: PSPNet <ref type="bibr" target="#b71">[72]</ref> and UNet <ref type="bibr" target="#b45">[46]</ref>; method of projecting 3D predictions to 2D images: 3DMV <ref type="bibr" target="#b7">[8]</ref>; and joint 2D-3D-input based methods: FuseNet <ref type="bibr" target="#b15">[16]</ref>, SSMA <ref type="bibr" target="#b55">[56]</ref> and RFBNet <ref type="bibr" target="#b9">[10]</ref>, as shown in <ref type="table">Table 2</ref>. Note that, the 3D and 2D results of our BPNet in <ref type="table" target="#tab_0">Table 1</ref> and 2 are produced by the same model. We can see that BPNet outperforms the 2D-only methods by a large margin (? 18.1 mIoU). We admit that this comparison is not completely fair since BPNet also leverages 3D information, but such comparison gives evidence that 3D information can boost 2D semantic segmentation. For the method of projecting 3D predictions to 2D images, like 3DMV, although the 3D information is utilized, it still cannot perform as well as BPNet as it is not tailor-made for 2D semantic segmentation. For the joint 2D-3D-input based methods, like FuseNet, SSMA, and RFBNet, both 2D and 3D information are employed, BPNet still outperforms them a lot (? 7.8 mIoU), thanks to our bidirectional projection module (BPM) that effectively interacts with 2D and 3D features to integrate the strengths of these two visual domains.</p><p>Also, we qualitatively compare the results of 2D-only network, UNet34, BPNet, and ground truth in "2D" rows part in <ref type="figure" target="#fig_1">Figure 3</ref>. We can see that BPNet not only correctly distinguishes backpack from the floor but also segments sharper and more accurate boundaries for the "chair" and "table". It is because of the underlying geometric clues provided by 3D features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To explore the effectiveness of different configurations for BPNet, we conduct the following ablation experiments  <ref type="table">Table 3</ref>. 2D and 3D semantic segmentation results of different projection levels and directions on the validation set of ScanNetV2.</p><p>on the validation set of ScanNetV2. To save training time, we use MinkowskiUNet18A for the 3D part and UNet34 for the 2D part in our BPNet, and the voxel size is set to 5cm in the first three ablations. Since there are too many 2D frames in the validation sequences, we follow the benchmark method to sub-sample a frame from every 100 frames to form the new validation set for 2D semantic segmentation.</p><p>Ablation for projection level. As shown in <ref type="figure">Figure 1</ref>, the BPM is applied in all the four pyramid levels (P2, P3, P4, and P5) for our full method. For ablation, we apply BPM at a certain level and compare the results of baseline methods (UNet34 and MinkowskiUNet18A), our full method, and its variants. From the first seven rows in <ref type="table">Table 3</ref>, we can see that our framework with BPM at every single level performs better than the 2D-only network, UNet34, or the 3D-only network, MinkowskiUNet18A. It means bidirectionally interacting 2D and 3D features even only at a single level can improve both 2D and 3D semantic segmentation, since useful information from the other domains is introduced. More importantly, our framework with BPM at all the four levels outperforms all the other variants, which shows bidirectionally interacting 2D and 3D features at both low-and high-levels can better integrate the advantages in the 2D and 3D domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unidirectional projection vs. Bidirectional projection.</head><p>To evaluate the effectiveness of projection directions, we replace the bidirectional projection module (BPM) with the unidirectional projection module (UPM), i.e., UPM 2D ? 3D and UPM 2D ? 3D , and compare the results in <ref type="table">Table 3</ref>. We can see that our method with UPM 2D ? 3D and UPM 2D ? 3D outperforms the 2D-or 3D-only baselines, but our method with BPM performs best for both 2D and 3D results. It evidences that 2D and 3D semantic segmentation can better benefit each other by bidirectionally interacting features. Ablation for the number of 2D views. We then explore the influence of the number of 2D views, and the results are shown in <ref type="table">Table 4</ref>. For 2D semantic segmentation, BPNet with one 2D view performs best while with five views perform worst. This is because when there is only one view, the 3D to 2D information transformation can be best focused. On the other hand, when the number of views increasing from one to three, the 3D semantic segmentation result becomes better and better, but it decreases slightly when the view number further increasing to five. It may because too few views cannot provide sufficient 2D information while too many views can hinder the network from extracting useful information while discarding the redundant information.</p><p>Ablation for voxel size. Finally, we evaluate the influence of voxel size for both 3D and 2D semantic segmentation, as shown in <ref type="table">Table 5</ref>. Comparing the second and third rows, we can see that more fine-grained voxels can greatly improve the 3D semantic segmentation for the 3D-only network. And comparing the last two rows and others, we can see that decreasing voxel size can not only improve the performance of 3D semantic segmentation but also greatly improve the 2D semantic segmentation interestingly. It evidences that the 3D information is transformed into 2D CNNs and the higher quality 3D information can better boost the 2D semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">BPNet on NYUv2</head><p>Although our BPNet is designed for the 3D scene with 2D images, we can also convert the RGB-D data into the 3D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy SceneNet <ref type="bibr" target="#b14">[15]</ref> 52.5 Hermans et al. <ref type="bibr" target="#b17">[18]</ref> 54.3 SemanticFusion <ref type="bibr" target="#b36">[37]</ref> 59.2 Dai et al. <ref type="bibr" target="#b6">[7]</ref> 60.7 3DMV <ref type="bibr" target="#b7">[8]</ref> 71.2 BPNet 73.5 <ref type="table">Table 6</ref>. Semantic segmentation results (13-class task) on NYUv2 <ref type="bibr" target="#b37">[38]</ref> using dense pixel classification accuracy metric. Note that the reported result of Dai et al. is on the 11-class task.</p><p>scene to perform semantic segmentation. Note that, RGB-D data is known as 2.5D data rather than 3D data as the depth is view-dependent. To evaluate the generality of BPNet on 2.5D data, we apply it to the popular RGB-D semantic segmentation dataset, NYUv2 <ref type="bibr" target="#b37">[38]</ref>. It contains 1,449 densely labeled pairs of aligned RGB and depth images. We followed the official training and testing set splits, which use 795 instances for training and 654 for testing. We converted the depth to point clouds according to the depth camera's matrix and back-projected the 2D annotations for the generated point clouds as our 3D labels. Followed 3DMV <ref type="bibr" target="#b7">[8]</ref>, we adopt the configuration of the 13-class label and report the dense pixel classification accuracy compared with typical RGB-D based methods and joint 2D-3D method 3DMV in <ref type="table">Table 6</ref>. Overall, our BP-Net performs favorably against the typical RGB-D and joint 2D-3D baselines. The results on ScanNetV2 and NYUv2 demonstrate the generality of BPNet for different types of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a bidirectional projection network, BPNet, to jointly perform 2D and 3D semantic segmentation, that leverages the complementary advantages in 2D and 3D data. We enable the bidirectional feature interacting between 2D and 3D CNNs in multiple pyramid levels via the proposed bidirectional projection module (BPM), such that the strengths of these two visual domains can be integrated for better scene recognition. BPNet achieves top performance on the ScanNetV2 benchmark and consistently outperforms our baseline with a single 2D/3D network. Also, with our BPNet, we can get more consistent 2D and 3D results. We believe the insight behind the BPM can also benefit other visual recognition tasks where 2D and 3D observations are both available, and it would advance related techniques in the community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative 3D and 2D result examples of the 3D-only network, MinkowskiNet [6], 2D-only network, UNet34, our BPNet, and the ground truths. Different semantics are labeled as corresponding colors as shown in the bottom color palette. We highlight the differences between the results of BPNet and others by red boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Method mIoU bath bed bkshf cab chair cntr curt desk door floor other pic fridge shower sink sofa table toilet wall window PointNet++ [43] 33.9 58.4 47.8 45.8 25.6 36.0 25.0 24.7 27.8 26.1 67.7 18.3 11.7 21.2 14.5 36.4 34.6 23.2 54.8 52.3 25.2 SPLATNet ? [50] 39.3 47.2 51.1 60.6 31.1 65.6 24.5 40.5 32.8 19.7 92.7 22.7 00.0 00.1 24.9 27.1 51.0 38.3 59.3 69.9 26.7 3DMV ? [8] 48.4 48.4 53.8 64.3 42.4 60.6 31.0 57.4 43.3 37.8 79.6 30.1 21.4 53.7 20.8 47.2 50.7 41.3 69.3 60.2 53.9 Comparison with the typical streams of methods on ScanNetV2 3D Semantic label benchmark, including point cloud based, sparse convolution based, and joint 2D-3D-input (marked with ?) based methods.</figDesc><table><row><cell>FAConv[69]</cell><cell>63.0 60.4 74.1 76.6 59.0 74.7 50.1 73.4 50.3 52.7 91.9 45.4 32.3 55.0 42.0 67.8 68.8 54.4 89.6 79.5 62.7</cell></row><row><cell>MCCNN [19]</cell><cell>63.3 86.6 73.1 77.1 57.6 80.9 41.0 68.4 49.7 49.1 94.9 46.6 10.5 58.1 64.6 62.0 68.0 54.2 81.7 79.5 61.8</cell></row><row><cell>FPConv [33]</cell><cell>63.9 78.5 76.0 71.3 60.3 79.8 39.2 53.4 60.3 52.4 94.8 45.7 25.0 53.8 72.3 59.8 69.6 61.4 87.2 79.9 56.7</cell></row><row><cell>MVPNet  ? [24]</cell><cell>64.1 83.1 71.5 67.1 59.0 78.1 39.4 67.9 64.2 55.3 93.7 46.2 25.6 64.9 40.6 62.6 69.1 66.6 87.7 79.2 60.8</cell></row><row><cell>DCM-Net [47]</cell><cell>65.8 77.8 70.2 80.6 61.9 81.3 46.8 69.3 49.4 52.4 94.1 44.9 29.8 51.0 82.1 67.5 72.7 56.8 82.6 80.3 63.7</cell></row><row><cell>PointConv [62]</cell><cell>66.6 78.1 75.9 69.9 64.4 82.2 47.5 77.9 56.4 50.4 95.3 42.8 20.3 58.6 75.4 66.1 75.3 58.8 90.2 81.3 64.2</cell></row><row><cell cols="2">PointASNL [64] 66.6 70.3 78.1 75.1 65.5 83.0 47.1 76.9 47.4 53.7 95.1 47.5 27.9 63.5 69.8 67.5 75.1 55.3 81.6 80.6 70.3</cell></row><row><cell>KP-FCNN [55]</cell><cell>68.4 84.7 75.8 78.4 64.7 81.4 47.3 77.2 60.5 59.4 93.5 45.0 18.1 58.7 80.5 69.0 78.5 61.4 88.2 81.9 63.2</cell></row><row><cell cols="2">MinkowskiNet [6] 73.6 85.9 81.8 83.2 70.9 84.0 52.1 85.3 66.0 64.3 95.1 54.4 28.6 73.1 89.3 67.5 77.2 68.3 87.4 85.2 72.7</cell></row><row><cell>BPNet (Ours)  ?</cell><cell>74.9 90.9 81.8 81.1 75.2 83.9 48.5 84.2 67.3 64.4 95.7 52.8 30.5 77.3 85.9 78.8 81.8 69.3 91.6 85.6 72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Method mIoU bath bed bkshf cab chair cntr curt desk door floor other pic fridge shower sink sofa table toilet wall window PSPNet [72] 47.5 49.0 58.1 28.9 50.7 6.7 37.9 61.0 41.7 43.5 82.2 27.8 26.7 50.3 22.8 61.6 53.3 37.5 82.0 72.9 56.0 UNet34 [46] 48.9 55.3 62.6 26.6 50.3 23.5 37.9 52.4 49.8 41.6 84.5 28.6 32.1 54.0 12.8 60.8 55.3 38.5 81.6 73.6 56.6 3DMV [8] 49.8 48.1 61.2 57.9 45.6 34.3 38.4 62.3 52.5 38.1 84.5 25.4 26.4 55.7 18.2 58.1 59.8 42.9 76.0 66.1 44.6 FuseNet ? [16] 53.5 57.0 68.1 18.2 51.2 29.0 43.1 65.9 50.4 49.5 90.3 30.8 42.8 52.3 36.5 67.6 62.1 47.0 76.2 77.9 54.1 SSMA ? [56] 57.7 69.5 71.6 43.9 56.3 31.4 44.4 71.9 55.1 50.3 88.7 34.6 34.8 60.3 35.3 70.9 60.0 45.7 90.1 78.6 59.9 RFBNet ? [10] 59.2 61.6 75.8 65.9 58.1 33.0 46.9 65.5 54.3 52.4 92.4 35.5 33.6 57.2 47.9 67.1 64.8 48.0 81.4 81.4 61.4 BPNet (Ours) ? 67.0 82.2 79.5 83.6 65.9 48.1 45.1 76.9 65.6 56.7 93.1 39.5 39.0 70.0 53.4 68.9 77.0 57.4 86.5 83.1 67.5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Ours W/ UPM 2D ? 3D P2, P3, P4, P5 62.2 69.7 Ours W/ UPM 2D ? 3D P2, P3, P4, P5 65.0 68.8</figDesc><table><row><cell>Method</cell><cell>Projection Level</cell><cell cols="2">mIoU 2D 3D</cell></row><row><cell>UNet34</cell><cell>-</cell><cell>61.5</cell><cell>-</cell></row><row><cell>MinkowskiUNet18A</cell><cell>-</cell><cell>-</cell><cell>68.0</cell></row><row><cell>Ours W/ BPM</cell><cell>P2</cell><cell cols="2">63.5 70.3</cell></row><row><cell>Ours W/ BPM</cell><cell>P3</cell><cell cols="2">64.1 70.5</cell></row><row><cell>Ours W/ BPM</cell><cell>P4</cell><cell cols="2">62.3 69.7</cell></row><row><cell>Ours W/ BPM</cell><cell>P5</cell><cell cols="2">61.8 68.5</cell></row><row><cell>Ours W/ BPM</cell><cell cols="3">P2, P3, P4, P5 65.1 70.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>65.8 65.1 63.9 63.6 3D mIoU 68.1 68.6 70.6 70.5 70.5Table 4. 2D and 3D semantic segmentation results of different view numbers on the validation set of ScanNetV2.</figDesc><table><row><cell>Number of Views</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="2">2D mIoU 66.5 Method</cell><cell>Voxel Size</cell><cell cols="3">mIoU 2D 3D</cell></row><row><cell>UNet34</cell><cell></cell><cell>-</cell><cell cols="2">61.5</cell><cell>-</cell></row><row><cell>MinkowskiUNet18A</cell><cell></cell><cell>5cm</cell><cell>-</cell><cell></cell><cell>68.0</cell></row><row><cell>MinkowskiUNet18A</cell><cell></cell><cell>2cm</cell><cell>-</cell><cell></cell><cell>72.1</cell></row><row><cell>Ours</cell><cell></cell><cell>5cm</cell><cell cols="2">65.1</cell><cell>70.6</cell></row><row><cell>Ours</cell><cell></cell><cell>2cm</cell><cell cols="2">71.9</cell><cell>73.9</cell></row><row><cell cols="6">Table 5. 2D and 3D semantic segmentation results of different</cell></row><row><cell cols="4">voxel sizes on the validation set of ScanNetV2.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This project is supported by Shenzhen Science and Technology Program (No.JCYJ20180507182410327) and The Science and Technology Plan Project of Guangzhou (No.201704020141).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno>COMPSTA. 2010. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A subdivision algorithm for computer display of curved surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Catmull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<pubPlace>UTAH UNIV SALT LAKE CITY SCHOOL OF COMPUTING</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richlyannotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rfbnet: deep multimodal networks with residual fusion blocks for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00135</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient 2d and 3d facade segmentation using auto-context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds. TOG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere-Pau</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lvar</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ropinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fpconv: Learning local flattening for point convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLRW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semanticfusion: Dense 3d semantic mapping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Pushmeet Kohli Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Volumetric and multiview cnns for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dualconvmesh-net: Joint geodesic and euclidean convolutions on 3d meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3D-assisted feature synthesis for novel views of an object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">SEGCloud: Semantic segmentation of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Young</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Selfsupervised model adaptation for multimodal semantic segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fusion-aware point convolution for online semantic 3d scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">ICNet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
