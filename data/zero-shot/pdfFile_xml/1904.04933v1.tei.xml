<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards High-fidelity Nonlinear 3D Face Morphable Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
							<email>tranluan@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<email>liufeng6@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards High-fidelity Nonlinear 3D Face Morphable Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: With novel enhancements in both learning objective as well as the network architecture, our proposed nonlinear 3D morphable model enables, for the first time, regressing high-fidelity facial shape (geometry) and albedo (skin reflectence) by directly estimating model latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Embedding 3 D morphable basis functions into deep neural networks opens great potential for models with better representation power. However, to faithfully learn those models from an image collection, it requires strong regularization to overcome ambiguities involved in the learning process. This critically prevents us from learning high fidelity face models which are needed to represent face images in high level of details. To address this problem, this paper presents a novel approach to learn additional proxies as means to side-step strong regularizations, as well as, leverages to promote detailed shape/albedo. To ease the learning, we also propose to use a dual-pathway network, a carefully-designed architecture that brings a balance between global and local-based models. By improving the nonlinear 3 D morphable model in both learning objective and network architecture, we present a model which is superior in capturing higher level of details than the linear or its precedent nonlinear counterparts. As a result, our model achieves state-of-the-art performance on 3 D face reconstruction by solely optimizing latent representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision and computer graphics fields have had much interest in the longstanding problem of 3D face reconstruction -creating a detailed 3D model of a person's face from a collection or a single photograph. The problem is important with many applications, including but not limited to face recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50]</ref>, video editing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41]</ref>, avatar puppeteering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref> or virtual make-up <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Recently, an incredible amount of attention is drawn into the simplest but most challenging form of this problem: monocular face reconstruction. Inferring a 3D face mesh from a single 2D photo is arduous and ill-posed since the image formation process blends multiple facial components (shape, albedo) as well as environment (lighting) into a single color for each pixel. To better handle the ambiguity, one must rely on additional prior assumptions, such as constraining faces to lie in a restricted subspace, e.g., 3D Morphable Models (3DMM) <ref type="bibr" target="#b5">[6]</ref> learned from a small 3D scans collection. Many state-of-the-art approaches, either learning-based <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> or optimization-based face reconstruction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>, heavily rely on such priors. While yielding impressive results, these algorithms do not generalize well beyond the underlying model's restricted low-dimensional subspace. As a consequence, the reconstructed 3D face may fail to recover important facial features, contain incorrect details or not well aligned to the input face.</p><p>Recently, with the flourishing in neural network, a few attempts have tried to use deep neural networks to replace the 3DMM basis functions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>. This increases the model representation power and learns model directly from unconstrained 2D images to better capture in-the-wild variations. However, even with better representation powers, these models still rely on many constraints <ref type="bibr" target="#b38">[39]</ref> to regularize the model learning. Hence, their objectives involve the conflicting requirements of a strong regularization for a global shape vs. a weak regularization for capturing higher level details. E.g., in order to faithfully separate shading and albedo, albedo is usually assumed to be piecewise constant <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, which prevents learning albedo with high level of details. In this work, besides learning the shape and albedo, we propose to learn additional shape and albedo proxies, on which we can enforce regularizations. This also allows us to flexibly pair the true shape with strongly regularized albedo proxy to learn the detailed shape or vice versa. As a result, each element can be learned with high fidelity without sacrificing the other element's quality.</p><p>On a different note, many 3DMM models fail to represent small details because of their parameterization. Many global 3D face parameterizations have been proposed to overcome the ambiguities associated with single image face fitting such as noise or occlusion. However, because they are designed to model the whole face at once, it is challenging to use them to represent small details. Meanwhile, local-based models can be more expressive than global approaches but with the cost of being less constrained to realistically represent human faces. We propose using dual-pathway networks to provide a better balance between global and local-based models. From the latent space, there is a global pathway focusing on the inference of global face structure and multiple local pathways generating details of different semantic facial parts. Their corresponding features are then fused together for successive process generation of the final shape and albedo. This network also helps to specialize filters in local pathways for each facial part which both improves the quality and saves the computation power.</p><p>In this paper, we improve the nonlinear 3D face morphable model in both learning objective and architecture:</p><p>? We solve the conflicting objective problem by learning shape and albedo proxies with proper regularization.</p><p>? The novel pairing scheme allows learning both detailed shape and albedo without sacrificing one.</p><p>? The global-local-based network architecture offers more balance between robustness and flexibility.</p><p>? Our model allows high-fidelity 3D face reconstruction by solely optimizing latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>Linear 3DMM. The first generic 3D face model is built by Blanz and Vetter <ref type="bibr" target="#b5">[6]</ref> using principal component analysis (PCA) on 3D scans. Since this seminal work, there has been a large amount of effort on improving 3DMM modeling mechanism. Paysan et al. <ref type="bibr" target="#b29">[30]</ref> replace the previous UV space alignment <ref type="bibr" target="#b5">[6]</ref> by Nonrigid Iterative Closest Point <ref type="bibr" target="#b1">[2]</ref> to directly align 3D scans. <ref type="bibr">Vlasic</ref>   <ref type="bibr" target="#b43">[44]</ref> use convolution neural networks by representing both geometry and skin reflectance in UV space. Despite having greater representation power, these models still have difficulty in recovering small details in the input images due to strong regularizations in their learning objectives. Global/local-based facial parameterization. Although, global 3D face parameterizations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref> can remedy the vagueness associated with monocular face tracking <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>; they can't represent small geometry details without making them exceedingly large and unwieldy. Hence, region or local-based models are proposed to overcome this problem. Blanz and Vetter <ref type="bibr" target="#b5">[6]</ref> and Tena et al. <ref type="bibr" target="#b36">[37]</ref> learn a regionbased PCA, where Blanz and Vetter <ref type="bibr" target="#b5">[6]</ref> segment the face into semantic subregions (eyes, nose, mouth), while Tena et al. <ref type="bibr" target="#b36">[37]</ref> further split into smaller regions to increase the model's expressiveness. Other approaches include a regionbased blendshape <ref type="bibr" target="#b17">[18]</ref> or localized multilinear model <ref type="bibr" target="#b8">[9]</ref>.</p><p>All these models bring more flexibility than the global one but at the cost of being less constrained on realistically representing human faces. Our approach offers a balance between global and local models by using a dual-pathway network architecture. Bagautdinov et al. <ref type="bibr" target="#b2">[3]</ref> try to achieve a similar objective with compositional VAE by introducing multiple layers of hidden variables, but at a cost of extremely large numbers of hidden variables. Residual learning. Residual learning has been used in many vision tasks. In super resolution, Kim et al. <ref type="bibr" target="#b20">[21]</ref> propose to learn the difference between the high-resolution target and the low-resolution input rather than estimating the target itself. In face alignment <ref type="bibr" target="#b18">[19]</ref>, or missing data imputation task <ref type="bibr" target="#b45">[46]</ref>, residual learning is used in many cascade of networks to iteratively refine their estimation by learning the difference with the true target. In this work, we leverage residual learning idea but with a different purpose to overcome conflicting objectives in learning 3D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>For completeness, we start by briefly summarizing the traditional linear 3DMM, the recently proposed nonlinear 3DMM learning method including their limitations. Then we introduce our proposed improvements in both learning objective and network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear 3DMM</head><p>The 3D Morphable Model (3DMM) <ref type="bibr" target="#b5">[6]</ref> provides parametric models representing faces using two components: shape (geometry) and albedo (skin reflectance). Blanz et al. <ref type="bibr" target="#b5">[6]</ref> describe the 3D face space with PCA. The 3D face mesh S ? R 3Q with Q vertices is computed as:</p><formula xml:id="formula_0">S = F S (f S |? S ) = ? S f S ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">F S (f S |? S ) is a function of f S ? R l S , parameterized by ? S .</formula><p>In linear model, F S is simply a matrix multiplication (the mean shape is omitted for clarity). The albedo of the face A ? R 3Q is defined within a template shape, describing the R, G, B colors of Q corresponding vertices. A is also formulated in a similar fashion:</p><formula xml:id="formula_2">A = F A (f A |? A ) = ? A f A .<label>(2)</label></formula><p>To synthesize 2D face images, the 3D mesh is projected onto the image plan with the weak perspective projection model. Then, the texture and 2D image is rendered using an illumination model, i.e., Spherical Harmonics <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Nonlinear 3DMM</head><p>Recently, Tewari et al. <ref type="bibr" target="#b38">[39]</ref>, Tran and Liu <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> concurrently propose to use deep neural network to present 3DMM bases. Essentially, mappings F S and F A are now represented as neural networks with parameters ? S , ? A respectively. Tewari et al. <ref type="bibr" target="#b38">[39]</ref> straightforwardly use multilayer perceptron as their networks. Meanwhile, Tran and Liu <ref type="bibr" target="#b43">[44]</ref> leverage spatial relation of vertices by presenting both S and A in a UV space, denoted S UV , A UV . Mappings F * are convolution neural networks (CNNs) with an extra sampling step converting from R UV to R 3Q . To make the framework end-to-end trainable, they also learn a model fitting module, E, which is another CNN. Beside estimating shape, albedo latent vectors f S , f A , the encoder E also estimates projection matrix M as well as lighting coefficients L. The objective of the whole network is to reconstruct the original input image via a differentiable rendering layer R:</p><formula xml:id="formula_3">arg min E,F S ,F A I L rec (?, I),<label>(3)</label></formula><formula xml:id="formula_4">I = R (E M (I), E L (I), F S (E S (I)), F A (E A (I))) .</formula><p>Reconstruction loss. There are many design options for the reconstruction loss. The straightforward choice is comparing images in the pixel space, with typical l 1 or l 2 loss. To better handle outliers, the robust l 2,1 is adopted, where the distance in the RGB color space is based on l 2 and the summation is based on l 1 -norm to enforce sparsity <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>:</p><formula xml:id="formula_5">L i rec = 1 |V| q?V ? (q) ? I(q) 2 ,<label>(4)</label></formula><p>where V is the set of pixels covered by the estimated mesh. The closeness between images? and I can also be enforced in the feature space (perceptual loss):</p><formula xml:id="formula_6">L f rec = 1 |C| j?C 1 W j H j C j ||? j (?) ? ? j (I)|| 2 2 .<label>(5)</label></formula><p>The loss is summed over C, a subset of layers of the network ?. Here ? j (I) is the activations of the j-th layer of ? with dimension W j ? H j ? C j obtained when processing I. The final reconstruction loss is a weighted average between the image and feature reconstruction losses:</p><formula xml:id="formula_7">L rec (?, I) = L i rec (?, I) + ? f L f rec (?, I).<label>(6)</label></formula><p>Sparse Landmark Alignment. To help achieve better model fitting, which in turn helps to improve the model learning itself, the landmark alignment loss is used as an auxiliary task. The loss is defined by Euclidean distance between estimated and groundtruth landmarks:</p><formula xml:id="formula_8">L lan = M * S(:, d) 1 ? U 2 2 ,<label>(7)</label></formula><p>where U ? R 2?68 is the manual labels of 2D landmark locations, d stores the indexes of 68 vertices corresponding to the sparse 2D landmarks in the 3D face mesh. In <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, the landmark loss is only applied on E to prevent learning implausible shapes as the loss only affects a tiny subsets of vertices related to the keypoints. Different regularization.</p><p>To overcome ambiguity and faithfully recover different elements (shape, albedo, lighting), many regularizations are needed.</p><p>Albedo Symmetry:</p><formula xml:id="formula_9">L sym (A) = A uv ? flip(A uv ) 1 ,<label>(8)</label></formula><p>where flip() is a horizontal image flip operation. Albedo Constancy:</p><formula xml:id="formula_10">L con (A) = v uv j ?Ni ?(v uv i , v uv j ) A uv (v uv i ) ? A uv (v uv j ) p 2 . (9) The weight ?(v uv i , v uv j ) = exp ?? c(v uv i ) ? c(v uv j )</formula><p>, helps to penalize more on pixels with the same chromaticity Proxies free shape and albedo from strong regularizations, allow them to learn models with high level of details.</p><p>(i.e., c(x) = I(x)/|I(x)|), where the color is referenced from the input image using the current estimated projection. N i denotes a set of 4-pixel neighborhood of pixel v uv i . Shape Smoothness: This is a Laplacian regularization on the vertex locations.</p><formula xml:id="formula_11">L smo (S) = v uv i ?S uv S uv (v uv i ) ? 1 |N i | v uv j ?Ni S uv (v uv j ) 2 .<label>(10)</label></formula><p>The overall objective can be summarized as:</p><formula xml:id="formula_12">L = L rec (?, I) + L lan + L reg ,<label>(11)</label></formula><p>with L reg = L sym (A) + ? con L con (A) + ? smo L smo (S). (12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Nonlinear 3DMM with Proxy and Residual</head><p>Proxy and Residual Learning. Strong regularization has been shown to be critical in ensuring the plausibility of the learned models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45]</ref>. However, the strong regularization also prevents the model from recovering high-level details in either shape or albedo. Hence, this prevents us from achieving the ultimate goal of learning a high-fidelity 3DMM model.</p><p>In this work, we propose to learn additional proxy shape (S) and proxy albedo (?), on which we can apply the regularization. All presented regularizations will now be moved to proxies: L * reg = L sym (?) + ? con L con (?) + ? smo L smo (S). (13) There will be no regularization applied directly to the actual shape S and albedo A, other than a weak regularization encouraging each to be close to its proxy:</p><formula xml:id="formula_13">L res = ?S 1 + ?A 1 = S ?S 1 + A ?? 1 .<label>(14)</label></formula><p>By pairing two shapes S,S and two albedos A,?, we can render four different output images <ref type="figure" target="#fig_0">(Fig. 2)</ref>. Any of them can be used to compare with the original input image. We rewrite our reconstruction loss as:</p><formula xml:id="formula_14">L * rec = L rec (?(S,?), I) + L rec (?(S, A), I) + L rec (?(S,?), I).<label>(15)</label></formula><p>Pairing strongly regularized proxies and weakly regularized components is a critical point in our approach. Using proxies allows us to learn high-fidelity shape and albedo without sacrificing quality of either component. This pairing is inspired by the observation that Shape from Shading techniques are able to recover detailed face mesh by assuming over regularized albedo or even using the mean albedo <ref type="bibr" target="#b33">[34]</ref>. Here, L rec (?(S,?), I) loss promotes S to recover more details as? is constrained by piece-wise constant L con (?) objective. Vice versa, L rec (?(S, A), I) aims to learn better albedo. In order for these two losses to work as desired, proxiesS and? should perform well enough to approximate the input images by themselves. Without L rec (?(S,?), I), a valid solution that minimizes L rec (?(S,?), I) is combination of a constant albedo proxy and noisy shape creating surface normal with dark shading in necessary regions, i.e., eyebrows.</p><p>Another notable design choice is that we intentionally left out the loss function on?(S, A), even though this theoretically is the most important objective. This is to avoid the case that the shape S learns an in-between solution that works well with both?, A and vice versa. Occlusion Imputation. With proposed objective function, our model is able to faithfully reconstruct input images. However, we empirically found that besides high- fidelity visible regions, the model tends to keep invisible region smooth. The reason might be that, there is no supervision on those areas other than the residual magnitude loss pulling the shape and albedo closer to their proxies. To learn a more meaningful model, which is beneficial to other applications, i.e., face editing or face synthesis, we propose to use a soft symmetry loss <ref type="bibr" target="#b42">[43]</ref> on occluded regions:</p><formula xml:id="formula_15">L res-sym (S) = T (?S uv z ? flip(?S uv z )) 1 ,<label>(16)</label></formula><p>where T is a visibility mask of each pixel in UV space, approximated based on estimated surface normal direction. Even though the shape itself is not symmetric, i.e., face with asymmetric expression; we enforce symmetrical property on its depth residual ?S z (only use shape's z-dimension).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Global-Local-Based Network Architecture</head><p>While global-based models are usually robust to noise and mismatches, they are usually over-constrained and do not provide sufficient flexibility to represent high-frequency deformations as local-based models. In order to take the best of both worlds, we propose to use dual-pathway networks for our shape and albedo decoders.</p><p>Here, we transfer the success of combining local and global models in image synthesis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref> to 3D face modeling. The general architecture of a decoder is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. From the latent vector, there is a global pathway focusing on inferring the global structure and a local pathway with four small sub-networks generating details of different facial parts, including eyes, nose and mouth. The global pathway is built from fractional strided convolution layers with five up-sampling steps. Meanwhile, each sub-network in the local pathway has the similar architecture but shallower with only three up-sampling steps. Using different small sub-networks for each facial part offers two benefits: i) with less up-sampling steps, the network is better able to represent high-frequency details in early layers; ii) each sub-network can learn part-specific filters, which is more computationally efficient than applying across global face.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, to fuse two pathways' features, we firstly integrate four local pathways' outputs into one single feature tensor. Different from other works that synthesize face images with different yaw angles <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> with no fixed keypoints' locations, our 3DMM generates facial albedo as well as 3D shape in UV space with predefined topology. Merging these local feature tensors is efficiently done with the zero padding operation. The max-pooling fusion strategy is also used to reduce the stitching artifacts on the overlapping areas. Then the resultant feature is simply concatenated with the global pathway's feature, which has the same spatial resolution. Successive convolution layers integrate information from both pathways and generate the final albedo/shape (or their proxies).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We study different aspects of the proposed framework, in terms of framework design, model representation power, and applications to facial analysis.</p><p>The training is similar to <ref type="bibr" target="#b44">[45]</ref>, which also include a pretrain stage with supervised losses. Adopting Basel Face Model (BFM) <ref type="bibr" target="#b29">[30]</ref>'s facial mesh triangle topology, we use a subset of Q = 39, 111 vertices on the face region only. The model is trained on 300W-LP dataset <ref type="bibr" target="#b51">[52]</ref>, which contains 122, 450 in-the-wild face images, in a wide pose range.</p><p>The model is optimized using Adam optimizer with a learning rate of 0.001. We set the following parameters: U = 192, V = 224, l S = l A = 320. ? values are selected to bring losses to similar magnitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Reconstruction Loss Functions. We study effects of different reconstruction losses on quality of the reconstructed images ( <ref type="figure" target="#fig_2">Fig. 4)</ref>. As expected, the model trained with l 2,1 loss only results in blurry reconstruction, similar to other l p loss. To make the reconstruction more realistic, we explore other options such as gradient difference <ref type="bibr" target="#b26">[27]</ref> or perceptual loss <ref type="bibr" target="#b16">[17]</ref>. While adding the gradient difference loss creates more details in the reconstruction, combining perceptual loss with l 2,1 gives the best results with high level of details and realism. For the rest of the paper we will refer <ref type="figure">Figure 5</ref>: Image reconstruction with our 3DMM model using the proxy and the true shape and albedo. Our shape and albedo can faithfully recover details of the face. Note: for the shape, we show the shading in UV space -a better visualization than the raw S UV . <ref type="figure">Figure 6</ref>: Affect of soft symmetry loss on our shape model. to the model trained using this combination. Understanding image pairing. <ref type="figure">Fig. 5</ref> shows fitting results of our model on a 2D face image. By using the proxy or the final components (shape or albedo) we can render four different reconstructed images with different quality and characteristics. The image generated by two proxies S,? is quite blurry but is still be able to capture major variations in the input face. By pairing S and the proxy?, S is enforced to capture high level of details to bring the image closer to the input. Similarly, A is also encouraged to capture more details by pairing with the proxyS. The final image?(S, A) inherently achieves high level of details and realism even without direct optimization. Residual Soft Symmetry Loss. We study effects of the residual soft symmetry loss on recovering details on occluded face region. As shown in <ref type="figure">Fig. 6</ref>, without L res-sym , the learned model can result in an unnatural shape, in which one side of the face is over-smooth, on occluded regions, while the other side still has high level of details. Our model learned with L res-sym can consistently create details across the face, even in occluded areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Representation Power</head><p>We compare the representation power of the proposed nonlinear 3DMM with Basel Face Model <ref type="bibr" target="#b29">[30]</ref>, the most commonly used linear 3DMM. We also make comparisons with the recently proposed nonlinear 3DMM <ref type="bibr" target="#b43">[44]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Reconstruction error (l 2,1 )</p><p>Linear <ref type="bibr" target="#b51">[52]</ref> 0.1287 Nonlinear <ref type="bibr" target="#b44">[45]</ref> 0.0427 Nonlinear + GL (Ours) 0.0386 Nonlinear + GL + Proxy (Ours) 0.0363</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Texture.</head><p>We evaluate our model's power to represent in-the-wild facial texture on AFLW2000-3D dataset <ref type="bibr" target="#b51">[52]</ref>. Given a face image, also with the groundtruth geometry and camera projection, we can jointly estimate an albedo parameter f A and a lighting parameter L whose decoded texture can reconstruct the original image. To accomplish this, we use SGD on f A and L with the initial parameters estimated by our encoder E. For the linear model, Zhu et al. <ref type="bibr" target="#b51">[52]</ref> fitting results of Basel albedo using Phong illumination model <ref type="bibr" target="#b30">[31]</ref> is used. As in <ref type="figure" target="#fig_3">Fig. 7</ref>, nonlinear model significantly outperforms the Basel Face model. Despite, being close to the original image, Tran and Liu <ref type="bibr" target="#b44">[45]</ref> model reconstruction results are still blurry. Using global-local-based network architecture ("+GL") with the same loss functions helps to bring the image closer to the input. However, these models are still constrained by regularizations on the albedo. By learning using proxy technique ("+Proxy"), our model can learn more realistic albedo with more high frequency details on the face. This conclusion is further supported with quantitative comparison in Tab. 1. We report the averaged l 2,1 reconstruction error over the face portion of each image. Our model achieves the lowest averaged reconstruction error among four models, 0.0363, which is a 15% error reduction of the recent nonlinear 3DMM work <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape.</head><p>Similarly, we also compare models' power to represent real-world 3D scans. Using ten 3D face meshes shape, we optimize the feature fS to approximate the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tran and Liu Linear 3DMM</head><p>Our <ref type="figure">Figure 9</ref>: The distance between the input images and their reconstruction from three models. For better visualization, images are sorted based on their distance to our model's reconstructions.</p><p>provided by <ref type="bibr" target="#b29">[30]</ref>, which share the same triangle topology with us, we can optimize the shape parameter to generate, through the decoder, shapes matching the groundtruth scans. The optimization objective is defined based on vertex distances (Euclidean) as well as surface normal direction (cosine distance), which empirically improves reconstructed meshes' fidelity compared to optimizing the former only. <ref type="figure" target="#fig_4">Fig. 8</ref> shows the visual comparisons between different reconstructed meshes. Our reconstructions closely match the face shapes details. To make quantitative comparisons, we use NME -averaged per-vertex Euclidean distances between the recovered and groundtruth meshes, normalized by inter-ocular distances. The proposed model has a significantly smaller reconstruction error than the linear model, and is also smaller than the nonlinear model by Tran and Liu <ref type="bibr" target="#b44">[45]</ref> (0.0139 vs. 0.0146 <ref type="bibr" target="#b44">[45]</ref>, and 0.0241 <ref type="bibr" target="#b29">[30]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Identity-Preserving</head><p>We explore the effect of our proposed 3DMM on preserving identity when reconstructing face images. Using DR-GAN <ref type="bibr" target="#b47">[48]</ref>, a pretrained face recognition network, we can compute the cosine distance between the input and its reconstruction from different models. <ref type="figure">Fig. 9</ref> shows the plot of these score distributions. At each horizontal mark, there are exactly three points presenting distances between an im-  age with its reconstructions from three models. Images are sorted based on the distance to our reconstruction. For the majority of the cases (77.2%), our reconstruction has the smallest difference to the input in the identity space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">3D Reconstruction</head><p>Using our model F S , F A , together with the model fitting CNN E, we can decompose a 2D photograph into different components: 3D shape, albedo and lighting <ref type="figure" target="#fig_5">(Fig. 10</ref>). Here we compare our 3D reconstruction results with different lines of works: linear 3DMM fitting <ref type="bibr" target="#b39">[40]</ref>, nonlinear 3DMM fitting <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45]</ref> and approaches beyond 3DMM <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>. Comparisons are made on CelebA dataset <ref type="bibr" target="#b24">[25]</ref>.</p><p>For linear 3DMM model, the representative work, MoFA by Tewari et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>, learns to regress 3DMM parameters in an unsupervised fashion. Even being trained on inthe-wild images, it is still limited to the linear subspace, with limited power to recovering in-the-wild texture. This results in the surface shrinkage when dealing with challenging texture, i.e., facial hair as discussed in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. Besides, even with regular skin texture their reconstruction is still blurry and has less details compared to ours <ref type="figure" target="#fig_6">(Fig. 11</ref>).  The most related work to our proposed model is Tewari et al. <ref type="bibr" target="#b38">[39]</ref>, Tran and Liu <ref type="bibr" target="#b44">[45]</ref>, in which 3DMM bases are embedded in neural networks. With more representation power, these models can recover details that the traditional 3DMM usually can't, i.e. make-up, facial hair. However, the model learning process is attached with strong regularization, which limits their ability to recover high-frequency details of the face. Our propose model enhances the learning process in both learning objective and network architecture to allow higher-fidelity reconstructions <ref type="figure" target="#fig_0">(Fig. 12</ref>).</p><p>To improve 3D reconstruction quality, many approaches also try to move beyond the 3DMM such as Richardson et al. <ref type="bibr" target="#b33">[34]</ref>, Sela et al. <ref type="bibr" target="#b34">[35]</ref> or Tran et al. <ref type="bibr" target="#b42">[43]</ref>. The current state-of-the-art 3D monocular face reconstruction method by Sela et al. <ref type="bibr" target="#b34">[35]</ref> using a fine detail reconstruction step to help reconstructing high fidelity meshes. However, their first depth map regression step is trained on synthetic data generated by the linear 3DMM. Besides domain gap between synthetic and real, it faces a more serious problem of lacking facial hair in the low-dimension texture. Hence, this network's output tends to ignore these unexplainable regions, which leads to failure in later steps. Our network is more robust in handling these in-the-wild variations <ref type="figure" target="#fig_1">(Fig. 13</ref>). The approach of Tran et al. <ref type="bibr" target="#b42">[43]</ref> shares a similar objective with us to be both robust and maintain high level of details in 3D reconstruction. However, they use an over-constrained foundation, which loses personal charac- teristics of the each face mesh. As a result, the 3D shapes look similar across different subjects <ref type="figure" target="#fig_1">(Fig. 13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Facial Editting</head><p>With more precise 3D face mesh reconstruction, the quality of successive tasks is also improved. Here, we show an application of our model on face editing: adding stickers or tattoos onto faces. Using the estimated shape as well as the projection matrix, we can unwrap the facial texture into the UV space. Thanks to the lighting decomposition, we can also remove the shading from the texture to get the detailed albedo. From here we can directly edit the albedo by adding sticker, tattoo or make-up. Finally, the edited images can be rendered using the modified albedo together with other original elements. <ref type="figure" target="#fig_2">Fig. 14 shows</ref> our editing results by adding stickers into different people's face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In realization that the strong regularization and globalbased modeling are the roadblocks to achieve high-fidelity 3DMM model, this work presents a novel approach to improve the nonlinear 3DMM modeling in both learning objective and network architecture. Hopefully, with insights and findings discussed in the paper, this work can be a step toward unlocking the possibility to build a model which can capture mid and high-level details in the face. Through which, high-fidelity 3D face reconstruction can be achieved solely by doing model fitting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed framework. Each shape or albedo decoder consist of two branches to reconstruct the true element and its proxy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4xFigure 3 :</head><label>3</label><figDesc>The proposed global-local-based network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Reconstruction results with different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparisons on texture representation power. Our model can better reconstruct in-the-wild facial texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Shape representation power comparison. Given a 3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Model fitting on faces with diverse skin color, pose, expression, lighting. Our model faithfully recovers these cues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>3D reconstruction comparison to Tewari et al.<ref type="bibr" target="#b39">[40]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>3D reconstruction comparisons to nonlinear 3DMM approaches by Tewari et al.<ref type="bibr" target="#b38">[39]</ref> or Tran and Liu<ref type="bibr" target="#b44">[45]</ref>. Our model can reconstruct face images with higher level of details. Please zoom-in for more details. Best view electronically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>3D reconstruction comparisons to Sela et al.<ref type="bibr" target="#b34">[35]</ref> or Tran et al.<ref type="bibr" target="#b42">[43]</ref>, which go beyond latent space representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Adding stickers to faces. The sticker is naturally added into faces following the surface normal or lighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Texture representation power quantitative comparison (Average reconstruction error on non-occluded face portion.)</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expression invariant 3D face recognition with a morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal step nonrigid ICP algorithms for surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling facial geometry using compositional VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reanimating faces in images and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curzio</forename><surname>Volker Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Yannis Panagakis, and Stefanos Zafeiriou. 3D face morphable models &quot;In-thewild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Online modeling for realtime facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilinear wavelets: A statistical shape space for human faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The integration of optical flow and deformable models with applications to human face shape and motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Sarmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reconstructing detailed dynamic face geometry from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reconstruction of personalized 3D face rigs from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large pose 3D face reconstruction from a single image via direct volumetric CNN regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Aaron S Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning controls for blend shape based realistic facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushkar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr&amp;apos;ed&amp;apos;eric</forename><surname>Desbrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Siggraph</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose-invariant 3D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lightness and retinex theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John J</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josa</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Face poser: Interactive modeling of 3D facial expressions using facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Qing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simulating makeup through physics-based manipulation of intrinsic image layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jatuporn Toy Leksut, and G?rard Medioni. Do we really need to collect millions of faces for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tun Trn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visiolization: generating novel facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond principal components: Deep Boltzmann Machines for face modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Nhan Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gia</forename><surname>Kha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien D</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Bui Tuong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural face editing with intrinsic image disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactive region-based linear 3D face models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Rafael</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">High-fidelity monocular face reconstruction based on an unsupervised model-based face autoencoder. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FaceVR: Real-time facial reenactment and eye gaze control in virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extreme 3D face reconstruction: Looking past occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlinear 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On learning 3D face morphable model from in-the-wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09560</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Missing modalities imputation via cascaded residual autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Disentangled representation learning GAN for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Representation learning by rotating your faces. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face transfer with multilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovan</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards large-pose face frontalization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Facial retargeting with automatic range of motion alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Botsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
