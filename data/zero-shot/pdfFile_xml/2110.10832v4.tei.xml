<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
							<email>devansharpit@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Domain Generalization (DG) settings, models trained independently on a given set of training domains have notoriously chaotic performance on distribution shifted test domains, and stochasticity in optimization (e.g. seed) plays a big role. This makes deep learning models unreliable in real world settings. We first show that this chaotic behavior exists even along the training optimization trajectory of a single model, and propose a simple model averaging protocol that both significantly boosts domain generalization and diminishes the impact of stochasticity by improving the rank correlation between the in-domain validation accuracy and out-domain test accuracy, which is crucial for reliable early stopping. Taking advantage of our observation, we show that instead of ensembling unaveraged models (that is typical in practice), ensembling moving average models (EoA) from independent runs further boosts performance. We theoretically explain the boost in performance of ensembling and model averaging by adapting the well known Bias-Variance trade-off to the domain generalization setting. On the DomainBed benchmark, when using a pre-trained ResNet-50, this ensemble of averages achieves an average of 68.0%, beating vanilla ERM (w/o averaging/ensembling) by ? 4%, and when using a pre-trained RegNetY-16GF, achieves an average of 76.6%, beating vanilla ERM by 6%. Our code is available at https://github.com/salesforce/ ensemble-of-averages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain generalization (DG, <ref type="bibr" target="#b4">[5]</ref>) aims at learning predictors that generalize well on data sampled from test distributions that are different from the training distribution. Currently, deep learning models have been shown to be poor at this form of generalization <ref type="bibr" target="#b9">[10]</ref>, and excel primarily in the IID setting <ref type="bibr" target="#b50">[51]</ref>.</p><p>While a number of algorithms have been proposed to mitigate this problem (cf <ref type="bibr" target="#b50">[51]</ref> for a survey), <ref type="bibr" target="#b17">[18]</ref> demonstrate that models trained using empirical risk minimization (ERM, <ref type="bibr" target="#b42">[43]</ref>) along with proper model selection (i.e. early stopping using validation set), using a subset of data from all the training domains, largely match or even outperform the performance of most existing domain generalization algorithms. This suggests that model selection plays an important role in domain generalization. Despite its importance, there has not been much investigation into the reliability of model selection. As we demonstrate in <ref type="figure" target="#fig_8">Figure 1</ref>, the out-domain performance varies greatly along the optimization trajectory of a model during training, even though the in-domain performance does not. This instability therefore hurts the reliability of model selection, and can become a problem in realistic settings where test domain data is unavailable, because it causes the rank correlation between in-domain validation accuracy and out-domain test accuracy to be weak.</p><p>In this paper, we first investigate a simple protocol for model averaging that both boosts DG within the ERM framework, and mitigates performance instability of deep models on out-domain data,  <ref type="figure" target="#fig_8">Figure 1</ref>: Model averaging improves out-domain performance stability. Left: In-domain validation accuracy and out-domain test accuracy during training of models using ERM. Right: Same as left, except validation and test predictions are made using a simple moving average of the model being optimized, along its optimization path. Details: The plots are for the TerraIncognita dataset with domain L38 used as the test domain, and others as training/validation data, and ResNet-50. Solid lines denote accuracy, dashed lines denote training loss, and dash-dot lines denote best accuracy achieved during training and all runs (for reference). Each color denotes a different run with a different random seed and training/validation split. Gist: Model averaging reduces out-domain performance instability, and makes the test curves correlate better with the validation curves, making model selection using in-domain validation set more reliable during optimization. We see a similar pattern when using ensemble of models, with and without model averaging, in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>specifically with respect to in-domain validation data. This makes model selection more reliable. Next, taking advantage of our observation, we show that ensembling moving average models further boosts performance, making it a better choice for practical scenarios. Note that we do not claim that model averaging or ensembling can fully solve the problem of DG. The observation that model averaging can boost domain generalization performance is not new, and was exposed by SWAD <ref type="bibr" target="#b7">[8]</ref>, which inspired our work. Our contribution in this respect are as follows:</p><p>1. Hyperparameter-free:In contrast to SWAD, which introduces three additional hyper-parameters for its model averaging algorithm that need tuning, we show that the simple strategy of maintaining a simple moving average (SMA) of the model parameters throughout the optimization trajectory, starting near initialization (Appendix <ref type="figure">Figure 5</ref>), works just as well (when a pre-trained model is used as initialization). Although model averaging technically requires two hyper-parameters-averaging frequency and starting iteration, through empirical analysis, we show that setting the frequency to 1 and setting the start iteration close to 0 works well on multiple datasets and architectures, making our proposal hyperparameter-free in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Computationally efficient:SWAD requires computing validation performance more frequently than is typically done (2x-6x on the DomainBed datasets), which is needed because it needs to find the start and end iteration between which model averaging is done. This increases compute requirements. This segment is selected based on the validation performance computed using the model being trained. Our proposal to instead use the SMA model to perform early stopping and inference, side-steps this need and does not require frequent validation performance check. We show that the root cause for this difference is that the model being trained has unstable performance on OOD data, while the SMA model has a more stable OOD performance (see <ref type="figure" target="#fig_8">Figure 1</ref> and <ref type="table" target="#tab_0">Table 2</ref>). Thus this observation results in our hyperparameter-free and more efficient model averaging strategy.</p><p>3. EoA: Taking advantage of our efficient model averaging protocol (section 2.2), we find that an ensemble of moving average models (EoA) outperforms a traditional ensemble of unaveraged models <ref type="table" target="#tab_4">(Table 4</ref>). We also show ablation analysis that the rank correlation between in-domain validation performance and out-domain test performance is better for the ensemble of average models <ref type="table" target="#tab_1">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical explanation:</head><p>To explain why both model averaging and ensembling improve OOD performance under a unified theoretical framework, we adapt the well known Bias-Variance decomposition to the domain generalization setting, and argue that the expected OOD loss for individual models comprises of both the bias and the variance term, while the expected OOD loss for ensembles and averaged models comprises mainly of the bias term only, and is thus strictly lower (section 3.2). Our explanation is in contrast with SWAD, which uses flat minima to explain the improved OOD generalization, which applies to model averaging, but is less straight forward for explaining the boost by ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Benchmarking: For benchmarking, we experiment with three different pre-trained models as initializations for DG training, with increasing pre-training dataset size and model size. In these experiments we find that EoA provides a larger gain over the corresponding ERM baseline with increasing dataset and model size. These gains range from 4% ? 6% <ref type="table" target="#tab_4">(Table 4</ref>). Notice that this claim is different from existing work <ref type="bibr" target="#b19">[20]</ref>, which states that the baseline ERM performance improves with larger pre-training data and model size.</p><p>2 Model Averaging</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Terminology</head><p>Online Model: For a given supervised learning objective function, let f ? (.) denote the deep network being optimized using gradient based optimizer, where ? denotes the parameters of this model. We refer to f ? as the online model, or unaveraged model. The output of f ? (.) is a vector of K logits corresponding to the K classes in the supervised task.</p><p>Moving Average (MA) Model: While the online model is being trained, we maintain a moving average of the online model's parameters. This process is sometime referred to as iterate averaging in existing literature. The deep network whose parameters are set to be this moving average is referred to as the moving average model, or more specifically simple moving average (SMA) model because of its use in our work. We denote the parameters of this model by?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Averaging Protocol</head><p>We use a simple moving average (SMA) of the online model. Instead of calculating the moving average starting from initialization (as done in Polyak-Ruppert averaging), we instead start after a certain number of iterations t 0 during training (tail averaging), and maintain the moving average until the end of training. As we discuss in the next section, t 0 is chosen to be close, but not equal to the initialization when a pre-trained model is used as initialization. At any iteration t, we denote:</p><formula xml:id="formula_0">? t = ? t , if t ? t 0 t?t0 t?t0+1 ?? t?1 + 1 t?t0+1 ? ? t , otherwise<label>(1)</label></formula><p>where ? t is the online model's state at iteration t. Note that effectively,? t := 1 t?t0+1 ? t t =t0 ? t . Further, at iteration t, if we need to calculate validation performance, we use? t to do so, and not ? t . As we show in the next section, the benefit of doing so is that the rank correlation between in-domain validation accuracy and out-domain test accuracy is significantly better when predictions are made using? t . This makes model selection more reliable for domain generalization. Finally, for a given run, model selection selects? t * for making test set predictions, such that? t * achieves the best validation performance. We discuss some theoretical perspectives on why model averaging can help domain generalization in section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ablation Analysis</head><p>Here we perform four ablation studies: 1) impact of the start iteration t 0 used in our SMA protocol in Eq. 1; 2) the frequency of model averaging; 3) instability reduction of SMA model compared to the online mode along the optimization trajectory on out-domain data; 4) correlation between in-domain and out-domain accuracy across independently trained models.</p><p>Due to space limitation, we show experiments for 1,2 and 4 in Appendix section C. In summary, we find that: 1) starting averaging close to initialization results in improved out-domain performance ( <ref type="figure">Figure 5</ref> in Appendix) when the parameters are initialized used a pre-trained model; 2) the frequency of SMA does not have a significant impact on performance, unless sampling is done at too large intervals ( <ref type="figure">Figure 6</ref> in Appendix); 4) the rank correlation is poor between validation and test accuracy of independently trained models <ref type="figure" target="#fig_11">(Figure 8</ref> in Appendix). An implication of this is that it is difficult to discover the best model (for out-domain performance) from a pool of independently trained models, based only on their in-domain validation performance (echoing the findings of <ref type="bibr" target="#b9">[10]</ref>). <ref type="table">Table 1</ref>: Spearman correlation (closer to 1 is better) between within-run in-domain validation accuracy and out-domain test accuracy on multiple datasets. Model averaging improves rank correlation for both individual models (left) and ensemble of averages (right).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Instability Reduction: Rank Correlation</head><p>We study the reliability of model selection for domain generalization when using online models vs moving average models, using rank correlation (see Appendix C.4 for definition). To do so, we train models on a dataset, both with and without model averaging, and compute Spearman correlation between the in-domain validation accuracy and out-domain test accuracy sampled at regular intervals during the training process. Since there are multiple runs where a given domain acts as the test domain, we calculate the mean and standard error of these values over these runs.</p><p>The rank correlations are shown in <ref type="table" target="#tab_0">Table 2</ref> (and <ref type="table" target="#tab_9">Table 8</ref> in Appendix) for the PACS, VLCS, Office-Home, TerraIncognita and DomainNet datasets. We find that in majority of the cases, using model averaging results in a significantly better rank correlation compared to using the online model. These experiments therefore suggest that the reliability of model selection is significantly higher within a run when using model averaging.</p><p>3 Ensemble of Averages (EoA) <ref type="bibr" target="#b17">[18]</ref> propose a rigorous framework for evaluation in the domain generalization setting which accounts for randomness due to seed and hyper-parameter values, and recommend reporting the average test accuracy over all the runs computed using a model selection criteria. However, in practice, it is desirable to have a single predictor that has a high accuracy. An ensemble combines predictions from multiple models, and is a well known approach for achieving this goal <ref type="bibr" target="#b10">[11]</ref> by exploiting function diversity <ref type="bibr" target="#b13">[14]</ref>. However, as we show, even ensembles suffer from instability in the domain generalization setting. Building on the observations of the previous section, we investigate the behavior of ensemble of moving average models and find that it mitigates this issue. We begin by describing the EoA protocol below.</p><p>EoA Protocol: We perform experiments with ensemble of multiple independently trained models (i.e., with different hyper-parameters and seeds). When each of these models are moving average models from their corresponding runs, we refer to this ensemble in short as the ensemble of averages (EoA). Identical to how we make predictions for traditional ensembles (specifically the bagging method <ref type="bibr" target="#b5">[6]</ref>), the class? predicted by an EoA for an input x is given by the formula:</p><formula xml:id="formula_1">y = arg max k Sof tmax( 1 E E i=1 f (x;? i )) k<label>(2)</label></formula><p>where E is the total number of models in the ensemble,? i denotes the parameters of the i th moving average model, and the sub-script (.) k denotes the k th element of the vector argument. Finally, the state? i of the i th moving average model used in the ensemble is selected from its corresponding run using its in-domain validation set performance (described in section 2.2). We now investigate the behavior of EoA compared with ensembles of online models on domain generalization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis</head><p>Qualitative visualization: For the purpose of contrasting the behavior of traditional ensembles vs ensemble of averages, we begin by qualitatively studying the stability of out-domain performance of these two ensembling techniques during the training process. To do so, we use the TerraIncognita  dataset, and fix one of its domains as the test domain while using the others as training/validation data. We then train 6 different models independently for 5, 000 iterations with different seeds, hyper-parameters and training-validation splits identical to the <ref type="bibr" target="#b17">[18]</ref> protocol. We also maintain moving average models corresponding to each of these 6 models. At every 300 iterations, we form an ensemble of the 6 online models from their corresponding runs and compute the out-domain test accuracy. Since, each run has a different training-validation split, we calculate the mean validation accuracy of each of these online models at that iteration. We follow an identical procedure for the moving average models and plot these performances in <ref type="figure" target="#fig_1">Figure 2</ref>. We find that the ensemble of averages has a better stability on out-domain test set compared to the ensemble of online models.</p><p>For clarity, note that this procedure for calculating test accuracy at regular intervals is different from what we proposed earlier for EoA for practical purposes. This experiment is only meant to highlight the fact that making predictions on out-domain data using an ensemble of online models suffers from instability along the optimization trajectory, while an ensemble of averages mitigates this issue. For plots on other domains of TerraIncognita, see <ref type="figure" target="#fig_8">Figure 10</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank correlation:</head><p>We now measure the rank correlation between in-domain validation accuracy and out-domain test accuracy for a quantitative evaluation. The details of the metric and motivations behind this experiment are same as those described in section 2.3.1. Here we use the same experimental setup described in the qualitative analysis above. But in addition, we also conduct experiments on VLCS, OfficeHome and DomainNet datasets. The results are shown in <ref type="table" target="#tab_1">Table 3</ref> (and <ref type="table" target="#tab_10">Table 9</ref> in Appendix). We find that in majority of the cases, using EoA results in a significantly better rank correlation compared to using the online model ensemble. These results show more concretely the fact that predictions by an ensemble of online models on out-domain data suffers from instability along the optimization trajectory, and EoA mitigates this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why does Ensembling and Model Averaging Improve Performance?</head><p>We explain the performance boost achieved by ensemble of averages (see next section) by adapting the Bias-Variance decomposition <ref type="bibr" target="#b16">[17]</ref> to the domain generalization setting. For classification tasks with one-hot labels, the Bias-Variance decomposition is given as <ref type="bibr" target="#b48">[49]</ref>, Finally (x, y) ? P out where P out is the out-domain distribution. Notice how T and (x, y) come from different distributions. For instance, in PACS dataset, P in could be the union of art, cartoon and photo domains, and P out could be the sketch domain.</p><formula xml:id="formula_2">E x,y E T [CE(y, f (x; T ))] = E x,y [CE(y,f (x))] Bias 2 + E x,T [KL(f (x), f (x; T ))]</formula><p>The L.H.S. of the above equation is the expected cross entropy loss on the out-domain distribution achieved by individual models, i.e., when we train an individual model on a particular instance of the training dataset T , the expected out-domain test loss is denoted by L.H.S. Importantly, the Bias term on the R.H.S. denotes the expected cross entropy loss on the out-domain distribution achieved by the functionf (.), which is essentially an ensemble. Finally, the variance term captures how much the Test Domain Accuracy (%) Test Domain Accuracy (%) prediction of individual models differs in expectation from the ensemble prediction, which makes this term strictly greater than zero.</p><formula xml:id="formula_3">PACS (w/ MA) PACS (w/o MA) VLCS (w/ MA) VLCS (w/o MA) OfficeHome (w/ MA) OfficeHome (w/o MA) TerraIncognita (w/ MA) TerraIncognita (w/o MA) DomainNet (w/ MA) DomainNet (w/o MA)</formula><formula xml:id="formula_4">PACS (w/ MA) PACS (w/o MA) VLCS (w/ MA) VLCS (w/o MA) OfficeHome (w/ MA) OfficeHome (w/o MA) TerraIncognita (w/ MA) TerraIncognita (w/o MA) DomainNet (w/ MA) DomainNet (w/o MA)</formula><p>Therefore, the above decomposition tells us that the expected test domain error of an ensemble is strictly less than that of an individual model. This interpretation directly explains why a traditional ensemble of unaveraged models can be expected to perform better than individual unaveraged models. However, it is still not clear why EoA performs better that a traditional ensemble in practice. To establish this connection, we note that in practice, we typically train a small number of independent models to form a traditional ensemble due to computational constraints. Thus such ensembles do not behave identically to the expected ensemblef (.) described above. Model averaging on the other hand has been shown to approximate an ensemble <ref type="bibr" target="#b22">[23]</ref>. To see this, consider without any loss of generality that the ensemble contains models with parameters {? 1 , ? 2 . . . ? T }, and denote? T := 1 T ? T t=1 ? t . Then note that the second order Taylor's expansion around? T of each model's k th dimension's prediction is given by,  Notice that f (.) is the model output and therefore the first and second order terms are the derivatives of the model output and not the loss gradient and Hessian. The first order term is zero due to? T := 1 T ? T t=1 ? t . A crucial difference of our analysis compared to <ref type="bibr" target="#b22">[23]</ref> is that they average model states that lie near different loss minima, while we perform tail averaging. Therefore, the term (? T ? ? t ) may not behave similar to that in their case. To shed light on its behavior, we plot the histogram of the second order term and the moving average model's logit f (? T ) k in Eq. 3 for the first dimension (k = 1) for test domain data in figure 4 (details and additional experiments provided in Appendix D). The histogram shows that the second order term concentrates near zeros while the logit values span a wider range, which implies that under the second order approximation, the model averaging protocol used in our work behaves like an ensemble. Finally, to study the impact of ensemble size on out-domain performance, we plot the test domain accuracy as a function of ensemble size in figure 3. The plots show that i. EoA outperforms traditional ensembles for all ensemble sizes (left); and ii. ensembles of larger size typically have better <ref type="table" target="#tab_4">Table 4</ref>: Performance benchmarking on 5 datasets of the DomainBed benchmark using two different pre-trained models. SWAD and MIRO are the previous SOTA. See <ref type="table">Table 10</ref> in Appendix for comparison with more methods. Note that ensembles do not have confidence interval because an ensemble uses all the models to make a prediction. Gray background shows our proposal. Our runs implies we ran experiments, but we did not propose it. Experiments use the training-domain validation protocol from <ref type="bibr" target="#b17">[18]</ref>. </p><formula xml:id="formula_5">1 T ? T t=1 f (? t ) k ? f (? T ) k + 1 T ? T t=1 (? T ? ? t ) T ?f (? T ) k ?? T + 0.5(? T ? ? t ) T ? 2 f (? T ) k ?? 2 T (? T ? ? t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DomainBed Benchmarking</head><p>We now benchmark our model averaging protocol (SMA) and ensemble of averages against online models (ERM, without MA) and ensemble of online models (ensembles). Note that all these models are trained using the ERM objective as before. We evaluate on PACS <ref type="bibr" target="#b26">[27]</ref>, VLCS <ref type="bibr" target="#b12">[13]</ref>, OfficeHome <ref type="bibr" target="#b44">[45]</ref>, TerraIncognita <ref type="bibr" target="#b2">[3]</ref> and DomainNet <ref type="bibr" target="#b34">[35]</ref> datasets in DomainBed. The training-evaluation protocols are the same as described in section 2.3 for moving average and online models, and in section 3 for ensembles. Full details can be found in section B in the Appendix.</p><p>Comparison with existing results using ResNet-50 pre-trained on ImageNet: Here we compare existing methods with our runs. All methods use ResNet-50 (25M parameters) <ref type="bibr" target="#b18">[19]</ref> pre-trained on ImageNet as initialization. Comparing ERM <ref type="bibr" target="#b17">[18]</ref> and ERM (our runs), we find that they perform similarly, especially considering we have used a smaller hyper-parameter space (further discussion in Appendix E). A comparison between SWAD and SMA shows that SWAD is slightly better (by 0.4% on average). However, recall that our protocol retains the advantage of not tuning any hyperparameters while SWAD has 3 additional ones that they tune separately in addition to the optimization hyper-parameters. Interestingly, traditional ensembles and SMA achieve similar performance (66.8% and 66.5% respectively). Finally, EoA outperforms all the existing results: ERM by 4% and SWAD (previous SOTA) by 1.1%. Importantly, note that while all non-ensemble models report the average test accuracy of multiple models following the protocol of <ref type="bibr" target="#b17">[18]</ref>, EoA test accuracy is achieved by a single predictor that combines the output of multiple models.</p><p>Experiments with larger pre-training datasets and larger models: In addition to ResNet-50 pretrained on ImageNet, we now also experiment with ResNeXt-50 32x4d (25M parameters), that is pre-trained using semi-weakly supervised objective on Instagram 1B images and ImageNet labeled data <ref type="bibr" target="#b47">[48]</ref>, and RegNetY-16GF (81M parameters) pre-trained using Instagram 3.6B images. Note that both ResNet-50 and ResNeXt-50 32x4d have similar number of parameters, while RegNetY-16GF has more than 3x the number of parameters. On the other hand, also notice that the three architectures are respectively pre-trained on an increasing size of datasets. The rationale behind this choice is that recent trends in deep learning has shown that models pre-trained on larger datasets and architectures achieve better downstream transfer performance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref>. Therefore, we expect the latter models to improve the ERM baseline, and our goal is to investigate the out-domain performance gain by model averaging and EoA relative to the corresponding ERM baseline with increasing pre-training dataset size and model size.</p><p>The experimental results are shown in  <ref type="bibr" target="#b19">[20]</ref>, which states that the baseline ERM performance improves with larger pre-training data and model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>In-domain Performance Improvement using Model Averaging We study the in-domain test accuracy on PACS and OfficeHome datasets using ImageNet pretrained ResNet-50 with and without our SMA protocol. In this experiment, we combine all the domains of PACS and split it into training/validation/test splits (0.8/0.1/0.1). We run 10 different runs with different seeds and randomly chosen splits for each dataset. The best model for each run is chosen using the validation set. The remaining optimization details are identical to those used in the previous section. The test accuracy mean and standard error using these best models are shown in <ref type="table" target="#tab_5">Table 5</ref>. As expected, SMA outperforms models without averaging.</p><p>5 Related Work 5.1 Model Averaging A theoretical perspective: In our model averaging protocol, we compute a simple moving average of the model parameters starting early during training. This is known as tail-averaging <ref type="bibr" target="#b23">[24]</ref>, which is slightly different from Polyak-Ruppert averaging <ref type="bibr" target="#b35">[36]</ref> in that the latter starts averaging from the very beginning of training. In the context of least square regression in the IID setting, <ref type="bibr" target="#b23">[24]</ref> theoretically study the behavior of tail averaging and show that the excess risk of the moving average model is upper bounded by a bias and a variance term. This bias term depends on the initialization state of the parameter, but interestingly, it decays exponentially with t 0 , where t 0 is the iteration at which model averaging is started. The variance term on the other hand depends on the covariance of the noise inherent in the data w.r.t. the optimal parameter, and is shown to decay at a faster rate when using model averaging, as opposed to a slower rate without averaging. This motivated them to propose tail-averaging.</p><p>Model averaging has also been shown to have a regularization effect <ref type="bibr" target="#b33">[34]</ref> similar to that of Tikhonov regularization <ref type="bibr" target="#b41">[42]</ref>. This regularization has been classically used in ill-posed optimization problems (typically least squared regression), which are under-specified. This property provides an interesting connection between model averaging and the under-specification problem discussed in <ref type="bibr" target="#b9">[10]</ref>, where the authors perform large scale experiments showing that the performance of multiple over-parameterized deep models, trained independently with different hyper-parameters and seeds, have a high variance on out-domain data, even though their in-domain performances are very close together. Based on this connection, a simple intuition why one can expect model averaging to help in domain generalization is its Tikhonov regularization effect. However, this intuition requires a more thorough investigation.</p><p>SWAD <ref type="bibr" target="#b7">[8]</ref>: SWAD propose flat minima as a means for improving domain generalization. Following the intuition of stochastic weight averaging (SWA, <ref type="bibr" target="#b22">[23]</ref>), they use model averaging to find flat minima. However, their proposal is different from sampling model states at regular intervals and towards the end of training (as done in SWA). SWAD selects contiguous model states along the optimization path for averaging, based on their validation loss. This is done to prevent including an under-performing state (determined using the in-domain validation set) in the moving average model. SWAD however adds additional hyper-parameters of its own: the validation loss threshold below which the the model states are selected, and patience parameters (number of iterations that determine the start and end of the averaging process). Note that this also requires computing validation loss more frequently during training. In this context, we show that instead of finding the start and end period for model averaging meticulously, we can simply start model averaging early during training and continue till the end. This difference arises from the fact that SWAD uses the online network to calculate validation performance while we use the SMA model in our protocol. This is explained further in section 2.2. The benefit our observations provide over SWAD is that they allow us to take advantage of model averaging without the additional hyper-parameters and compute required by SWAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Domain Generalization</head><p>Existing methods aimed at domain generalization can be broadly categorized into techniques that perform domain alignment, regularization, data augmentation, and meta-learning. Domain alignment is perhaps the most intuitive direction, in which methods aim to learn latent representations which have similar distributions across different domains <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref>. There are different variants of this idea, such as minimizing some divergence metric between the latent representation of different domains (E.g. DANN <ref type="bibr" target="#b15">[16]</ref>), or less strictly, minimizing the difference between the latent statistics of different domains (E.g. DICA <ref type="bibr" target="#b32">[33]</ref>, CORAL <ref type="bibr" target="#b40">[41]</ref>). In the meta learning category, source domains are typically split into 2 subsets to be used as the training and test domains in episodes to simulate the domain generalization setting <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Data augmentation is also a popular tool used for improving domain generalization. It ranges from introducing various types of augmentations to simulate unseen test domain conditions (E.g. style transfer <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">52]</ref>) to self-supervised learning involving matching the representations of an image with different augmentations (E.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>). Finally, different ways of regularizing models (implicit and explicit) have also been developed with the goal of encouraging domain-invariant feature learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref>. For instance, invariant risk minimization <ref type="bibr" target="#b1">[2]</ref> propose a regularization such that the classifier is optimal in all the environments. Representation Self-Challenging <ref type="bibr" target="#b20">[21]</ref> propose to suppress the dominant features that get activated on the training data, which forces the network to use other features that correlate with labels. Risk extrapolation <ref type="bibr" target="#b25">[26]</ref> propose a regularization that minimizes the variance between domain-wise loss, in the hope that it is representative of the variance including unseen test domains. See <ref type="bibr" target="#b50">[51]</ref> for a survey on DG methods.</p><p>Our investigation in this work is complementary to all these domain generalization methods. Additionally, one of our main focus is to also study and improve performance instability on out-domain data during training, which results in more reliable model selection. This aspect has not received much attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We investigated a hyperparameter-free and efficient protocol for model averaging in the ERM framework, and showed that it provides a significant boost to out-domain performance compared to un-averaged models. Building on this observation, we showed that an ensemble of moving average models performs better compared to an ensemble of un-averaged models. Importantly, we showed that in both cases, model averaging significantly improves the rank correlation between in-domain validation accuracy and out-domain test accuracy, which is crucial for reliable model selection using in-domain validation data. We experimented with three pre-trained models with increasing pre-training dataset and model size, and found that EoA provides a proportionally larger gain compared to the corresponding ERM baseline, and lies in the range of 4% ? 6%. Finally, we explain the performance boost of EoA by adapting the Bias-Variance trade-off perspective to the domain generalization setting. Further discussions along with limitations of our work are provided in Appendix E.</p><p>[52] Kaiyang Zhou, Chen Change Loy, and Ziwei Liu. Semi-supervised domain generalization with stochastic stylematch. arXiv preprint arXiv:2106.00592, 2021.   <ref type="figure">Figure 5</ref>: The impact of iteration t 0 at which we start simple moving averaging as described in Eq. 1, on the domain generalization performance for PACS and TerraIncognita datasets. The dominant pattern across all the experiments suggests that starting averaging earlier yields a stronger boost in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Broader Impact</head><p>Our work aims at improving out-domain performance of models. Thus it is naturally geared towards mitigating the effects of training dataset biases on the hypothesis learned by the model, which we believe has a positive societal impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training and Evaluation Protocols for DomainBed Benchmarking</head><p>We use the training protocol described in <ref type="bibr" target="#b17">[18]</ref> with minor changes: we use a smaller hyper-parameter search space (shown in <ref type="table" target="#tab_6">Table 6</ref>) and smaller number of random trials for computational reasons, and train on DomainNet dataset for 15, 000 iterations instead of 5, 000 similar to <ref type="bibr" target="#b7">[8]</ref>, because its training loss is quite high. For a dataset with D domains, we run a total of 6D random trials. This results in 6 experiments per domain, in which this domain is used as the test set, while the remaining domains are used as training/validation set (randomly split). This is also the reason why we use a smaller hyper-parameter search space, because otherwise the search space would be under-sampled. For moving average models, the iteration t 0 at which averaging is started (Eq. 1) is set to be 100 in all experiments unless specified otherwise. For ensembles (both with and without averaged models), the 6 models corresponding to the 6 experiments per domain, in which this domain is used as the test set (as described above), are used for ensembling as described in section 3.</p><p>All models are trained using the ERM objective and optimized using the Adam optimizer <ref type="bibr" target="#b24">[25]</ref>. We use ResNet-50 <ref type="bibr" target="#b18">[19]</ref> pre-trained on Imagenet as our initialization for training in all the experiments. In the final benchmarking experiment, we also use ResNeXt-50 32x4d, that is trained using semiweakly supervised objective on IG-1B targeted (containing 1 billion weakly labeled images) and ImageNet labeled data <ref type="bibr" target="#b47">[48]</ref>. This model was downloaded from Pytorch hub. For all models, the batch normalization <ref type="bibr" target="#b21">[22]</ref> statistics are kept frozen throughout training and inference. Validation accuracy is calculated every 300 iterations for all datasets except DomainNet where it is calculated every 1000 iterations. Unless specified otherwise, we use the said protocol in all the experiments. For model selection, we use the training-domain validation set protocol in <ref type="bibr" target="#b17">[18]</ref> with 80% ? 20% training-validation split, and the average out-domain test performance is reported across all runs for each domain.</p><p>All experiments were performed on Google Could Platform (GCP) using 24 NVIDIA A100 GPUs.  <ref type="figure">Figure 6</ref>: The impact of the frequency (number of iterations), at which model states are sampled for computing the simple moving average (SMA), on the domain generalization performance for PACS and TerraIncognita datasets. Broadly, we find that the frequency of sampling does not have a major influence on performance unless the sampling interval is too large: performance drops significantly on TerraIncognita only when the frequency is set to sampling every 1000 iterations. Experimentation Details: We use the training protocol described in <ref type="bibr" target="#b17">[18]</ref> with minor changes: we use a smaller hyper-parameter search space for feasibility, and train on DomainNet dataset for 15, 000 iterations instead of 5, 000 similar to <ref type="bibr" target="#b7">[8]</ref>, because its training loss is quite high. Unless specified otherwise, we use the said protocol in all the experiments. For model selection, we use the trainingdomain validation set protocol in <ref type="bibr" target="#b17">[18]</ref>, where the average out-domain test performance is reported across all runs. For more details, see section B in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Details:</head><p>We use a subset of the DomainBed benchmark: PACS dataset (4 domains, 7 classes, and 9,991 images), TerraIncognita dataset (4 domains, 10 classes, and 24,788 images), VLCS dataset (4 domains, 5 classes, and 10,729 images), OfficeHome dataset (4 domains, 65 classes, and 15,588 images), and DomainNet dataset (6 domains, 345 classes, and 586,575 images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Start Iteration</head><p>We investigate how domain generalization performance is impacted by the choice of iteration t 0 when we start model averaging in Eq. 1. In this section, we simply refer to it as start iteration, which should not be confused with the start of the training process. For experiments, we use the PACS and TerraIncognita datasets. To investigate a wide range of start and end iterations, for all experiments in this section, we train models for 10, 000 iterations.</p><p>We consider starting model averaging from iterations in {0, 100, 500, 2000, 4000, 6000, 8000}. We plot the performance in <ref type="figure">Figure 5</ref> for each value. We find that the test performance, averaged across all the domains for both datasets, decreases if we start model averaging later during the training, and choosing t 0 close to initialization yields the best performance. We believe using the initialization state in model averaging causes a slight dip in performance because loss is initially high. Based on these experiments, instead of tuning start iteration as a hyper-parameter, we arbitrarily choose 100 as the start iteration for the remaining experiments in this paper. This choice of starting averaging later during training is called tail averaging, and the theoretical motivation for this choice are discussed in more detail in section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Averaging Frequency</head><p>When performing simple model averaging described in Eq. 1, instead of averaging iterates from every iteration, we can alternatively sample iterates at a larger interval. We study the impact of averaging frequency on out-domain test performance. Once again, we use the PACS and TerraIncognita datasets. We train models for 10, 000 iterations, and sample iterates at intervals in {10 0 , 10 1 , 10 2 , 10 3 } iterations. Test accuracy is once again computed using the protocol of <ref type="bibr" target="#b17">[18]</ref> for each case. The performance as a function of the iterate sampling interval used in SMA is shown in <ref type="figure">Figure 6</ref>. Broadly, we find that the frequency of sampling does not have a major impact on performance unless the sampling interval is too large, which happens in the case of TerraIncognita, where performance drops significantly when the sampling interval is set to 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Rank Correlation</head><p>Rank correlation metrics aim to quantify the degree to which an increase in one random variable's value is consistent with an increase in the other random variable's value. Therefore, instead of Pearson's correlation, they are better suited for studying the relationship between the in-domain and out-domain performance for the purpose of model selection because we select the best model during an optimization based on ranking the validation performance (early stopping). We consider Spearman correlation in our experiments. Its value vary between ?1 and +1, where ?1 implies the ranking of the two random variables are exactly the reverse of each other, and +1 implies the ranking of the two random variables are exactly the same as each other. A value of 0 implies there is no relationship between the two variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Instability Reduction: Qualitative Analysis</head><p>Here we try to qualitatively study the robustness of model selection using in-domain validation set, on out-domain performance. To do so, consider the ideal scenario where the in-domain validation performance correlates well with the out-domain performance. In this case, training longer should not be a problem in general, because if the model starts overfitting beyond a certain point, model selection can take care of it. In such a situation, we would expect the out-domain performance to either improve with longer training, or remain stable.</p><p>We use TerraIncognita dataset for this experiment. We consider training duration to be 1, 000 to 10, 000 iterations, at intervals of 1, 000. We plot the performance in <ref type="figure" target="#fig_10">Figure 7</ref> for online model (left) and moving average model (right). We find that the performance of moving average models is more stable compared to online models, suggesting that model selection is more reliable when using moving average models. <ref type="figure" target="#fig_1">Figure 12</ref> in the appendix shows the training loss, in-domain validation accuracy and out-domain test accuracy for all the runs used in the above experiment. It shows that the out-domain test performance is unstable during optimization without model averaging, which causes problem for model selection using the in-domain validation performance, as is evident in the above experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Cross-run rank correlation</head><p>In addition to the experiments in 2.3.1, there is another way in which it makes sense to study the rank correlation between validation and test performance. Suppose we set one of the domains of PACS as our test domain, and the remaining as training/validation data, and perform multiple independent runs with different seeds/hyper-parameter values. At each iteration during training, we can gather the tuple (validation, test) accuracy for each of these runs, and then study the rank correlation between them. The utility of this perspective is to assess the reliability of model selection in terms of selecting a single model across multiple independently trained models, based on their validation performance. We study this rank correlation for PACS and TerraIncognita datasets. The results are shown in <ref type="figure" target="#fig_11">Figure  8</ref> in the appendix. We find that the cross-run rank correlations are poor (not consistently close to 1) for both online model (without averaging) and moving average model. This implies that in-domain validation performance based model selection is not a reliable approach for selecting a model from a pool of multiple independently trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Why Does Ensembling Improve Performance?</head><p>We describe the histogram experiment presented in section 3.2. For this experiment, we use the TerraIncognita dataset with L46 as the test domain. We use one of the runs with model averaging from the experiments done in section 4.1. We begin by re-writing the Taylor's expansion more precisely for our model averaging protocol,</p><formula xml:id="formula_6">1 T ? t 0 + 1 ? T t=t0 f (x; ? t ) k ? f (x;? T ) k + 0.5 ? 1 T ? t 0 + 1 ? T t=t0 (? T ? ? t ) T ? 2 f (x;? T ) k ?? 2 T (? T ? ? t )<label>(3)</label></formula><p>where? T :=  For each training duration, the out-domain test accuracy is calculated using model selection over the in-domain validation data. Not using model averaging leads to unreliable model selection as evident in the instability of the out-domain performance. Model averaging is able to reduce this instability. <ref type="table">Table 7</ref>: Spearman correlation (closer to 1 is better) between within-run in-domain validation accuracy and out-domain test accuracy on the multiple datasets in the DomainBed benchmark for individual models (left) and ensembles (right). In most cases, using model averaging results in a significantly better rank correlation, which makes model selection more reliable.  integer valued t between t 0 and T , we only use ? t for t = 300 * i for i ? {1, 2, . . . , 16} due to computational constraints. This should not affect our conclusion because as shown in section C.3, averaging frequency does not have a significant impact on performance. Finally, we record the logit values and the second order term for 1000 randomly selected samples x from the test domain data.</p><p>Note that TerraIncognita has 10 classes and so we have the above equation for k ? {1, 2, . . . , 10}.</p><p>We plot the histogram of the logit values and the second order term corresponding to each class's output separately. All the plots are shown in <ref type="figure" target="#fig_13">figure 9</ref>. The conclusion is the same as described in the main text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Discussions and Limitations</head><p>Domain Generalization Limitations: The Bias-Variance trade-off analysis of EoA in section 3.2 shows that the expected loss of individual models constitutes both the bias and variance term, while that of ensembles is dominated by the bias term alone, and is thus strictly lower. Thus ensembles (either explicitly or implicitly by model averaging) is guaranteed to improve performance on OOD data compared to the corresponding individual unaveraged models. However, this strategy cannot go beyond getting rid of the variance term, and other strategies need to be used to reduce the bias term, which will further improve the OOD performance.</p><p>Additionally, our proposal does not make use of the environment ID of samples. A popular strategy to utilize this information is to increase some form of alignment between the latent representations of different domains, which has been shown to be one of the terms in the upper bound of the test domain generalization error <ref type="bibr" target="#b3">[4]</ref>. While a number of prior work has proposed variants of this strategy (eg. CORAL, DANN, see section 5.2 for a discussion), <ref type="bibr" target="#b17">[18]</ref> has showed that ERM with appropriate training-validation protocol performs at least as well as these methods. Further, <ref type="bibr" target="#b43">[44]</ref> has recently argued supported by empirical evidence that domain alignment is neither necessary nor sufficient for domain generalization. Thus it remains an open question what other strategies can be used to utilize the environment ID to boost domain generalization Functional Diversity: Model averaging mitigates instability within a run, which makes model selection more reliable. However, we note that the gap in performance between different runs still exists, though it is smaller on average compared with online models (see training evolution plots in appendix for reference). On another note, while our analysis in section 3.2 suggests that model averaging behaves as an ensemble, we believe that it does not offer as much of functional diversity as independently trained models. This is because if it did, model averaging should have had a much better performance compared with a traditional ensembles (since there are T ? t 0 + 1 models in model averaging while only 6 models in the traditional ensemble in our experiments), but this is not the case (see their performance in table 10 for a comparison). This implies that there is still diversity among the independently trained models with model averaging. This is also inline with <ref type="bibr" target="#b13">[14]</ref> which shows that ensembling methods such as model averaging and Monte Carlo dropout <ref type="bibr" target="#b14">[15]</ref> do not provide diversity in function space as much as ensembles of independently trained models. Perhaps this is why EoA performs better compared to both individual moving average models and traditional ensembles, by better approximating the expected ensemble behavior.</p><p>Scalability: Following the protocol of <ref type="bibr" target="#b17">[18]</ref>, we used samples from all the training domains in each mini-batch update. However, in settings where the number of domains is very large, this approach can be prohibitive. As an alternative, we also performed preliminary experiments in which we stochastically picked one of the training domains at every iteration, and sampled a mini-batch from that domain to update parameters. We found that this protocol resulted in a similar performance as that achieved by the protocol used in our work.  Computational Complexity: The computational overhead due to SMA is practically negligible (compared to back-propagation) since it merely involves a running average estimate of the parameters. So its complexity is the same as that of training a vanilla supervised deep network. On the other hand, since EoA trains an ensemble of models, the complexity scales linearly with the number of models used in the ensemble compared with SMA on a single model, if these models are trained sequentially. Of course, these different models can be trained in parallel if resources are available, given each model is trained independently of one another, in which case the complexity of EoA remains the same as that of vanilla supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information leaking considerations:</head><p>We present many experiments where validation and test performances are studied. It is therefore natural to wonder if there was any information leak from the test set while performing this analysis. We note that in the model averaging protocol we investigated, there were two moving parts: iteration t 0 at which model averaging is started, and averaging frequency. We studied them in section C.2 and C.3 respectively. For both of them, we proposed to fix their values universally instead of tuning them on each dataset. Specifically, <ref type="bibr" target="#b23">[24]</ref> propose tail averaging in which iterates from every iteration are used for computing the simple moving average. We found this proposal to work well empirically in our analysis, and therefore set averaging frequency to 1. For start <ref type="table">Table 10</ref>: Performance benchmarking on 5 datasets of the DomainBed benchmark using two different pre-trained models. SWAD is the previous SOTA. Note that ensembles do not have confidence interval because an ensemble uses all the models to make a prediction. Gray background shows our proposal. Our runs implies we ran experiments, but we did not propose it. iteration t 0 on the other hand, <ref type="bibr" target="#b23">[24]</ref> theoretically show that the initial bias term in the excess error upper bound decays exponentially with t 0 . In line with this theory, our analysis showed that an iteration close to but not at initialization worked well. So we arbitrarily set t 0 = 100. Note that these are not their optimal values, but are rather arbitrary choices guided by our investigation and existing theory. Aside from these two objects (which we fix in all experiments except the aforementioned ablation), there are no hyper-parameters introduced by the averaging protocol or the ensemble of averages studied in this paper, and all other experiments are purely observational. Finally, we followed the protocol of <ref type="bibr" target="#b17">[18]</ref> for training and evaluation.</p><p>Smaller HP search space: We use a smaller hyper-parameter search space compared with that in <ref type="bibr" target="#b17">[18]</ref>. Nonetheless, we find that on average, our runs of the ERM baseline performance (without model averaging) yield 64% test accuracy on average compared with 63.8% reported in <ref type="bibr" target="#b17">[18]</ref> on the 5 datasets we used. We also note that model averaging and ensemble of averages, that we study in our work, are not competing with ERM baseline, in the sense that these techniques essentially rely on the quality of the baseline model to further boost performance. Therefore any boost in the ERM baseline performance is likely to improve the model averaging and EoA performance. This is also evident in our benchmarking experiments in <ref type="table" target="#tab_4">Table 4</ref>, where using ResNeXt-50 32x4d pre-trained on a larger dataset <ref type="bibr" target="#b47">[48]</ref> has a better ERM baseline performance compared to ResNet-50 pre-trained on ImageNet. This results in a further boost of 3.9% and 5% test accuracy on average when using model averaging and EoA respectively.    Each color represents a different run with randomly chosen seed, hyper-parameters and training-validation split following <ref type="bibr" target="#b17">[18]</ref>. Gist: Out-domain test performance is unstable without model averaging, which causes problem for model selection using in-domain validation performance. Model averaging is able to mitigate this instability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Ensemble of moving averages (EoA) (right) has better out-domain test performance stability compared with ensemble of online models (left), w.r.t. in-domain validation accuracy. Details: The plots are for the TerraIncognita dataset with domain L38 used as the test domain, and others as training/validation domain, and ResNet-50. Each ensemble has 6 different models from independent runs with different random seeds, hyper-parameters, and training/validation split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Variancewhere CE denotes the cross entropy loss, KL denotes KL divergence,T = {(x in i , y in i )} N i=1 are N IIDsamples drawn from the in-domain training distribution P in , f (x; T ) denotes the prediction of the model f on sample x such that the model is trained on the dataset T , andf (x) = E T [f (x; T )].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 : 1 .</head><label>31</label><figDesc>Left: Effect of ensemble size (number of models in an ensemble) on out-domain performance (mean and standard error) for models with and without moving average (MA) parameters for ResNet-50 pre-trained on ImageNet. Right: Using the performance of ensemble of size 1 (shown in the left plot) as reference, right plot shows the percentage point improvement for ensembles of size &gt; The plots show that i) ensemble of averages (solid lines in left plot) are consistently better than ensemble of models without averaging (dashed lines in left plot); ii) ensemble of averages consistently improves performance over averaged models (ensemble of size 1 in right plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>The scale of terms-moving average model's logit and the second order term in Eq. 3. The latter concentrates around 0, suggesting our model averaging protocol approximates ensembles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] In Appendix E, we have discussed various aspects including the limitations of our and existing methods in addressing domain generalization, functional diversity of ensembles vs model averaging strategy, and more. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix A. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] See section 3.2. (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [No] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>C</head><label></label><figDesc>Additional Ablation Analysis of Our Model Averaging Protocol (Eq. 1) C.1 Dataset and Experiment Details for SMA Ablation Analysis in Section 2.3, C.2, and C.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Qualitatively accessing the reliability of model selection while varying the training duration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Spearman correlation between cross-run in-domain validation accuracy and out-domain test accuracy for PACS dataset (top) and TerraIncognita dataset (bottom). The cross-run rank correlations are poor (not consistently close to 1) for both online model (without avg) and moving average model. This implies that in-domain validation performance based model selection is not a reliable approach for selecting a single model from a pool of multiple independently trained models. See section 2.3.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>The scale of terms-moving average model's logit and the second order term in Eq. 3. The latter concentrates around 0, suggesting our model averaging protocol approximates ensembles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Evolution of training loss, in-domain validation accuracy and out-domain test accuracy for ResNet-50 (pre-trained on ImageNet) trained on PACS without model averaging (top 4 rows) and with model averaging (bottom 4 rows) for 5, 000 iterations with the domain mentioned in the title used as test domain and remain domains as training/validation data. Each color represents a different run with randomly chosen seed, hyper-parameters and training-validation split following [18]. Evolution of training loss, in-domain validation accuracy and out-domain test accuracy for ResNet-50 (pre-trained on ImageNet) trained on TerraIncognita without model averaging (top 4 rows) and with model averaging (bottom 4 rows) for 10, 000 iterations with the domain mentioned in the title used as test domain and remain domains as training/validation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Individual Models</cell></row><row><cell cols="2">TerraIncognita w/o avg</cell><cell>w/ avg</cell></row><row><cell>L100</cell><cell cols="2">0.21 ? 0.07 0.90 ? 0.05</cell></row><row><cell>L38</cell><cell cols="2">0.12 ? 0.13 0.83 ? 0.05</cell></row><row><cell>L43</cell><cell cols="2">0.30 ? 0.06 0.67 ? 0.18</cell></row><row><cell>L46</cell><cell cols="2">0.03 ? 0.11 0.52 ? 0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>: Ensembles</cell><cell></cell></row><row><cell cols="3">TerraIncognita w/o avg w/ avg</cell></row><row><cell>L100</cell><cell>0.48</cell><cell>1</cell></row><row><cell>L38</cell><cell>0.17</cell><cell>0.95</cell></row><row><cell>L43</cell><cell>0.59</cell><cell>0.38</cell></row><row><cell>L46</cell><cell>0.08</cell><cell>0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>To investigate models with the same size, but one pre-trained on a larger dataset, we compare the results of ResNet-50 and ResNeXt-50 32x4d. On average across all five datasets, the gain of SMA over ERM (our runs) is 2.5% for ResNet-50 and 3.9% for ResNeXt-50 32x4d. The gain of EoA over ERM is larger: 4% vs 5% respectively. This suggests that pre-training the model on a larger dataset increases the gain of model averaging and EoA over the corresponding ERM baseline, while the ERM performance itself improves.Next, to investigate the impact of both larger model size and larger pre-training dataset, we compare the results of ResNeXt-50 32x4d and RegNetY-16GF. On average across all five datasets, the gain of SMA over ERM (our runs) is 3.9% for ResNeXt-50 32x4d and 5% for RegNetY-16GF. The gain of EoA over ERM is again larger: 5% vs 6% respectively. This suggests that increasing both model size and pre-training dataset size allow model averaging and EoA to provide larger out-domain gains over the corresponding ERM baseline. Notice that these claims are different from existing work</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>SMA outperforms ERM without model averaging in the IID setting. averaging) 94.39 ? 0.46 77.09 ? 0.57 SMA (ours) 96.77 ? 0.20 83.56 ? 0.21</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>OfficeHome</cell></row><row><cell>ERM (no</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameter search space for all experiments.</figDesc><table><row><cell cols="4">Hyper-parameter Default value</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Random distribution</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">[18]</cell><cell>Ours</cell></row><row><cell cols="3">Learning rate</cell><cell>5e ? 5</cell><cell cols="4">10 Uniform(?5,?3.5)</cell><cell>5e ? 5</cell></row><row><cell cols="3">Batch size</cell><cell>32</cell><cell></cell><cell cols="3">2 Uniform(3,5.5)</cell><cell>32</cell></row><row><cell cols="3">ResNet dropout</cell><cell>0</cell><cell cols="5">RandomChoice([0, 0.1, 0.5]) RandomChoice([0, 0.1, 0.5])</cell></row><row><cell cols="3">Weight decay</cell><cell>0</cell><cell cols="4">10 Uniform(?6,?2)</cell><cell>10 Uniform(?6,?4)</cell></row><row><cell>Test Domain Accuracy</cell><cell>80 90</cell><cell cols="2">0 1000 2000 3000 4000 5000 6000 7000 8000 SMA Start Iteration PACS (with moving average)</cell><cell>Art Cartoon Photo Sketch Avg</cell><cell>Test Domain Accuracy</cell><cell>40 50 60</cell><cell cols="2">SMA Start Iteration 0 1000 2000 3000 4000 5000 6000 7000 8000 TerraIncognita (with moving average)</cell><cell>L100 L38 L43 L46 Avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Individual Models</cell></row><row><cell>PACS</cell><cell>w/o avg</cell><cell>with avg</cell></row><row><cell>Art</cell><cell cols="2">0.31 ? 0.04 0.62 ? 0.04</cell></row><row><cell>Cartoon</cell><cell cols="2">0.25 ? 0.10 0.52 ? 0.03</cell></row><row><cell>Photo</cell><cell cols="2">0.09 ? 0.07 -0.38 ? 0.15</cell></row><row><cell>Sketch</cell><cell cols="2">0.24 ? 0.06 0.53 ? 0.06</cell></row><row><cell cols="2">TerraIncognita w/o avg</cell><cell>with avg</cell></row><row><cell>L100</cell><cell cols="2">0.21 ? 0.07 0.90 ? 0.05</cell></row><row><cell>L38</cell><cell cols="2">0.12 ? 0.13 0.83 ? 0.05</cell></row><row><cell>L43</cell><cell cols="2">0.30 ? 0.06 0.67 ? 0.18</cell></row><row><cell>L46</cell><cell cols="2">0.03 ? 0.11 0.52 ? 0.14</cell></row><row><cell>VLCS</cell><cell>w/o avg</cell><cell>with avg</cell></row><row><cell>Caltech101</cell><cell cols="2">0.21 ? 0.10 0.16 ? 0.15</cell></row><row><cell>LabelMe</cell><cell cols="2">0.30 ? 0.08 0.02 ? 0.14</cell></row><row><cell>Sun09</cell><cell cols="2">0.27 ? 0.12 0.32 ? 0.11</cell></row><row><cell>VOC2007</cell><cell cols="2">0.17 ? 0.11 0.38 ? 0.05</cell></row><row><cell>OfficeHome</cell><cell>w/o avg</cell><cell>with avg</cell></row><row><cell>Art</cell><cell cols="2">0.05 ? 0.11 0.80 ? 0.04</cell></row><row><cell>Clipart</cell><cell cols="2">0.33 ? 0.04 0.84 ? 0.04</cell></row><row><cell>Product</cell><cell cols="2">0.61 ? 0.04 0.80 ? 0.04</cell></row><row><cell>RealWorld</cell><cell cols="2">0.41 ? 0.06 0.74 ? 0.04</cell></row><row><cell>DomainNet</cell><cell>w/o avg</cell><cell>with avg</cell></row><row><cell>Clip</cell><cell cols="2">0.96 ? 0.01 1 ? 0</cell></row><row><cell>Info</cell><cell cols="2">0.80 ? 0.05 1 ? 0</cell></row><row><cell>Paint</cell><cell cols="2">0.87 ? 0.02 1 ? 0</cell></row><row><cell>Quick</cell><cell cols="2">0.65 ? 0.04 1 ? 0</cell></row><row><cell>Real</cell><cell cols="2">0.91 ? 0.01 1 ? 0</cell></row><row><cell>Sketch</cell><cell cols="2">0.82 ? 0.04 1 ? 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Ensembles</cell></row><row><cell>PACS</cell><cell cols="2">w/o avg with avg</cell></row><row><cell>Art</cell><cell>0.06</cell><cell>0.78</cell></row><row><cell>Cartoon</cell><cell>0.33</cell><cell>0.81</cell></row><row><cell>Photo</cell><cell>-0.12</cell><cell>-0.52</cell></row><row><cell>Sketch</cell><cell>0.43</cell><cell>0.70</cell></row><row><cell cols="3">TerraIncognita w/o avg with avg</cell></row><row><cell>L100</cell><cell>0.48</cell><cell>1</cell></row><row><cell>L38</cell><cell>0.17</cell><cell>0.95</cell></row><row><cell>L43</cell><cell>0.59</cell><cell>0.38</cell></row><row><cell>L46</cell><cell>0.08</cell><cell>0.61</cell></row><row><cell>VLCS</cell><cell cols="2">w/o avg with avg</cell></row><row><cell>Caltech101</cell><cell>0.52</cell><cell>0.81</cell></row><row><cell>LabelMe</cell><cell>0.05</cell><cell>0.38</cell></row><row><cell>Sun09</cell><cell>0.63</cell><cell>0.82</cell></row><row><cell>VOC2007</cell><cell>0.55</cell><cell>0.65</cell></row><row><cell>OfficeHome</cell><cell cols="2">w/o avg with avg</cell></row><row><cell>Art</cell><cell>0.27</cell><cell>0.92</cell></row><row><cell>Clipart</cell><cell>0.66</cell><cell>0.95</cell></row><row><cell>Product</cell><cell>0.20</cell><cell>0.95</cell></row><row><cell>RealWorld</cell><cell>0.09</cell><cell>0.78</cell></row><row><cell>DomainNet</cell><cell cols="2">w/o avg with avg</cell></row><row><cell>Clip</cell><cell>1</cell><cell>1</cell></row><row><cell>Info</cell><cell>0.88</cell><cell>1</cell></row><row><cell>Paint</cell><cell>0.98</cell><cell>1</cell></row><row><cell>Quick</cell><cell>0.95</cell><cell>1</cell></row><row><cell>Real</cell><cell>0.97</cell><cell>1</cell></row><row><cell>Sketch</cell><cell>0.97</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Out-domain accuracy for PACS dataset. ? 1.0 80.4 ? 0.6 94.8 ? 0.1 76.2 ? 1.7 84.4</figDesc><table><row><cell>Algorithm</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>S</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ERM 86.4 Ensemble 88.3</cell><cell>83.6</cell><cell>96.5</cell><cell>81.9</cell><cell>87.6</cell></row><row><cell>SMA</cell><cell cols="5">89.1 ? 0.1 82.6 ? 0.2 97.6 ? 0.0 80.5 ? 0.9 87.5</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 90.5</cell><cell>83.4</cell><cell>98.0</cell><cell>82.5</cell><cell>88.6</cell></row><row><cell></cell><cell cols="2">ResNeXt-50 32x4d [48]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">84.7 ? 1.6 87.6 ? 0.1 97.6 ? 0.4 85.7 ? 0.1 88.9</cell></row><row><cell>Ensemble</cell><cell>90.2</cell><cell>89.2</cell><cell>98.1</cell><cell>87.2</cell><cell>91.2</cell></row><row><cell>SMA</cell><cell cols="5">92.6 ? 0.3 90.9 ? 0.8 99.1 ? 0.3 88.3 ? 0.5 92.7</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 93.1</cell><cell>91.8</cell><cell>99.2</cell><cell>88.9</cell><cell>93.2</cell></row><row><cell></cell><cell cols="2">RegNetY-16GF [40]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="4">90.2 ? 0.6 92.6 ? 0.8 97.6 ? 0.1 87.8 ? 2</cell><cell>92</cell></row><row><cell>Ensemble</cell><cell>93.75</cell><cell>95.35</cell><cell>98.02</cell><cell>93.38</cell><cell>95.1</cell></row><row><cell>SMA</cell><cell cols="5">93.8 ? 0.3 95.8 ? 0.2 99.2 ? 0.2 93.4 ? 0.2 95.5</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 94.09</cell><cell>96.33</cell><cell>99.52</cell><cell>93.31</cell><cell>95.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Out-domain accuracy for VLCS dataset.</figDesc><table><row><cell>Algorithm</cell><cell>C</cell><cell>L</cell><cell>S</cell><cell>V</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">98.5 ? 0.5 62.4 ? 1.4 72.1 ? 0.0 75.4 ? 0.1 77.1</cell></row><row><cell>Ensemble</cell><cell>98.7</cell><cell>64.5</cell><cell>72.1</cell><cell>78.9</cell><cell>78.5</cell></row><row><cell>SMA</cell><cell cols="5">99.0 ? 0.2 63.0 ? 0.2 74.5 ? 0.3 76.4 ? 1.1 78.2</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 99.1</cell><cell>63.1</cell><cell>75.9</cell><cell>78.3</cell><cell>79.1</cell></row><row><cell></cell><cell cols="2">ResNeXt-50 32x4d [48]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">97.0 ? 0.4 67.8 ? 0.7 75.7 ? 0.2 75.5 ? 0.6 79.0</cell></row><row><cell>Ensemble</cell><cell>98.4</cell><cell>66.1</cell><cell>76.4</cell><cell>80.5</cell><cell>80.3</cell></row><row><cell>SMA</cell><cell cols="5">98.8 ? 0.2 63.3 ? 0.6 77.7 ? 0.2 79.2 ? 0.8 79.7</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 98.7</cell><cell>64.1</cell><cell>78.2</cell><cell>80.6</cell><cell>80.4</cell></row><row><cell></cell><cell cols="2">RegNetY-16GF [40]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">96.0 ? 0.4 66.0 ? 0.8 76.1 ? 0.7 76.2 ? 2.1 78.6</cell></row><row><cell>Ensemble</cell><cell>97.88</cell><cell>67.28</cell><cell>78.46</cell><cell>78.55</cell><cell>80.6</cell></row><row><cell>SMA</cell><cell cols="5">98.1 ? 0.2 65.7 ? 1.1 79.2 ? 1.1 79.6 ? 0.2 80.7</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 98.23</cell><cell>66.00</cell><cell>79.49</cell><cell>80.63</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Out-domain accuracy for OfficeHome dataset.</figDesc><table><row><cell>Algorithm</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">60.5 ? 0.7 54.5 ? 0.8 74.7 ? 0.8 76.6 ? 0.2 66.6</cell></row><row><cell>Ensemble</cell><cell>65.6</cell><cell>58.5</cell><cell>78.7</cell><cell>80.5</cell><cell>70.8</cell></row><row><cell>SMA</cell><cell cols="4">66.7 ? 0.5 57.1 ? 0.1 78.6 ? 0.1 80.0 ? 0</cell><cell>70.6</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 69.1</cell><cell>59.8</cell><cell>79.5</cell><cell>81.5</cell><cell>72.5</cell></row><row><cell></cell><cell cols="2">ResNeXt-50 32x4d [48]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">64.7 ? 1.0 60.6 ? 0.3 77.1 ? 0.4 81.3 ? 0.2 70.9</cell></row><row><cell>Ensemble</cell><cell>74.1</cell><cell>67.3</cell><cell>83.9</cell><cell>86.0</cell><cell>77.8</cell></row><row><cell>SMA</cell><cell cols="5">76.7 ? 0.4 67.8 ? 0.0 84.0 ? 0.1 85.8 ? 0.1 78.6</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 79.0</cell><cell>70.0</cell><cell>85.2</cell><cell>86.5</cell><cell>80.2</cell></row><row><cell></cell><cell cols="2">RegNetY-16GF [40]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell cols="5">70.7? 1.3 60.0 ? 0.5 82.4 ? 0.5 82.1 ? 0.4 73.8</cell></row><row><cell>Ensemble</cell><cell>79.44</cell><cell>68.68</cell><cell>86.28</cell><cell>87.63</cell><cell>80.5</cell></row><row><cell>SMA</cell><cell cols="5">81.1 ? 0.4 72.3 ? 0.6 86.6 ? 0.1 88.2 ? 0.1 82.0</cell></row><row><cell cols="2">Ensemble of Averages (EoA) 83.89</cell><cell>73.95</cell><cell>88.22</cell><cell>89.37</cell><cell>83.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">T ?t0+1 ? T t=t0 ? t . Notice the first order term has been omitted since it is zero. As an important detail, instead of computing the second order term in the above equation for all</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving out-of-distribution generalization via multi-task self-supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Isabela Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2178" to="2186" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised learning across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Swad: Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08604</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Domain generalization by mutual-information regularization with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10789</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Underspecification presents challenges for credibility in modern machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Alexander D&amp;apos;amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03395</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02757</idno>
		<title level="m">Deep ensembles: A loss landscape perspective</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural networks and the bias/variance dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<title level="m">search of lost domain generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-challenging improves crossdomain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="124" to="140" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Kidambi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens Van Der Maaten</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iterate averaging as regularization for stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gergely</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3222" to="3242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fishr: Invariant gradient variances for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02934</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gradient matching for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Seely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09937</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Laurens van der Maaten. Revisiting weakly supervised pre-training of visual perception models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>De Freitas Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugra</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08371</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the stability of inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Nikolayevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tikhonov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="195" to="198" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Statistical learning theory wiley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlamimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An empirical investigation of domain generalization with empirical risk minimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Heterogeneous domain generalization via domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3622" to="3626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking bias-variance trade-off for generalization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10767" to="10777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2100" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02503</idno>
		<title level="m">Table 14: Out-domain accuracy for TerraIncognita dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Domain generalization: A survey</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Algorithm clip info paint quick real sketch Avg</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>Out-domain accuracy for Domainet dataset. ResNet-50 ERM 63.4 ? 0.2 20.6 ? 0.1 50.0 ? 0.1 13.8 ? 0.4 62.1 ? 0.2 51.9 ? 0.3 43.6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
