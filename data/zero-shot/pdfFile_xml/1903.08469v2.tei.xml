<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">Sini?a?egvi?</orgName>
								<orgName type="institution" key="instit2">Computing University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kre?o</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">Sini?a?egvi?</orgName>
								<orgName type="institution" key="instit2">Computing University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">Sini?a?egvi?</orgName>
								<orgName type="institution" key="instit2">Computing University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent success of semantic segmentation approaches on demanding road driving datasets has spurred interest in many related application fields. Many of these applications involve real-time prediction on mobile platforms such as cars, drones and various kinds of robots. Real-time setup is challenging due to extraordinary computational complexity involved. Many previous works address the challenge with custom lightweight architectures which decrease computational complexity by reducing depth, width and layer capacity with respect to general purpose architectures. We propose an alternative approach which achieves a significantly better performance across a wide range of computing budgets. First, we rely on a light-weight general purpose architecture as the main recognition engine. Then, we leverage light-weight upsampling with lateral connections as the most cost-effective solution to restore the prediction resolution. Finally, we propose to enlarge the receptive field by fusing shared features at multiple resolutions in a novel fashion. Experiments on several road driving datasets show a substantial advantage of the proposed approach, either with ImageNet pre-trained parameters or when we learn from scratch. Our Cityscapes test submission entitled SwiftNetRN-18 delivers 75.5% MIoU and achieves 39.9 Hz on 1024?2048 images on GTX1080Ti.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is an important dense prediction task in which the inference targets posterior distribution over a known set of classes in each image pixel <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. Currently, the best results are achieved with deep fully convolutional models which require extraordinary computational resources. Many important applications such as autonomous navigation or driver assistance require inference on very large images in order to cover a wide field of view * equal contribution <ref type="figure">Figure 1</ref>. Speed-accuracy trade-off for different semantic semantic segmentation approaches on Cityscapes test on GTX1080Ti (except for DG2s which reports only validation performance). Red dots represent our method. Other methods are displayed in green, whereas blue dots show estimated frame rates of the corresponding methods on our GPU (please refer to subsection 4.2 for details). Our submissions achieve the best accuracy and the best speed among all approaches aiming at real-time operation. and perceive pedestrians at distances of over 200m. At the same time, these applications require a very low latency in order to be able to bring real-time decisions. The resulting requirements intensify computational strain and make realtime implementations a challenging research objective.</p><p>Many real-time semantic segmentation approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> address this goal by introducing custom lightweight architectures which are not suited for largescale visual recognition. Most of these approaches initialize training from scratch, and thus miss a huge regularization arXiv:1903.08469v2 [cs.CV] 12 Apr 2019 opportunity offered by knowledge transfer <ref type="bibr" target="#b25">[26]</ref> from larger and more diverse recognition datasets <ref type="bibr" target="#b29">[30]</ref>. Consequently, these methods incur a comparatively large overfitting risk. Some approaches alleviate this shortcoming by pre-training on ImageNet <ref type="bibr" target="#b27">[28]</ref>. However, our experiments suggest that the resulting benefits tend to be smaller than in architectures which are designed for competitive ImageNet performance.</p><p>A simple model for semantic segmentation starts with a fully convolutional encoder which gradually decreases the resolution and increases the number of feature maps of the resulting representation. Instead of performing global pooling (as we would do in image-wide classification) one can proceed by attaching a pixel-wise loss to obtain the predictions <ref type="bibr" target="#b5">[6]</ref>. The resulting model would lead to a very fast evaluation on modern hardware, however its accuracy would be rather low due to the following problems. Firstly, small objects (e.g. distant traffic signs) would not be recognized due to low resolution of pixel-wise predictions, which is usually 32 times smaller than the input image. Secondly, the receptive field of such models would not be large enough to classify pixels at large objects (e.g. nearby buses or trucks). These problems can be alleviated with various techniques such as dilated convolutions <ref type="bibr" target="#b36">[37]</ref>, learned upsampling <ref type="bibr" target="#b19">[20]</ref>, lateral connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16]</ref> and resolution pyramids <ref type="bibr" target="#b5">[6]</ref>. However, not all of these techniques are equally suited for real-time operation.</p><p>In this paper, we argue that a competitive blend of efficiency and prediction accuracy can be achieved by models based on lightweight ImageNet-grade classification architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>. Additionally, we propose a novel approach to increase the receptive field of deep model predictions, based on a resolution pyramid with shared parameters <ref type="bibr" target="#b5">[6]</ref>. The proposed approach incurs a very modest increase of the model capacity and is therefore especially suited for datasets with large objects and few annotated images. Finally, we show that the resolution of the predictions can be efficiently and accurately upsampled by a lightweight decoder with lateral connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. We show that the resulting semantic segmentation models can be evaluated under various computing budgets, and that they are feasible even on embedded GPU platforms. We present experiments both with ImageNet pre-training and learning from scratch on different road driving datasets. Our experiments achieve state-of-the art semantic segmentation accuracy among all existing approaches aiming at real-time execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>As described in the introduction, semantic segmentation models have to face two major problems: restoring the input resolution and increasing the receptive field. The simplest way to restore input resolution is to avoid downsampling. This is usually achieved by replacing stride-2 poolings with non-strided poolings, and doubling the dilation factor in subsequent convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>. However, this approach increases the resolution of deep latent representations, which implies a large computational complexity. Furthermore, dilated convolutions incur significant additional slow-down due to necessity to rearrange the image data before and after calling optimized implementations.</p><p>Another way to achieve dense image prediction relies on trained upsampling <ref type="bibr" target="#b19">[20]</ref>, which results in an encoderdecoder architecture. The idea is to perform recognition on subsampled latent representation to reduce complexity and then to restore the resolution by upsampling the representation (or the predictions). This setup can be naturally augmented by introducing lateral connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16]</ref> to blend semantically rich deep layers with spatially rich shallow layers. The upsampling path has to be as lean as possible (to achieve efficiency and prevent overfitting) but no leaner (to avoid underfitting). It turns out that the sweet spot is computationally inexpensive, which makes this setup especially well-suited for real-time operation.</p><p>Early approaches to enlarge the receptive field of logit activations were based on dilated convolutions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>. A more involved approach is known as spatial pyramid pooling (SPP) <ref type="bibr" target="#b6">[7]</ref>. SPP averages features over aligned grids with different granularities <ref type="bibr" target="#b16">[17]</ref>. We use a convolutional adaptation of that idea, in which the feature pyramid is upsampled to the original resolution <ref type="bibr" target="#b40">[41]</ref> and then concatenated with the input features. Thus, subsequent convolutions obtain access to broad spatial pools and that increases their receptive field. The combination of dilated convolutions and SPP is known as? trous SPP, or ASPP for short <ref type="bibr" target="#b2">[3]</ref>. However, SPP and ASPP may hurt generalization due to large capacity. In this paper, we study resolution pyramids as an alternative way to increase the receptive field and at the same time promote scale invariance <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref>. Most previous pyramidal approaches to semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b22">23]</ref> fuse only the deepest representations extracted at different resolutions. Different from them, we combine representations from different abstraction levels before joining the upsampling path within the decoder. This results in a better gradient flow throughout the pyramidal representation which is advantageous when the objects are large and the training data is scarce.</p><p>Efficient recognition architectures leverage optimized building blocks which aim at reducing computational load while preserving accuracy. Grouped convolutions reduce the number of floating point operations and the number of parameters by enclosing the information flow within smaller groups of feature maps. Various methods have been proposed to discover prominent inter-group connections. ShuffleNet <ref type="bibr" target="#b38">[39]</ref> uses channel shuffling to pass information across convolutional groups. CondenseNet <ref type="bibr" target="#b10">[11]</ref> incorporates a training strategy which locates important connections in grouped convolutions and prunes those which are redundant. Neural architecture search <ref type="bibr" target="#b41">[42]</ref> is an approach that leverages reinforcement learning to jointly learn the model architecture and the corresponding parameters. The resulting architectures achieve competitive ImageNet performance when the computational budget is restricted. Depthwise separable convolutions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref> decrease computational complexity by splitting a regular convolution in two. Firstly, a k ? k convolution is separably applied to each input channel. This can be viewed as a group convolution where the number of groups corresponds to the number of channels C. In other words, there are C kernels k?k?1. Secondly, a 1?1 convolution is applied to propagate interchannel information. Replacing standard convolutions with depthwise separable convolutions lowers the number of parameters and increases the inference speed at the cost of some drop in performance <ref type="bibr" target="#b9">[10]</ref>. Strong regularization effect of depthwise separable convolutions can be relaxed by inverted residual blocks <ref type="bibr" target="#b30">[31]</ref> which lead to compact residual models suitable for mobile applications.</p><p>Most semantic segmentation approaches aiming at realtime operation refrain from using encoders designed for competitive ImageNet performance. ICNet <ref type="bibr" target="#b39">[40]</ref> proposes a custom encoder which processes an image pyramid with shared parameters and fuses multi-scale representations before entering the decoder which restores the resolution. ERFNet <ref type="bibr" target="#b27">[28]</ref> redefines a residual block as a composition of a 3 ? 1 followed by a 1 ? 3 convolution, which yields a 33% reduction in parameters. Vallurupalli et al. <ref type="bibr" target="#b34">[35]</ref> propose the DG2s approach as an efficient ERFNet variant with the following modifications in residual blocks: i) depthwise separable convolutions, and ii) channel shuffling operation before pointwise convolutions. ESPNet <ref type="bibr" target="#b23">[24]</ref> factorizes convolutions in a similar manner and refrains from shared parameters across the image pyramid in order to produce a fast and compact architecture.</p><p>Our method is most related to semantic segmentation approaches which use lightweight encoders trained on Ima-geNet and benefit from such initialization. Similar to our work, Nekrasov et al. <ref type="bibr" target="#b24">[25]</ref> rely on MobileNet V2 <ref type="bibr" target="#b30">[31]</ref> and NASNet <ref type="bibr" target="#b41">[42]</ref> encoders and feature a thick decoder with lateral connections. This is similar to our single scale model, however, our decoder has much less capacity, which allows us to report a half of their number of floating point operations without sacrificing recognition accuracy on road driving datasets. LinkNet <ref type="bibr" target="#b1">[2]</ref> uses a small ResNet-18 backbone and a lightweight decoder to achieve satisfying performance-speed ratio. Our single scale model is similar to LinkNet, however we omit convolutional layers at full resolution in order to substantially reduce memory imprint and greatly increase the processing speed. Mazzini et al. <ref type="bibr" target="#b22">[23]</ref> use a dilated encoder initialized from the DRN-D-22 model <ref type="bibr" target="#b37">[38]</ref>, and a decoder with one lateral connection. They also learn nonlinear upsampling to improve accuracy at object boundaries. Instead of using dilated convolutions, our decoder upsamples predictions by exclusively relying on lateral connections, which results in a 4-fold speed-up. Additionally, we succeed to leverage full resolution images during training which achieves an improvement of 5 percentage points on Cityscapes test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed segmentation method</head><p>Our method assumes the following requirements. The model should be based on an ImageNet pre-trained encoder in order to benefit from regularization induced by transfer learning. The decoder should restore the resolution of encoded features in order for the predictions to retain detail. The upsampling procedure must be as simple as possible in order to maintain real-time processing speed. Gradient flow should be promoted throughout the network to support training from random initialization in an unusual event that ImageNet pre-training turns out not to be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic building blocks</head><p>The proposed segmentation method is conceived around three basic building blocks which we briefly describe in the following paragraphs. These building blocks are going to be used in our two models which we propose in subsequent subsections.</p><p>Recognition encoder We advocate the usage of compact ImageNet pre-trained model as segmentation encoders. We propose to use ResNet-18 <ref type="bibr" target="#b7">[8]</ref> and MobileNet V2 <ref type="bibr" target="#b30">[31]</ref> for a number of reasons. These models are a good fit for fine tuning due to pre-trained parameters being publicly available. They are also suitable for training from scratch due to moderate depth and residual structure. Finally, these models are compatible with real-time operation due to small operation footprint. Computationally, ResNet-18 is around six times more complex than MobileNet V2. However, MobileNet V2 uses depthwise separable convolutions which are not directly supported in GPU firmware (the cuDNN library). Therefore, MobileNet V2 tends to be slower than ResNet-18 in most experimental setups. Note that the same issue disqualifies usage of the DenseNet architecture <ref type="bibr" target="#b11">[12]</ref>, since it requires efficient convolution over a non-contiguous tensor, which is still not supported in cuDNN.</p><p>Upsampling decoder The recognition encoder transforms the input image into semantically rich visual features. These features must have a coarse spatial resolution in order to save memory and processing time. The purpose of the decoder is to upsample these features to the input resolution. We advocate a simple decoder organized as a sequence of upsampling modules with lateral connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. The proposed ladder-style upsampling modules have two inputs: the low resolution features (which should be upsampled), and the lateral features from an earlier layer of the encoder. The low resolution features are first upsampled with bilinear interpolation to the same resolution as the lateral features. Upsampled input features and lateral encoder features are then mixed with elementwise summation and finally blended with a 3?3 convolution.</p><p>We propose to route the lateral features from the output of the elementwise sum within the last residual block at the corresponding level of subsampling, as shown in <ref type="figure">Figure 2</ref>. Note that routing the lateral features from the output of the subsequent ReLU leads to a considerable drop in validation accuracy. Replacing the standard 3?3 convolution with either a 1?1 convolution, or a depthwise separable convolution also decreases the validation accuracy. <ref type="figure">Figure 2</ref>. Structural diagram of the last residual unit within a convolutional block operating on common spatial resolution. We do not use pre-activation [9] since we could not find a pre-trained parameterization for ResNet-18. The lateral connection is taken from the output of the elementwise sum after the last residual block. The output of the ReLU node is forwarded to the next residual block.</p><p>Module for increasing the receptive field As discussed before, there are two viable possibilities for increasing the receptive field while maintaining real-time speed: i) spatial pyramid pooling, and ii) pyramid fusion. The SPP block gathers the features produced by the encoder at several pooling levels and produces a representation with a varying level of detail. We demonstrate the use of SPP in our single scale model. Our SPP block is a simplified version of the pyramid pooling module from PSPNet <ref type="bibr" target="#b40">[41]</ref>. The pyramid fusion produces genuine multi-scale representations which need to be carefully fused within the decoder in order to avoid overfitting to unsuitable level of detail. We propose a pyramid pooling approach which blends representations at different levels of abstraction and thus enlarges the receptive field without sacrificing spatial resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single scale model</head><p>The proposed single scale model transforms the input image into dense semantic predictions throughout a downsampling recognition encoder and upsampling decoder, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Yellow trapezoids designate convolution groups, that is, parts of the encoder which operate on the same spatial resolution. All considered encoders consist of four such convolution groups. The first convolution group produces features at the H/4?W/4 resolution, while each following group increases the subsampling by the fac-tor of 2. Thus the features at the far end of the encoder are H/32?W/32. These features are fed into the spatial pyramid pooling layer (designated by a green diamond) in order to increase the model receptive field. The resulting tensor is finally routed to the decoder whose upsampling modules are shown in blue.</p><p>Note that decoder and encoder are asymmetric: the encoder has many convolutions per convolution group while decoder has only one convolution per upsampling module. Furthermore, the dimensionality of encoder features increases along the downsampling path, while the dimensionality of the decoder features is constant. Therefore, lateral connections have to adjust dimensionality with 1?1 convolutions designated with red squares. Upsampling modules operate in three steps: i) the low resolution representation is bilinearly upsampled, ii) the obtained representations is summed with the lateral connection, iii) the summation is blended using a 3?3 convolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Interleaved pyramid fusion model</head><p>While using a compact encoder is beneficial for fast inference, this also results in a decreased receptive field and a smaller capacity compared to general purpose convolutional models for visual recognition. To counteract these drawbacks, we propose to exploit image pyramids to enlarge the receptive field of the model and reduce the model capacity requirements.</p><p>The proposed model is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Two encoder instances (yellow) are applied to the input image at different levels of the resolution pyramid. This results in increased receptive field of the activations which sense the lowest res-olution of the image pyramid. Furthermore, shared parameters enable recognition of objects of different sizes with the common set of parameters, which may relax the demand for model capacity. In order to enforce lateral connections and improve the gradient flow throughout the encoder, we concatenate the feature tensors from neighbouring levels of different encoders (we can do that since they have equal spatial resolution). This concatenation is designated with green circles. After concatenation, interleaved encoder features are projected onto the decoder feature space by 1?1 convolutions designated with red squares. The decoder (blue) operates in the same manner as in the single-scale model, however now we have an additional upsampling module for each additional level of the image pyramid. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct semantic segmentation experiments on two datasets: Cityscapes <ref type="bibr" target="#b4">[5]</ref> and CamVid <ref type="bibr" target="#b0">[1]</ref>. We report mIoU accuracy, computational complexity and the execution speed of the trained models. The speed measurements are performed on a desktop GPU (GTX 1080Ti) and on an embedded System on a chip module (Jetson TX2). We also present ablation and validation experiments which provide a more detailed insight into the impact of various design choices. Please note that additional experiments can be found in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training details</head><p>We train all our models with the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer with the learning rate set to 4 ? 10 ?4 . We decay the learning rate with cosine annealing <ref type="bibr" target="#b20">[21]</ref> to the minimum value of 1 ? 10 ?6 in the last epoch (we do not perform any warm restarts). The weight decay is set to 1?10 ?4 . In experiments with ImageNet pre-training, we update pre-trained parameters with 4 times smaller learning rate and apply 4 times smaller weight decay. We train on jittered square crops with batch size 12. The jittering consists of random horizontal flipping, and scaling with random factors between 0.5 and 2. We use 768?768 crops for full Cityscapes resolution, and 448?448 crops for half Cityscapes resolution and CamVid. We train for 200 epochs on Cityscapes and 400 epochs on CamVid. We train for additional 200 epochs in experiments without ImageNet pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Measuring the computational complexity</head><p>We express the computational complexity of the trained models with two metrics: i) billions of floating point operations (GFLOP), and ii) number of processed frames per second (FPS). The GFLOP metric provides the number of fused multiply-add operations required to evaluate the model. Such platform-agnostic measure of the computational complexity is suitable for CPUs where all multiplications require roughly equal processing time. Unfortunately, the GFLOP metric poorly corresponds with the actual processing time on GPU platforms, since efficient implementations are available only for a small subset of all building blocks used to express current deep models. Consequently, both metrics are required for a complete description of algorithm suitability for real-time operation.</p><p>The FPS metric directly corresponds to the processing time on a particular hardware platform. Such metric does not necessarily correlate across platforms, although rough estimations can be done, as we show below. We simulate real-time applications by setting batch size to 1. We measure the time elapsed between transferring the input data to the GPU, and receiving the semantic predictions into RAM as shown in the code snippet shown in <ref type="figure" target="#fig_2">Figure 5</ref>. We conduct all measurements on a single GTX1080Ti with CUDA 10.0, CUDNN 7.3 and PyTorch 1.0rc1. We exclude the batch normalization layers <ref type="bibr" target="#b12">[13]</ref> from measurements since in real-time applications they would be fused with preceding convolutional layers. We report mean FPS values over 1000 forward passes. Results are shown in Table 1. The column FPS norm provides a rough estimate on how would other methods perform on our hardware. The scaling factors are: 1.0 for GTX1080Ti, 0.61 for TitanX Maxwell, 1.03 for TitanX Pascal, and 1.12 for Titan XP. These scaling factors were calculated using publicly available benchmarks: goo.gl/N6ukTz, goo.gl/BaopYQ. The column GFLOPs@1MPx shows an estimated number of FLOPs for an input image of 1MPx, as a resolutionagnostic metric of computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cityscapes</head><p>The Cityscapes dataset is a collection of high resolution images taken from the driver's perspective during daytime and fine weather. It consists of 2975 training, 500 validation, and 1525 test images with labels from 19 classes. It also provides 20000 coarsely labeled images which we do not use during our experiments. <ref type="table">Table 1</ref> evaluates the accuracy (class mIoU) and efficiency (GFLOP, FPS) of our methods and compares them to other real-time methods. Our single scale method based on the ResNet-18 encoder achieves 75.5% test mIoU, and delivers 39.9 FPS on full Cityscapes resolution (1024?2048 pixels). To the best of our knowledge, this result outperforms all other approaches aiming at real-time operation. The corresponding submission to the Cityscapes evaluation server is enti-tled SwiftNetRN-18. <ref type="table">Table 1</ref> also presents experiments in which our models are trained from scratch. The accuracy decreases for 5 mIoU percentage points (pp) with respect to the corresponding experiments with ImageNet pre-trained initialization. This shows that ImageNet pre-training represents an important ingredient for reaching highly accurate predictions. We notice that custom encoders like ERFNet <ref type="bibr" target="#b27">[28]</ref> get less benefits from ImageNet pre-training: only 1.7% pp as shown in <ref type="table">Table 1</ref>. <ref type="figure">Figure 7</ref> presents examples of segmentations on Cityscapes val images. We show examples for both single scale and pyramid models. We did not achieve measurable improvements with the pyramid model over the single scale model on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CamVid</head><p>The CamVid dataset contains 701 densely annotated frames. We use the usual split into 367 train, 101 validation and 233 test images. We train on combined train and validation subsets and evaluate semantic segmentation into 11 classes on the test subset. <ref type="table">Table 2</ref> shows that we obtain an improvement of roughly 1.5 pp mIoU when using the pyramid model with pre-trained ResNet-18 and MobileNetV2 backbones. <ref type="figure">Figure 8</ref> shows frames from the CamVid test subset where the pyramid model performed better. <ref type="table">Table 2</ref> also shows that ImageNet pre-training contributes more on CamVid than on Cityscapes (7-9pp of mIoU performance). This is not surprising since CamVid has almost 20 times less training pixels.</p><p>A small size of the dataset poses a considerable challenge when training from scratch due to high overfitting risk. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Validation of the upsampling capacity</head><p>The number of feature maps along the upsampling path is the most important design choice of the decoder. We validate this hyper-parameter and report the results in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation of lateral connections</head><p>To demonstrate the importance of lateral connections between the encoder and the decoder, we train a single scale model without lateral connections. For this experiment, we discard the 1?1 convolution layers located on the skip connections and abandon the elementwise summations in upsampling modules. Training such a model on full Cityscapes train images causes the validation accuracy to drop from 75.35% to 72.93%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Execution profile</head><p>To obtain a better insight into the execution time of our models, we report separate processing times and the GFLOP metrics for the downsampling path (encoder and SPP), and the upsampling path (decoder). <ref type="table" target="#tab_2">Table 4</ref> shows the results for the single scale model and input resolution of 2048?1024. Note the striking discrepancy of time and GFLOPs for the two downsampling paths. ResNet-18 is almost twice as fast than MobileNet v2 despite requiring 6 times more multiplications. Note also that our decoder is twice as fast as the ResNet-18 encoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Size of the receptive field</head><p>We estimate the effective receptive field of our models by considering the central pixel in each image X of Cityscapes val. The estimate is based on gradients ?yi ?X <ref type="bibr" target="#b21">[22]</ref> where y are the logits for the central pixel while i is argmax(y). <ref type="table">Table 5</ref> expresses the effective receptive fields as standard deviations of pixels with top 5% gradient ?yi ?X . The results show that both SPP and interleaved pyramidal fusion substantially increase the receptive field.  <ref type="table">Table 5</ref>. Effective receptive fields (ERF) expressed as standard deviation of pixels with top 5% image gradients with respect to the dominant class of the central pixel, measured on Cityscapes val.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Processing speed on Jetson TX2</head><p>We evaluate the proposed methods on NVIDIA Jetson TX2 module under CUDA 9.0, CUDNN 7.1, and PyTorch v1.0rc1. Due to limited number of CUDA cores, all bilinear interpolations had to be replaced with nearest neighbour interpolation. Results are reported in <ref type="figure">Figure 6</ref>. The Mo-bileNet V2 backbone outperforms ResNet-18 for 20-30% on most resolutions due to lower number of FLOPs. However, ResNet-18 is faster on the lowest resolution. Note that our implementations do not use TensorRT optimizations. <ref type="figure">Figure 6</ref>. Processing speed in frames per second of the proposed architecture on NVIDIA Jetson TX2 module for two different backbones and various input resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Real-time performance is a very important trait of semantic segmentation models aiming at applications in robotics and intelligent transportation systems. Most previous work in the field involves custom convolutional encoders trained from scratch, and decoders without lateral skip-connections. However, we argue that a better speedaccuracy trade-off is achieved with i) compact encoders designed for competitive ImageNet performance and ii) lightweight decoders with lateral skip-connections. Additionally, we propose a novel interleaved pyramidal fusion scheme which is able to further improve the results on large objects close to the camera. We provide a detailed analysis of prediction accuracy and processing time on Cityscapes and CamVid datasets for models based on ResNet-18 and MobileNetv2. Our Cityscapes test submis-sion achieves 75.5% mIoU by processing 1024?2048 images at 39.9 Hz on a GTX1080Ti. To the best of our knowledge, this result outperforms all previous approaches aiming at real-time application. The source code is available at https://github.com/orsic/swiftnet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Structural diagram of the proposed single scale model. Yellow trapezoids designate convolution groups within the encoder which may be pre-trained on ImageNet. The green diamond designates the spatial pyramid pooling layer, the red squares designate bottleneck layers, and blue trapezoids designate lightweight upsampling modules. Logits are upsampled to original image resolution with bilinear interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Structural diagram of the proposed model with interleaved pyramidal fusion. Encoder parameters (yellow) are shared across all pyramid levels and may be pre-trained on Imagenet. Features of the same resolutions are concatenated (green circles), fed into a 1?1 bottleneck convolution (red squares) and blended within the decoder (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>device = torch . device ( ' cuda ') model .eval () model .to( device ) with torch . no grad (): input = model . prepare data ( batch ).to( device ) logits = model . forward ( input) torch .cuda. synchronize () t0 = 1000 * perf counter () for in range(n): input = model . prepare data ( batch ).to( device ) logits = model . forward ( input ) , pred = logits .max (1) out = pred.data.byte ().cpu () torch .cuda. synchronize () t1 = 1000 * perf counter () fps = (1000 * n) / (t1 ? t0) Measurement of the processing time under PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 Table 1 .</head><label>21</label><figDesc>shows that the pyramid model achieves better Results of semantic segmentation on Cityscapes. We evaluate our best result on the online test benchmark and compare it with relevant previous work, where possible. We also report the computational complexity (GFLOP, FPS) GPU on which the inference was performed, and the image resolution on which the training and inference were performed. The column GFLOPs@1Mpx shows the GFLOPs metric when the input resolution is 1MPx. The column FPS norm shows or estimates the FPS metric on GTX 1080Ti. The default SwiftNet configuration is the single scale model presented in 3.2. Label pyr denotes the pyramid fusion model presented in 3.3. Label ens denotes the ensemble of the single scale model and the pyramid model. The symbol ? designates ImageNet pre-training.results than the single scale model. This supports our choice of sharing encoder parameters across pyramid levels.</figDesc><table><row><cell>model</cell><cell cols="2">subset mIoU</cell><cell>FPS</cell><cell>FPS norm</cell><cell>GPU</cell><cell>resolution</cell><cell cols="3">GFLOPs GFLOPs@1Mpx # params</cell></row><row><cell>D* [35]</cell><cell>val</cell><cell>68.4</cell><cell>-</cell><cell>-</cell><cell>TitanX M</cell><cell>1024x512</cell><cell>5.8</cell><cell>11.6</cell><cell>0.5M</cell></row><row><cell>DG2s [35]</cell><cell>val</cell><cell>70.6</cell><cell>-</cell><cell>-</cell><cell>TitanX M</cell><cell>1024x512</cell><cell>19.0</cell><cell>38</cell><cell>1.2M</cell></row><row><cell>Ladder DenseNet ?[16]</cell><cell>val</cell><cell>72.8</cell><cell>31.0</cell><cell>30.1</cell><cell>TitanX</cell><cell>1024x512</cell><cell>-</cell><cell>-</cell><cell>9.8M</cell></row><row><cell>ICNet [40]</cell><cell>test</cell><cell>69.5</cell><cell>30.3</cell><cell>49.7</cell><cell>TitanX M</cell><cell>2048x1024</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ESPNet [24]</cell><cell>test</cell><cell>60.3</cell><cell>112</cell><cell>108.7</cell><cell>TitanX</cell><cell>1024x512</cell><cell>-</cell><cell>-</cell><cell>0.4M</cell></row><row><cell>ERFNet [28]</cell><cell>test</cell><cell>68.0</cell><cell>11.2</cell><cell>18.4</cell><cell>TitanX M</cell><cell>1024x512</cell><cell>27.7</cell><cell>55.4</cell><cell>20M</cell></row><row><cell>GUNet ?[23]</cell><cell>test</cell><cell>70.4</cell><cell>37.3</cell><cell>33.3</cell><cell>TitanXP</cell><cell>1024x512</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ERFNet ?[28]</cell><cell>test</cell><cell>69.7</cell><cell>11.2</cell><cell>18.4</cell><cell>TitanX M</cell><cell>1024x512</cell><cell>27.7</cell><cell>55.4</cell><cell>20M</cell></row><row><cell>SwiftNetRN-18 SwiftNetMN V2</cell><cell>val val</cell><cell>70.4 69.4</cell><cell>39.9 27.7</cell><cell>39.3 27.7</cell><cell>GTX 1080Ti</cell><cell>2048x1024 2048x1024</cell><cell>104.0 41.0</cell><cell>52.0 20.5</cell><cell>11.8M 2.4M</cell></row><row><cell>SwiftNetRN-18 ?</cell><cell>val</cell><cell>70.2</cell><cell>134.9</cell><cell>134.9</cell><cell></cell><cell>1024x512</cell><cell>26.0</cell><cell>52.0</cell><cell>11.8M</cell></row><row><cell>SwiftNetRN-18 pyr ? SwiftNetMN V2 ?</cell><cell>val val</cell><cell>74.4 75.3</cell><cell>34.0 27.7</cell><cell>34.0 27.7</cell><cell>GTX 1080Ti</cell><cell>2048x1024 2048x1024</cell><cell>114.0 41.0</cell><cell>57.0 20.5</cell><cell>12.9M 2.4M</cell></row><row><cell>SwiftNetRN-18 ?</cell><cell>val</cell><cell>75.4</cell><cell>39.9</cell><cell>39.3</cell><cell></cell><cell>2048x1024</cell><cell>104.0</cell><cell>52.0</cell><cell>11.8M</cell></row><row><cell>SwiftNetRN-18 pyr ?</cell><cell>test</cell><cell>75.1</cell><cell>34.0</cell><cell>34.0</cell><cell></cell><cell>2048x1024</cell><cell>114.0</cell><cell>57.0</cell><cell>12.9M</cell></row><row><cell>SwiftNetRN-18 ?</cell><cell>test</cell><cell>75.5</cell><cell>39.9</cell><cell>39.3</cell><cell>GTX 1080Ti</cell><cell>2048x1024</cell><cell>104.0</cell><cell>52.0</cell><cell>11.8M</cell></row><row><cell>SwiftNetRN-18 ens ?</cell><cell>test</cell><cell>76.5</cell><cell>18.4</cell><cell>18.4</cell><cell></cell><cell>2048x1024</cell><cell>218.0</cell><cell>109.0</cell><cell>24.7M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 3 .</head><label>33</label><figDesc>The results show that the model accuracy saturates at 128 dimensions. Consequently, we pick this value as a sensible speed-accuracy trade-off in all other experiments. Validation of the number of feature maps in the upsampling path. The models were trained on Cityscapes train subset at 512?1024 while the evaluation is performed on Cityscapes val. All models use ImageNet initialization.</figDesc><table><row><cell>model</cell><cell cols="2">upsampling features mIoU</cell></row><row><cell></cell><cell>64</cell><cell>69.50</cell></row><row><cell>SwiftNetRN-18</cell><cell>128 192</cell><cell>70.35 70.26</cell></row><row><cell></cell><cell>256</cell><cell>70.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Inference speed along the downsampling (encoder and SPP) and the upsampling (decoder) paths for the single scale model. The columns dn time and up time display the execution times, while the columns dn FLOPs and up FLOPs show the number of floating point operations for 2048?1024 images.</figDesc><table><row><cell>model</cell><cell cols="4">dn time up time dn FLOPs up FLOPs</cell></row><row><cell>RN-18</cell><cell>15.0ms</cell><cell>8.1ms</cell><cell>76.1B</cell><cell>30.9B</cell></row><row><cell cols="2">MN-V2 26.7ms</cell><cell>7.5ms</cell><cell>12.1B</cell><cell>28.9B</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been supported by the European Regional Development Fund under the project System for increased driving safety in public urban rail traffic (Safe-TRAM) under grant KK.01.2.1.01.0022, and by European Regional Development Fund (DATACROSS) under grant KK.01.1.1.01.0009, and Microblink Ltd. We would like to thank Josip Krapac for helpful discussions. The Titan Xp used to train some of the evaluated models was donated by NVIDIA Corporation.  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">xx(x):xx-xx</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing</title>
		<meeting><address><addrLine>St. Petersburg, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-10" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The cityscapes dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharw?chter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Condensenet: An efficient densenet using learned group convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional scale invariance for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Causevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW CVRSUAD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">117</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
	<note>Proceedings, Part X</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Light-weight refinenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">125</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus</title>
		<meeting><address><addrLine>OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference Munich</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study of realtime semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vallurupalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Annamaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno>abs/1403.1687</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection-snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient semantic segmentation using gradual grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vallurupalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Annamaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nagori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factorized convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="545" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="636" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
