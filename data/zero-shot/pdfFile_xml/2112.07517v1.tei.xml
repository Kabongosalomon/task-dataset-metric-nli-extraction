<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Style and Semantic Memory Mechanism for Domain Generalization *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>JD</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>JD</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>JD</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
							<email>xinmei@ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>JD</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Style and Semantic Memory Mechanism for Domain Generalization *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mainstream state-of-the-art domain generalization algorithms tend to prioritize the assumption on semantic invariance across domains. Meanwhile, the inherent intradomain style invariance is usually underappreciated and put on the shelf. In this paper, we reveal that leveraging intra-domain style invariance is also of pivotal importance in improving the efficiency of domain generalization. We verify that it is critical for the network to be informative on what domain features are invariant and shared among instances, so that the network sharpens its understanding and improves its semantic discriminative ability. Correspondingly, we also propose a novel "jury" mechanism, which is particularly effective in learning useful semantic feature commonalities among domains. Our complete model called STEAM can be interpreted as a novel probabilistic graphical model, for which the implementation requires convenient constructions of two kinds of memory banks: semantic feature bank and style feature bank. Empirical results show that our proposed framework surpasses the state-of-the-art methods by clear margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning models are usually deployed in scenarios where the test data are unknown beforehand. This phenomenon can lead to dangerous consequences especially when the predictions are used for life-threatening occasions such as medical diagnosis. The prediction might be seriously erroneous due to the distributional gap between training data and test data. It is therefore critical for machine learning algorithms to maintain safe and reliable predictions that generalize well across domains. The goal of domain generalization (DG) approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b54">55]</ref> is to solve this issue by leveraging labeled data from multiple training domains. However, we observed that mainstream state-ofthe-art DG algorithms tend to only prioritize the semantic * This work was performed at JD AI Research.</p><p>invariance assumption across domains, while the style invariance within each domain is usually ignored. In this paper, we reveal that intra-domain style invariance is also of pivotal importance to improve DG approaches. Particularly, we propose a novel model to incorporate both intra-domain style invariance and inter-domain semantic invariance for DG tasks. The proposed framework is called STEAM, which relies on a STyle and sEmAntic Memory mechanism to practically implement our proposed assumptions.</p><p>In contrast to existing DG works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref>, STEAM further benefits from the hypothesis that instances from the same domain should share style information. The motivation is that the simple constraint helps efficiently disentangle the style feature, and therefore eases the search for true semantic feature with a reduced degree of freedom. To reach this goal, we resort to the recently prevailing selfsupervised learning paradigm that makes our assumptions practically accessible. Our first objective is to achieve intradomain style invariance by conveniently resorting to contrastive loss. Given the invariance assumption, style features corresponding to each domain are discovered, and the network can further learn semantic features along the directions mostly orthogonal to the domain styles. Since the semantic feature is considered as the true causal factor affecting the instance category, STEAM effectively helps reduce overfitting to the domain styles through the above mechanisms. Most importantly, we also force the network to respect the conventional semantic invariance among domains. Specifically, we require that each pair of samples from the same class to compute a similarity score with all the semantic features in "memory". These two samples need to reach a consensus on every such similarity score when sweeping through all the stored semantic features. We name this procedure "jury" mechanism, and we elaborate the mathematical net effect of such mechanism in the paper.</p><p>To summarize, our contributions in this paper include: 1. We explore the assumption of instance level intra-domain style invariance and justify the empirical advantage by incorporating such assumptions into the DG framework. 2. We propose a "jury" mechanism, which efficiently learns domain-agnostic semantic features beneficial for classification tasks. Such mechanism is capable of efficiently preventing semantic features from overfitting to domain styles. <ref type="bibr" target="#b2">3</ref>. We observe that the proposed STEAM not only generalizes well on DG benchmarks, but also can be conveniently modified for domain adaptation (DA) problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain Adaptation (DA) algorithms aim to exploit both annotated training data in the source domain and unlabeled samples in the target domain. Mainstream DA approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50]</ref> usually penalize distributional misalignment between source and target data via, e.g., maximum mean discrepancy loss <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> or adversarial loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>. Given the success of DA algorithms, multisource domain adaptation (MSDA) methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref> consider scenarios where multiple sources are available for training to better improve generalization.</p><p>Domain Generalization (DG) algorithms also assume access to multiple labeled training source domains. However, target data is unavailable during training for DG approaches, leading to a more challenging yet a more practical setting than DA problems. Many early DG approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> borrowed the idea of distribution alignment from DA to reduce the distributional gap between multiple training sources. Some recent DG methods consider generating extra synthetic images given the multiple source domains, so that test data distributed closely to the training data are "in-distribution" with the training data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53]</ref>. Some methods decompose the network parameters into domain-specific and domain-invariant parts during training, while only the domain-invariant parameters are used for predictions at test time. For instance, <ref type="bibr" target="#b17">[18]</ref> develops a low-rank parameterized CNN model where each layer of the network is decomposed into "common" and "specific" components. In <ref type="bibr" target="#b33">[34]</ref>, only the last layer of the network is decomposed to serve the goal of DG. Several normalization and meta-learning strategies are also considered for domain generalization such as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Contrastive Learning has shown impressive performance in the context of self-supervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>. Representative contrastive learning <ref type="bibr" target="#b14">[15]</ref> proposed efficient memory bank constructions so that the historical features are conveniently stored and reused even if batchsize is small. We feel inspired from these approaches <ref type="bibr" target="#b14">[15]</ref> where the memory bank help retain temporal consistency between features, and we introduce such a memory mechanism to best facilitate our motivations. However, our approach is neither targeting a pre-train task nor an unsupervised learning problem, in contrast to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b46">47]</ref>. Rather, we look into DG problems and we aim to learn the desired feature invariances respecting our unique hypothesis. We notice a related contrastive learning method in <ref type="bibr" target="#b26">[27]</ref>, that enforces a particular prediction regularizer across augmentations to improve in-class consistency. We argue that the work in <ref type="bibr" target="#b26">[27]</ref> is an unsupervised algorithm that completely distinguishes its nature from our DG task, while the loss proposed here also leads to an entirely different interpretation and application.</p><p>We observe that existing DG methods along the decomposition path either: hinge on inter-domain semantic invariance features assumption, so that the network becomes agnostic to styles; or they rely on style decomposition approaches to synthesize more data. In contrast, our proposed method enjoys two exclusive novel assumptions: 1. We are the first to impose instance level intra-domain style invariance during the training. Having these domain-specific style features at hand, we effectively reduce the degree of freedom of the problem in further learning the useful semantic features. 2. We design a novel "jury" mechanism that is different from any existing DG method, which achieves significant improvement in domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In the regime of out of distribution (OOD) detection, it has been shown that data distribution is heavily affected by population-level background statistics <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>. Owing to this issue, OOD inputs can rather be classified into in-distribution classes with high confidence, given the presence of dominant background noise. Recent observations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> have shown that deep generative models can even assign a higher likelihood to OOD inputs. One reason is that simple parameterization on marginal input distribution can be significantly confounded and dominated by background statistics, and does not learn much useful parameterization on the variation of semantic features. Accordingly, work such as <ref type="bibr" target="#b34">[35]</ref> aims to mute the effect of these background statistics via the likelihood ratio method, to achieve better OOD detection based on cleaner semantic features. Although the literature on OOD detection investigates completely orthogonal topics and directions against the DG community, we feel intrigued and motivated to reformulate domain generalization problems with novel background noise priors. The ultimate goal is to relieve the network optimization on the background statistics, so that the network steers away from its overfitting to these domain specific background noise, while focusing on learning true semantic distributional commonalities among domains.</p><p>We therefore do not seek to directly close the distribution gap between data distributions across domains (that GAN and MMD methods normally do). We impose prior knowledge that instances from the same domain share hidden invariant background statistics. Under this constraint, the network reduces uncertainty and redundancy when searching for semantic features that might be glimmering in comparison to the dominant background style statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>For DG problems, we consider D source domains D =</p><formula xml:id="formula_0">{D d } D d=1 , with each d-th domain D d includes N d training pairs {(x d,i , y d,i )} N d i=1 , where x d,i is the i-th sample in D d , and y d,i ? {1, 2, ..., n c } is the label of x d,i</formula><p>. n c is the number of classes shared across domains. The goal of DG approach is to learn a model from multiple labeled source domains that generalizes well to an unseen target domain D T .</p><p>We define notations frequently used throughout the paper. We assume each training sample is projected to feature embedding via CNN encoders <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. Specifically, the image x d,i firstly is input into feature extractor:</p><formula xml:id="formula_1">z d,i = E f (x d,i , ? e,f ),</formula><p>where function E f extracts image feature z d,i out of x d,i by a CNN parameterization ? e,f . A semantic encoder E c then reads in z d,i and produces semantic feature c d,i = E c (z d,i , ? e,c ). In parallel, a style encoder simultaneously inputs the z d,i feature and maps it into style representation via s d,i = E s (z d,i , ? e,s ). We define a classifier C(c d,i , ?), where function C is parameterized by ? and used to classify semantic features among n c possible classes. Here, the above encoder parameters ? e = {? e,f , ? e,c , ? e,s } and classifier parameters ? are CNN parameters learned during training. Subscript e associated with each notation, e.g., ? e,c , is intended to be reminiscent of "encoder".</p><p>We name the proposed method "Style and Semantic Memory Mechanism (STEAM)" for Domain Generalization. The overall STEAM model can be interpreted as a probabilistic graphical model in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Each semantic feature c d,i and style feature s d,i both depend on statistics (rectangles) from every other instances available in memory (given unlimited memory, the whole dataset then). Our motivation is that human intrinsically define classes by contrasting. Take for instance, Labrador and Husky are both defined as dogs, apparently because they share higher similarity than with any other species. In comparison, conventional DG methods as shown as in <ref type="figure" target="#fig_0">Fig. 1</ref>(a) only models each instance's semantic feature c d,i independently from other c d ? ,j . Note STEAM hinges on enormous historical training data. It is therefore critical that our loss function breaks free from the limitation of batchsize. A simple solution would be to directly store the learned features out of the encoder into a static memory bank for later usage <ref type="bibr" target="#b46">[47]</ref>. Unfortunately, similar to the observation in <ref type="bibr" target="#b14">[15]</ref>, we find features stored in this way cannot retain any temporal representation consistency due to the rapidly update of encoder via backpropagation. We thus simultaneously maintain an alternative memory encoder:</p><formula xml:id="formula_2">E m = {E m,f , E m,c , E m,s } with parameters ? m = {? m,f , ? m,c , ? m,</formula><p>s } that is able to slowly release the historical feature representations into the memory bank in an momentum updated way.</p><p>Specifically, we make sure the constructed memory encoder shall involve identical architecture and functioning components mirroring everything in E c , E f , E s above: We have memory feature encoder: E m,f (?, ? m,f ), memory style feature encoder E m,s (?, ? m,s ), and memory semantic feature encoder E m,c (?, ? m,c ). The only difference is that the parameters ? e = {? e,f , ? e,c , ? e,s } are updated via backpropagation, whereas the parameters of memory encoder ? m = {? m,f , ? m,c , ? m,s } are only momentum updated according to the changes of ? e . Subscript m is intended to be reminiscent of "memory". We elaborate the usage of memory encoders in Section 3.2, and Section 3.3.</p><p>Generally, a bird's-eye view of our whole framework is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>: We maintain an encoder to extract features of input images, and use a parallel memory encoder to generate and release memory features in the memory bank. Within both encoder and memory encoder structure, we both include a style encoder component and semantic encoder component. We explain the usage of these components in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Intra-domain Invariance on Style Features</head><p>Our first objective is to impose cross-instance intradomain style invariance via a memory bank construction. The plan is to maintain D domain specific style banks that gradually become agnostic to semantics as the training progresses, while each style bank retains intra-domain invariant and inter-domain contrastive style features at instance level.</p><p>We compute the style feature of each x d,i via memory encoder:  <ref type="figure" target="#fig_1">Figure 2</ref>. The framework of STEAM. We train an encoder for style and semantic feature extraction. We maintain a memory encoder to obtain D number of parallel style memory banks and one semantic memory bank. We use contrastive loss based on style banks to achieve intra-domain style invariance. We construct a memory semantic feature bank ("jury") to achieve inter-domain semantic invariance.</p><formula xml:id="formula_3">s d,i = E m,s (E m,f (x d,i , ? m,f ), ? m,</formula><p>Recall that we desire to figure out the shared invariant style features embedded behind each domain. To achieve this goal, we require every style feature out of the style en-</p><formula xml:id="formula_4">coder s d,i = E s (E f (x d,i , ? e,f )</formula><p>, ? e,s ) to have high similarity score with any stored style features from the same domain in memory V s d , whereas similarity score between style feature s d,i and all features from other domain banks V s d ? , d ? ? = d remains low. We find contrastive loss <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43]</ref> a natural fit to fulfill this goal:</p><formula xml:id="formula_5">Ls = ? 1 Zs d,i,j log exp( s d,i , v s d,j /? ) exp( s d,i , v s d,j /? ) + d ? ? =d B ?=1 exp( s d,i , v s d ? ,? /? ) ,<label>(1)</label></formula><p>where Z s = B ? d N d normalizes sample number, ? is a temperature parameter and ?x 1 ,</p><formula xml:id="formula_6">x 2 ? = x T 1 x 2 /?x 1 ??x 2 ? denote the cosine similarity between x 1 and x 2 .</formula><p>It is transparent that Eq. (1) is essentially a softmax function aiming to distinguish each (s d,i , v s d,j ) intra-domain pair from the total B ?(D ?1) number of (s d,i , v s d ? ,? ), d ? ? = d inter-domain pairs. As the training proceeds, the instance level style feature s d,i compares with all the members in the domain style bank v s d,j , j ? [1, B], in order to reach consensus on intra-domain style invariance of domain d. In other words, Eq. (1) penalizes feature misalignment between s d,i and v s d,j , whereas the style features from distinct domains are pushed away, i.e., any increase in inner product</p><formula xml:id="formula_7">?s d,i , v s d ? ,? ?, d ? ? = d increases loss Eq. (1)</formula><p>. These D number of domain specific style banks therefore gradually become agnostic to semantics during the training, as each bank V s d retains intra-domain invariant and inter-domain contrastive style features at instance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inter-domain Invariance on Semantic Features</head><p>We have tentative style features at hand. We now turn to our second objective: to learn inter-domain semantic invariant feature via another memory bank. Bear in mind that these semantic features are considered the true causal variables that determine the semantics of samples independent of domains. We define x + d,i to be a "variant" of each training input x d,i . We call x + d,i a "variant" of x d,i , because it is considered as semantically identical to x d,i . The sample</p><p>x + d,i is randomly chosen from all possible D domains that either is from the same class of x d,i , or simply is selected from the augmentation pool (RandAug <ref type="bibr" target="#b12">[13]</ref>) of image x d,i .</p><p>Feature c d,i is considered the true causal variable that determines the semantics of training samples. We desire that a semantic feature</p><formula xml:id="formula_8">c d,i = E c (E f (x d,i , ? e,f )</formula><p>, ? e,c ) to return a high similarity score with the semantic feature of any possible x + d,i , whereas c d,i remains distinct to samples from other classes. We certainly cannot build n c classes number of parallel semantic banks analogously to what we did for style banks, because there might be millions of classes. We correspondingly come up with a novel "jury" mechanism that relaxes our objective.</p><p>Construction of Semantic Jury Memory Bank. The semantic "jury" bank's construction relies on memory encoders: We sequentially push any arriving semantic mem-</p><formula xml:id="formula_9">ory featurec + d,i = E m,c (E m,f (x + d,i , ? m,f ), ? m,c )</formula><p>into the single semantic feature memory bank V c regardless of whose "variant"c + d,i is and which domain d is. We update the entries in V c , again, like maintaining a queue structure: entries stored in V c are denoted as:</p><formula xml:id="formula_10">V c = [v c 1 , ...v c j , ..., v c B ], j ? [1, B].</formula><p>Note features c d,i andc + d,i are semantically identical, as they represent the different samples from the same class. But how far are c d,i andc + d,i in the semantic embedding space?</p><p>We leave the judgment to the "jury". All the queuing semantic features in the bank have the right to bid, contribute, and weigh their contribution into the similarity measurement betweenc + d,i and c d,i . Mathematically, we denote the probability p(x d,i ; ? e,c , ? e,f , V c ) = [p e 1 , ...p e j , ..., p e B ], j ? [1, B] as similarity score between c d,i with respect to all stored semantic features in semantic memory V c , where each probability entry p e j is defined as:</p><formula xml:id="formula_11">p e j = exp( c d,i , v c j /? ) v c ?V c exp(?c d,i , v c ? /? ) .<label>(2)</label></formula><p>Similarly, we also compute p(</p><formula xml:id="formula_12">x + d,i ; ? m,c , ? m,f , V c ) = [p m 1 , ..., p m B ]</formula><p>, which is the similarity scores betweenc + d,i</p><p>with respect to each of the member in the V c :</p><formula xml:id="formula_13">p m j = exp(?c + d,i , v c j ?/? ) v c ?V c exp( c + d,i , v c /? ) .<label>(3)</label></formula><p>The motivation here is that, the "jury" V c would cross check every "jury" member's similarity score with bothc + d,i and c d,i . The bank V c summaries these two distribution respectively into p(x d,i ; ? e,c , ? e,f , V c ) and</p><formula xml:id="formula_14">p(x + d,i ; ? m,c , ? m,f , V c ) as in Eq. (2) and Eq. (3).</formula><p>The assumption here is that, if x + d,i and x d,i truly share invariant semantic features, then their similarity score across the entire semantic feature bank V c shall be as close as possible, too. We therefore penalize cross entropy between</p><formula xml:id="formula_15">p(x + d,i ; ? m,c , ? m,f , V c ) and p(x d,i ; ? e,c , ? e,f , V c ): Lc = ? 1 Zc d,i p(x + d,i ; ?m,c, ? m,f , V c ) log p(x d,i ; ?e,c, ? e,f , V c ),<label>(4)</label></formula><p>where Z c = d N d normalizes over samples. Eq.</p><p>(4) penalizes misalignment between probability p(x d,i ; ? e,c , ? e,f , V c ) and p(x + d,i ; ? m,c , ? m,f , V c ) via similarity cross-checks with all the features stored in the V c . As a result, features in the current V c jointly vote on distributional similarity between x + d,i and x d,i . This strategy strongly contrasts with conventional contrastive learning, where for MoCo like mechanisms, only a single positive key is considered for each query, whereas negative keys in the memory bank are only considered as negative and contrastive to the positives.</p><p>But why not directly penalize the inner product ?c d,i ,c + d,i ? or the likes which seems a more obvious option? If one takes a closer look at Eq. (4), the net effect of the "jury" mechanism is that, any feature member, say v c j in the bank V c having a relatively high similarity score with both c d,i andc + d,i would vote for agreement on this similarity, and then v c j becomes confident to contribute itself to jointly supervise the optimization direction to further improve semantic invariance among c d,i ,c + d,i and v c j . The strength of this vote depends on v c j 's similarity score with each c d,i andc + d,i . This makes one reminiscent of popular multi-view contrastive learning strategy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, where introducing more views for each instance is always beneficial for learning. Conversely, all feature members in the jury showing low similarity scores with both c d,i and c + d,i would automatically tend to contrast itself away from both c d,i andc + d,i . The loss therefore inherently summarizes the compounding effect of all features in the bank via such joint similarity cross-check mechanism, in comparison to any conventional contrastive learning approaches that banks are only considered "negative". We notice a related work <ref type="bibr" target="#b26">[27]</ref>, that also enforces invariant prediction regularizer across augmentations. We argue that the work in <ref type="bibr" target="#b26">[27]</ref> is a completely unsupervised algorithm that distinguishes itself from our DG task. Notably, the proposed bank definition here out of the specific DG invariance hypothesis is also orthogonal to <ref type="bibr" target="#b26">[27]</ref>, and the loss Eq. (4) proposed here leads to entirely different interpretation and implementation.</p><p>To make sure the features are indeed semantically dis-criminative, we apply supervised classification loss on x d,i :</p><formula xml:id="formula_16">L cls = ? 1 N N i=1 y d,i log C(c d,i , ?),<label>(5)</label></formula><p>where classifier C(c d,i , ?) predicts the class of sample x d,i by only using its semantic feature c d,i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoupling Semantics from Styles</head><p>Last but not least, current loss on semantic features c d,i is completely disconnected from its style features s d,i , and cannot benefit from the style invariance assumption at all. A simple solution to fix this is to enforce orthogonality <ref type="bibr" target="#b3">[4]</ref> between c d,i and s d,i . Let H c and H s be matrices whose rows are the semantic representations c d,i and style representations s d,i from x d,i . We constrain the semantic feature to go towards the orthogonal direction against style features, such that the semantic encoder E c (?, ? e,c ) and classifier C(c d,i , ?) could mostly avoid overfitting to the style features. This is formalized into the squared Frobenius norm:</p><formula xml:id="formula_17">L o = ?H T c H s ? 2 F .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Overall Loss Function</head><p>Taking into account all the discussions above, the eventual loss function is:</p><formula xml:id="formula_18">L = L cls + L s + L c + L o ,<label>(7)</label></formula><p>where all present losses are equally weighted. The overall loss is backpropagated through the network in a batchwise manner. The parameters of encoder ? e = {? e,f , ? e,c , ? e,s } and classifier ? are updated via backpropagation, whereas the parameters of memory encoder ? m = {? m,f , ? m,c , ? m,s } are momentum updated as:</p><formula xml:id="formula_19">? m = ? ? ? m + (1 ? ?) ? ? e ,<label>(8)</label></formula><p>where ? ? [0, 1) is a momentum coefficient. We only use obtained encoder parameters ? e,f , ? e,c and classifier parameters ? during test time inference. For a fair comparison with prior works, we follow the standard leave-one-domain-out evaluation procedure as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, where one domain is chosen as the unseen target and the remaining domains are used as source domains for training. For Digits-DG, PACS and Office-Home, the authors of <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> have specified particular train and val splits for each domain to ensure a fair comparison. They use the entire train + val target data as the test data. We use the same data split definition for our experiments. For Do-mainNet, according to <ref type="bibr" target="#b8">[9]</ref>, we employ their dataset division and report the accuracy on the test split of target domain.</p><p>Implementation details. Following the backbone setting of <ref type="bibr" target="#b52">[53]</ref>, we use 4 conv layers and a softmax layer for Digits-DG dataset. ReLU and 2 ? 2 max-pooling are inserted after each convolution layer. The model is trained with SGD, initial learning rate of 0.05 and batch size of 128 for 50 epochs. For both PACS and Office-Home, we use ResNet-18 pretrained on ImageNet as the CNN backbone, as in <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b54">55]</ref>. We train the model with SGD, initial learning rate of 0.002 and batch size of 30 for 60 epochs. The learning rate is further decayed by the cosine annealing rule. For DomainNet, we experiment with ResNet-18 and ResNet-50 backbone architectures, as in <ref type="bibr" target="#b8">[9]</ref>. For all experiments, the semantic encoder, style encoder and classifier are all imple-mented using a fully connected (FC) layer. The size of style and semantic memory bank is set as 2, 048.</p><p>Baselines. To evaluate our method, we consider comparisons with the following approaches: (1) Vanilla simply trains the plain classification model on all available source domains using all annotations, the model is then directly used to classify target samples. Results on Digits-DG. <ref type="table" target="#tab_1">Table 1</ref> shows that, our method exhibits clear advantages over the existing state-of-the-art methods. Please note that our model is especially effective and commanding on challenging DG directions, e.g., MNIST-M and SVHN, as they seem to have large domain variations compared with other directions. This is a valid justification that the instance level style invariance along with the proposed "jury" mechanism together is a legitimate idea to deal with domain generalization problems. Compared with the methods that hinge on marginal distribution alignment across domains, e.g., MMD-AAE and CCSA, our model even demonstrates around 8.5% performance boost on average. This well validates our assumption in Section 3.2, that intra-domain style invariance seems to be a more practical and suitable hypothesis in presence of computing background statistics hidden in domain styles. In other words, the prior knowledge on intra-domain style invariance effectively reduce the uncertainty when search- ing for optimal network parameters so that the parameters reduces overfitting to domain styles.</p><p>Results on PACS. This part of results is shown in Table 1. Our method achieves the best performance on all test domains. Note the recently proposed EISNet also involves the usage of a feature memory. However, the memory in EISNet is only used for the sake of hard triplets selection without consideration on feature invariance. In De-cAug <ref type="bibr" target="#b0">[1]</ref>, similar semantic-style orthogonal regularization loss was used. However, orthogonality is perhaps the most widely used tool everywhere, and we simply employ orthogonality as an auxiliary regularizer. Nevertheless, our motivation and hypothesis are completely different from DecAug, highlighting the outstanding performance given our instance-level style invariance and "jury" mechanism.</p><p>Results on Office-Home. The results are shown in Table 1. Our STEAM achieves the best average performance. Notably, the simple vanilla model shows strong results on this benchmark. Most of baselines provide only marginal improvements than vanilla model that are under 1.0%. As discussed in L2A-OT, this might be owing to the fact that Office-Home is relatively a large composition of data, compared with PACS and Digits-DG, thus offering inherently bigger domain diversity in training data already. In contrast, our method shows the best performance on average with an impressive 2.1% improvement over the Vanilla.</p><p>Results on DomainNet. DomainNet is considered as perhaps the most challenging benchmark, owing to its dataset size, both in terms of image number and category numbers. <ref type="table" target="#tab_2">Table 2</ref> shows that, on DomainNet dataset, the Vanilla baseline achieves competitive results in comparison to domain generalization methods MetaReg and DMG, while our method again surpasses all competitors. We observe that our model is leading in the table with improved average performance, especially when ResNet-18 and ResNet-50 are used as the backbone architectures, showing respectively 2.1% and 2.5% accuracy improvement. At this point, it is worthy to mention that our method has achieved better robustness, for consistently offering better performance than other baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Domain Adaptation</head><p>Datasets and implementation details. To justify STEAM's validity on the application of MSDA, we implement STEAM under the problem definition described in Section 3.6. We firstly consider evaluation on PACS dataset again with the same problem definition and setting of <ref type="bibr" target="#b48">[49]</ref>. Next, we follow <ref type="bibr" target="#b53">[54]</ref> and use miniDomainNet for evaluation, which is a sampled subset of DomainNet and reformatted into a smaller image size (96 ? 96). In general, miniDo-mainNet consists of 4 domains (Clipart, Painting, Real and Sketch) across 126 classes, which mostly resembles data diversity of the original DomainNet. For PACS, we use ResNet-18 pretrained on ImageNet as the CNN backbone, by following training protocols in <ref type="bibr" target="#b48">[49]</ref>. Batch size is 32. For miniDomainNet, we use ResNet-18 as the CNN backbone, the same used as in <ref type="bibr" target="#b53">[54]</ref>. Similarly, we use SGD with momentum as the optimizer, and the learning rate decays according to cosine annealing rule. The model is trained with an initial learning rate of 0.005 for 60 epochs. For each mini-batch, we sample from each domain 64 images.</p><p>Results. Given the standard test protocol in <ref type="bibr" target="#b32">[33]</ref> on PACS, we use one domain as target and the remaining as sources. Classification accuracy on the target domain test set is reported. We compare our STEAM with two state-of-the-art multi-source domain adaptation approaches: M 3 SDA <ref type="bibr" target="#b32">[33]</ref> and CMSS <ref type="bibr" target="#b48">[49]</ref>. In addition, we also present the following methods as our baselines: DANN <ref type="bibr" target="#b13">[14]</ref>, MDAN <ref type="bibr" target="#b51">[52]</ref>, WBN <ref type="bibr" target="#b25">[26]</ref>, MCD <ref type="bibr" target="#b36">[37]</ref>. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Our method achieves the state-of-the-art average accuracy of 93.0%. On the most challenging sketch domain, we obtain 85.1%, clearly outperforming other baselines. On the miniDomainNet, we compare with the same methods presented in DAEL <ref type="bibr" target="#b53">[54]</ref>, including MCD <ref type="bibr" target="#b36">[37]</ref>, DCTN  <ref type="bibr" target="#b47">[48]</ref>, DANN <ref type="bibr" target="#b13">[14]</ref>, M 3 SDA <ref type="bibr" target="#b32">[33]</ref>, MME <ref type="bibr" target="#b35">[36]</ref>. The results are shown in <ref type="table">Table 4</ref>. Our method achieves 66.3% average accuracy, again, justifying significant advantages out of our interesting invariance hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Analysis</head><p>Ablation study. We investigate the impact of each component in Eq. <ref type="formula" target="#formula_18">(7)</ref> by comparing several variations of STEAM using the Digits-DG and PACS datasets. Vanilla: a conventional supervised learning formulation that uses all source domains for training, i.e., with training loss L cls ; Vanilla-style: we add intra-domain style invariance loss and orthogonal regularization to Vanilla model, i.e., train using loss L cls + L s + L o ; Vanilla-semantic: we only add inter-domain invariance constraint on semantic features, i.e., training loss L cls + L c ; STEAM: our complete training scheme, with training loss defined in Eq. <ref type="bibr" target="#b6">(7)</ref>. Note we don't separate the L s and L o here, but please see supplementary file for more related ablation studies. <ref type="table" target="#tab_4">Table 5</ref> and 6 display the results. Notably, we observe three phenomenons: (1) Vanilla-style outperforms Vanilla by average 4.2% and 3.5% on Digits-DG and PACS, supporting the advantage of "intra-domain style invariance" hypothesis for domain generalization alone; (2) Vanillasemantic also surpasses over Vanilla by 6.8% on Digits-DG, and 4.9% on PACS, which well validates the effectiveness of semantic invariance imposed through "jury" mechanism alone. (3) By leveraging on both intra-domain style invariance and inter-domain semantic invariance assumptions, our entire STEAM framework further drastically improves the performance across all settings, indicating the complementary roles of these two factors.</p><p>Design choices for STEAM. We now explore alternative definitions for each loss term, to better understand the essence behind STEAM. (1) Domain classification: we replace the entire style bank and associated instance level contrastive loss described in Eq. (1) by a domain classifier. We then use the domain classifier to predict the domain label, so that the style features becomes domaindiscrinimative. (2) L2-matching: we replace Eq. (4), by directly minimizing the l2-distance of each semantic feature pairs ?c d,i ?c + d,i ? 2 2 where c d,i andc + d,i share identical class label. (3) Contrastive: we define semantic featuresc + d,i as the positive key of c d,i , with random instances sampled from the memory encoders forming V c as negative samples. We then perform conventional instance level  Info-NCE loss on (c d,i ,c + d,i ). <ref type="figure" target="#fig_2">Fig. 3</ref> shows that NONE of the above replacements using alternative losses have achieved any better design than our original STEAM framework. The Domain classification achieves some descent test accuracy though (worse than STEAM), while apparently the idea of instance level style invariance is a much stronger prior than simply requiring the style features to be linear separable. L2-matching performs worse than our STEAM, meaning directly minimizing the geometry distance within each (c d,i ,c + d,i ) pair has ignored important information to be inter-semantically contrastive. Finally, if we would use plain Info-NCE loss as in contrastive learning, i.e., Contrastive, worse performance is observed, showing the superiority of our proposed "jury" mechanism for inter-domain semantic invariance learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel algorithm capitalizing on both Style and sEmAntic memory mechanism (STEAM) for domain generalization tasks. Importantly, we find leveraging on intra-domain style invariance can lead to a significant improvement on the efficacy of domain generalization. The intra-domain style invariance prior can help improve the learning of semantic features, owing to reduced overfitting to domain styles during training. We introduce efficient memory bank construction policies for both style and semantic features that store useful statistics for computing our losses. Specifically, our semantic feature bank serves as a "jury" and helps effectively improve intra-class invariance cross different domains. Empirical results verify our assumption on various benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Probabilistic graphical model comparisons for (a) conventional causal model for DG (b) causal model for STEAM, based on memory bank constructions. For STEAM, all the training samples have an impact on deciding the semantic and style feature for each instance.? d,i is the class prediction given feature c d,i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 )</head><label>2</label><figDesc>CrossGrad [41] perturbs input using adversarial gradients from a domain classifier. (3) CCSA [28] explores a contrastive semantic alignment loss for domain-invariant representation learning. (4) MMD-AAE [20] imposes an MMD loss on the hidden layers of an autoencoder. (5) JiGen [7] utilizes an auxiliary self-supervision loss so that the features can be used to solve the Jigsaw puzzle task. (6) Epi-FCR [19] designs an episodic training strategy. (7) EISNet [45] develops a momentum metric learning scheme with the K-hard negative mining to improve the network generalization ability. (8) L2A-OT [53] synthesizes extra data from pseudo-novel domains to augment the source domains. (9) DecAug [1] generates extra data augmentations through perturbing the disentangled style feature and semantic features. (10) DMG [9] learns domain specific masks for generalization on different domains. (11) MixStyle [55] mixes instance level feature statistics of training samples across various sources to introduce more domain diversity via synthesizing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Study on different design choices of STEAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Entropy Feature extractor Semantic encoder Encoder Style encoder Feature extractor Style encoder Memory Encoder Semantic encoder Momentum update Labels Classifier Classification loss Style contrastive loss Similarity Similarity Inner-product Inner-product Semantic memory Style feature enqueue Semantic feature enqueue ...</head><label></label><figDesc>s ), and we sequentially push each arrivings d,i into the "domain style bank" V s d . We define D number of parallel style memory banks V s =(V s 1 , ..., V s d , ..., V s D , d ? [1, D]) and dynamically update each V s d bank like queues: We push each newly arriving style representations d,i into the queue tail of V s d , and remove the oldest style feature from the queue head as in [15]. Assuming the memory size of each bank V s d is a constant B. The entries stored in V s d are denoted with brand new subscripts as: [v s d,1 , ...v s d,j , ...., v s d,B ], j ? [1, B], where each v s d,j dynamically stores the style feature vectors d,? at the j position of the V s d bank.</figDesc><table><row><cell>Orthogonal loss</cell><cell>...</cell><cell>style memory Cross Domain-specific ...</cell></row></table><note>...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance (%) comparisons with the state-of-the-art approaches for DG. MNIST-M [14], SVHN [30] and SYN [14]) with an evident domain shift in font style, stroke color and background. (2) PACS [18] is a widely used domain generalization benchmark, which is composed of four domains (Art Painting, Cartoon, Photo and Sketch). Each domain includes samples from 7 different categories, including a total of 9, 991 samples. (3) Office-Home [44] contains around 15, 500 images of 65 classes, distributed across 4 domains (Artistic, Clipart, Product and Real world). (4) DomainNet [33] is a recently established large-scale dataset for multi-source domain adaptation and domain generalization, which includes about 0.6 million images in 345 classes distributed across 6 domains (i.e., Clipart, Infograph, Quickdraw, Painting, Real, Sketch).</figDesc><table><row><cell>Method</cell><cell cols="5">Digits-DG MNIST MNIST-M SVHN SYN Avg</cell><cell cols="5">PACS Art Cartoon Photo Sketch Avg</cell><cell cols="5">Office-Home Artistic Clipart Product Real World Avg</cell></row><row><cell>MMD-AAE [20]</cell><cell>96.5</cell><cell>58.4</cell><cell>65.0</cell><cell cols="2">78.4 74.6</cell><cell>75.2</cell><cell>72.7</cell><cell>96.0</cell><cell>64.2</cell><cell>77.0</cell><cell>56.5</cell><cell>47.3</cell><cell>72.1</cell><cell>74.8</cell><cell>62.7</cell></row><row><cell>CCSA [28]</cell><cell>95.2</cell><cell>58.2</cell><cell>65.5</cell><cell cols="2">79.1 74.5</cell><cell>80.5</cell><cell>76.9</cell><cell>93.6</cell><cell>66.8</cell><cell>79.4</cell><cell>59.9</cell><cell>49.9</cell><cell>74.1</cell><cell>75.7</cell><cell>64.9</cell></row><row><cell>JiGen [7]</cell><cell>96.5</cell><cell>61.4</cell><cell>63.7</cell><cell cols="2">74.0 73.9</cell><cell>79.4</cell><cell>75.3</cell><cell>96.0</cell><cell>71.6</cell><cell>80.5</cell><cell>53.0</cell><cell>47.5</cell><cell>71.5</cell><cell>72.8</cell><cell>61.2</cell></row><row><cell>CrossGrad [41]</cell><cell>96.7</cell><cell>61.1</cell><cell>65.3</cell><cell cols="2">80.2 75.8</cell><cell>79.8</cell><cell>76.8</cell><cell>96.0</cell><cell>70.2</cell><cell>80.7</cell><cell>58.4</cell><cell>49.4</cell><cell>73.9</cell><cell>75.8</cell><cell>64.4</cell></row><row><cell>Epi-FCR [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.1</cell><cell>77.0</cell><cell>93.9</cell><cell>73.0</cell><cell>81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EISNet [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.9</cell><cell>76.4</cell><cell>95.9</cell><cell>74.3</cell><cell>82.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>L2A-OT [53]</cell><cell>96.7</cell><cell>63.9</cell><cell>68.6</cell><cell cols="2">83.2 78.1</cell><cell>83.3</cell><cell>78.2</cell><cell>96.2</cell><cell>73.6</cell><cell>82.8</cell><cell>60.6</cell><cell>50.1</cell><cell>74.8</cell><cell>77.0</cell><cell>65.6</cell></row><row><cell>DecAug [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.0</cell><cell>79.6</cell><cell>95.3</cell><cell>75.6</cell><cell>82.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MixStyle [55]</cell><cell>96.5</cell><cell>63.5</cell><cell>64.7</cell><cell cols="2">81.2 76.5</cell><cell>84.1</cell><cell>78.8</cell><cell>96.1</cell><cell>75.9</cell><cell>83.7</cell><cell>58.7</cell><cell>53.4</cell><cell>74.2</cell><cell>75.9</cell><cell>65.5</cell></row><row><cell>Vanilla</cell><cell>95.8</cell><cell>58.8</cell><cell>61.7</cell><cell cols="2">78.6 73.7</cell><cell>77.0</cell><cell>75.9</cell><cell>96.0</cell><cell>69.2</cell><cell>79.5</cell><cell>58.9</cell><cell>49.4</cell><cell>74.3</cell><cell>76.2</cell><cell>64.7</cell></row><row><cell>STEAM</cell><cell>96.8</cell><cell>67.5</cell><cell>76.0</cell><cell cols="2">92.2 83.1</cell><cell>85.5</cell><cell>80.6</cell><cell>97.5</cell><cell>82.9</cell><cell>86.6</cell><cell>62.1</cell><cell>52.3</cell><cell>75.4</cell><cell>77.5</cell><cell>66.8</cell></row><row><cell cols="2">4. Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">4.1. Evaluation on Domain Generalization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Datasets. We perform DG tasks via extensive evalu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ations on the following benchmarks: (1) Digits-DG [53]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">includes 4 domains (MNIST [17],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Leave-one-domain-out results on DomainNet for DG.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Clp</cell><cell>Inf</cell><cell>Pnt Qdr Rel</cell><cell>Skt Avg.</cell></row><row><cell></cell><cell>Vanilla</cell><cell cols="4">56.5 18.4 45.3 12.4 57.9 38.8 38.2</cell></row><row><cell>ResNet-18</cell><cell>Multi-Headed [9] MetaReg [2] DMG [9] STEAM</cell><cell cols="4">55.4 17.5 40.8 11.2 52.9 38.6 36.1 53.6 21.0 45.2 10.6 58.4 42.3 38.5 60.0 18.7 44.5 14.1 54.7 41.7 39.0 58.3 22.1 47.4 14.4 58.6 45.9 41.1</cell></row><row><cell></cell><cell>Vanilla</cell><cell cols="4">64.0 23.6 51.0 13.1 64.4 47.7 44.0</cell></row><row><cell>ResNet-50</cell><cell>Multi-Headed [9] MetaReg [2] DMG [9] STEAM</cell><cell cols="4">61.7 21.2 46.8 13.8 58.4 45.4 41.2 59.7 25.5 50.1 11.5 64.5 50.0 43.6 65.2 22.1 50.0 15.6 59.6 49.0 43.6 64.6 27.0 54.0 15.8 65.6 52.2 46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Domain adaptation results on PACS.</figDesc><table><row><cell>Method</cell><cell>Art</cell><cell cols="3">Cartoon Photo Sketch</cell><cell>Avg</cell></row><row><cell>Source-only</cell><cell>74.9</cell><cell>72.1</cell><cell>94.5</cell><cell>64.7</cell><cell>76.6</cell></row><row><cell>DANN [14]</cell><cell>81.9</cell><cell>77.5</cell><cell>91.8</cell><cell>74.6</cell><cell>81.5</cell></row><row><cell>MDAN [52]</cell><cell>79.1</cell><cell>76.0</cell><cell>91.4</cell><cell>72.0</cell><cell>79.6</cell></row><row><cell>WBN [26]</cell><cell>89.9</cell><cell>89.7</cell><cell>97.4</cell><cell>58.0</cell><cell>83.8</cell></row><row><cell>MCD [37]</cell><cell>88.7</cell><cell>88.9</cell><cell>96.4</cell><cell>73.9</cell><cell>87.0</cell></row><row><cell>M 3 SDA [33]</cell><cell>89.3</cell><cell>89.9</cell><cell>97.3</cell><cell>76.7</cell><cell>88.3</cell></row><row><cell>CMSS [49]</cell><cell>88.6</cell><cell>90.4</cell><cell>96.9</cell><cell>82.0</cell><cell>89.5</cell></row><row><cell>STEAM</cell><cell>94.0</cell><cell>93.7</cell><cell>99.3</cell><cell>85.1</cell><cell>93.0</cell></row><row><cell cols="6">Table 4. Domain adaptation results on miniDomianNet.</cell></row><row><cell>Method</cell><cell cols="4">Clipart Painting Real Sketch</cell><cell>Avg</cell></row><row><cell>Source-only</cell><cell>63.4</cell><cell>49.9</cell><cell>61.5</cell><cell>44.1</cell><cell>54.7</cell></row><row><cell>MCD [37]</cell><cell>62.9</cell><cell>45.7</cell><cell>57.5</cell><cell>45.8</cell><cell>53.0</cell></row><row><cell>DCTN [48]</cell><cell>62.0</cell><cell>48.7</cell><cell>58.8</cell><cell>48.2</cell><cell>54.4</cell></row><row><cell>DANN [14]</cell><cell>65.5</cell><cell>46.2</cell><cell>58.6</cell><cell>47.8</cell><cell>54.6</cell></row><row><cell>M 3 SDA [33]</cell><cell>64.1</cell><cell>49.0</cell><cell>57.7</cell><cell>49.2</cell><cell>55.0</cell></row><row><cell>MME [36]</cell><cell>68.0</cell><cell>47.1</cell><cell>63.3</cell><cell>43.5</cell><cell>55.5</cell></row><row><cell>DAEL [54]</cell><cell>69.9</cell><cell>55.1</cell><cell>66.1</cell><cell>55.7</cell><cell>61.7</cell></row><row><cell>STEAM</cell><cell>71.4</cell><cell>61.9</cell><cell>71.1</cell><cell>60.9</cell><cell>66.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation on Digits-DG. MT, MM, SV, and SY indicates MNIST, MNIST-M, SVHN, and SYN, respectively. Method L cls Ls Lo Lc MT MM SV SY Avg.</figDesc><table><row><cell>Vanilla Vanilla-style Vanilla-semantic</cell><cell>? ? ?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>95.8 58.8 61.7 78.6 73.7 96.3 63.6 69.3 82.4 77.9 96.0 65.2 74.5 86.2 80.5</cell></row><row><cell>STEAM</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>96.5 67.5 76.0 92.2 83.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation on PACS for domain generaliztion. A, C, P, and S indicates Art, Cartoon, Photo, and Sketch, respectively.</figDesc><table><row><cell cols="2">Method Vanilla Vanilla-style Vanilla-semantic</cell><cell cols="4">L cls Ls Lo Lc ? ? ? ? ? ?</cell><cell cols="3">A 77.0 75.9 96.1 69.2 79.5 C P S Avg. 80.2 78.7 96.5 76.6 83.0 83.3 77.6 96.7 80.1 84.4</cell></row><row><cell>STEAM</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="3">85.5 80.6 97.5 82.9 86.6</cell></row><row><cell></cell><cell cols="8">STEAM Domain classification L2-matching Contrastive</cell></row><row><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86.6</cell><cell></cell></row><row><cell>(%)</cell><cell>83</cell><cell>83.1</cell><cell>80.9</cell><cell></cell><cell>80.1</cell><cell>83.9</cell><cell>83.5</cell><cell>82.4</cell></row><row><cell>Accuracy</cell><cell>73 78</cell><cell></cell><cell></cell><cell>76.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Digits-DG</cell><cell></cell><cell cols="2">PACS</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">.6. Extension to MSDA Another advantage of the proposal is that, loss Eq.<ref type="bibr" target="#b6">(7)</ref> can be transferred to deal with multi-source domain adaptation (MSDA) tasks with few modifications. The only difference is that for target data without any annotations, only those augmentation images of x d,i are used to define x + d,i and there is no classifier applied on target data semantic features, as the class label is not available. Except for this difference, all target data can be conveniently plugged into our algorithm with a domain ID, d ?[1, D]. The training procedure then progresses under the exactly same losses and network constructions as we did for DG tasks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Decaug: Out-of-distribution generalization via decomposed feature representation and semantic augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-H Gary</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using metaregularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring object relation in mean teacher for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Joint contrastive learning with infinite possibilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to balance specificity and invariance for in and out of domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mocycle-gan: Unpaired video-to-video translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative ensembles for robust anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12292</idno>
		<title level="m">Contextual transformer networks for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning via invariant causal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovana</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don&apos;t know? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring category-agnostic clusters for open-set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient domain generalization via common-specific low-rank decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to optimize domain specific normalization for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Input complexity and out-ofdistribution detection with likelihood-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicen?</forename><surname>David?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Frustratingly simple domain generalization via image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Somavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11207</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning from extrinsic and intrinsic supervisions for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caizi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A low rank promoting prior for unsupervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02696</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ser-Nam Lim, and Abhinav Shrivastava. Curriculum manager for source selection in multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation with subspace learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Seco: Exploring sequence supervision for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07325</idno>
		<title level="m">Domain adaptive ensemble learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
