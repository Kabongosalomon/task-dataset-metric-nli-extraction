<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONFORMER-BASED SELF-SUPERVISED LEARNING FOR NON-SPEECH AUDIO TASKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeeta</forename><surname>Srivastava</surname></persName>
							<email>srivastava.206@osu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
							<email>yunwang@fb.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AI</orgName>
								<orgName type="institution" key="instit2">USA ? Reality Labs Research</orgName>
								<address>
									<region>Meta</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
							<email>androstj@fb.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AI</orgName>
								<orgName type="institution" key="instit2">USA ? Reality Labs Research</orgName>
								<address>
									<region>Meta</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
							<email>anuragkr@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
							<email>chunxiliu@fb.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AI</orgName>
								<orgName type="institution" key="instit2">USA ? Reality Labs Research</orgName>
								<address>
									<region>Meta</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kritika</forename><surname>Singh</surname></persName>
							<email>skritika@fb.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AI</orgName>
								<orgName type="institution" key="instit2">USA ? Reality Labs Research</orgName>
								<address>
									<region>Meta</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
							<email>ysaraf@fb.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AI</orgName>
								<orgName type="institution" key="instit2">USA ? Reality Labs Research</orgName>
								<address>
									<region>Meta</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CONFORMER-BASED SELF-SUPERVISED LEARNING FOR NON-SPEECH AUDIO TASKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>representation learning</term>
					<term>conformer</term>
					<term>sound events</term>
					<term>wav2vec</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameterefficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audioonly self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, self-supervised learning (SSL) has proven to be an effective method for representation learning without the need for labeled supervision, not only in the language <ref type="bibr" target="#b0">[1]</ref> and visual <ref type="bibr" target="#b1">[2]</ref> domains but also in the audio domain. In the audio domain, SSL for speech tasks have shown extremely promising results <ref type="bibr" target="#b2">[3]</ref>, but SSL for non-speech audio tasks has been explored to a lesser extent. Examples of SSL for non-speech tasks include multimodal self-supervised learning <ref type="bibr" target="#b3">[4]</ref> and audio captioning <ref type="bibr" target="#b4">[5]</ref>, but an extensive evaluation of SSL for non-speech audio-only tasks remains to be seen.</p><p>A number of self-supervised representation learning frameworks have been proposed, of which contrastive learning based methods have been found to be particularly effective in both image and speech domains. It aims to push semantically comparable samples closer together and dissimilar samples apart in the feature space. Wav2vec 2.0 <ref type="bibr" target="#b2">[3]</ref> is one such contrastive learning framework which have been demonstrated to be highly efficient in speech-related tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Wav2vec 2.0 uses transformers to build contextualized representations of the speech sequence, but transformers can only simulate global dependencies. While convolutional networks (CNN) can capture local associations, obtaining global information from the model would necessitate the use of additional layers or parameters. Conformers <ref type="bibr" target="#b7">[8]</ref>, on the other hand, captures both local and global dependencies while requiring fewer parameters as compared to convolutional mod-els. Prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> have shown that conformers can outperform transformers and CNN models on both speech and non-speech tasks.</p><p>In this work, we combine the conformer architecture with contrastive learning to learn representations for non-speech audio in a self-supervised manner. We replace the transformer in wav2vec 2.0 with a conformer in order to capture both global and local information in the audio signal. Unlike prior self-supervised works on speech representation learning (e.g. <ref type="bibr" target="#b2">[3]</ref>) which directly learn from raw audio waveforms, we use logmel spectrograms as the fundamental representation of the audio signal. Besides their proven efficacy in different audio tasks <ref type="bibr" target="#b10">[11]</ref>, logmels are much more compact and lead to better memory and compute efficiency. The network is pre-trained on a large-scale unlabeled dataset of ?67,000 hours to learn audio representations, and then fine-tuned on several audio tasks to show the generalization capabilities of the proposed SSL framework.</p><p>The contributions of this work are the following: (1) We propose a conformer-based SSL framework for general-purpose audio representation learning applicable to many audio tasks, which can reduce the need for labeled data by two-thirds; (2) On the AudioSet <ref type="bibr" target="#b11">[12]</ref> benchmark, our fine-tuned representations achieve a new state-of-the-art (SOTA) mean average precision (mAP) of 0.415 for self-supervised learning with only audio, outperforming the previous best score of 0.329; (3) Our method can surpass or match the performance of previous systems pre-trained in a supervised way on 3 out of 4 classification tasks: acoustic scenes, music and human actions; (4) We show the effect of the design choices during pre-training and identify the main parameters to tune to avoid overfitting during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SELF-SUPERVISED CONFORMER TRAINING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Upstream Data</head><p>The data for pre-training was selected from de-identified audio tracks of publicly shared Facebook user videos. We ran an in-house version of TALNet <ref type="bibr" target="#b12">[13]</ref> on the audio tracks of 440 million user videos up to five minutes long, and obtained estimated probabilities of the 527 types of acoustic events in the AudioSet ontology. Then, for each event type, we selected 9,500 videos with the highest probabilities (some videos might be selected for multiple event types). The resultant dataset contains ?3.9M videos totaling ?67k hours. We expect that this large dataset captures enough relevant acoustic phenomena and is sufficiently diverse for different non-speech audio tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Architecture</head><p>Frontend. We use 64-dimensional logmel spectrograms extracted with a window size of 64 ms and a hop size of 20 ms as the input X. Encoder. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> frames into a single frame; then a linear layer converts the stacked spectrogram into latent representation vectors Z = [z1, ..., z T /R ]. Before feeding the latent representation Z to the context encoder, we mask a proportion of it, similar to the original wav2vec 2.0 framework. The context encoder consists of a linear layer, multiple conformer blocks <ref type="bibr" target="#b7">[8]</ref> and another linear layer at the end. We experiment with two context encoder architectures of differing sizes:</p><p>(i) conformer small (cf S): 12 conformer blocks with 256-D encoder embeddings and 8 attention heads (18.4M parameters).</p><p>(ii) conformer large (cf L): 12 conformer blocks with 768-D encoder embeddings and 12 attention heads (88.1M parameters). Both encoders use a feed-forward network (FFN) dimension of 1024, dropout 0.1, kernel size 31 in the first conformer block and 15 for the rest. We further explore different design choices in Section 5.1.2. Additionally, following <ref type="bibr" target="#b13">[14]</ref>, we remove the original relative positional encoding, and reuse the existing convolution module for positional encoding by swapping the order of the convolution and multi-head self-attention modules, which speeds up both training and inference. Contrastive Loss. We learn audio representations by solving a contrastive task: identify the true latent representation zt for a masked time step t within a set of K + 1 candidatesZ = {z1, . . . ,zK+1}, which includes zt and K distractors. Similar to the original wav2vec 2.0, the distractors are uniformly sampled from other masked time steps of the same audio snippet. Given the context network output ct centered over a masked time step t, the contrastive loss is defined as:</p><formula xml:id="formula_0">L = ? log exp(sim(ct, zt)) z?Z exp(sim(ct,z))<label>(1)</label></formula><p>where sim(x, y) is the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hyper-parameters</head><p>For all pre-training experiments, we use the Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with ?1 = 0.9, ?2 = 0.98. We warm up the learning rate for the first 10k updates linearly to a peak of 3 ? 10 ?4 , and then decay it linearly to zero until 300k updates. We regularize the model using weight decay with a decay factor of 0.01. For the contrastive loss we use K = 100 distractors and mask 30% of the latent representations Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DOWNSTREAM TASKS</head><p>We evaluate the generalizability of the learnt representations on sound event detection (SED) and other non-speech audio tasks. Within SED, we evaluate on the AudioSet dataset separately in Sec. 5, because it is much larger than the data used in all the other downstream tasks, and has been used in several priors works as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sound Event Classification</head><p>AudioSet <ref type="bibr" target="#b11">[12]</ref>: AudioSet contains ? 2 million 10-second audio clips from YouTube videos, labeled using an ontology of 527 sound classes. Each clip may have multiple labels. The Balanced training set of AudioSet is a subset with at least 59 clips per class. The balanced training, full training, and evaluation sets contain 22k, 2M, and 20k samples, respectively. For AudioSet, we use the larger model (cf L). ESC-50 <ref type="bibr" target="#b15">[16]</ref>: ESC-50 consists of 2,000 5-seconds audio clips belonging to 50 environmental sound categories. It contains 40 samples for each category, and comes divided into 5 folds for cross validation.</p><p>FSDKaggle2019 <ref type="bibr" target="#b16">[17]</ref>: FSDKaggle2019 is a multi-label dataset consisting of 29,266 audio files annotated with 80 labels form the Au-dioSet ontology. It further consists of a curated set and a noisy set: the former is annotated by humans but the labels may be incomplete; the latter contains many wrong labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Other Audio Tasks: Scenes, Music, and Human Actions</head><p>Acoustic scene classification: We use the dataset from Task 1a of the 2019 DCASE challenge <ref type="bibr" target="#b17">[18]</ref>. It consists of 9,185 and 4,185 segments in the training and the test sets respectively, belonging to 10 acoustic scene classes such as "airport", "park", and "metro station". Music tagging: This is a multi-label classification problem where each recording can have one genre label and multiple instrument labels. For this task, we use the MagnaTagATune <ref type="bibr" target="#b18">[19]</ref> dataset which consists of 25,863 music clips, each clip lasting 29 seconds. We follow the most common 12:1:3 split of training, validation, and evaluation data, and only use the 50 most popular tags out of the 188. Human action classification: This task involves recognizing human actions such as "dancing" and "playing guitar" in video recordings. The Kinetics700 dataset <ref type="bibr" target="#b19">[20]</ref> is a collection of 650k 10-second video clips covering 700 human action classes. This problem has primarily been addressed using images in a uni- <ref type="bibr" target="#b20">[21]</ref> or multimodal <ref type="bibr" target="#b21">[22]</ref> approach; so far there is only one audio-only solution <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FINE-TUNING</head><p>After the self-supervised pre-training, the conformer model can generate an embedding vector (the context representation) for each frame of the input audio. The downstream tasks, however, require predictions over the entire input audio. For AudioSet, we first add a linear classification layer with the sigmoid activation to predict the  frame-level probabilities of each event type, then add a linear softmax pooling layer <ref type="bibr" target="#b12">[13]</ref> to pool them into global probabilities. For all other downstream tasks, we first average-pool the embeddings across all the frames, then stack a linear classification layer to make predictions. During fine-tuning, we allow the weights of the conformer to update. We find this is critical to make the SSL schema work. This is different from some existing works on transfer learning, where a shallow classifier is stacked on top of an encoder with frozen weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Processing</head><p>To deal with the highly skewed label distribution in Full AudioSet, we apply data balancing to ensure that the training sees different sound classes at approximately equal frequencies. We also apply data augmentations in the following order to all downstream tasks: Temporal jittering: Prior to extracting logmel features, we shift the waveform by a random amount between -200 and 200 samples. The maximum shift (200 samples) is equal to half the frame length. SpecAugment <ref type="bibr" target="#b22">[23]</ref>: For each 10 s video, we mask out the logmel features of one random temporal interval up to 2 s. We do not mask out any frequency bins because it does not work well with Mixup. Mixup <ref type="bibr" target="#b23">[24]</ref>: For a minibatch of n videos, with logmel features denoted by {x1, . . . , xn}, we shuffle the instances to get {x 1 , . . . , x n }, and then mix up the two batches according to</p><formula xml:id="formula_1">x i = ?ixi + (1 ? ?i)x i , i = 1, . . . , n<label>(2)</label></formula><p>The mixing coefficient ?i is sampled from the beta distribution B(0.5, 0.5), and it is enforced that ?i ? 1 ? ?i. The labels of x i inherits those of xi; we do not mix them with the labels of x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Consistency Loss</head><p>In addition to the binary cross-entropy (BCE) loss, we employ a consistency loss as a regularizer for training. For each minibatch, we apply the data augmentation twice with different random numbers, and compute the symmetric KL divergence between the model's predicted event probabilities on the two versions of data. This KL divergence is multiplied by 2 and added to the BCE loss.  <ref type="table">Table 2</ref>: Performance comparison with prior works on Full Au-dioSet. Input forms include waveforms (w), logmel (l), video (v), and text (t). We label systems that are pre-trained on AudioSet, or update the encoder weights during fine-tuning. Higher mAP is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyperparameter Tuning</head><p>We find that the fine-tuning process is prone to overfitting, but it can be avoided by carefully tuning the learning rate, the learning rate schedule, and the batch size. We utilize a three-stage learning rate scheduler <ref type="bibr" target="#b22">[23]</ref> for all downstream datasets. The three stages are: warmup, hold, and exponential decay, and they typically last for 30%, 30%, and 40% of all the updates. While high-resource datasets perform best with larger batches, smaller datasets prefer smaller batches. For example, we use a batch size of 640 and 64 for Full AudioSet and MagnaTagATune, respectively. We also regularize the model more for low-resource datasets by adding dropout to the output layer of the pre-trained conformer. We sweep different combinations of the parameters to find the one that optimizes validation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation on AudioSet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Effect of Pre-training</head><p>First, we evaluate the merit of pre-training in our proposed SSL schema in <ref type="table" target="#tab_0">Table 1</ref>. On both the Balanced and the Full training sets, we compare the performance of the cf L model trained from scratch vs pre-trained then fine-tuned. The model trained from scratch performs 50% and 12% worse than the pre-trained and fine-tuned version on Balanced and Full AudioSet, respectively. This emphasizes the necessity of pre-training, especially for smaller downstream datasets. The merit of pre-training can also be quantified by the reduced need for labeled data. By varying the amount of fine-tuning data from 20k to 1.9M audio clips, we find that pre-training and fine-tuning with 600k clips approximately matches the performance of training from scratch with 1.9M clips. That is, pre-training with ?67k hours of unlabeled data reduces the need for labeled data by two-thirds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Ablation Studies</head><p>We perform detailed ablation studies to investigate the effect of the amount of pre-training data and the design choices regarding the model structure, on both the Full and and Balanced training sets.</p><p>Amount of pre-training data. As shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, the performance on Full AudioSet stays relatively unchanged as we vary the amount of pre-training data from 6k to 60k hours. In other words, we can reduce the need for labeled data by two thirds even with only  <ref type="table">Table 3</ref>: Performance of both versions of our conformer on downstream tasks, compared with prior works using either self-supervised (SS) or supervised (S) pre-training. Most prior works use shallow classifiers; only <ref type="bibr" target="#b34">[35]</ref> uses fine-tuning as we do. All metrics are the higher, the better. ? Performance using logmel representations only. Combining waveform and logmel features produces a higher accuracy of 90.5%.</p><p>6k hours of unlabeled data. On Balanced AudioSet, however, the performance increases significantly and monotonically with every bit of extra pre-training data. This demonstrates that a sufficient amount of unlabeled data can make up for the scarcity of labeled data. Number of conformer blocks. We vary the depth of the model from 8 conformer blocks (58.9M parameters) to 20 conformer blocks (146.6M parameters), keeping all other hyperparameters identical to the large model. <ref type="figure" target="#fig_1">Fig. 2b</ref> shows that the largest performance gain occurs when going from 8 to 12 blocks. Beyond that, the improvement is minimal (if any) for Full AudioSet, but still moderate for Balanced AudioSet from 16 to 20 blocks.</p><p>Number of attention heads. Attention heads allow varied levels of focus on different parts of the sequence, producing better predictions than a single weighted average. We vary the number of attention heads from 4 to 24 in our large model, using the same number of heads in all layers. As shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>, there is a slight improvement from 4 to 12 heads, but the performance drops once the number of attention heads exceeds 12 for both Full and Balanced AudioSet.</p><p>In general, the pre-training data size and design choices have a larger impact when labeled data for the downstream task is limited. <ref type="table">Table 2</ref> lists the Full AudioSet test performance of our system and some prior works. Depending on the type of data used for pre-training, we divide the works into four categories: self-supervised pre-training with audio only, self-supervised pre-training with multimodal data, supervised pre-training, and no pre-training. Not all comparisons are apple to apple: some works pre-train on AudioSet, and others (including ours) fine-tune the weights of the entire network, both of which may give them an advantage. Still, we report our performance with ?6k hours of pre-training data without any augmentation, in order to match previous works that pre-train on AudioSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Comparison with Prior Works</head><p>We give a quick overview of the previous works using audioonly self-supervised pre-training. <ref type="bibr" target="#b26">[27]</ref> uses contrastive predictive coding (CPC) to single out the representation of a future step at a given distance, but also adds a nonlinear learnable similarity metric and adversarial perturbation. All the others <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> learn encoders such that a pair of audio clips sampled within a certain temporal proximity have similar representations; the difference lies in how the pairs are presented to the encoder, and the loss function. <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b25">[26]</ref> present both clips as spectrograms, and use the BCE loss and triplet loss, respectively. <ref type="bibr" target="#b27">[28]</ref> presents one clip as a waveform and the other as a spectrogram, and uses a one-vs-many contrastive loss. <ref type="bibr" target="#b28">[29]</ref> separates a mixed signal into two channels and forms a pair using one of the channels and the original signal, and combines the BCE and the one-vs-many contrastive losses.</p><p>The current state-of-the-art performance on AudioSet using audioonly SSL is held by <ref type="bibr" target="#b27">[28]</ref>, with an mAP of 0.329. Our method out-performs it by a huge 25% relative, and achieves an mAP of 0.411. However, it should be pointed out that we update encoder weights during fine-tuning, while all the existing works in this category, including <ref type="bibr" target="#b27">[28]</ref>, use a shallow classifier upon frozen encoder weights.</p><p>On the multimodal self-supervised learning front, most methods use audio and visual signals to learn relationships between them <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>. In addition to audio and video, some works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref> also make use of textual information, but perform worse than our framework with only audio. Our audio-only learning method is competitive even to the best multimodal SSL work on AudioSet <ref type="bibr" target="#b30">[31]</ref>, inferior by only 3% relative.</p><p>Supervised pre-training on ImageNet <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> outperforms our proposed SSL framework, but it requires a large amount of labeled data for pre-training. However, we notice some systems trained from scratch <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> achieve exceedingly high performance. This may indicate that CRNNs may be better suited for SED than conformers. <ref type="table">Table 3</ref> summarizes the results for all the other downstream tasks. We compare the performance of both our conformer models (cf S and cf L) with prior works of transfer learning using either self-supervised or supervised pre-training. Most of these prior works use shallow classifiers; only <ref type="bibr" target="#b33">[34]</ref> fine-tunes the entire network as we do. To our knowledge, no self-supervised prior work exists for FSDKaggle2019, DCASE2019, or Kinetics700.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on Other Downstream Tasks</head><p>For ESC50 and MagnaTagATune, we outperform baselines of selfsupervised pre-training with our cf L architecture. For music tagging, we nearly match the baseline system <ref type="bibr" target="#b10">[11]</ref> pre-trained in a supervised fashion; for the classification of acoustic scenes and human actions, cf L (cf S) outperforms supervised pre-training baselines by 11.9% (6.5%) and 30.5% (13.3%), respectively. However, if we compare against supervised pre-training baselines for SED tasks, our best cf L architecture performs 7.6% (ESC50) and 19.1% (FSDKaggle curated) worse. This significant disparity can be attributed to the fact that the baseline systems were pre-trained on Full AudioSet, which has a substantial overlap in the label space with both ESC50 and FSDKaggle, providing the baseline systems with an edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we have proposed and evaluated a conformer-based selfsupervised framework for learning representations for non-speech audio. The self-supervised pre-training can reduce the need for labeled data by two-thirds. On the widely known AudioSet, our framework produces an mAP of 0.415, outperforming the audio-only self-supervised learning SOTA of 0.329; it also achieves performance comparable to supervised pre-trained baselines on several other downstream non-speech tasks. Our ablation studies shows the significance of critical design choices such as the model depth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>, the logmel spectrograms are fed into two types of encoders: feature and context encoders. In the feature encoder, the time stacking layer stacks every R = 4 consecutive arXiv:2110.07313v3 [cs.SD] 7 Jan 2022 a) Logmel wav2vec 2.0 conformer architecture. b) A conformer block. c) An illustration of how wav2vec 2.0 generates context representation and solves the contrastive task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Effect of the amount of pre-training data, and the number of conformer blocks and attention heads (Full &amp; Balanced AudioSet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance (measured in mAP) of the cf L model on the test set of AudioSet, with and without pre-training, and with different amounts of fine-tuning data.</figDesc><table><row><cell></cell><cell>Balanced</cell><cell></cell><cell></cell><cell></cell><cell>Full</cell></row><row><cell>Model</cell><cell>(20k)</cell><cell cols="4">60k 200k 600k (?2M)</cell></row><row><cell></cell><cell>1%</cell><cell cols="4">3% 10% 30% 100%</cell></row><row><cell>From scratch</cell><cell>0.138</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.366</cell></row><row><cell>Pre-train + fine-tune</cell><cell>0.276</cell><cell cols="4">0.305 0.337 0.356 0.415</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? This work was done during an internship at Facebook.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">VATT: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09990</idno>
		<title level="m">CL4AC: A contrastive loss for audio captioning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved language identification through cross-lingual self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04082</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring wav2vec 2.0 on speaker verification and language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06185</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pushing the limits of semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolution-augmented transformer for semisupervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="100" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Do sound event representations generalize to other audio tasks? A case study in audio transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1214" to="1218" />
		</imprint>
	</monogr>
	<note>Interspeech. ISCA, 2021</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="776" to="780" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="31" to="35" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A better and faster end-to-end model for streaming ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5634" to="5638" />
			<date type="published" when="2021" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Audio tagging with noisy labels and minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02975</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A multi-device dataset for urban acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09840</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Evaluation of algorithms using games: The case of music tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>West</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="387" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="121" to="125" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of semantic audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. IEEE</title>
		<imprint>
			<biblScope unit="page" from="126" to="130" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Contrastive predictive coding of audio with an adversary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="826" to="830" />
		</imprint>
	</monogr>
	<note>in Interspeech. ISCA, 2020</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-format contrastive learning of audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06508</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-supervised learning from automatically separated sound scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multimodal self-supervised learning of general audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12807</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">PSLA: Improving audio event classification with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01243</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">AST: Audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01778</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A sequential self teaching approach for improving generalization in sound event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ithapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="5447" to="5457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PANNs: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">ERANNs: Efficient residual audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verbitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vyshegorodtsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01621</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Contrastive learning of musical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spijkervet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09410</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
