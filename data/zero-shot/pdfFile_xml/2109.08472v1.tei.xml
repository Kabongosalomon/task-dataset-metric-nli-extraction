<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActionCLIP: A New Paradigm for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
							<email>mengmengwang@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazheng</forename><surname>Xing</surname></persName>
							<email>jiazhengxing@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>yongliu@iipc.zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ActionCLIP: A New Paradigm for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The canonical approach to video action recognition dictates a neural model to do a classic and standard 1-of-N majority vote task. They are trained to predict a fixed set of predefined categories, limiting their transferable ability on new datasets with unseen concepts. In this paper, we provide a new perspective on action recognition by attaching importance to the semantic information of label texts rather than simply mapping them into numbers. Specifically, we model this task as a video-text matching problem within a multimodal learning framework, which strengthens the video representation with more semantic language supervision and enables our model to do zero-shot action recognition without any further labeled data or parameters requirements. Moreover, to handle the deficiency of label texts and make use of tremendous web data, we propose a new paradigm based on this multimodal learning framework for action recognition, which we dub "pretrain, prompt and fine-tune". This paradigm first learns powerful representations from pre-training on a large amount of web imagetext or video-text data. Then it makes the action recognition task to act more like pre-training problems via prompt engineering. Finally, it end-to-end fine-tunes on target datasets to obtain strong performance. We give an instantiation of the new paradigm, Ac-tionCLIP, which not only has superior and flexible zero-shot/fewshot transfer ability but also reaches a top performance on general action recognition task, achieving 83.8% top-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video action recognition is the first step of video understanding, and it is an active research area in recent years. We have observed that it mainly went through two stages, feature engineering and architecture engineering. Since there were no sufficient data for learning high-quality models before the birth of large datasets like Kinetics <ref type="bibr" target="#b4">[5]</ref>, early methods focused on the feature engineering, where researchers considered the temporal information inside the videos and * Corresponding author. used their knowledge to design specific hand-crafted representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42]</ref>. Then, with the advent of deep neural networks and large benchmarks, we are now in the second stage, architecture engineering. Lots of well-designed networks sprang up by reasonably absorbing the temporal dimension like two-stream networks <ref type="bibr" target="#b44">[45]</ref>, 3D convolutional neural networks (CNN) <ref type="bibr" target="#b11">[12]</ref>, compute-efficient networks <ref type="bibr" target="#b14">[15]</ref> and transformer-based networks <ref type="bibr" target="#b1">[2]</ref>.</p><p>Though the features and network architectures have been well-studied in the last few years, they are trained to predict a fixed set of predefined categories within a unimodal framework as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). This predetermined manner limits their generality and employment since additional labeled training data is required to transfer to any other new and unseen concepts. Instead of directly mapping labels to numbers like traditional works, learning from the raw text will be a promising solution which could be a much broader source of supervision and provide a more comprehensive representation. Reminiscent of how our humans do this job, we can recognize both known and unknown videos by associating the semantic information from the visual appearance to natural language sources rather than numbers. In this paper, we explore the natural language supervision in a multimodal framework as shown <ref type="figure" target="#fig_0">Figure 1</ref>(b) with two objectives, i) strengthening the representation of the traditional action recognition with more semantic language supervision, and ii) enabling our model to realize zero-shot transfer without any further labeled data or parameters requirements. Our multimodal framework includes two separate unimodal encoders for videos and labels and a similarity calculation module. The training objective is to pull the pairwise video and label representations close to each other, thus the learned representations will be more semantic than unimodal methods. In the inference phase, it becomes a video-text matching problem rather than a classical 1-of-N majority vote task and is capable of zero-shot prediction.</p><p>However, labels of existing fully-supervised action recognition datasets are always too succinct to construct rich sentences for language learning. Collecting and annotating new video datasets require huge storage resources and enormous human effort and time. On the other hand, a sea of videos with noisy but rich text labels are stored and generated on the web every day. Is there a way to energize the abundant web data for action recognition? Pretraining may be a solution that is demonstrated in ViViT <ref type="bibr" target="#b1">[2]</ref>. But it is not easy to pre-train with a large magnitude of web data. It is expensive on storage hardware, computational resources and experiment cycles 1 . This triggers another motivation of this paper, could we directly adapt a pre-trained multimodal model into this task, avoiding the above dilemma? We find this is possible. Formally, we define a new paradigm "pre-train, prompt, and fine-tune" for video action recognition. Although it is appealing to pretrain the whole model end-to-end with large-scale videotext datasets such as HowTo100M <ref type="bibr" target="#b31">[32]</ref>, we are restricted by the enormous computation cost. Luckily, we find it is also worked to use a pre-trained model. Here we use the word "pre-train" rather than "pre-trained" in the new paradigm to keep the pre-training function. Then, instead of adapting the pre-trained model in specific benchmarks by substituting the final classification layers and objective functions, we reformulate our task to look more like those solved during the original pre-training procedure via prompt. Promptbased learning <ref type="bibr" target="#b24">[25]</ref> is regarded as a sea change to natural language processing (NLP), but it is not active in vision tasks, especially has not been exploited in action recognition. We believe it will have attractive prospects in many <ref type="bibr" target="#b0">1</ref>  <ref type="bibr" target="#b7">[8]</ref> reports that pre-training a ViT-H/14 model on JFT takes 2.5k TPUv3-core-days vision-text-related tasks and explore it in action recognition here. Finally, we fine-tune the whole model on target datasets. We implement an instantiation of this paradigm, ActionCLIP, which employs CLIP <ref type="bibr" target="#b35">[36]</ref> as the pre-trained model. It obtains a top performance of 83.8% top-1 accuracy on Kinetics-400. Our contributions can be summarized as follows:</p><p>? We formulate the action recognition task as a multimodal learning problem rather than a traditional unimodal classification task. It strengthens the representations with more semantic language supervision and enlarges the generality and employment of the model in zero-shot/few-shot situations.</p><p>? We propose a new paradigm for action recognition, which we dub "pre-train, prompt, and fine-tune". In this paradigm, we could directly reuse powerful largescale web data pre-trained models by designing appropriate prompts, significantly reducing the pre-training cost.</p><p>? Comprehensive experiments demonstrate the potential and effectiveness of our method, which consistently outperforms the state-of-the-art methods on several public benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Action Recognition</head><p>We have observed that video action recognition mainly went through two stages, feature engineering and architecture engineering. In the first stage, lots of hand-craft descriptors are designed for spatio-temporal representations, like Cuboids <ref type="bibr" target="#b6">[7]</ref>, 3D 3DHOG <ref type="bibr" target="#b16">[17]</ref> and Dense Trajectories <ref type="bibr" target="#b41">[42]</ref>. However, these features lack generalization since they are not end-to-end learned in large-scale datasets. Now we are in the second stage, architecture engineering. We coarsely classify these architectures into four categories, two-stream networks, 3D CNNs, computeefficient networks and transformer-based networks. Twostream-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49</ref>] are introduced to model appearance and dynamics separately with two networks and fuse two streams through the middle or at last. 3D CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40]</ref> intuitively learn spatiotemporal features from RGB frames directly which extend the common 2D CNNs with an extra temporal dimension. Due to the heavy computational burden of 3D CNNs, many compute-efficient networks are designed to find the trade-off between precision and speed <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. Transformer-based networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9]</ref> employ and modify recent strong vision transformers to jointly encode the spatial and temporal features. Yet, most works of both stages are unimodal, without considering the semantic information contained in the labels. We propose a new paradigm "pre-trained, prompt, and fine-tune" based on a video-text multimodal learning framework for action recognition, which sheds light on the language modeling of label words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vision-text Multi-modality in Action Recognition</head><p>Vision-text multi-modality is a hot topic in several vision-text related fields recently, like pre-training <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, vision-text retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref> and so on. Video action recognition could be interpreted as a text-insufficient video-totext retrieval problem. Therefore, it may also be feasible to apply vision-text multi-modal learning in this task. However, to the best of our knowledge, we have not found mature and effective methods from this perspective in general video action recognition. We do find several visiontext multi-modality works in self-supervised video representation learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1]</ref> and zero-shot action recognition <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b52">53]</ref>. Yet, the former is prone to just learn a strong pre-trained video representation with a large web dataset and still neglects the label texts features when doing specific classification, just attaching and learning a linear classifier on top of the learned vision representation. The latter mainly concentrates on the embedding space designation with a pre-trained vision model and a simple text embedding like Word2Vec <ref type="bibr" target="#b32">[33]</ref>, paying less attention to the upstream general action recognition task. Different from them, in this paper, we focus on the vision-text multimodality learning in general action recognition and build a bridge for it and zero-shot/few-shot action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multimodal Learning Framework</head><p>Previous comprehensive video action recognition methods treat this task as a classic and standard 1-of-N majority vote problem, mapping labels into numbers. This pipeline completely ignores the semantic information contained in the label texts. We instead model this task as a video-text multimodal learning problem, in contrast to pure video modeling. We believe learning from the supervision of natural language could not only enhance the representation power but also enable flexible zero-shot transfer.</p><p>Formally, given an input video x and a label y from a predefined label set Y, the prior works usually train a model to predict the conditional probability P (y|x, ?) and turn y into a number or a one-hot vector to indicate its index of the whole label set length |Y|. In the inference phase, the highest-scoring index of the prediction is regarded as the corresponding category. We try to break this routine and model the problem as P (f (x, y)|?), where y is the original words of the label and f is a similarity function. Then, the testing is more likely a matching process, the label words of the highest similarity score is the classification result:</p><formula xml:id="formula_0">y = argmax y?Y P (f (x, y)|?)<label>(1)</label></formula><p>As shown <ref type="figure" target="#fig_0">Figure 1</ref>(b), we learn separate unimodal encoders g V , g W for video and label words inside a dualstream framework. The video encoder g V extracts spatiotemporal features for the visual modality and could be any well-designed architectures. The language encoder g W is used to extract features of input label texts and could be a wide variety of language models. Then, to pull the pairwise video and label representations close to each other, we define symmetric similarities between the two modalities with cosine distances in the similarity calculation module:</p><formula xml:id="formula_1">s(x, y) = v ? w v w , s(y, x) = w ? v w v<label>(2)</label></formula><p>where v = g V (x) and w = g W (y) are encoded features of x and y, respectively. Then the softmax-normalized videoto-text and text-to-video similarity scores can be calculated as:</p><formula xml:id="formula_2">p x2y i (x) = exp(s(x, y i )/? ) N j=1 exp(s(x, y j )/? ) , p y2x i (y) = exp(s(y, x i )/? ) N j=1 exp(s(y, x j )/? )<label>(3)</label></formula><p>where ? is a learnable temperature parameter and N is the number of training pairs. Let q x2y (x), q y2x (y) indicate the ground-truth similarity scores, where the negative pair has a probability of 0 and the positive pair has a probability of 1. Since the amount of videos are much larger than the fixed labels, it will inevitably appear multiple videos belonging to one label in a batch. Therefore, it may exist more than one positive pair in both q x2y i (x) and q y2x i (y). It is not proper to regard the similarity score learning as a 1-in-N classification problem with cross-entropy loss. Instead, we define the Kullback-Leibler (KL) divergence as the video-text contrastive loss to optimize our framework as:</p><formula xml:id="formula_3">L = 1 2 E (x,y)?D [KL(p x2y (x), q x2y (x))+KL(p y2x (y), q y2x (y))]<label>(4)</label></formula><p>where D is the whole training set. Based on the multimodal framework, we can simply carry out zero-shot prediction as the normal testing process in Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The New Paradigm</head><p>When considering the above multimodal learning framework, we need to consider the deficiency of label words. The most intuitive way is to take advantage of vast web image-text or video-text data. To cater for this, we propose a new "pre-train, prompt and fine-tune" paradigm for action recognition.  Pre-train. As prior arts suggested, pre-training has a large impact on vision-language multimodal learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref>. Since the training data is directly collected from the web, one of the hot topics is to design appropriate objectives to handle these noisy data during this procedure. We find there are mainly three upstream pre-training proxy tasks in the pre-training procedure: multimodal matching (MM), multimodal contrastive learning (MCL) and masked language modeling (MLM). MM predicts whether a pair of modalities is matched or not. MCL aims to draw pairwise unimodal representations close to each other. MLM utilizes the features of both modalities to predict the masked words. However, this paper does not focus on this step due to the restriction of enormous computation cost. We directly choose to apply a pre-trained model and make efforts on the following two steps. Prompt. Prompt in NLP means the original input is modified using a template into a textual string prompt that has some unfilled slots to fill with expected results. Here we borrow the word "prompt" for the meaning of adjusting and reformulating the downstream tasks to act more like the upstream pre-training tasks. Notably, the traditional practice is adapting the pre-trained model to the downstream classification task via attaching a new linear layer to the pre-trained feature extractor, which is reversed to ours. Here we make two kinds of prompts, textual prompt and visual prompt. The former is significant for label text extension. Given a label y, we first define a set of permissible values Z, then the prompted textual input y is obtained by a filling function f f ill (y, z), where z ? Z. There are three varieties of f f ill , prefix prompt, cloze prompt and suffix prompt.</p><p>They are classified based on the filling locations. For visual prompt, its designation mainly depends on the pretrained model. If the model is pre-trained on video-text data, it is almost no extra reformulation for the visual part since the model is already trained to output video representations. While if the model is pre-trained with image-text data, then we should empower the model to learn the important temporal relationship of videos. Formally, given a video x, we introduce prompt function as f tem (h I (x)), where h I is the visual encoding network of pre-trained models. Similarly, f tem has three variants based on where it works against h I , pre-network prompt, in-network prompt and post-network prompt. With the elaborate designation of prompt, we could even avoid the above unreachable computational "pre-train" step by keeping the learned ability of a pre-trained model. Note that in the new paradigm, the pretrained model should not be largely modified due to catas-trophic forgetting <ref type="bibr" target="#b28">[29]</ref>, where the pre-trained model loses its ability to do things that it was able to do in the pre-training.</p><p>We also demonstrate this point in our experiments. Fine-tune. When there are sufficient downstream training datasets like Kinetics, it is no doubt that fine-tuning on specific datasets will dramatically improve the performance. Also, if the prompt introduces extra parameters, it is necessary to train these parameters and learn with the whole framework end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">New Paradigm Instantiation Details</head><p>Each component of the new paradigm has a wide variety of choices. As presented in <ref type="figure" target="#fig_1">Figure 2</ref>, we show an instantiation example here and conduct all the experiments with this instantiation.</p><p>We employ a firsthand pre-trained model, CLIP <ref type="bibr" target="#b35">[36]</ref> to avoid the enormous computational resources at the first pre-training step. This instantiation model is called ActionCLIP as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a). CLIP is an efficient image-text representation trained with the MCL task, similar to our multimodal learning framework. <ref type="figure" target="#fig_1">Figure 2(b)</ref> shows concrete examples of the textual prompts used in the instantiation. We define Z to be K discrete manual sentences which is the most natural way based on human introspection. Then the prompted input y is fed into the language encoder g W that is the same with pre-trained language model h W . For the vision model, based on the pre-trained image encoder h I of CLIP, we employ three types of visual prompts as follows.</p><p>Pre-network Prompt. This type operates on the inputs before feeding into the encoder, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c). Given a video x, we simply forward all spatio-temporal tokens extracted from the video through the visual encoder to jointly learn spatio-temporal attentions. Except for the spatial positional embedding, an extra learnable temporal positional embedding will be added to the token embedding to indicate the frame index. g V could use the original pretrained image encoder h I . We call this type Joint for short. In-network Prompt. We attempt a parameter-free prompt abbreviated as Shift for this type as shown in <ref type="figure" target="#fig_1">Figure 2(d)</ref>. We introduce the temporal shift module <ref type="bibr" target="#b23">[24]</ref>, which shifts part of the feature channels along the temporal dimension and facilitates information exchanged among neighboring input frames. We insert the module between every two adjacent layers of g V . The architecture and pre-trained weights of g V could directly reuse h I since this module brings no parameters. Post-network prompt. Given a video x with F extracted frames, we sequentially encode spatial and temporal features with two separate encoders in this prompt. The first is a spatial encoder g sp V which is responsible for only modeling interactions between tokens extracted from the same temporal index. We use h I as our g sp V . The extracted framelevel representations u i ? R d are then concatenated into u ? R F ?d , and then fed to a temporal encoder g te V to model interactions between tokens from different temporal indices. We offer four choices for g te V , MeanP, Conv1D, LSTM and Transf, presented in <ref type="figure" target="#fig_1">Figure 2</ref>(e-g). MeanP is short for mean pooling on the temporal dimension. Conv1D is a 1d convolutional layer applied on the temporal dimension. LSTM is a recurrent neural network and Transf means a L t layer temporal vision transformer encoder. Since the temporal dimensions of Conv1D, LSTM and Transf keep the same with input u, the subsequent operations are the same as MeanP.</p><p>Then we end-to-end fine-tune the whole network with the training objective Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Network architectures. Our textual encoder g W follows that of CLIP which is a 12-layer, 512-wide Transformer with 8 attention heads and the activations from the highest layer at [EOS] are treated as the feature representation w. We use ViT-B/32 and ViT-B/16 of CLIP's visual encoder h I . They are all 12-layer vision transformers, with different input patch sizes of 32 and 16 respectively. The [Class] token of their highest layers' outputs are used. We use K=18 permissible values Z for textual prompt. For visual prompts, the layer of Conv1D and LSTM is 1, Transf has L t =6 layers. Two versions of Transf are implemented, they are different in not using or using [Class] token. We distinguish them as Transf and Transf cls . Training. We use AdamW optimizer with a base learning rate of 5 ? 10 ?6 for pre-trained parameters and 5 ? 10 ?5 for new modules with learnable parameters. Models are trained with 50 epochs and the weight decay is 0.2. The learning rate is warmed up for 10% of the total training epochs and decayed to zero following a cosine schedule for the rest of the training. The spatial resolution of the input frames is 224 ? 224. We use the same segmentbased input frame sampling strategy as <ref type="bibr" target="#b45">[46]</ref> with 8, 16 or 32 frames. Even the largest model of our method, ViT-B/16 could be trained with 4 NVIDIA GeForce RTX 3090 GPUs on Kinetics-400 when inputting 8 frames, and the training process takes about 2.5 days. Compared to X3D and Slow-Fast, both trained with 128 GPUs for 256 epochs, our training is much faster and requires fewer GPUs (?30). Inference. The input resolution is 224?224 in all the experiments. Following <ref type="bibr" target="#b14">[15]</ref>, we use multi-view inference with 3 spatial crops and 10 temporal clips of each video only for the best performance model. The final prediction is from the averaged similarity scores of all views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>In this section, we do extensive ablation experiments to demonstrate our method with the instantiation, Action-CLIP. Models in this section use 8-frame input, Transf for temporal modeling, ViT-B/32 as the backbone and single view testing on Kinetics-400, unless specified otherwise. Is the "multimodal framework" helpful? To compare with the traditional video-unimodal 1-of-N classification model, we implement a variant called unimodality which has the same backbone, pre-trained weights and temporal modeling strategy (before the final linear layer) with our Action-CLIP. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. When exploiting the semantic information of label texts with our multimodal learning framework, it dramatically improves the performance with 2.91% top-1 accuracy gains, demonstrating that the multimodal framework is helpful to learn powerful representations for action recognition. Is the "pre-train" step important? In <ref type="table">Table 2</ref>, we validate the impact of this step by experimenting with random initialized or CLIP pre-trained vision and language encoders. In particular, from the large gap (40.10% vs. 78.36%) between model V3 and V4, we find that the visual encoder needs proper initialization, otherwise the model will fail to obtain a strong performance. The language encoder has a smaller influence, since model V2 could also get a comparative result (76.63%) compared with model V4 (78.36%). When both the visual and language encoders are randomly initialized, model V1 is hard to learn a good representation and drops a large margin of 41.4% from model V4. Therefore, the final conclusion is that the "pre-train" step is important, especially for the visual encoder. Is the "prompt" step important? <ref type="table">Table 3</ref> shows the results of textual prompt. It can be seen that using only the label words drops 0.54% compared with using textual prompt, demonstrating the validness of this simple, discrete and human-comprehensible textual prompt. For the visual prompt, note that MeanP is the simplest temporal fusion way and we compare other visual prompts with it. As shown in <ref type="table" target="#tab_3">Table 4</ref>, we find Joint and Shift obviously decrease the performance by 2.74% and 5.38%, respectively. We believe the reason is the catastrophic forgetting phenomenon since the input pattern is changed in Joint and the features of pre-trained image encoder h I are changed in Shift. These operations may break the original learned strong representations and yield performance drop. Postnetwork prompts are more suitable and safer options to keep the learned character. Specifically, LSTM and Conv1D cause a negligible top-1 drop but they all improve the top-5 accuracy. Transf cls and Transf improve the top-1 results with 1.01% and 1.25%. We choose Transf as our final visual prompt since it has the best results. In a word, the designation of prompt is significant since proper prompts could avoid catastrophic forgetting and maintain the representation power of existing pre-trained models, giving a shortcut to the usage of tremendous web data. Is the "fine-tune" step important? We demonstrate this step by separately freezing the parameters of the pre-trained language encoder h W and image encoder h I . The results are presents in <ref type="table" target="#tab_4">Table 5</ref>. When the two encoders are all frozen and only the visual prompt Transf is trained, the performance decreases 6.15% on top-1 accuracy. When all the parameters are end-to-end fine-tuned, we obtain the best results of 78.36%. It will have a negative influence on the accuracy if either of the pre-trained encoders is frozen. Therefore, the "fine-tune" step is indeed crucial to specific datasets, which is consistent with our perceptual intuition. Backbones and input frames. In <ref type="table" target="#tab_5">Table 6</ref>, we experiment  ActionCLIP with different backbones and input frames configurations. The input frames vary from 8, 16 to 32. Two different backbones are used, ViT-B/32 and ViT-B/16. The conclusion is intuitive that larger models and more input frames yield better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Runtime Analysis</head><p>For different backbones and input frame configurations, we present their model sizes, FLOPs and inference speeds in <ref type="table" target="#tab_6">Table 7</ref>. The textual encoder of all backbones has the same architecture, which has 37.8M parameters. We show the whole parameter in the table. Notably, ViT-B/32 has a little more parameters than ViT-B/16, which comes from the linear projection layer before feeding into the vision transformer. While ViT-B/32 has much a faster inference speed (3.3?) and fewer FLOPs (4?) than ViT-B/16. Moreover, we provide two very recent methods for comparison, TimeSformer <ref type="bibr" target="#b2">[3]</ref> and ViViT <ref type="bibr" target="#b1">[2]</ref> with their highest configurations which obtain similar accuracy with ActionCLIP. Specifically, compared with the highest configuration of ActionCLIP, TimeSformer needs more input frames (3?) and much more computational FLOPs (12.7?) to obtain its best performance, which is still worse than ActionCLIP (82.3% vs. 80.7%). Similarly, ViViT has much more computational FLOPs (7.1?) to obtain its best results with a larger input solution 320?320, while ActionCLIP's input is 224?224 and it surpasses ViViT with 1% top-1 accuracy gap and runs faster (3.1?) than ViViT. In conclusion, Ac-tionCLIP is a cost-effective and efficient method for action recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Zero-shot/few-shot Recognition</head><p>In this section, we demonstrate the attractive zeroshot/few-shot recognition ability of our ActionCLIP (ViT-B/16). We implement two representative methods for comparison, STM <ref type="bibr" target="#b14">[15]</ref> which is a well-designed temporalencoded 2D network, and 3D-ResNet-50 which is the slow path of SlowFast <ref type="bibr" target="#b11">[12]</ref>. We use 8-frame input, single view inference in all models of this section. We first conduct the zero-shot/few-shot experiments on Kinetics-400. Action-CLIP uses pre-trained model of CLIP with MeanP visual prompt (since Transf has no pre-trained parameters), STM and 3D-ResNet-50 are pre-trained on ImageNet. Then, we validate on UCF-101 and HMDB-51 with Kinetics-400 pre-trained models (ActionCLIP uses Transf here). As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the results demonstrate the strong transfer power of ActionCLIP under these data-poor conditions, while the traditional unimodality methods are not able to do zero-shot recognition and their few-shot performance is ineffective compared with ActionCLIP even pre-trained on large Kinetics-400. <ref type="table">Table 8</ref>. Comparison with previous work on Kinetics-400. "-" indicates the numbers are not available for us. "+" in the second column means the different input of two paths of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Frames </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-art Methods</head><p>In this section, we evaluate the performance of our method on a diverse set of action recognition datasets: Kinetics-400 <ref type="bibr" target="#b4">[5]</ref>, Charades <ref type="bibr" target="#b37">[38]</ref>, UCF-101 <ref type="bibr" target="#b38">[39]</ref> and HMDB-51 <ref type="bibr" target="#b17">[18]</ref>. ViT-B/16, Transf prompt and multi-view testing are used in ActionCLIP. The results of UCF-101 and HMDB-51 are shown in Appendix. Kinetics-400. <ref type="table">Table 8</ref> compares to prior methods on Kinetics-400. There are four parts in this table, corresponding to 3D-CNN-based methods, 2D-CNN-based methods, transformer-based methods and our method. According to the table, the third section achieves better results with strong vision transformers than the first and second parts. Among them, the first four methods build on the 12-layer ViT-B/16 model for parameter-accuracy balance, so do our Action-CLIP. ViViT instead uses a larger model, 24-layer ViT-L for better results. Also, it introduces JFT for pre-training for further gain. Our ActionCLIP achieves 82.6% top-1 accuracy with only 16-frame input, which exceeds all the methods in the first and second parts of the table and most transformer-based methods that may use more input frames like 250 frames of ViT-B-VTN. An interesting discovery is that our top-5 accuracy is always higher than other methods. We think this benefits from our multimodal framework's different inference process, which calculates the similarity <ref type="table">Table 9</ref>. Comparison with previous work on Charades. "-" indicates the numbers are not available for us. "+" in the second column means the different input of two paths of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Frames between the semantic representations of videos and all labels. ActionCLIP further reaches a leading performance of 83.8% when increasing the input to 32 frames. We believe that more input frames, larger models and larger input resolutions will yield better results and leave it to future work. The current performance of ActionCLIP could already reveal the potential of the multimodal learning framework and the proposed new paradigm for action recognition.</p><p>Charades. This is a dataset with longer-range activities and it has multiple actions inside every video. We show Kinetics-400 pre-trained models in <ref type="table">Table 9</ref>. Mean Average Precision (mAP) is used for evaluation. ActionCLIP achieves the top performance of 44.3 mAP, which demonstrates its effectiveness on multi-label video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper provides a new perspective for action recognition by regarding it as a video-text multimodal learning problem. Unlike the canonical approaches that model the task as a video unimodality classification problem, we propose a multimodal learning framework to exploit the semantic information of label texts. Then, we formulate a new paradigm, i.e., "pre-train, prompt, and fine-tune" to enable our framework to directly reuse powerful large-scale web data pre-trained models, greatly reducing the pre-training cost. We implement an instantiation of the new paradigm, ActionCLIP, which has a superior performance on both general and zero-shot/few-shot action recognition. We hope our work could provide a new perspective for this task, especially raising attention on language modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Existing unimodality pipeline (a) and our multimodal framework (b). They are different in the usage of labels. (a) maps labels into numbers or one-hot vectors while (b) exploits the semantic information of label text itself and tries to pull the corresponding video representation close to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(d) In-network Prompt: Shift MeanP MeanP Choosing from ? Prefix prompt? label?a video of action Cloze prompt: this is label, a video of action Suffix prompt: human action of label (b) Textual Prompt Examples Overview of ActionCLIP. We present the whole architecture in (a), which consists of two single-modal encoders, a similarity calculation module and all possible prompt locations. (b) shows several examples of the textual prompt. (c) and (d) are the in-network and pre-network visual prompts details, respectively. (e),(f) and (g) give the details of post-network visual prompts, MeanP, Conv1D, LSTM and Transf. Pos. is short for positional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Zero-shot/few-shot results on Kinetics-400 (left), HMDB-51 (middle) and UCF-101 (right). ActionCLIP leads the performance under these hard data-poor circumstances. It can do zero-shot recognition on all three datasets while STM and 3D-ResNet-50 are not available under this condition. Also, ActionCLIP is good at few-shot classification where the performance gap is obvious compared to the other two methods. Brown dashed lines demonstrate zero-shot accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pre-network Prompt In-network Prompt Pre-trained Image Encoder Post-network Prompt Frame-level representations Video Encoder ...</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Similarity</cell><cell></cell><cell cols="2">Video</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Similarity Calculation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Label text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Text Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>1 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>...</cell></row><row><cell>0</cell><cell>1 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Spatial Pos. + Patch Embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Temporal Pos. + Input</cell></row><row><cell>play</cell><cell>a</cell><cell>human</cell><cell></cell><cell>action of</cell><cell>watering</cell><cell cols="2">plants</cell><cell>0</cell><cell>1 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Pre-network Prompt: Joint</cell></row><row><cell></cell><cell></cell><cell cols="4">Textual Prompt Label: Watering Plants</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Linear Projection</cell><cell>...</cell><cell>A ViT Layer</cell><cell>Module</cell><cell>Temporal Shift</cell><cell>A ViT Layer</cell><cell>Module</cell><cell>Temporal Shift</cell><cell>A ViT Layer</cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Main Architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Mean Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">1D Convolutional Layer/LSTM</cell><cell>Temporal Transformer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell>0</cell><cell>1 1</cell><cell>2</cell><cell>3</cell><cell>1 4</cell><cell>5</cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Temporal Pos. + Input</cell></row><row><cell></cell><cell cols="6">(e) Post-network Prompt: MeanP</cell><cell></cell><cell cols="6">(f) Post-network Prompt: Conv1D/LSTM</cell><cell>(g) Post-network Prompt: Transf</cell></row></table><note>6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation of the "multimodal framework".</figDesc><table><row><cell></cell><cell cols="2">Unimodality ActionCLIP</cell></row><row><cell>Top-1</cell><cell>75.45</cell><cell>78.36</cell></row><row><cell>Top-5</cell><cell>92.51</cell><cell>94.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation of the "pre-train" step. Ablation of the textual prompt.</figDesc><table><row><cell cols="4">Model Language Vision</cell><cell>Top1 Top5</cell></row><row><cell>V1</cell><cell cols="2">random</cell><cell cols="2">random 36.96 63.02</cell></row><row><cell>V2</cell><cell cols="2">random</cell><cell>CLIP</cell><cell>76.63 91.94</cell></row><row><cell>V3</cell><cell></cell><cell>CLIP</cell><cell cols="2">random 40.10 66.35</cell></row><row><cell>V4</cell><cell></cell><cell>CLIP</cell><cell>CLIP</cell><cell>78.36 94.25</cell></row><row><cell></cell><cell></cell><cell cols="3">only label textual prompt</cell></row><row><cell></cell><cell>Top-1</cell><cell>77.82</cell><cell></cell><cell>78.36</cell></row><row><cell></cell><cell>Top-5</cell><cell>93.95</cell><cell></cell><cell>94.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation of the visual prompt.</figDesc><table><row><cell cols="2">Visual prompt</cell><cell>Top-1 Top-5</cell></row><row><cell>Pre-network</cell><cell>Joint</cell><cell>74.37 93.09</cell></row><row><cell>In-network</cell><cell>Shift</cell><cell>71.73 91.07</cell></row><row><cell></cell><cell>MeanP</cell><cell>77.11 93.79</cell></row><row><cell></cell><cell>LSTM</cell><cell>77.09 93.86</cell></row><row><cell>Post-network</cell><cell cols="2">Conv1D 77.04 94.06</cell></row><row><cell></cell><cell cols="2">Transf cls 78.12 94.25</cell></row><row><cell></cell><cell>Transf</cell><cell>78.36 94.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Is the "fine-tune" step important? " " means do finetuning while "?" means fixing the parameters without fine-tuning.</figDesc><table><row><cell cols="4">Model Language Video Top-1 Top-5</cell></row><row><cell>V1</cell><cell>?</cell><cell>?</cell><cell>72.21 91.61</cell></row><row><cell>V2</cell><cell>?</cell><cell></cell><cell>73.88 91.61</cell></row><row><cell>V3</cell><cell></cell><cell>?</cell><cell>73.78 92.17</cell></row><row><cell>V4</cell><cell></cell><cell></cell><cell>78.36 94.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Influence of backbones and input frames.</figDesc><table><row><cell cols="3">Backbone Input frames Top1 Top5</cell></row><row><cell>ResNet-50</cell><cell>8</cell><cell>73.31 92.02</cell></row><row><cell>ViT-B/32</cell><cell>8</cell><cell>78.36 94.25</cell></row><row><cell></cell><cell>8</cell><cell>81.09 95.49</cell></row><row><cell>ViT-B/16</cell><cell>16</cell><cell>81.68 95.87</cell></row><row><cell></cell><cell>32</cell><cell>82.32 96.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Parameters, FLOPs and inference speed comparison. All the results of ActionCLIP are tested on one NVIDIA GeForce RTX 3090 GPU and use single-view testing.</figDesc><table><row><cell>Backbone</cell><cell cols="5">Frames Top-1 GFLOPs Params Runtime</cell></row><row><cell cols="2">TimesSformer-L 96</cell><cell cols="2">80.7 7140</cell><cell>-</cell><cell>-</cell></row><row><cell>ViViT-L/16 320</cell><cell>32</cell><cell cols="2">81.3 3992</cell><cell>-</cell><cell>4.2V/s</cell></row><row><cell>ViT-B/32</cell><cell>8</cell><cell>78.4</cell><cell cols="3">35.4 144.1M 144.7V/s</cell></row><row><cell></cell><cell>8</cell><cell cols="4">81.1 140.8 141.7M 43.2V/s</cell></row><row><cell>ViT-B/16</cell><cell>16</cell><cell cols="4">81.7 281.6 141.7M 21.2V/s</cell></row><row><cell></cell><cell>32</cell><cell cols="4">82.3 563.1 141.7M 13.0V/s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We would like to thank Zeyi Huang for his constructive suggestions and comments on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2008-19th British Machine Vision Conference</title>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depthwise spatio-temporal stft convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhakar</forename><surname>Kumawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07651</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11669" to="11676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06803</idno>
		<title level="m">Temporal adaptive module for video recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning multimodal representations for unseen activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13915</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="625" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1529" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mict: Mixed 3d/2d convolutional tube for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
