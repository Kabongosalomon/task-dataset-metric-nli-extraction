<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 03-05. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyan</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosi</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guixu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyan</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosi</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guixu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Woodstock, NY</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">18</biblScope>
							<date type="published">June 03-05. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>ACM Reference Format: 2018. Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation. In Woodstock &apos;18: ACM Symposium on Neural Gaze Detection, June 03-05, 2018, Woodstock, NY . ACM, New York, NY, USA, 9 pages. https:// ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Reconstruction; Matching KEYWORDS Stereo image super-resolution</term>
					<term>disparity estimation</term>
					<term>mutually boosted</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Under stereo settings, the problem of image super-resolution (SR) and disparity estimation are interrelated that the result of each problem could help to solve the other. The effective exploitation of correspondence between different views facilitates the SR performance, while the high-resolution (HR) features with richer details benefit the correspondence estimation. According to this motivation, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the stereo image super-resolution and disparity estimation in a unified framework and interact them with each other to further improve their performance. Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for left and right views. Besides the cross-view information exploitation in the low-resolution (LR) space, HR representations produced by the SR process are utilized to perform HR disparity estimation with higher accuracy, through which the HR features can be aggregated to generate a finer SR result. Afterward, the proposed HR Disparity Information Feedback (HRDIF) mechanism delivers information carried by HR disparity back to previous layers to further refine the SR image reconstruction. Extensive experiments demonstrate the effectiveness and advancement of SSRDE-FNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the development of dual cameras, stereo images have shown greater impact in many applications, such as smartphones, drones, and autonomous vehicles. However, the stereo images often suffer from resolution degradation in practice. Therefore, a technology that can restore the high-resolution (HR) left and right views in a 3D scene is essential. In the binocular system, parallax effects between the low resolution (LR) images cause a sub-pixel shift between them. Therefore, making full use of cross-view information can help reconstruct high-quality SR images since one view may have additional information relative to the other.</p><p>Recently, several deep learning based methods have been proposed to capture cross-view information by modeling the disparity. For example, <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b39">37]</ref> leverage the parallax attention module (PAM) proposed by Wang et al. <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30]</ref> to search for correspondences along the horizontal epipolar line without parallax limit; In <ref type="bibr" target="#b37">[35]</ref>, a pre-trained disparity network <ref type="bibr" target="#b9">[9]</ref> was used to deploy the disparity prior into image reconstruction. Although continuous improvements have been achieved in stereo image SR, the utilization of cross-view information is still insufficient and less effective.</p><p>In fact, under stereo settings, disparity estimation and image SR are interrelated that the result of each problem could help to solve the other one, and each task benefits from the gradual improvement over the other task. However, previous methods have not explored this mutually boosted property. Moreover, all these methods exploit correspondent information only in the LR space, which usually does not provide enough accuracy in high-frequency regions due to the loss of fine-grained details in LR features. Thus, the positive additional information brought by these correspondences is still limited, hindering sufficient feature aggregation and further SR performance improvements. Thus, it is highly desirable to model disparity in a more powerful way and have a guidance mechanism that can fully interact between super-resolution and disparity estimation.</p><p>To address the aforementioned problem, we propose a novel method that can handle stereo image super-resolution and HR disparity estimation in an end-to-end framework <ref type="figure">(Figure 1</ref>), interacting in a mutually boosted manner. We perform disparity estimation in the HR space to overcome the accuracy limitation of LR correspondence and better guide the stereo SR. To achieve this efficiently, we leverage the features from LR space and the reconstructed HR space to estimate disparity in a coarse-to-fine manner. In the framework, the guidance and interaction of super-resolution and disparity estimation are three-folds: (i). the coarse correspondence estimation in LR space benefits the cross-view information exploration for SR, initial SR results and HR features for both views are produced; <ref type="bibr" target="#b0">1</ref> School of Computer Science and Technology, East China Normal University, Shanghai, China ? 51194506008@stu.ecnu.edu.cn * fmfang@cs.ecnu.edu.cn <ref type="figure">Figure 1</ref>: The architecture of SSRDE-FNet, which introduces the HR disparity information feedback mechanism.</p><p>(ii). the HR representations from (i) with richer details serve as finer features for HR disparity estimation, which reduces the search range of HR disparity for better accuracy and efficiency; (iii). The HR disparity can further benefit SR reconstruction. Specifically, we align the HR features of the two views using HR disparity maps and perform attention-driven feature aggregation to produce the enhanced HR features, upon which a finer SR result is generated. To achieve a more essential facilitation of HR disparity to stereo SR, we propose the HR Disparity Information Feedback (HRDIF) mechanism that feeds the enhanced HR features and the HR disparity back to previous layers for the refinement of low-level features in the SR process. In summary, the main contributions of this paper are as follows:</p><p>? We propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet) that can simultaneously solve the stereo image super-resolution and disparity estimation in a unified framework. To the best of our knowledge, this is the first end-to-end network that can achieve the mutual boost of these two tasks. ? We propose a novel HR Disparity Information Feedback (HRDIF) mechanism for HR disparity and promote the quality of the SR image in an iterative manner. ? Extensive experiments illustrate that the proposed model can restore high-quality SR images, and the model achieves state-of-the-art results in the field of stereo image superresolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Image Super-Resolution</head><p>Image Super-Resolution aims to reconstruct a super-resolution (SR) image from its degraded low-resolution (LR) one, which is an extremely hot topic in the computer vision field. Since the pioneer work of Super-Resolution Convolutional Neural Network (SRCNN <ref type="bibr" target="#b4">[4]</ref>), learning-based methods have dominated the research of single image super-resolution (SISR). Methods like VDSR <ref type="bibr" target="#b10">[10]</ref>, SRDenseNet <ref type="bibr" target="#b29">[28]</ref>, EDSR <ref type="bibr" target="#b19">[18]</ref>, MSRN <ref type="bibr" target="#b15">[15]</ref>, and RDN <ref type="bibr" target="#b41">[39]</ref> achieved excellent performance and greatly promoted the development of SISR. However, due to the lack of reference features, the development of SISR has encountered a bottleneck, and its performance is difficult to further improve. Therefore, stereo image super-resolution has received great attention in recent years since it has the available left and right view information. The critical challenge for enhancing spatial resolution from stereo images is how to register corresponding pixels with sub-pixel accuracy. Bhavsar et al. <ref type="bibr" target="#b0">[1]</ref> argued that the twin problems of image SR and HR disparity estimation are intertwined under stereo settings. They formulate the two problems into one energy function, and minimize it by iteratively updating the HR image and disparity map. The following conventional methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">23]</ref> usually follow this pipeline, however, these methods usually take a large amount of computation time. Recently, several deep learning-based stereo SR methods have emerged by using the parallax. For example, StereoSR <ref type="bibr" target="#b7">[7]</ref> stacks stereo images with horizontal shift intervals to feed into the network to learn stereo correspondences. However, the maximum parallax that can be processed is fixed as 64. To explore correspondences without disparity limit, Wang et al. <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30]</ref> proposed PASSRnet, with a parallax-attention module (PAM) that has a global receptive field along the epipolar line for global correspondence capturing. Ying et al. <ref type="bibr" target="#b39">[37]</ref> and Song et al. <ref type="bibr" target="#b28">[27]</ref> also made use of the PAM, while <ref type="bibr" target="#b39">[37]</ref> incorporated several PAMs to different stages of the pre-trained SISR networks to enhance the cross-view interaction. In iPASSR <ref type="bibr" target="#b34">[32]</ref>, a symmetric bi-directional PAM (biPAM) and an inline occlusion handling scheme are proposed to further improve SR performance. Besides the PAM based methods, Yan et al. <ref type="bibr" target="#b37">[35]</ref> uses a pre-trained disparity flow network to predict the disparity map based on the input stereo pair, and incorporates the disparity prior to better utilize the cross-view nature. Lei et al. <ref type="bibr" target="#b13">[13]</ref> builds up an interaction module-based stereo SR network (IMSSRnet), in which the interaction module is composed of a series of interaction units with a residual structure. Above methods all explore the correspondence information between stereo images only in the LR space, limiting the positive effects provided by cross-view. Our work hunts for the mutual contributions between the stereo image SR and HR disparity estimation, leading to higher image quality and more accurate disparity, which is new in literature w.r.t learning-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disparity Estimation</head><p>Disparity estimation has been investigated to obtain correspondence between a stereo image pair <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b26">25]</ref>, which can be utilized to capture long-range dependency for stereo SR. Existing end-to-end disparity estimation networks usually include cost volume computation, cost aggregation, and disparity prediction. 2D CNN based methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b35">33]</ref> generally adopt a correlation layer for 3D cost volume construction, while 3D CNN based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b40">38]</ref> mostly use direct feature concatenation to construct 4D cost volume and use 3D convolution for cost aggregation. However, learning matching costs from 4D cost volumes suffers from a high computational and memory burden. Apart from supervised methods, several unsupervised learning methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b42">40]</ref> have been developed to avoid the use of costly ground truth depth annotations. Most relevantly, Wang et al. <ref type="bibr" target="#b30">[29]</ref> uses cascaded PAM to regress matching costs in a coarse-to-fine manner, getting rid of the limitation of fixed maximum disparity in cost volume techniques. However, as Gu at al. <ref type="bibr" target="#b6">[6]</ref> pointed out, due to computational limitation, methods usually calculate matching cost at a lower resolution by the downsampled feature maps and rely on interpolation operations to generate HR disparity. Differently, they decompose the single cost volume into a cascade formulation of multiple stages for efficient HR stereo matching. Inspired by this, we achieve the HR disparity estimation in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>As shown in <ref type="figure">Figure 1</ref>, we develop a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet) in this paper. The goal of our method is to obtain SR images , of both view and relevant HR disparity maps , , from LR stereo images input , , and interact the two tasks in a mutually boosted way. In this section, we first introduce the overall insights and network architecture in Sec. 3.1. Then, we detail the novel proposed HR Disparity Information Feedback (HRDIF) mechanism in Sec. 3.2. Finally, the loss functions are presented in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SSRDE-FNet</head><p>A key to improve stereo SR is utilizing disparity for sub-pixel information registration, and a key to disparity estimation accuracy is the resolution of input features. To let these two tasks make mutually effective contribution to each other, the modeling power of both tasks are important. Thus, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet). As shown in <ref type="figure">Figure.</ref> 1, SSRDE-FNet is essentially a recurrent network with the proposed HR Disparity Information Feedback (HRDIF) mechanism. In each iteration, two SR reconstruction steps are involved. The HR disparity is achieved in a coarse-to-fine way, coarse disparity is first estimated from LR features and the finer one is estimated from the reconstructed HR features. The advantages of this method are: (1) Stereo image SR can utilize cross-view information in multi-scales since both LR and HR correspondences can be obtained, leading to more sufficient feature aggregation; (2) The coarse-to-fine manner leads to a more compact and efficient network.</p><p>Stereo Image SR Backbone We develop a lightweight stereo SR network as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a), which leverages both intraview and cross-view LR information for image reconstruction. Since hierarchical features have been demonstrated to be effective in both SISR <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b41">39]</ref> and disparity estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">8]</ref>, we are also committed to maximizing the use of hierarchical features in the model. Specifically, after a convolution layer that extracts shallow features, four RDBs <ref type="bibr" target="#b41">[39]</ref> are stacked to extract hierarchical features. Finally, we make full use of the features from all the RDBs by concatenating them and fusing them with a 1 ? 1 convolution. Meanwhile, in order to alleviate the training conflict that may suffered by directly sharing features across different tasks <ref type="bibr" target="#b27">[26]</ref> and explore more adaptive features for LR disparity estimation, a transition block is performed on and , expressed as: * = ( ), * = ( ).</p><p>Among them, and denote the extracted features, * and * denote the transformed features, and denotes the transition block (TB). As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b)), we apply a Spatial Pyramid Pooling (SPP) module in the TB for multi-scale feature extraction, which can further improve model performance.</p><p>Under LR space, we explore cross-view information by sampling disparity across the entire horizontal-range of a scene. To achieve this, bi-directional parallax attention module (biPAM <ref type="bibr" target="#b34">[32]</ref>) is adopted. In this work, it serves as both self-attention LR feature registration and coarse disparity estimation for HR disparity initialization, thus its reliability is important. However, even with deep features, matching from unaries is far from reliable. To this end, we cascade biPAMs for matching cost aggregation. Therefore, the operation of the ? biPAM can be defined as: </p><formula xml:id="formula_1">C ? = C ?1 ? + ( ? ) ? ( ? ) , C ? = C ?1 ? + ( ? ) ? ( ? ) , * , = * , ?1 + ? , * , = * , ?1 + ? ,<label>(2)</label></formula><p>where denotes two 3?3 convolutions. and are both 1 ? 1 convolution. ? is geometry-aware matrix multiplication, T is transposition operation that exchanges the last two dimensions of a matrix. Finally, the softmax is applied on C ? and C ? to generate parallax attention map M ? and M ? . Therefore, the warped feature maps ? , ? for sub-pixel registration are generated by the corresponding parallax attention map and inline occlusion inline occlusion handling <ref type="bibr" target="#b34">[32]</ref>. For each view, its own feature and the warped feature from the other view are then sent to the feature fusion module (FFM) for cross-view information aggregation. Instead of directly concatenate the two features, we build a residual based aggregation module ( <ref type="figure" target="#fig_0">Fig. 2(c)</ref>). To allow the network to concentrate on more informative features that are complementary from cross-view, we first compute the residual between the two features, and then apply a RDB <ref type="bibr" target="#b41">[39]</ref> on the residual features, the output features are then added back to the view's own feature. Take the left view as example, the operation can be defined as:</p><formula xml:id="formula_2">= ? ? , = ( ( ) + ),<label>(3)</label></formula><p>where denotes the fused features for left view and denotes the channel attention layer. Such inter-residual projection allows the network to focus only on the distinct information between feature sources while bypassing the common knowledge, enabling a more discriminative feature aggregation compared with trivial adding or concatenating. Finally, the fused features , go through the reconstruction module that has the same architecture with the feature extraction module, and a sub-pixel convolutional layer is applied to produce the HR feature , . Meanwhile, the SR images 0 , 0 are reconstructed at this step by adding the corresponding bicubic upsampled LR images:</p><formula xml:id="formula_3">0 = ( ) + ( ), 0 = ( ) + ( ).<label>(4)</label></formula><p>The main role of the two super-resolved images is to guarantee the effectiveness of the HR features , , which serve as important inputs to the subsequent HR disparity estimation module.</p><p>HR Disparity Estimation Module The downside to rely only on coarse matching is that the resulting correspondences lack fine details. Although LR correspondences have been demonstrated to benefit the stereo SR <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b31">30]</ref>, the low-level LR features limit the accuracy in correspondence matching, especially in high-frequency regions like object boundaries, which is the most important goal of SR. Thus, we suggest to also estimate the HR disparity map for more fine-gained correspondence information. To ensure the effectiveness of high-level HR features , , we connect the image reconstruction loss on the first step HR results 0 , 0 , thus the HR features , can be seen as containing the information of HR images, and serve as reliable representations for HR disparity estimation. However, directly estimating from scratch costs massive computation cost, a more efficient strategy should be adopted. We found that the disparity maps Disp and Disp regressed from the parallax attention maps M ? and M ? have relative high accuracy in most regions (see the 1 column of Tab. 4), which can be obtained as:</p><formula xml:id="formula_4">Disp = ?1 ?? =0 ? M ? (:, :, ), Disp = ?1 ?? =0 ? M ? (:, :, ),<label>(5)</label></formula><p>where is the width of the input LR image. Thus, we only construct partial cost volumes C , C based on coarse estimation and disparity residual hypotheses to achieve disparity maps with higher resolution and accuracy. As shown in <ref type="figure" target="#fig_2">Fig.6</ref>, the upsampled disparity maps ( (Disp ), (Disp )) are used as initialization of the HR disparity estimation for the left and right view, respectively. The disparity searching range can then be narrowed, we task the network of only finding a residual to add or subtract from the coarse prediction, blending in high-frequency details. Specially, we denote the disparity searching residual for each pixel in high resolution as ? . Take the left view as an example, when performing ? SR, for the ? pixel in HR space, the disparity range for building the left cost volume is [ ( (Disp ) ( ) ? ? /2, 0), ( (Disp )( ) +? /2, )]. By uniformly sampling disparity hypotheses in this range (in this work, we set = ? = 24), 3D cost volume with size ? ? can be obtained through feature correlation operation <ref type="bibr" target="#b21">[20]</ref>. To learn more context information, we aggregate the cost volume using hourglass architecture. Then through soft-argmax operation, we can regress the HR disparity Disp , Disp for both view, with higher accuracy. For occlusion handling, we use the estimated disparity maps to check the geometric consistency and estimate the valid masks to be used in the loss functions:</p><formula xml:id="formula_5">V = 1 ? ? (0.2 Disp ? (Disp , Disp ) ), V = 1 ? ? (0.2 Disp ? (Disp , Disp ) ),<label>(6)</label></formula><p>where (Disp , Disp ) represents using Disp to warp Disp . The HR disparity is in turn used to explore additional information from different views in the HR space, thus the registered HR features can be obtained by: ? = ( , Disp ), ? = <ref type="figure">Figure 4</ref>: Illustration of our HR disparity information feedback (HRDIF) mechanism. (Please zoom in for details) ( , Disp ). For HR cross-view information aggregation, the residual-based module is adopted (similar to FFM), the only difference is that an additional attention map for each view is introduced to improve the aggregation reliability. Take the left view as example, the attention map measure the similarity of and ? : = (5 1 ( ) ? 2 ( ? )), where 1 and 2 are both 3 ? 3 convolutional layers, ? is the element-wise multiplication. Therefore, the aggregated HR left features are:</p><formula xml:id="formula_6">= ( ? ? ) ? , = ( ( ) + ).<label>(7)</label></formula><p>where adaptively weights down the regions with too large difference with the original view and emphasis the regions that are favorable for providing complementary information. Similarly, we can get the aggregated right HR feature . Afterwards, better SR images can be reconstructed through , :</p><formula xml:id="formula_7">1 = ( ) + ( ), 1 = ( ) + ( ).<label>(8)</label></formula><p>This section introduces a whole feed-forward pipeline for performing the two tasks. Three stages of task interactions have been shown: Firstly, LR disparity (correspondence) promotes image SR by adding extra details. Secondly, image SR promotes HR disparity estimation accuracy by providing fine-gained HR representations. Thirdly, the more accurate disparity promotes the quality of the SR images by aggregating features in the HR space. The interactions mentioned above all act in a straightforward way, however, we intend to further explore a more essential and intrinsic connection of the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HRDIF Mechanism</head><p>The flow of information from the LR image to the final SR image is purely feed-forward in all previous stereo SR network architectures <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b39">37]</ref>, which cannot fully exploit effective high-resolution features in representing the LR to HR relation. The purely feed-forward network also makes it impossible for the HR disparity map to send useful information to the preceding low-level features, thus cannot refine these features in the SR process. To this end, we intend to project the useful information carried by the HR disparity back to preceding layers. Since the essential influence of the disparity to SR task is acting on the feature level, i.e., by registering the sup-pixel feature of two views and aggregating to obtain the enriched representations, we propose two strategies to feedback the HR disparity and act upon the feature space (Figure5, this illustration is based on the left view, the similar operation can be done on the right branch).</p><p>Firstly, the HR disparity information is embedded in the aggregated HR features , , thus we recommend to feed them back to refine the low-level features. Different from original feedback operation in <ref type="bibr" target="#b16">[16]</ref> that simply send the high-level features of the view back to low-level layer, our feedback HR features contain information both from intra-view and cross-view. To handle the spatial resolution gap, we back-project the HR features to LR space, and leverage a simple attention strategy to highlight the high-frequency regions in the downsampled features to compensate for the resolution loss. As shown in the downside branch of <ref type="figure">Fig.5</ref>, for the ? iteration, we first apply strided convolution to ?1 to obtain the back-projected feature .</p><p>= ( ?1 ).</p><p>Secondly, in order to get the high-frequency regions, we apply average pooling to , then a deconvolution layer is applied to project the feature back to original resolution, obtaining . In addition, the attention map is calculated by computing the residual between and .</p><formula xml:id="formula_9">= ( ), = ( ), = ( ? ).<label>(10)</label></formula><p>Then, the highlighted regions activated by is added to :</p><formula xml:id="formula_10">= + ( ? ),<label>(11)</label></formula><p>where is a hyper-parameter used to control the importance of the attention weights. We name this feedback operation as AHFF (Aggregated HR Feature Feedback). It is worth noting that one of the requirements that contains in a feedback system is providing an LR input at each iteration, i.e., to ensure the availability of low-level information which is needed to be refined. Thus, for the ? iteration, the LR feature ?1 from the ( ? 1) ? iteration is meant to be refined by . Instead of directly leveraging the coarse original feature ?1 , we propose the second HR disparity information feedback strategy to enrich the low-level representations. As shown in the upside of <ref type="figure">Figure.</ref>5, we first apply spatial-to-depth operation upon the , ?1 ? R ? , obtaining LR disparity cube of size R ? ? 2 . We leverage each disparity slice in the cube to warp ?1 , obtaining 2 warped feature maps of the right view. Each warped feature map is concatenated with the same left feature ?1 , and each concatenated feature map is going through a residual block and a 1 ? 1 convolution for fusion. Finally, we sum up the 2 fused LR feature maps to get ?1 . The operation can be defined as:</p><formula xml:id="formula_11">?1 = 2 ?? =0 ( ( ( ?1 , ?1, ? ))).<label>(12)</label></formula><p>We name this strategy as LRE (Low-level Representations Enrichment). Finally, ?1 and are concatenated and fused to reduce the channel back to the same with ?1 , and the new LR feature for the new iteration is generated according to:</p><formula xml:id="formula_12">= ( ( ?1 , )).<label>(13)</label></formula><p>In this way, the low-level features carry information from the HR disparity, and this feature enhancement dose favor to the whole pipeline right from the beginning. Finally, we adopt the last SR output as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Functions</head><p>Since our work aims to achieve stereo SR and disparity estimation simultaneously, we set loss constraints for both tasks. Note that we learn the disparity in an unsupersived manner and do not use groundtruth (GT) disparities during the training phase. We introduce SR loss L , biPAM loss L , and disparity loss L to train our network. The overall loss function of our network is defined as:</p><formula xml:id="formula_13">L = L + 1 L + 2 L ,<label>(14)</label></formula><p>where both 1 and 2 are set to 0.1 in this work. SR Loss. The SR loss is essentially an 1 loss function that is used to measure the difference between the SR images and GT images, i.e., for T iterations,</p><formula xml:id="formula_14">L = ?? =0 ? SR ,0 ? HR ? 1 + ? SR ,0 ? HR ? 1 + ? SR ,1 ? HR ? 1 + ? SR ,1 ? HR ? 1 ,<label>(15)</label></formula><p>where SR and SR represent the restored left and right images, and HR and HR represent their corresponding HR images. BiPAM Loss. We formulate the BiPAM loss as a combination of photometric, smoothness, cycle and consistency terms, connecting to bi-directional parallax-attention maps M ? , M ? , t=1,...,T. That is, L = L ? + L + L ? + L . The loss is employed in a residual manner <ref type="bibr" target="#b34">[32]</ref> to overcome illuminance variation. Please refer to <ref type="bibr" target="#b34">[32]</ref> for details. Disparity Loss. Besides tying loss on the parallax-attention maps, we also enforce direct constraints on all the estimated disparity maps, namely , , , , , , , for = 1, ..., . We first penalize the reconstruction loss on HR images using each disparity map (LR disparity upsamples to the same size of HR images), for the left view,</p><formula xml:id="formula_15">L = 1 = ?? ?V , =1 1 ? S (HR ( ), HR ? ( )) 2 + (1 ? ) HR ( ) ? HR ? ( ) 1 , = 1, ..., ,<label>(16)</label></formula><p>where HR ? = (HR , Disp , ). S is a structural similarity index (SSIM) function, represents a valid pixel in the valid mask, is the number of valid pixels, and is empirically set to 0.85. The loss for the right view is also calculated as the similar method.</p><p>Moreover, we constrain edge-aware smoothness loss on HR disparity, which is defined as:</p><formula xml:id="formula_16">L = 1 ?? p?V ? D , (p) 1 ? ?? HR (p) ? 1 + ? D , (p) 1 ? ?? HR (p) ? 1 , = 1, ..., ,<label>(17)</label></formula><p>where ? and ? are gradients in the and directions respectively. Finally, residual based cycle and consistency losses <ref type="bibr" target="#b34">[32]</ref> are also used to constrain HR disparity maps. The total disparity loss can be written as: L = L + L + L + 0.1 * L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Settings</head><p>Following iPASSR <ref type="bibr" target="#b34">[32]</ref>, we adopt 60 Middlebury images and 800 images from Flickr1024 <ref type="bibr" target="#b32">[31]</ref> as the training dataset during training. For images from the Middlebury dataset, we followed <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b39">37]</ref> to perform bicubic downsampling by a factor of 2 to generate HR ground truth images to match the spatial resolution of Flickr1024 dataset. To produce LR images, we downscale the HR images on particular scaling factors by using the bicubic operation and then cropped 30 ? 90 patches with a stride of 20 as input samples. Our network was implemented using PyTorch and trained on NVIDIA V100 GPU. All models were optimized by the Adam <ref type="bibr" target="#b11">[11]</ref> with 1 = 0.9 and 2 = 0.999. The batch size is set to 16, the initial learning rate is set to 2 ? 10 ?4 and reduced to half after every 30 epochs.</p><p>To evaluate SR results, 20 images from KITTI 2012 <ref type="bibr" target="#b5">[5]</ref>, 20 images from KITTI 2015 <ref type="bibr" target="#b22">[21]</ref>, 5 images from Middlebury, and 112 images from Flickr1024 are utilized as the test dataset. For fair comparison with <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b39">37]</ref>, we followed these methods to calculate peak signalto-noise ratio (PSNR) and structural similarity (SSIM) scores on the left views with their left boundaries (64 pixels) being cropped, and these metrics were calculated on RGB color space. Moreover, to comprehensively evaluate the quality of the reconstructed stereo SR image, we also report the average PSNR and SSIM scores on stereo image pairs (i.e., (Left + Right) /2) without any boundary cropping. Meanwhile, in order to evaluate disparity estimation results, we apply the end-point-error (EPE) in both non-occluded region (NOC) and all (ALL) pixels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with SOTA Methods</head><p>We compare SSRDE-FNet with several state-of-the-art methods, including four SISR methods(VDSR, EDSR, RDN, and RCAN) and five stereo image SR methods (i.e., StereoSR, PASSRnet, SRResNet+SAM, IMSSRnet, and iPASSR). Moreover, to achieve fair comparison with SISR methods, we retrained these methods on the same training datasets as our method.</p><p>Quantitative Evaluations: In <ref type="table" target="#tab_2">Table 1</ref>, we show the quantitative comparisons with these SR methods. Among both SISR and stereo image SR methods, our FSSRHD-net achieves the best results on all datasets and upsampling factors (?2, ?4). This fully demonstrates the effectiveness and advancement of the proposed SSRDE-FNet.</p><p>Visual Comparison: In <ref type="figure" target="#fig_2">Figures 5 and 6</ref>, we show the visual comparisons on ?2 and ?4, respectively. According to the figure, we can clearly observe that most compared SR methods cannot recover clear and right image edges. In contrast, our SSRDE-FNet can reconstruct high-quality SR images with rich details and clear edges. This further validates the effectiveness of our SSRDE-FNet.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In order to verify the effectiveness of the proposed mutually boost strategies, we designed a series of ablation experiments. In addition, all ablation studies are conducted on the ?4 stereo image SR task. It is worth noting that the baseline model does not use the HR disparity estimation mechanism and the feedback strategy. This means that the baseline model has only one step of SR reconstruction, as shown in Figure2. Effectiveness of HR disparity estimation boost SR 1)Effectiveness of the HR disparity estimation method. In order to verify that the feature aggregation by the HR disparity in HR space benefits the SR performance, we designed three models, including "baseline", "baseline+ Up disp", and "baseline + HR disp". Among them, "baseline+ Up disp" means that the high-resolution disparity directly achieved by the interpolation operation and "base-line+ HR disp" represents our proposed method. Meanwhile, all of these three model are in purely feed-forward manner. The PSNR and SSIM results are presented in <ref type="table" target="#tab_4">Table 2</ref>. According to these results, we can draw the following conclusions: <ref type="bibr" target="#b0">(1)</ref>. high-resolution disparity can effectively improve the quality of the reconstructed SR images; <ref type="bibr" target="#b1">(2)</ref>. the more precise disparity can bring higher performance improvement; (3) the high-resolution disparity provided by our method enables the model to achieve the best results.</p><p>2) Effectiveness of the HR disparity information feedback mechanism (HRDIF): To verify that the HR disparity truly contribute to stereo SR in the HRDIF mechanism, but not just the original feedback operation that plays a major role, we compare two models that both have the feedback operation. The variant removes the HR disparity estimation model, directly use the and as the high-level features to feedback. We name this variant as SSR-FNet (Stereo SR Feedback Network), which also means adding HR Feature Feedback (HFF) to the baseline. The feedback manner in the variant is just concatenating the down-projected HR feature and the low-level features of the last iteration. Although noticeable improvement can be observed, the PSNR drops 0.11 dB as compared to our SSRDE-FNet. The experiment indicates that our method does benefit from the HR disparity information feedback mechanism, instead of only rely on the power of the original feedback structure. Moreover, to verify the effectiveness of strategy of the low-level representations enhancement (LRE) in HRDIF, we remove this operation and directly concatenate ?1 and for the ? iteration, a slight PSNR drop can be observed.    3) SR performance improvements in a single inference: As mentioned, each iteration of SSRDE-FNet contains two SR reconstruction steps. In our experiments, we iterate the network twice (T=2) to balance the efficiency and performance. We then compare the PSNR values of all intermediate SR images. The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Each intermediate result outperforms the former one, and the final result achieves a PSNR gain of 0.22dB over the first result. This demonstrates that the HR disparity surely benefits the information flow across time.</p><p>Effectiveness of SR boost disparity estimation 1) Comparison of disparity accuracy: We compare the estimated HR disparity and upsampled disparity of the baseline to the ground truth on the KITTI2012 and KITTI2015 datasets, shown in <ref type="table">Table.</ref>4. We also include the disparity regressed from two stereo SR methods for comparison, including PASSRnet and iPASSR. The disparity maps estimated from LR stereo images are upsampled for fair evaluation. Even using our baseline, our disparity EPE error is obviously lower than that of other state-of-the-art stereo SR methods. By interacting stereo SR task and disparity estimation task in our network, the final HR disparity become much more accurate as compared to the straightforward baseline, with about 2 ? 3 pixel EPE error drop. A visualization disparity result is shown in <ref type="figure" target="#fig_3">Figure.7</ref>.</p><p>2) The disparity accuracy improvements within a single inference of SSRDE-FNet: To show the changing process of the disparity estimation accuracy, we calculate the EPE error on each intermediate disparity estimation in a single inference process of SSRDE-FNet.</p><p>The mean EPE error change in KITTI 2012 and KITTI 2015 are shown in Tab. 5. It can be observed that in each iteration, the estimated HR disparity (step2) has 0.5 ? 0.6 pixel EPE error drop compared to the coarse estimation (step1). More obvious disparity accuracy improvements can be achieved after the HRDIF, since the low-level features are refined and lead to better disparity accuracy right from the LR space. The results above demonstrate that both stereo SR and disparity estimation are improved along time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we propose to explore the mutually boosted property of stereo image super-resolution and high-resolution disparity estimation, and build a novel end-to-end deep learning framework, namely SSRDE-FNet. Our model is essentially a feedback network with a proposed HR Disparity Information Feedback (HRDIF) mechanism. By fully interacting the two tasks and making guidance to each other, we achieve to improve both tasks during a single inference. Experiments have demonstrated our state-of-the-art stereo SR performance and the disparity estimation improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the proposed SR backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of HR disparity estimation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results (?4) on image "testing 2" from Flickr1024 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Visualization result of the disparity map on KITTI 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>School of Computer Science and Technology, East China Normal University, Shanghai, China ? 51194506008@stu.ecnu.edu.cn * fmfang@cs.ecnu.edu.cn</figDesc><table /><note>1? = (* , ?1 ),? = (* , ?1 ),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>School of Computer Science and Technology, East China Normal University, Shanghai, China ? 51194506008@stu.ecnu.edu.cn * fmfang@cs.ecnu.edu.cn</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results achieved by different methods on the KITTI 2012, KITTI 2015, Middlebury, and Flickr1024 datasets. # represents the number of parameters of the networks. Here, PSNR/SSIM values achieved on both the left images (i.e., Left) and a pair of stereo images (i.e., (Left + Right) /2) are reported. The best results are in bold faces and the second best results are underlined.</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell>#</cell><cell></cell><cell></cell><cell></cell><cell>Left</cell><cell></cell><cell></cell><cell></cell><cell>(Left + Right) /2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">KITTI 2012</cell><cell cols="2">KITTI 2015</cell><cell>Middlebury</cell><cell cols="2">KITTI 2012</cell><cell>KITTI 2015</cell><cell>Middlebury</cell><cell>Flickr1024</cell></row><row><cell>VDSR</cell><cell>?2</cell><cell cols="3">0.66M 30.17/0.9062</cell><cell cols="2">28.99/0.9038</cell><cell>32.66/0.9101</cell><cell cols="2">30.30/0.9089</cell><cell>29.78/0.9150</cell><cell>32.77/0.9102</cell><cell>25.60/0.8534</cell></row><row><cell>EDSR</cell><cell>?2</cell><cell cols="3">38.6M 30.83/0.9199</cell><cell cols="2">29.94/0.9231</cell><cell>34.84/0.9489</cell><cell cols="2">30.96/0.9228</cell><cell>30.73/0.9335</cell><cell>34.95/0.9492</cell><cell>28.66/0.9087</cell></row><row><cell>RDN</cell><cell>?2</cell><cell cols="3">22.0M 30.81/0.9197</cell><cell cols="2">29.91/0.9224</cell><cell>34.85/0.9488</cell><cell cols="2">30.94/0.9227</cell><cell>30.70/0.9330</cell><cell>34.94/0.9491</cell><cell>28.64/0.9084</cell></row><row><cell>RCAN</cell><cell>?2</cell><cell cols="3">15.3M 30.88/0.9202</cell><cell cols="2">29.97/0.9231</cell><cell>34.80/0.9482</cell><cell cols="2">31.02/0.9232</cell><cell>30.77/0.9336</cell><cell>34.90/0.9486</cell><cell>28.63/0.9082</cell></row><row><cell>StereoSR</cell><cell>?2</cell><cell cols="3">1.08M 29.42/0.9040</cell><cell cols="2">28.53/0.9038</cell><cell>33.15/0.9343</cell><cell cols="2">29.51/0.9073</cell><cell>29.33/0.9168</cell><cell>33.23/0.9348</cell><cell>25.96/0.8599</cell></row><row><cell>PASSRnet</cell><cell>?2</cell><cell cols="3">1.37M 30.68/0.9159</cell><cell cols="2">29.81/0.9191</cell><cell>34.13/0.9421</cell><cell cols="2">30.81/0.9190</cell><cell>30.60/0.9300</cell><cell>34.23/0.9422</cell><cell>28.38/0.9038</cell></row><row><cell>IMSSRnet</cell><cell>?2</cell><cell>6.84M</cell><cell cols="2">30.90/-</cell><cell></cell><cell>29.97/-</cell><cell>34.66/-</cell><cell>30.92/-</cell><cell></cell><cell>30.66/-</cell><cell>34.67/-</cell><cell>-/-</cell></row><row><cell>iPASSR</cell><cell>?2</cell><cell cols="3">1.37M 30.97/0.9210</cell><cell cols="2">30.01/0.9234</cell><cell>34.41/0.9454</cell><cell cols="2">31.11/0.9240</cell><cell>30.81/0.9340</cell><cell>34.51/0.9454</cell><cell>28.60/0.9097</cell></row><row><cell>SSRDE-FNet (ours)</cell><cell>?2</cell><cell cols="9">2.10M 31.08/0.9224 30.10/0.9245 35.02/0.9508 31.23/0.9254 30.90/0.9352 35.09/0.9511 28.85/0.9132</cell></row><row><cell>VDSR</cell><cell>?4</cell><cell cols="3">0.66M 25.54/0.7662</cell><cell cols="2">24.68/0.7456</cell><cell>27.60/0.7933</cell><cell cols="2">25.60/0.7722</cell><cell>25.32/0.7703</cell><cell>27.69/0.7941</cell><cell>22.46/0.6718</cell></row><row><cell>EDSR</cell><cell>?4</cell><cell cols="3">38.9M 26.26/0.7954</cell><cell cols="2">25.38/0.7811</cell><cell>29.15/0.8383</cell><cell cols="2">26.35/0.8015</cell><cell>26.04/0.8039</cell><cell>29.23/0.8397</cell><cell>23.46/0.7285</cell></row><row><cell>RDN</cell><cell>?4</cell><cell cols="3">22.0M 26.23/0.7952</cell><cell cols="2">25.37/0.7813</cell><cell>29.15/0.8387</cell><cell cols="2">26.32/0.8014</cell><cell>26.04/0.8043</cell><cell>29.27/0.8404</cell><cell>23.47/0.7295</cell></row><row><cell>RCAN</cell><cell>?4</cell><cell cols="3">15.4M 26.36/0.7968</cell><cell cols="2">25.53/0.7836</cell><cell>29.20/0.8381</cell><cell cols="2">26.44/0.8029</cell><cell>26.22/0.8068</cell><cell>29.30/0.8397</cell><cell>23.48/0.7286</cell></row><row><cell>StereoSR</cell><cell>?4</cell><cell cols="3">1.42M 24.49/0.7502</cell><cell cols="2">23.67/0.7273</cell><cell>27.70/0.8036</cell><cell cols="2">24.53/0.7555</cell><cell>24.21/0.7511</cell><cell>27.64/0.8022</cell><cell>21.70/0.6460</cell></row><row><cell>PASSRnet</cell><cell>?4</cell><cell cols="3">1.42M 26.26/0.7919</cell><cell cols="2">25.41/0.7772</cell><cell>28.61/0.8232</cell><cell cols="2">26.34/0.7981</cell><cell>26.08/0.8002</cell><cell>28.72/0.8236</cell><cell>23.31/0.7195</cell></row><row><cell>SRRes+SAM</cell><cell>?4</cell><cell cols="3">1.73M 26.35/0.7957</cell><cell cols="2">25.55/0.7825</cell><cell>28.76/0.8287</cell><cell cols="2">26.44/0.8018</cell><cell>26.22/0.8054</cell><cell>28.83/0.8290</cell><cell>23.27/0.7233</cell></row><row><cell>IMSSRnet</cell><cell>?4</cell><cell>6.89M</cell><cell cols="2">26.44/-</cell><cell></cell><cell>25.59/-</cell><cell>29.02/-</cell><cell>26.43/-</cell><cell></cell><cell>26.20/-</cell><cell>29.02/-</cell><cell>-/-</cell></row><row><cell>iPASSR</cell><cell>?4</cell><cell cols="3">1.42M 26.47/0.7993</cell><cell cols="2">25.61/0.7850</cell><cell>29.07/0.8363</cell><cell cols="2">26.56/0.8053</cell><cell>26.32/0.8084</cell><cell>29.16/0.8367</cell><cell>23.44/0.7287</cell></row><row><cell>SSRDE-FNet (ours)</cell><cell>?4</cell><cell cols="9">2.24M 26.61/0.8028 25.74/0.7884 29.29/0.8407 26.70/0.8082 26.43/0.8118 29.38/0.8411 23.59/0.7352</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Left</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR(dB):</cell><cell>32.71</cell><cell>32.71</cell><cell>32.68</cell><cell></cell><cell>30.21</cell><cell>32.47</cell><cell>32.76</cell><cell>33.35</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR(dB):</cell><cell>32.95</cell><cell>32.96</cell><cell>32.87</cell><cell></cell><cell>30.23</cell><cell>32.51</cell><cell>32.77</cell><cell>33.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR</cell><cell></cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell cols="2">StereoSR</cell><cell>PASSRnet</cell><cell>iPASSR</cell><cell>Ours</cell></row><row><cell></cell><cell cols="10">Figure 5: Qualitative results (?2) on image "motorcycle" from Middlebury dataset.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Right Left</cell><cell cols="2">PSNR(dB):</cell><cell>29.44</cell><cell>29.45</cell><cell>29.48</cell><cell></cell><cell>28.75</cell><cell>28.89</cell><cell>29.45</cell><cell>29.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR(dB):</cell><cell>29.47</cell><cell>29.50</cell><cell>29.44</cell><cell></cell><cell>27.48</cell><cell>28.96</cell><cell>29.55</cell><cell>30.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR</cell><cell></cell><cell>EDSR</cell><cell>RDN</cell><cell cols="3">RCAN SRResNet+SAM PASSRnet</cell><cell>iPASSR</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>School of Computer Science and Technology, East China Normal University, Shanghai, China ? 51194506008@stu.ecnu.edu.cn * fmfang@cs.ecnu.edu.cn</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on different settings of SSRDE-FNet on Middlebury. The average PSNR and SSIM score of the SR left and right images are shown.</figDesc><table><row><cell>Method</cell><cell cols="4">Disparity method HRDIF HFF PSNR/SSIM</cell></row><row><cell></cell><cell cols="3">Up disp HR disp AHFF LRE</cell><cell></cell></row><row><cell>baseline</cell><cell></cell><cell></cell><cell></cell><cell>29.16/0.8361</cell></row><row><cell>baseline + Up disp</cell><cell>?</cell><cell></cell><cell></cell><cell>29.20/0.8370</cell></row><row><cell>baseline + HR disp</cell><cell>?</cell><cell></cell><cell></cell><cell>29.27/0.8383</cell></row><row><cell>SSR-FNet</cell><cell></cell><cell></cell><cell></cell><cell>? 29.27/0.8385</cell></row><row><cell>SSRDE-FNet w/o LRE</cell><cell>?</cell><cell>?</cell><cell></cell><cell>29.35/0.8407</cell></row><row><cell>SSRDE-FNet (Ours)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>29.38/0.8411</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The PSNR changing of intermediate SR outputs on Middlebury.</figDesc><table><row><cell></cell><cell cols="2">Iteration 1</cell><cell cols="2">Iteration 2</cell></row><row><cell></cell><cell>Step 1</cell><cell>Step 2</cell><cell>Step 1</cell><cell>Step 2</cell></row><row><cell>Middlebury</cell><cell>29.16</cell><cell>29.25</cell><cell>29.32</cell><cell>29.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average disparity EPE errors (lower is better) on KITTI 2012 and KITTI 2015 for 4? SR. Best results are shown in boldface.</figDesc><table><row><cell></cell><cell></cell><cell>Baseline</cell><cell>Estimated HR</cell><cell>PASSRnet</cell><cell>iPASSR</cell></row><row><cell></cell><cell></cell><cell>disparity</cell><cell>disparity</cell><cell>[30]</cell><cell>[32]</cell></row><row><cell>KITTI 2012</cell><cell>Noc All</cell><cell>6.72 7.81</cell><cell>3.90 5.12</cell><cell>11.33 12.29</cell><cell>7.88 8.96</cell></row><row><cell>KITTI 2015</cell><cell>Noc All</cell><cell>5.71 6.38</cell><cell>3.52 4.28</cell><cell>9.36 9.91</cell><cell>6.57 7.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Disparity accuracy improvements across inference time on KITTI 2012 and KITTI 2015 dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Iteration 1</cell><cell cols="2">Iteration 2</cell></row><row><cell></cell><cell></cell><cell cols="4">Step 1 Step 2 Step 1 Step 2</cell></row><row><cell>KITTI 2012</cell><cell>Noc ALL</cell><cell>7.13 8.14</cell><cell>6.50 7.53</cell><cell>4.59 5.79</cell><cell>3.90 5.12</cell></row><row><cell>KITTI 2015</cell><cell>Noc ALL</cell><cell>6.98 7.60</cell><cell>6.47 7.11</cell><cell>4.06 4.81</cell><cell>3.52 4.28</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Resolution Enhancement in Multi-Image Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhavsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1721" to="1728" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">StereoDRNet: Dilated Residual StereoNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohan Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11778" to="11787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pyramid Stereo Matching Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a Deep Convolutional Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feitong Tan, and Ping Tan. 2020. Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<imprint>
			<biblScope unit="page" from="2492" to="2501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancing the Spatial Resolution of Stereo Images Using a Parallax Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung-Hwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Geometry and Context for Deep Stereo Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous Super-Resolution of Depth and Images Using a Single Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Stereoscopic Image Super-Resolution via Interaction Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoting</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xin-Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Occlusion Aware Stereo Matching via Cooperative Unsupervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-scale Residual Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangfu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guixu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feedback Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3862" to="3871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Learning for Disparity Estimation Through Feature Constancy</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient Deep Learning for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-Level Context Ultra-Aggregation for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Yu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3278" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining multi-view stereo and super resolution in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haesol</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<meeting>The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Progressive Fusion for Unsupervised Binocular Depth Estimation Using Cycled Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>St?phane Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mihai Marian Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2380" to="2395" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-Task Learning as Multi-Objective Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stereoscopic Image Super-Resolution with Stereo Consistent Feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonil</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somi</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jungang Yang, and Wei An. 2020. Parallax Attention for Unsupervised Stereo Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Parallax Attention for Stereo Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12242" to="12251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3852" to="3857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Symmetric Parallax Attention for Stereo Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<idno>ArXiv abs/2011.03802</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AANet: Adaptive Aggregation Network for Efficient Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1956" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Bilateral Learning for Stereo Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpu</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="613" to="617" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disparity-Aware Domain Adaptation in Stereo Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahetiyaer</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13176" to="13184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SegStereo: Exploiting Semantic Information for Disparity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Stereo Attention Module for Stereo Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GA-Net: Guided Aggregation Net for End-To-End Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual Dense Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1576" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
