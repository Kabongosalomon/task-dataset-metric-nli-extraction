<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 TACKLING THE GENERATIVE LEARNING TRILEMMA WITH DENOISING DIFFUSION GANS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
							<email>zxiao@uchicago.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
							<email>kkreis@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 TACKLING THE GENERATIVE LEARNING TRILEMMA WITH DENOISING DIFFUSION GANS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000? faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively. Project page and code: https://nvlabs.github.io/denoising-diffusion-gan. * Work done during an internship at NVIDIA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past decade, a plethora of deep generative models has been developed for various domains such as images <ref type="bibr">(Karras et al., 2019;</ref><ref type="bibr" target="#b45">Razavi et al., 2019)</ref>, audio <ref type="bibr" target="#b40">(Oord et al., 2016a;</ref><ref type="bibr">Kong et al., 2021)</ref>, point clouds  and graphs <ref type="bibr" target="#b8">(De Cao &amp; Kipf, 2018)</ref>. However, current generative learning frameworks cannot yet simultaneously satisfy three key requirements, often needed for their wide adoption in real-world problems. These requirements include (i) high-quality sampling, (ii) mode coverage and sample diversity, and (iii) fast and computationally inexpensive sampling. For example, most current works in image synthesis focus on high-quality generation. However, mode coverage and data diversity are important for better representing minorities and for reducing the negative social impacts of generative models. Additionally, applications such as interactive image editing or real-time speech synthesis require fast sampling. Here, we identify the challenge posed by these requirements as the generative learning trilemma, since existing models usually compromise between them. <ref type="figure" target="#fig_0">Fig. 1</ref> summarizes how mainstream generative frameworks tackle the trilemma. Generative adversarial networks (GANs) <ref type="bibr" target="#b18">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b7">Brock et al., 2018)</ref> generate high-quality samples rapidly, but they have poor mode coverage <ref type="bibr" target="#b49">(Salimans et al., 2016;</ref><ref type="bibr" target="#b72">Zhao et al., 2018)</ref>. Conversely, variational autoencoders (VAEs) <ref type="bibr">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b46">Rezende et al., 2014)</ref> and normalizing flows <ref type="bibr" target="#b11">(Dinh et al., 2016;</ref><ref type="bibr">Kingma &amp; Dhariwal, 2018</ref>) cover data modes faithfully, but they often suffer from low sample quality. Recently, diffusion models <ref type="bibr" target="#b54">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b23">Ho et al., 2020;</ref><ref type="bibr" target="#b59">Song et al., 2021c)</ref> have emerged as powerful generative models. They demonstrate surprisingly good results in sample quality, beating GANs in image generation . They also obtain good mode coverage, indicated by high likelihood <ref type="bibr" target="#b16">Kingma et al., 2021;</ref><ref type="bibr" target="#b25">Huang et al., 2021)</ref>. Although diffusion models have been applied to a variety of tasks <ref type="bibr">(Dhariwal &amp; Nichol; Austin et al.; Mittal et al.; Luo &amp; Hu)</ref>, sampling from them often requires thousands of network evaluations, making their application expensive in practice.</p><p>In this paper, we tackle the generative learning trilemma by reformulating denoising diffusion models specifically for fast sampling while maintaining strong mode coverage and sample quality. We investigate the slow sampling issue of diffusion models and we observe that diffusion models commonly assume that the denoising distribution can be approximated by Gaussian distributions. However, it is known that the Gaussian assumption holds only in the infinitesimal limit of small denoising steps <ref type="bibr" target="#b54">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b15">Feller, 1949)</ref>, which leads to the requirement of a large number of steps in the reverse process. When the reverse process uses larger step sizes (i.e., it has fewer denoising steps), we need a non-Gaussian multimodal distribution for modeling the denoising distribution. Intuitively, in image synthesis, the multimodal distribution arises from the fact that multiple plausible clean images may correspond to the same noisy image.</p><p>Inspired by this observation, we propose to parametrize the denoising distribution with an expressive multimodal distribution to enable denoising for large steps. In particular, we introduce a novel generative model, termed as denoising diffusion GAN, in which the denoising distributions are modeled with conditional GANs. In image generation, we observe that our model obtains sample quality and mode coverage competitive with diffusion models, while taking only as few as two denoising steps, achieving about 2000? speed-up in sampling compared to the predictor-corrector sampling by <ref type="bibr" target="#b59">Song et al. (2021c)</ref> on CIFAR-10. Compared to traditional GANs, we show that our model significantly outperforms state-of-the-art GANs in sample diversity, while being competitive in sample fidelity.</p><p>In summary, we make the following contributions: i) We attribute the slow sampling of diffusion models to the Gaussian assumption in the denoising distribution and propose to employ complex, multimodal denoising distributions. ii) We propose denoising diffusion GANs, a diffusion model whose reverse process is parametrized by conditional GANs. iii) Through careful evaluations, we demonstrate that denoising diffusion GANs achieve several orders of magnitude speed-up compared to current diffusion models for both image generation and editing. We show that our model overcomes the deep generative learning trilemma to a large extent, making diffusion models for the first time applicable to interactive, real-world applications at a low computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In diffusion models <ref type="bibr" target="#b54">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref>, there is a forward process that gradually adds noise to the data x 0 ? q(x 0 ) in T steps with pre-defined variance schedule ? t :</p><formula xml:id="formula_0">q(x 1:T |x 0 ) = t?1 q(x t |x t?1 ), q(x t |x t?1 ) = N (x t ; 1 ? ? t x t?1 , ? t I),<label>(1)</label></formula><p>where q(x 0 ) is a data-generating distribution. The reverse denoising process is defined by:</p><formula xml:id="formula_1">p ? (x 0:T ) = p(x T ) t?1 p ? (x t?1 |x t ), p ? (x t?1 |x t ) = N (x t?1 ; ? ? (x t , t), ? 2 t I),<label>(2)</label></formula><p>where ? ? (x t , t) and ? 2 t are the mean and variance for the denoising model and ? denotes its parameters. The goal of training is to maximize the likelihood p ? (x 0 ) = p ? (x 0:T )dx 1:T , by maximizing the evidence lower bound (ELBO, L ? log p ? (x 0 )). The ELBO can be written as matching the true denoising distribution q(x t?1 |x t ) with the parameterized denoising model p ? (x t?1 |x t ) using:</p><formula xml:id="formula_2">L = ? t?1 E q(xt) [D KL (q(x t?1 |x t ) p ? (x t?1 |x t ))] + C,<label>(3)</label></formula><p>where C contains constant terms that are independent of ? and D KL denotes the Kullback-Leibler (KL) divergence. The objective above is intractable due to the unavailability of q(x t?1 |x t ). Instead,</p><formula xml:id="formula_3">t xt xt q(x0) q(x1) q(x2) q(x3) q(x4) q(x5) q(x0|x5=X) q(x1|x5=X) q(x2|x5=X) q(x3|x5=X) q(x4|x5=X) x5=X</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marginal Diffused Data Distributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True Denoising Distributions</head><p>Figure 2: Top: The evolution of 1D data distribution q(x 0 ) through the diffusion process. Bottom:, The visualization of the true denoising distribution for varying step sizes conditioned on a fixed x 5 . The true denoising distribution for a small step size (i.e., q(x 4 |x 5 = X)) is close to a Gaussian distribution. However, it becomes more complex and multimodal as the step size increases. <ref type="bibr" target="#b54">-Dickstein et al. (2015)</ref> show that L can be written in an alternative form with tractable distributions (see Appendix A of <ref type="bibr" target="#b23">Ho et al. (2020)</ref> for details). <ref type="bibr" target="#b23">Ho et al. (2020)</ref> show the equivalence of this form with score-based models trained with denoising score matching <ref type="bibr" target="#b56">(Song &amp; Ermon, 2019;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sohl</head><p>Two key assumptions are commonly made in diffusion models: First, the denoising distribution p ? (x t?1 |x t ) is modeled with a Gaussian distribution. Second, the number of denoising steps T is often assumed to be in the order of hundreds to thousands of steps. In this paper, we focus on discrete-time diffusion models. In continuous-time diffusion models <ref type="bibr" target="#b59">(Song et al., 2021c)</ref>, similar assumptions are also made at the sampling time when discretizing time into small timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DENOISING DIFFUSION GANS</head><p>We first discuss why reducing the number of denoising steps requires learning a multimodal denoising distribution in Sec. 3.1. Then, we present our multimodal denoising model in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MULTIMODAL DENOISING DISTRIBUTIONS FOR LARGE DENOISING STEPS</head><p>As we discussed in Sec. 2, a common assumption in the diffusion model literature is to approximate q(x t?1 |x t ) with a Gaussian distribution. Here, we question when such an approximation is accurate.</p><p>The true denoising distribution q(x t?1 |x t ) can be written as q(x t?1 |x t ) ? q(x t |x t?1 )q(x t?1 ) using Bayes' rule where q(x t |x t?1 ) is the forward Gaussian diffusion shown in Eq. 1 and q(x t?1 ) is the marginal data distribution at step t. It can be shown that in two situations the true denoising distribution takes a Gaussian form. First, in the limit of infinitesimal step size ? t , the product in the Bayes' rule is dominated by q(x t |x t?1 ) and the reversal of the diffusion process takes an identical functional form as the forward process <ref type="bibr" target="#b15">(Feller, 1949)</ref>. Thus, when ? t is sufficiently small, since q(x t |x t?1 ) is a Gaussian, the denoising distribution q(x t?1 |x t ) is also Gaussian, and the approximation used by current diffusion models can be accurate. To satisfy this, diffusion models often have thousands of steps with small ? t . Second, if data marginal q(x t ) is Gaussian, the denoising distribution q(x t?1 |x t ) is also a Gaussian distribution. The idea of bringing data distribution q(x 0 ) and consequently q(x t ) closer to Gaussian using a VAE encoder was recently explored in LSGM . However, the problem of transforming the data to Gaussian itself is challenging and VAE encoders cannot solve it perfectly. That is why LSGM still requires tens to hundreds of steps on complex datasets.</p><p>In this paper, we argue that when neither of the conditions are met, i.e., when the denoising step is large and the data distribution is non-Gaussian, there are no guarantees that the Gaussian assumption on the denoising distribution holds. To illustrate this, in <ref type="figure">Fig. 2</ref>, we visualize the true denoising distribution for different denoising step sizes for a multimodal data distribution. We see that as the denoising step gets larger, the true denoising distribution becomes more complex and multimodal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODELING DENOISING DISTRIBUTIONS WITH CONITIONAL GANS</head><p>Our goal is to reduce the number of denoising diffusion steps T required in the reverse process of diffusion models. Inspired by the observation above, we propose to model the denoising distribution with an expressive multimodal distribution. Since conditional GANs have been shown to model complex conditional distributions in the image domain <ref type="bibr" target="#b34">(Mirza &amp; Osindero, 2014;</ref><ref type="bibr">Ledig et al., 2017;</ref><ref type="bibr" target="#b27">Isola et al., 2017)</ref>, we adopt them to approximate the true denoising distribution q(x t?1 |x t ).</p><p>Specifically, our forward diffusion is set up similarly to the diffusion models in Eq. 1 with the main assumption that T is assumed to be small (T ? 8) and each diffusion step has larger ? t . Our training is formulated by matching the conditional GAN generator p ? (x t?1 |x t ) and q(x t?1 |x t ) using an adversarial loss that minimizes a divergence D adv per denoising step:</p><formula xml:id="formula_4">min ? t?1 E q(xt) [D adv (q(x t?1 |x t ) p ? (x t?1 |x t ))] ,<label>(4)</label></formula><p>where D adv can be Wasserstein distance, Jenson-Shannon divergence, or f-divergence depending on the adversarial training setup <ref type="bibr" target="#b18">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b39">Nowozin et al., 2016)</ref>.</p><p>In this paper, we rely on non-saturating GANs <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref>, that are widely used in successful GAN frameworks such as StyleGANs <ref type="bibr">(Karras et al., 2019;</ref><ref type="bibr">2020b)</ref>. In this case, D adv takes a special instance of f-divergence called softened reverse KL <ref type="bibr" target="#b52">(Shannon et al., 2020)</ref>, which is different from the forward KL divergence used in the original diffusion model training in Eq. 3 1 .</p><p>To set up the adversarial training, we denote the time-dependent discriminator as</p><formula xml:id="formula_5">D ? (x t?1 , x t , t) : R N ? R N ? R ? [0, 1], with parameters ?.</formula><p>It takes the N -dimensional x t?1 and x t as inputs, and decides whether x t?1 is a plausible denoised version of x t . The discriminator is trained by:</p><formula xml:id="formula_6">min ? t?1 E q(xt) E q(xt?1|xt) [?log(D ? (x t?1 , x t , t)] + E p ? (xt?1|xt) [?log(1?D ? (x t?1 , x t , t))] ,<label>(5)</label></formula><p>where fake samples from p ? (x t?1 |x t ) are contrasted against real samples from q(x t?1 |x t ). The first expectation requires sampling from q(x t?1 |x t ) which is unknown. However, we use the identity</p><formula xml:id="formula_7">q(x t , x t?1 ) = dx 0 q(x 0 )q(x t , x t?1 |x 0 ) = dx 0 q(x 0 )q(x t?1 |x 0 )q(x t |x t?1 )</formula><p>to rewrite the first expectation in Eq. 5 as:</p><formula xml:id="formula_8">E q(xt)q(xt?1|xt) [? log(D ? (x t?1 , x t , t))] = E q(x0)q(xt?1|x0)q(xt|xt?1) [? log(D ? (x t?1 , x t , t))]</formula><p>. Given the discriminator, we train the generator by max ? t?1 E q(xt) E p ? (xt?1|xt) [log(D ? (x t?1 , x t , t))], which updates the generator with the non-saturating GAN objective <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref>.</p><p>Parametrizing the implicit denoising model: Instead of directly predicting x t?1 in the denoising step, diffusion models <ref type="bibr" target="#b23">(Ho et al., 2020)</ref> can be interpreted as parameterizing the denoising model by p ? (x t?1 |x t ) := q(x t?1 |x t , x 0 = f ? (x t , t)) in which first x 0 is predicted using the denoising model f ? (x t , t), and then, x t?1 is sampled using the posterior distribution q(x t?1 |x t , x 0 ) given x t and the predicted x 0 (See Appendix B for details). The distribution q(x t?1 |x 0 , x t ) is intuitively the distribution over x t?1 when denoising from x t towards x 0 , and it always has a Gaussian form for the diffusion process in Eq. 1, independent of the step size and complexity of the data distribution (see Appendix A for the expression of q(x t?1 |x 0 , x t )). Similarly, we define p ? (x t?1 |x t ) by:</p><formula xml:id="formula_9">p ? (x t?1 |x t ) := p ? (x 0 |x t )q(x t?1 |x t , x 0 )dx 0 = p(z)q(x t?1 |x t , x 0 = G ? (x t , z, t))dz, (6) where p ? (x 0 |x t ) is the implicit distribution imposed by the GAN generator G ? (x t , z, t) : R N ?R L ? R ? R N that outputs x 0 given x t and an L-dimensional latent variable z ? p(z) := N (z; 0, I).</formula><p>Our parameterization has several advantages: Firstly, our p ? (x t?1 |x t ) is formulated similar to DDPM <ref type="bibr" target="#b23">(Ho et al., 2020)</ref>. Thus, we can borrow some inductive bias such as the network structure design from DDPM. The main difference is that, in DDPM, x 0 is predicted as a deterministic mapping of x t , while in our case x 0 is produced by the generator with random latent variable z. This is the key difference that allows our denoising distribution p ? (x t?1 |x t ) to become multimodal and complex in contrast to the unimodal denoising model in DDPM. Secondly, note that for different t's, x t has different levels of perturbation, and hence using a single network to predict x t?1 directly at different t may be difficult. However, in our case the generator only needs to predict unperturbed x 0 and then add back perturbation using q(x t?1 |x t , x 0 ). <ref type="figure" target="#fig_2">Fig. 3</ref> visualizes our training pipeline. <ref type="bibr">1</ref> At early stages, we examined training a conditional VAE as p ? (xt?1|xt) by minimizing T t=1 E q(t) [DKL (q(xt?1|xt) p ? (xt?1|xt))] for small T . However, conditional VAEs consistently resulted in poor generative performance in our early experiments. In this paper, we focus on conditional GANs and we leave the exploration of other expressive conditional generators for p ? (xt?1|xt) to future work. Real / Fake? </p><formula xml:id="formula_10">x' 0 x' t -1 q(x t -1 | x 0 ) q(x t | x t -1 ) q(x t -1 | x t , x' 0 ) D(x t -1 , x t , t) G(x t , z, t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantage over one-shot generator:</head><p>One natural question for our model is, why not just train a GAN that can generate samples in one shot using a traditional setup, in contrast to our model that generates samples by denoising iteratively? Our model has several advantages over traditional GANs. GANs are known to suffer from training instability and mode collapse <ref type="bibr">(Kodali et al., 2017;</ref><ref type="bibr" target="#b49">Salimans et al., 2016)</ref>, and some possible reasons include the difficulty of directly generating samples from a complex distribution in one-shot, and the overfitting issue when the discriminator only looks at clean samples. In contrast, our model breaks the generation process into several conditional denoising diffusion steps in which each step is relatively simple to model, due to the strong conditioning on x t . Moreover, the diffusion process smoothens the data distribution (Lyu, 2012), making the discriminator less likely to overfit. Thus, we expect our model to exhibit better training stability and mode coverage. We empirically verify the advantages over traditional GANs in Sec. 5. Since then, there have been a number of improvements and alternatives to diffusion models. <ref type="bibr" target="#b59">Song et al. (2021c)</ref> generalize diffusion processes to continuous time, and provide a unified view of diffusion models and denoising score matching <ref type="bibr" target="#b64">(Vincent, 2011;</ref><ref type="bibr" target="#b56">Song &amp; Ermon, 2019)</ref>. Jolicoeur-Martineau et al. (2021b) add an auxiliary adversarial loss to the main objective. This is fundamentally different from ours, as their auxiliary adversarial loss only acts as an image enhancer, and they do not use latent variables; therefore, the denoising distribution is still a unimodal Gaussian. Other explorations include introducing alternative noise distributions in the forward process , jointly optimizing the model and noise schedule <ref type="bibr" target="#b16">(Kingma et al., 2021</ref>) and applying the model in latent spaces .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>One major drawback of diffusion or score-based models is the slow sampling speed due to a large number of iterative sampling steps. To alleviate this issue, multiple methods have been proposed, including knowledge distillation (Luhman &amp; Luhman, 2021), learning an adaptive noise schedule (San-Roman et al., 2021), introducing non-Markovian diffusion processes <ref type="bibr">Kong &amp; Ping, 2021)</ref>, and using better SDE solvers for continuous-time models <ref type="bibr" target="#b29">(Jolicoeur-Martineau et al., 2021a)</ref>. In particular,  uses x 0 sampling as a crucial ingredient to their method, but their denoising distribution is still a Gaussian. These methods either suffer from significant degradation in sample quality, or still require many sampling steps as we demonstrate in Sec. 5.</p><p>Among variants of diffusion models, <ref type="bibr" target="#b16">Gao et al. (2021)</ref> have the closest connection with our method. They propose to model the single-step denoising distribution by a conditional energy-based model (EBM), sharing the high-level idea of using expressive denoising distributions with us. However, they motivate their method from the perspective of facilitating the training of EBMs. More importantly, although only a few denoising steps are needed, expensive MCMC has to be used to sample from each denoising step, making the sampling process slow with ?180 network evaluations. Im-ageBART <ref type="bibr" target="#b13">(Esser et al., 2021a)</ref> explores modeling the denoising distribution of a diffusion process on discrete latent space with an auto-regressive model per step in a few denoising steps. However, the auto-regressive structure of their denoising distribution still makes sampling slow.</p><p>Since our model is trained with adversarial loss, our work is related to recent advances in improving the sample quality and diversity of GANs, including data augmentation <ref type="bibr" target="#b73">(Zhao et al., 2020;</ref><ref type="bibr">Karras et al., 2020a)</ref>, consistency regularization <ref type="bibr" target="#b70">(Zhang et al., 2020;</ref><ref type="bibr" target="#b74">Zhao et al., 2021)</ref> and entropy regularization <ref type="bibr" target="#b10">(Dieng et al., 2019)</ref>. In addition, the idea of training generative models with smoothed distributions is also discussed in  for auto-regressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate our proposed denoising diffusion GAN for the image synthesis problem. We begin with briefly introducing the network architecture design, while additional implementation details are presented in Appendix C. For our GAN generator, we adopt the NCSN++ architecture from <ref type="bibr" target="#b59">Song et al. (2021c)</ref> which has a U-net structure <ref type="bibr" target="#b47">(Ronneberger et al., 2015)</ref>. The conditioning x t is the input of the network, and time embedding is used to ensure conditioning on t. We let the latent variable z control the normalization layers. In particular, we replace all group normalization layers <ref type="bibr" target="#b65">(Wu &amp; He, 2018)</ref> in NCSN++ with adaptive group normalization layers in the generator, similar to Karras et al. <ref type="formula" target="#formula_0">(2019)</ref>; <ref type="bibr" target="#b26">Huang &amp; Belongie (2017)</ref>, where the shift and scale parameters in group normalization are predicted from z using a simple multi-layer fully-connected network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">OVERCOMING THE GENERATIVE LEARNING TRILEMMA</head><p>One major highlight of our model is that it excels at all three criteria in the generative learning trilemma. Here, we carefully evaluate our model's performances on sample fidelity, sample diversity and sampling time, and benchmark against a comprehensive list of models on the CIFAR-10 dataset.</p><p>Evaluation criteria: We adopt the commonly used Fr?chet inception distance (FID) <ref type="bibr" target="#b22">(Heusel et al., 2017)</ref> and Inception Score (IS) <ref type="bibr" target="#b49">(Salimans et al., 2016)</ref> for evaluating sample fidelity. We use the training set as a reference to compute the FID, following common practice in the literature (see <ref type="bibr" target="#b23">Ho et al. (2020)</ref>; Karras et al. <ref type="formula" target="#formula_0">(2019)</ref> as an example). For sample diversity, we use the improved recall score from Kynk??nniemi et al. <ref type="formula" target="#formula_0">(2019)</ref>, which is an improved version of the original precision and recall metric proposed by <ref type="bibr" target="#b48">Sajjadi et al. (2018)</ref>. It is shown that an improved recall score reflects how the variation in the generated samples matches that in the training set <ref type="bibr">(Kynk??nniemi et al., 2019)</ref>. For sampling time, we use the number of function evaluations (NFE) and the clock time when generating a batch of 100 images on a V100 GPU.</p><p>Results: We present our quantitative results in <ref type="table" target="#tab_1">Table 1</ref>. We observe that our sample quality is competitive among the best diffusion models and GANs. Although some variants of diffusion models obtain better IS and FID, they require a large number of function evaluations to generate samples (while we use only 4 denoising steps). For example, our sampling time is about 2000? faster than the predictor-corrector sampling by <ref type="bibr" target="#b59">Song et al. (2021c)</ref> and ?20? faster than FastDDPM <ref type="bibr">(Kong &amp; Ping, 2021)</ref>. Note that diffusion models can produce samples in fewer steps while trading off the sample quality. To better benchmark our method against existing diffusion models, we plot the FID score versus sampling time of diffusion models by varying the number of denoising steps (or the error tolerance for continuous-time models) in <ref type="figure" target="#fig_4">Figure 4</ref>. The figure clearly shows the advantage of our model compared to previous diffusion models. When comparing our model to GANs, we observe that only StyleGAN2 with adaptive data augmentation has slightly better sample quality than ours. However, from <ref type="table" target="#tab_1">Table 1</ref>, we see that GANs have limited sample diversity, as their recall scores are below 0.5. In contrast, our model obtains a significantly better recall score, even higher than several advanced likelihood-based models, and competitive among diffusion models. We show qualitative samples of CIFAR-10 in <ref type="figure">Figure 5</ref>. In summary, our model simultaneously excels at sample quality, sample diversity, and sampling speed and tackles the generative learning trilemma by a large extent.   <ref type="bibr" target="#b59">(Song et al., 2021c)</ref> 9.89 2.20 0.59 2000 423.2 Score SDE (VP) <ref type="bibr" target="#b59">(Song et al., 2021c)</ref> 9.68 2.41 0.59 2000 421.5 Probability Flow (VP) <ref type="bibr" target="#b59">(Song et al., 2021c)</ref> 9.83 3.08 0.57 140 50.9 LSGM  9.87 2.10 0.61 147 44.5 DDIM, T=50  8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION STUDIES</head><p>Here, we provide additional insights into our model by performing ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of denoising steps:</head><p>In the first part of <ref type="table" target="#tab_3">Table 2</ref>, we study the effect of using a different number of denoising steps (T ). Note that T = 1 corresponds to training an unconditional GAN, as the conditioning x t contains almost no information about x 0 . We observe that T = 1 leads to significantly worse results with low sample diversity, indicated by the low recall score. This confirms the benefits of breaking generation into several denoising steps, especially for improving the sample diversity. When varying T &gt; 1, we observe that T = 4 gives the best results, whereas there is a slight degrade in performance for larger T . We hypothesize that we may require a significantly higher capacity to accommodate larger T , as we need a conditional GAN for each denoising step.</p><p>Diffusion as data augmentation: Our model shares some similarities with recent work on applying data augmentation to <ref type="bibr">GANs (Karras et al., 2020a;</ref><ref type="bibr" target="#b73">Zhao et al., 2020)</ref>. To study the effect of perturbing inputs, we train a one-shot GAN with our network structure following the protocol in <ref type="bibr" target="#b73">(Zhao et al., 2020)</ref> with the forward diffusion process as data augmentation. The result, presented in the second group of <ref type="table" target="#tab_3">Table 2</ref>, is significantly worse than our model, indicating that our model is not equivalent to augmenting data before applying the discriminator.</p><p>Parametrization for p ? (x t?1 |x t ):</p><p>We study two alternative ways to parametrize the denoising distribution for the same T = 4 setting. Instead of letting the generator produce estimated samples of x 0 , we set the generator to directly output denoised samples x t?1 without posterior sampling (direct denoising), or output the noise t that perturbs a clean image to produce x t (noise generation). Note that the latter case is closely related to most diffusion models where the network deterministically predicts the perturbation noise. In <ref type="table" target="#tab_3">Table 2</ref>, we show that although these alternative parametrizations work reasonably well, our main parametrization outperforms them by a large margin.</p><p>Importance of latent variable: Removing latent variables z converts our denoising model to a unimodal distribution. In the last line of <ref type="table" target="#tab_3">Table 2</ref>, we study our model's performance without any latent variables z. We see that the sample quality is significantly worse, suggesting the importance of multimodal denoising distributions. In <ref type="figure" target="#fig_6">Figure 8</ref>, we visualize the effect of latent variables by showing samples of p ? (x 0 |x 1 ), where x 1 is a fixed noisy observation. We see that while the majority of information in the conditioning x 1 is preserved, the samples are diverse due to the latent variables.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ADDITIONAL STUDIES</head><p>Mode Coverage: Besides the recall score in <ref type="table" target="#tab_1">Table 1</ref>, we also evaluate the mode coverage of our model on the popular 25-Gaussians and StackedMNIST. The 25-Gaussians dataset is a 2-D toy dataset, generated by a mixture of 25 two-dimensional Gaussian distributions, arranged in a grid. We train our denoising diffusion GAN with 4 denoising steps and compare it to other models in <ref type="figure">Figure  6</ref>. We observe that the vanilla GAN suffers severely from mode collapse, and while techniques like WGAN-GP <ref type="bibr" target="#b20">(Gulrajani et al., 2017)</ref> improve mode coverage, the sample quality is still limited.</p><p>In contrast, our model covers all the modes while maintaining high sample quality. We also train a diffusion model and plot the samples generated by 100 and 500 denoising steps. We see that diffusion models require a large number of steps to maintain high sample quality.</p><p>Real samples GAN WGAN-GP Ours, T=4 DDPM, T=500 DDPM, T=100 <ref type="figure">Figure 6</ref>: Qualitative results on the 25-Gaussians dataset.</p><p>StackMNIST contains images generated by randomly choosing 3 MNIST images and stacking them along the RGB channels. Hence, the data distribution has 1000 modes. Following the setting of Lin et al. <ref type="formula" target="#formula_0">(2018)</ref>, we report the number of covered modes and the KL divergence from the categorical distribution over 1000 categories of generated samples to true data in <ref type="table" target="#tab_4">Table 3</ref>. We observe that our model covers all modes faithfully and achieves the lowest KL compared to GANs that are specifically designed for better mode coverage or StyleGAN2 that is known to have the best sample quality.</p><p>Training Stability: We discuss the training stability of our model in Appendix D.</p><p>High Resolution Images: We train our model on datasets with larger images, including CelebA-HQ <ref type="bibr">(Karras et al., 2018)</ref> and LSUN Church <ref type="bibr" target="#b68">(Yu et al., 2015)</ref> at 256 ? 256px resolution. We report FID on these two datasets in <ref type="table" target="#tab_5">Table 4</ref> and 5. Similar to CIFAR-10, our model obtains competitive sample quality among the best diffusion models and GANs. In particular, in LSUN Church, our model outperforms DDPM and ImageBART (see <ref type="figure" target="#fig_5">Figure 7</ref> and Appendix E for samples). Although, some GANs perform better on this dataset, their mode coverage is not reflected by the FID score.</p><p>Stroke-based image synthesis: Recently, <ref type="bibr" target="#b32">Meng et al. (2021b)</ref> propose an interesting application of diffusion models to stroke-based generation. Specifically, they perturb a stroke painting by the forward diffusion process, and denoise it with a diffusion model. The method is particularly promising because it only requires training an unconditional generative model on the target dataset and does not require training images paired with stroke paintings like GAN-based methods <ref type="bibr" target="#b51">(Sangkloy et al., 2017;</ref><ref type="bibr" target="#b42">Park et al., 2019)</ref>. We apply our model to stroke-based image synthesis and show qualitative results in <ref type="figure" target="#fig_7">Figure 9</ref>. The generated samples are realistic and diverse, while the conditioning in the stroke paintings is faithfully preserved. Compared to <ref type="bibr" target="#b32">Meng et al. (2021b)</ref>, our model enjoys a 1100? speedup in generation, as it takes only 0.16s to generate one image at 256 resolution vs. 181s for <ref type="bibr" target="#b32">Meng et al. (2021b)</ref>. This experiment confirms that our proposed model enables the application of diffusion models to interactive applications such as image editing. Score SDE <ref type="bibr" target="#b59">(Song et al., 2021c)</ref> 7.23 LSGM  7.22 UDM <ref type="bibr">(Kim et al., 2021)</ref> 7.16</p><p>NVAE <ref type="bibr" target="#b61">(Vahdat &amp; Kautz, 2020)</ref> 29.7 VAEBM <ref type="bibr" target="#b66">(Xiao et al., 2021)</ref> 20.4 NCP-VAE <ref type="bibr" target="#b1">(Aneja et al., 2021)</ref> 24.8 <ref type="bibr">PGGAN (Karras et al., 2018)</ref> 8.03 Adv. LAE <ref type="bibr" target="#b44">(Pidhorskyi et al., 2020)</ref> 19.2 VQ-GAN <ref type="bibr" target="#b14">(Esser et al., 2021b)</ref> 10.2 DC-AE <ref type="bibr" target="#b43">(Parmar et al., 2021)</ref> 15.8    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Deep generative learning frameworks still struggle with addressing the generative learning trilemma. Diffusion models achieve particularly high-quality and diverse sampling. However, their slow sampling and high computational cost do not yet allow them to be widely applied in real-world applications. In this paper, we argued that one of the main sources of slow sampling in diffusion models is the Gaussian assumption in the denoising distribution, which is justified only for very small denoising steps. To remedy this, we proposed denoising diffusion GANs that model each denoising step using a complex multimodal distribution, enabling us to take large denoising steps. In extensive experiments, we showed that denoising diffusion GANs achieve high sample quality and diversity competitive to the original diffusion models, while being orders of magnitude faster at sampling. Compared to traditional GANs, our proposed model enjoys better mode coverage and sample diversity. Our denoising diffusion GAN overcomes the generative learning trilemma to a large extent, allowing diffusion models to be applied to real-world problems with low computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ETHICS AND REPRODUCIBILITY STATEMENT</head><p>Generating high-quality samples while representing the diversity in the training data faithfully has been a daunting challenge in generative learning. Mode coverage and high diversity are key requirements for reducing biases in generative models and improving the representation of minorities in a population. While diffusion models achieve both high sample quality and diversity, their expensive sampling limits their application in many real-world problems. Our proposed denoising diffusion GAN reduces the computational complexity of diffusion models to an extent that allows these models to be applied in practical applications at a low cost. Thus, we foresee that in the long term, our model can help with reducing the negative social impacts of existing generative models that fall short in capturing the data diversity.</p><p>We evaluate our model using public datasets, and therefore this work does not involve any human subject evaluation or new data collection.</p><p>Reproducibility Statement: We currently provide experimental details in the appendices with a detailed list of hyperparameters and training settings. To aid with reproducibility further, we will release our source code publicly in the future with instructions to reproduce our results. Next, we can plug the expression for x 0 in Eq. 13 into the mean of the Gaussian posterior distribution in Eq. 11, and we have?</p><formula xml:id="formula_11">t (x t , x 0 ) =? t x t , 1 ?? t x t ? ? 1 ?? t ? (x t , t) (14) = 1 ? ? t x t ? 1 ? ? t ? 1 ?? t ? (x t , t)<label>(15)</label></formula><p>after simplifications. Comparing this with Eq. 12, we observe that Eq. 12 simply corresponds to sampling from the Gaussian posterior distribution. Therefore, although <ref type="bibr" target="#b23">Ho et al. (2020)</ref> use an alternative re-parametrization, their denoising distribution can still be equivalently interpreted as p ? (x t?1 |x t ) := q(x t?1 |x t , x 0 = f ? (x t , t)), i.e, first predicting x 0 using the time-dependent denoising model, and then sampling x t?1 using the posterior distribution q(x t?1 |x t , x 0 ) given x t and the predicted x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS</head><p>In this section, we present our experimental settings in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 NETWORK STRUCTURE</head><p>Generator: Our generator structure largely follows the U-net structure <ref type="bibr" target="#b47">(Ronneberger et al., 2015)</ref> used in NCSN++ <ref type="bibr" target="#b59">(Song et al., 2021c)</ref>, which consists of multiple ResNet blocks <ref type="bibr" target="#b21">(He et al., 2016)</ref> and Attention blocks <ref type="bibr" target="#b63">(Vaswani et al., 2017)</ref>. Hyper-parameters for the network design, such as the number of blocks and number of channels, are reported in <ref type="table" target="#tab_8">Table 6</ref>. We follow the default settings in <ref type="bibr" target="#b59">Song et al. (2021c)</ref> for other network configurations not mentioned in the table, including Swish activation function, upsampling and downsampling with anti-aliasing based on Finite Impulse Response (FIR) <ref type="bibr" target="#b71">(Zhang, 2019)</ref>, re-scaling all skip connections by 1 ? 2 , using residual block design from BigGAN <ref type="bibr" target="#b7">(Brock et al., 2018)</ref> and incorporating progressive growing architectures <ref type="bibr">(Karras et al., 2020b)</ref>. See Appendix H of <ref type="bibr" target="#b59">Song et al. (2021c)</ref> for more details on these configurations.</p><p>We follow <ref type="bibr" target="#b23">Ho et al. (2020)</ref> and use sinusoidal positional embeddings for conditioning on integer time steps. The dimension for the time embedding is 4? the number of initial channels presented in <ref type="table" target="#tab_8">Table 6</ref>. Contrary to previous works, we did not find the use of Dropout helpful in our case.</p><p>The fundamental difference between our generator network and the networks of previous diffusion models is that our generator takes an extra latent variable z as input. Inspired by the success of Style-GANs, we provide z-conditioning to the NCSN++ architecture using mapping networks, introduced in <ref type="bibr">StyleGAN (Karras et al., 2019)</ref>. We use z ? N (0, I) for all experiments. We replace all the group normalization (GN) layers in the network with adaptive group normalization (AdaGN) layers to allow the input of latent variables. The latent variable z is first transformed by a fully-connected network (called mapping network), and then the resulting embedding vector, denoted by w, is sent to every AdaGN layer. Each AdaGN layer contains one fully-connected layer that takes w as input, and outputs the per-channel shift and scale parameters for the group normalization. The network's feature maps are then subject to affine transformations using these shift and scale parameters of the AdaGN layers. The mapping network and the fully-connected layer in AdaGN are independent of time steps t, as we found no extra benefit in incorporating time embeddings in these layers. Details about latent variables are also presented in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>Discriminator: We design our time-dependent discriminator with a convolutional network with ResNet blocks, where the design of the ResNet blocks is similar to that of the generator. The discriminator tries to discriminate real and fake x t?1 , conditioned on x t and t. The time conditioning is enforced by the same sinusoidal positional embedding as in the generator. The x t conditioning is enforced by concatenating x t and x t?1 as the input to the discriminator. We use LeakyReLU activations with a negative slope 0.2 for all layers. Similar to <ref type="bibr">Karras et al. (2020b)</ref>, we use a minibatch standard deviation layer after all the ResNet blocks. We present the exact architecture of discriminators in <ref type="table">Table 7</ref>.   <ref type="bibr">, 2, 3, 4]</ref> for each datapoint in a batch. Besides the main objective, we also add an R 1 regularization term <ref type="bibr" target="#b33">(Mescheder et al., 2018)</ref> to the objective for the discriminator. The R 1 term is defined as</p><formula xml:id="formula_12">R 1 (?) = ? 2 E q(x0)q(xt?1|x0)q(xt|xt?1) ? xt?1 D ? (x t?1 , x t , t) 2 ,</formula><p>where ? is the coefficient for the regularization. We use ? = 0.05 for CIFAR-10, and ? = 1 for CelebA-HQ and LSUN Church. Note that the R 1 regularization is a gradient penalty that encourages the discriminator to stay smooth and improves the convergence of GAN training <ref type="bibr" target="#b33">(Mescheder et al., 2018)</ref>.</p><p>Optimization: We train our models using the Adam optimizer <ref type="bibr">(Kingma &amp; Ba, 2015)</ref>. We use cosine learning rate decay <ref type="bibr">(Loshchilov &amp; Hutter, 2016)</ref> for training both the generator and discriminator. Similar to <ref type="bibr" target="#b23">Ho et al. (2020)</ref>; <ref type="bibr" target="#b59">Song et al. (2021c)</ref>; Karras et al. (2020a), we observe that applying an exponential moving average (EMA) on the generator is crucial to achieve high performance. We summarize the optimization hyper-parameters in <ref type="table" target="#tab_9">Table 8</ref>.</p><p>We train our models on CIFAR-10 using 4 V100 GPUs. On CelebA-HQ and LSUN Church we use 8 V100 GPUs. The training takes approximately 48 hours on CIFAR-10, and 180 hours on CelebA-HQ and LSUN Church.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 EVALUATION</head><p>When evaluating IS, FID and recall score, we use 50k generated samples for CIFAR-10 and LSUN Church, and 30k samples for CelebA-HQ (since the CelebA HQ dataset contains only 30k samples).</p><p>When evaluating sampling time, we use models trained on CIFAR-10 and generate a batch of 100 samples. We benchmark the sampling time on a machine with a single V100 GPU. We use Pytorch 1.9.0 and CUDA 11.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 ABLATION STUDIES</head><p>Here we introduce the settings for the ablation study in Sec. 5.2. We observe that training requires a larger number of training iterations when T is larger. As a result, we train the model for each T until the FID score does not increase any further. The number of training iteration is 200k for T = 1 and T = 2, 400k for T = 4 and 600k for T = 8. We use the same network structures and optimization settings as in the main experiments.</p><p>For the data augmentation baseline, we follow the differentiable data augmentation pipeline in <ref type="bibr" target="#b73">Zhao et al. (2020)</ref>. In particular, for every (real or fake) image in the batch, we perturbed it by sampling from a random timestep at the diffusion process (except the last diffusion step where the information of data is completely destroyed). We find the results insensitive to the number of possible perturbation levels (i.e, the number of steps in the diffusion process), and we report the result using a diffusion process with 4 steps. Since the perturbation by the diffusion process is differentiable due to the re-parametrization trick (Kingma &amp; Welling, 2014), we can train both the discriminator and generator with the perturbed samples. See <ref type="bibr" target="#b73">Zhao et al. (2020)</ref> for a detailed explanation for the training pipeline.</p><p>For the experiments on alternative parametrizations, we use T = 4 for the diffusion process and keep other settings the same as in the main experiments.</p><p>For the experiment on training a model without latent variables, similar to the main experiments, the generator takes the conditioning x t as its input, and the time conditioning is still enforced by the time embedding. However, the AdaGN layers are replaced by plain GN layers, such that no latent variable is needed, and the mapping network for z is removed. Other settings follow the main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 TOY DATA AND STACKEDMNIST</head><p>For the 25-Gaussian toy dataset, both our generator and discriminator have 3 fully-connected layers each with 512 hidden units and LeakyReLU activations (negative slope of 0.2). We enforce both the conditioning on x t and t by concatenation with the input. We use the Adam optimizer with a learning rate of 10 ?4 for both the generator and discriminator. The batch size is 512, and we train the model for 50k iterations.</p><p>Our experimental settings for StackedMNIST are the same as those for CIFAR-10, except that we train the model for only 150k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TRAINING STABILITY</head><p>In <ref type="figure" target="#fig_0">Fig. 10</ref>, we plot the discriminator loss for different time steps in the diffusion process when T = 4. We observe that the training of our denoising diffusion GAN is stable and we do not see any explosion in loss values, as is sometimes reported for other GAN methods such as <ref type="bibr" target="#b7">Brock et al. (2018)</ref>. The stability might be attributed to two reasons: First, the conditioning on x t for both generator and discriminator provides a strong signal. The generator is required to generate a few plausible samples given x t and the discriminator requires classifying them. The x t conditioning keeps the discriminator and generator in a balance. Second, we are training the GAN on relatively smooth distributions, as the diffusion process is known as a smoothening process that brings the distributions of fake and real samples closer to each other (Lyu, 2012). As we can see from <ref type="figure" target="#fig_0">Fig.  10</ref>, the discriminator loss for t &gt; 0 is higher than t = 0 (the last denoising step). Note that t &gt; 0 corresponds to training the discriminator on noisy images, and in this case the true and generator distributions are closer to each other, making the discrimination harder and hence resulting in higher discriminator loss. We believe that such a property prevents the discriminator from overfitting, which leads to better training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL QUALITATIVE RESULTS</head><p>We show additional qualitative samples of CIFAR-10, CelebA-HQ and LSUN Church Outdoor in <ref type="figure" target="#fig_0">Figure 11</ref>, <ref type="figure" target="#fig_0">Figure 12</ref> and <ref type="figure" target="#fig_0">Figure 13</ref>, respectively. F ADDITIONAL VISUALIZATION FOR p ? (x 0 |x t )</p><p>In <ref type="figure" target="#fig_0">Figure 14</ref> and <ref type="figure" target="#fig_0">Figure 15</ref>, we show visualizations of samples from p ? (x 0 |x t ) for different t. Note that except for p ? (x 0 |x 1 ), the samples from p ? (x 0 |x t ) do not need to be sharp, as they are only intermediate outputs of the sampling process. The conditioning is less preserved as the perturbation in x t increases, and in particular x T (x 4 in our example) contains almost no information of clean data x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G NEAREST NEIGHBOR RESULTS</head><p>In <ref type="figure" target="#fig_0">Figure 16</ref> and <ref type="figure" target="#fig_0">Figure 17</ref>, we show the nearest neighbors in the training dataset, corresponding to a few generated samples, where the nearest neighbors are computed using the feature distance of a pre-trained VGG network <ref type="bibr" target="#b53">(Simonyan &amp; Zisserman, 2014)</ref>. We observe that the nearest neighbors are significantly different from the samples, suggesting that our models generalize well.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Generative learning trilemma.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The training process of denoising diffusion GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Diffusion-based models<ref type="bibr" target="#b54">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref> learn the finite-time reversal of a diffusion process, sharing the idea of learning transition operators of Markov chains with Goyal et al. (2017); Alain et al. (2016); Bordes et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Sample quality vs sampling time trade-off.Figure 5: CIFAR-10 qualitative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on CelebA-HQ 256 and LSUN Church Outdoor 256.Additional results: Additional qualitative visualizations are provided in Appendix E, F, and G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Multi-modality of denoising distribution given the same noisy observation. Left: clean image x 0 and perturbed image x 1 . Right: Three samples from p ? (x 0 |x 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results on stroke-based synthesis. Top row: stroke paintings. Bottom two rows: generated samples corresponding to the stroke painting (best seen when zoomed in).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>The discriminator loss per denoising step during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Additional qualitative samples on CIFAR-10.Figure 14: Visualization of samples from p ? (x 0 |x t ) for different t on CelebA-HQ. For each example, the top row contains x t from diffusion process steps, where x 0 is a sample from the dataset. The bottom rows contain 3 samples from p ? (x 0 |x t ) for different t's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Visualization of samples from p ? (x 0 |x t ) for different t on LSUN Church. For each example, the top row contains x t from diffusion process steps, where x 0 is a sample from the dataset. The bottom rows contain 3 samples from p ? (x 0 |x t ) for different t's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>CIFAR-10 nearest neighbors in VGG feature space. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>CelebA-HQ nearest neighbors in the VGG feature space. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results for unconditional generation on CIFAR-10.</figDesc><table><row><cell>Model</cell><cell>IS?</cell><cell cols="4">FID? Recall? NFE ? Time (s) ?</cell></row><row><cell>Denoising Diffusion GAN (ours), T=4</cell><cell>9.63</cell><cell>3.75</cell><cell>0.57</cell><cell>4</cell><cell>0.21</cell></row><row><cell>DDPM (Ho et al., 2020)</cell><cell>9.46</cell><cell>3.21</cell><cell>0.57</cell><cell>1000</cell><cell>80.5</cell></row><row><cell>NCSN (Song &amp; Ermon, 2019)</cell><cell>8.87</cell><cell>25.3</cell><cell>-</cell><cell>1000</cell><cell>107.9</cell></row><row><cell>Adversarial DSM (Jolicoeur-Martineau et al., 2021b)</cell><cell>-</cell><cell>6.10</cell><cell>-</cell><cell>1000</cell><cell>-</cell></row><row><cell>Likelihood SDE (Song et al., 2021b)</cell><cell>-</cell><cell>2.87</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Score SDE (VE)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on CIFAR-10.</figDesc><table><row><cell>Model Variants</cell><cell>IS?</cell><cell cols="2">FID? Recall?</cell></row><row><cell>T = 1</cell><cell cols="2">8.93 14.6</cell><cell>0.19</cell></row><row><cell>T = 2</cell><cell cols="2">9.80 4.08</cell><cell>0.54</cell></row><row><cell>T = 4</cell><cell cols="2">9.63 3.75</cell><cell>0.57</cell></row><row><cell>T = 8</cell><cell cols="2">9.43 4.36</cell><cell>0.56</cell></row><row><cell>One-shot w/ aug</cell><cell cols="2">8.96 13.2</cell><cell>0.25</cell></row><row><cell>Direct denoising</cell><cell cols="2">9.10 6.03</cell><cell>0.53</cell></row><row><cell>Noise generation</cell><cell cols="2">8.79 8.04</cell><cell>0.52</cell></row><row><cell cols="3">No latent variable 8.37 20.6</cell><cell>0.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mode coverage on StackedMNIST.</figDesc><table><row><cell>Model</cell><cell cols="2">Modes? KL?</cell></row><row><cell>VEEGAN (Srivastava et al.)</cell><cell>762</cell><cell>2.173</cell></row><row><cell>PacGAN (Lin et al.)</cell><cell>992</cell><cell>0.277</cell></row><row><cell>PresGAN (Dieng et al.)</cell><cell>1000</cell><cell>0.115</cell></row><row><cell>InclusiveGAN (Yu et al.)</cell><cell>997</cell><cell>0.200</cell></row><row><cell>StyleGAN2 (Karras et al.)</cell><cell>940</cell><cell>0.424</cell></row><row><cell cols="2">Adv. DSM (Jolicoeur-Martineau et al.) 1000</cell><cell>1.49</cell></row><row><cell>VAEBM (Xiao et al.)</cell><cell>1000</cell><cell>0.087</cell></row><row><cell>Denoising Diffusion GAN (ours)</cell><cell>1000</cell><cell>0.071</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Generative results on CelebA-HQ-256</figDesc><table><row><cell>Model</cell><cell>FID?</cell></row><row><cell>Denoising Diffusion GAN (ours)</cell><cell>7.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Generative results on LSUN Church 256</figDesc><table><row><cell>Model</cell><cell>FID?</cell></row><row><cell>Denoising Diffusion GAN (ours)</cell><cell>5.25</cell></row><row><cell>DDPM (Ho et al., 2020)</cell><cell>7.89</cell></row><row><cell>ImageBART (Esser et al., 2021a)</cell><cell>7.32</cell></row><row><cell cols="2">Gotta Go Fast (Jolicoeur-Martineau et al.) 25.67</cell></row><row><cell>PGGAN (Karras et al., 2018))</cell><cell>6.42</cell></row><row><cell>StyleGAN (Karras et al., 2019)</cell><cell>4.21</cell></row><row><cell>StyleGAN2 (Karras et al., 2020b)</cell><cell>3.86</cell></row><row><cell>CIPS (Anokhin et al., 2021)</cell><cell>2.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.</figDesc><table><row><cell>Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training</cell></row><row><cell>generative adversarial networks with limited data. In Advances in neural information processing</cell></row><row><cell>systems, 2020a.</cell></row><row><cell>Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Ana-</cell></row><row><cell>lyzing and improving the image quality of stylegan. In Proceedings of the IEEE conference on</cell></row><row><cell>computer vision and pattern recognition, 2020b.</cell></row><row><cell>Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching</cell></row><row><cell>model for unbounded data score. arXiv preprint arXiv:2106.05527, 2021.</cell></row><row><cell>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International</cell></row><row><cell>Conference on Learning Representations, 2015.</cell></row><row><cell>Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In The International Con-</cell></row><row><cell>ference on Learning Representations, 2014.</cell></row><row><cell>Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.</cell></row><row><cell>arXiv preprint arXiv:2107.00630, 2021.</cell></row><row><cell>Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In</cell></row><row><cell>Advances in neural information processing systems, 2018.</cell></row><row><cell>Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.</cell></row><row><cell>arXiv preprint arXiv:1705.07215, 2017.</cell></row><row><cell>Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint</cell></row><row><cell>arXiv:2106.00132, 2021.</cell></row><row><cell>Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile</cell></row><row><cell>diffusion model for audio synthesis. In International Conference on Learning Representations,</cell></row><row><cell>2021.</cell></row><row><cell>Tuomas Kynk??nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved</cell></row><row><cell>precision and recall metric for assessing generative models. arXiv preprint arXiv:1904.06991,</cell></row><row><cell>2019.</cell></row><row><cell>Christian Ledig, Lucas Theis, Ferenc Husz?r, Jose Caballero, Andrew Cunningham, Alejandro</cell></row><row><cell>Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-</cell></row><row><cell>gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE</cell></row><row><cell>conference on computer vision and pattern recognition, 2017.</cell></row><row><cell>Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in</cell></row><row><cell>generative adversarial networks. In Advances in neural information processing systems, 2018.</cell></row><row><cell>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv</cell></row><row><cell>preprint arXiv:1608.03983, 2016.</cell></row><row><cell>Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved</cell></row><row><cell>sampling speed. arXiv preprint arXiv:2101.02388, 2021.</cell></row><row><cell>Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceed-</cell></row><row><cell>ings of the IEEE conference on computer vision and pattern recognition, 2021.</cell></row><row><cell>Siwei Lyu. Interpretation and generalization of score matching. arXiv preprint arXiv:1205.2629,</cell></row><row><cell>2012.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters for the generator network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Optimization hyper-parameters.</figDesc><table><row><cell>CIFAR10</cell><cell>CelebA-HQ LSUN Church</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A DERIVATION FOR THE GAUSSIAN POSTERIOR <ref type="bibr" target="#b23">Ho et al. (2020)</ref> provide a derivation for the Gaussian posterior distribution. We include it here for completeness. Consider the forward diffusion process in Eq. 1, which we repeat here:</p><p>q(x 1:T |x 0 ) = T t=1 q(x t |x t?1 ), q(x t |x t?1 ) = N (x t ; 1 ? ? t x t?1 , ? t I).</p><p>Due to the Markov property of the forward process, we have the marginal distribution of x t given the initial clean data x 0 :</p><p>where we denote ? t := 1?? t and? t := t s=1 ? s . Applying Bayes' rule, we can obtain the forward process posterior when conditioned on x 0 :</p><p>where the second equation follows from the Markov property of the forward process. Since all three terms in Eq. 9 are Gaussians, the posterior q(x t?1 |x t , x 0 ) is also a Gaussian distribution, and it can be written as</p><p>with mean? t (x t , x 0 ) and variance? t . Plugging the expressions in Eq. 7 and Eq. 8 into Eq. 10, we obtain?</p><p>after minor simplifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PARAMETRIZATION OF DDPM</head><p>In Sec. 3.2, we mention that the parametrization of the denoising distribution for current diffusion models such as <ref type="bibr" target="#b23">Ho et al. (2020)</ref> can be interpreted as</p><p>However, such a parametrization is not explicitly stated in <ref type="bibr" target="#b23">Ho et al. (2020)</ref> but it is discussed by . To avoid possible confusion, here we show that the parametrization of <ref type="bibr" target="#b23">Ho et al. (2020)</ref> is equivalent to what we describe in Sec 3.2.</p><p>Ho et al. (2020) train a noise prediction network ? (x t , t) which predicts the noise that perturbs data x 0 to x t , and a sample from p ? (x t?1 |x t ) is obtained as (see Algorithm 2 of Ho et al. <ref type="formula">(2020)</ref>)</p><p>where z ? N (0, I) except for the last denoising step where z = 0, and ? t = ? t is the standard deviation of the Gaussian posterior distribution in Eq. 11.</p><p>Firstly, notice that predicting the perturbation noise ? (x t , t) is equivalent to predicting x 0 . We know that x t is generated by adding ? N (0, I) noise as:</p><p>Hence, after predicting the noise with ? (x t , t) we can obtain a prediction of x 0 using:  Diffusion Process: For all datasets, we set the number of diffusion steps to be 4. In order to compute ? t per step, we use the discretization of the continuous-time extension of the process described in Eq. 1, which is called the Variance Preserving (VP) SDE by <ref type="bibr" target="#b59">Song et al. (2021c)</ref>. We compute ? t based on the continuous-time diffusion model formulation, as it allows us to ensure that variance schedule stays the same independent of the number of diffusion steps. Let's define the normalized time variable by t := t T which normalizes t ? {1, 2, . . . , T } to [0, 1]. The variance function of VP SDE is given by:</p><p>with the constants ? max = 20 and ? min = 0.1. Recall that sampling from t th step in the forward diffusion process can be done with q(x t |x 0 ) = N (x t ; ?? t x 0 , (1?? t )I). We compute ? t by solving 1 ?? t = ? 2 ( t T ):</p><p>for t ? {1, 2, . . . T }. This choice of ? t values corresponds to equidistant steps in time according to VP SDE. Other choices are possible, but we did not explore them.</p><p>Objective: We train our denoising diffusion GAN with the following adversarial objective: where the outer expectation denotes ancestral sampling from q(x 0 , x t?1 , x t ) and p ? (x t?1 |x t ) is our implicit GAN denoising distribution.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gsns: generative stochastic networks. Information and Inference: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="249" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A contrastive learning approach for training variational autoencoder priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image generators with conditionally-independent pixel synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Ivan Anokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Demochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sterkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korzhenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Refining deep generative models via discriminator gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Abdul Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Liang Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03006</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to generate samples from noise through infusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06975</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis K</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titsias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04302</idno>
		<title level="m">Prescribed generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08827</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the theory of stochastic processes, with particular reference to applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the [First] Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the [First] Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1949" />
			<biblScope unit="page" from="403" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning energy-based models by diffusion recovery likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02282</idno>
		<title level="m">Variational walkback: Learning a transition operator as a stochastic recurrent net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15282</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A variational perspective on diffusion-based generative models and score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02808</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gotta go fast when generating data with score-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Remi Tachet des Combes</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improved autoregressive modeling with distribution smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15089</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16091</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07582</idno>
		<title level="m">Non gaussian denoising diffusion models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dual contradistinctive generative autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial latent autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00035</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scribbler: Controlling deep image synthesis with sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bagby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08029</idno>
		<title level="m">Non-saturating gan training as divergence minimization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09258</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vaebm: A symbiosis between variational autoencoders and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Inclusive gan: Improving data and minority coverage in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03355</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bias and generalization in deep generative models: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Improved consistency regularization for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
