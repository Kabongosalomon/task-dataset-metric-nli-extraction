<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
							<email>vtankovich@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
							<email>yindaz@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
							<email>adarshkowdle@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
							<email>seanfa@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz Google</surname></persName>
							<email>sofien@google.com</email>
						</author>
						<title level="a" type="main">HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Figure 1: Example result of HITNet on the SceneFlow, KITTI, ETH3D and Middlebury datasets. Our approach predicts accurate depth with crisp edges. HITNet obtains state-of-art results on KITTI, ETH3D and Middlebury-v3 benchmarks.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stereo matching has been an active field of research for decades <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b18">19]</ref>. Traditional methods utilize handcrafted schemes to find local correspondences <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> and global optimization to exploit spatial context <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. The run-time efficiency of most of these approaches are correlated with the size of the disparity space, which prevents real-time applications. Efficient algorithms <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> avoid searching the full disparity space by using patchmatch [1] and super-pixel [31] techniques. A family of machine learning based approaches, using random forest and decision trees, are able to establish correspondences quickly <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. However, these methods require either camera specific learning or post processing. Recently, deep learning brought big improvements to stereo matching. Early works trained siamese networks to extract patch-wise features or predict matching costs <ref type="bibr" target="#b35">[36,</ref> 60, 58, 59]. End-to-end networks have been proposed to learn all steps jointly, yielding more accurate results <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Example result of HITNet on the SceneFlow, KITTI, ETH3D and Middlebury datasets. Our approach predicts accurate depth with crisp edges. HITNet obtains state-of-art results on KITTI, ETH3D and Middlebury-v3 benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper presents HITNet, a novel neural network architecture for real-time stereo matching. Contrary to many recent neural network approaches that operate on a full cost volume and rely on 3D convolutions, our approach does not explicitly build a volume and instead relies on a fast multiresolution initialization step, differentiable 2D geometric propagation and warping mechanisms to infer disparity hypotheses. To achieve a high level of accuracy, our network not only geometrically reasons about disparities but also infers slanted plane hypotheses allowing to more accurately perform geometric warping and upsampling opera-tions. Our architecture is inherently multi-resolution allowing the propagation of information across different levels. Multiple experiments prove the effectiveness of the proposed approach at a fraction of the computation required by stateof-the-art methods. At the time of writing, HITNet ranks 1 st -3 rd on all the metrics published on the ETH3D website for two view stereo, ranks 1 st on most of the metrics amongst all the end-to-end learning approaches on Middlebury-v3, ranks 1 st on the popular KITTI 2012 and 2015 benchmarks among the published methods faster than 100 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent research on depth from stereo matching has largely focused on developing accurate but computationally expensive deep learning approaches. Large convolutional neural networks (CNNs) can often use up to a second or even more to process an image pair and infer a disparity map. For active agents such as mobile robots or self driving cars such a high latency is undesirable and methods which are able to process an image pair in a matter of milliseconds are required instead. Despite this, only 4 out of the top 100 methods on the KITTI 2012 leaderboard are published approaches that take less than 100ms 1 .</p><p>A common pattern in end-to-end learning based approaches to computational stereo is utilizing a CNN which is largely unaware of the geometric properties of the stereo matching problem. In fact, initial end-to-end networks were based on a generic U-Net architecture <ref type="bibr" target="#b37">[38]</ref>. Subsequent works have pointed out that incorporating explicit matching cost volumes encoding the cost of assigning a disparity to a pixel, in conjunction with 3D convolutions provides a notable improvement in terms of accuracy but at the cost of significantly increasing the amount of computation <ref type="bibr" target="#b23">[24]</ref>. Follow up work <ref type="bibr" target="#b24">[25]</ref> showed that a downsampled cost volume could provide a reasonable trade-off between speed and accuracy. However, the downsampling of the cost volume comes at the price of sacrificing accuracy.</p><p>Multiple recent stereo matching methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref> have increased the efficiency of disparity estimation for active stereo while maintaining a high level of accuracy. These methods are mainly built on three intuitions: Firstly, the use of compact/sparse features for fast high resolution matching cost computation; Secondly, very efficient disparity optimization schemes that do not rely on the full cost volume; Thirdly, iterative image warps using slanted planes to achieve high accuracy by minimizing image dissimilarity. All these design choices are used without explicitly operating on a full 3D cost volume. By doing so these approaches achieve very fast and accurate results for active stereo but they do not directly generalize to passive stereo due to the lack of using a powerful machine learning system. This therefore raises the question if such mechanisms can be integrated into neural network based stereo-matching systems to achieve efficient and accurate results opening up the possibility of using passive stereo based depth sensing in latency critical applications.</p><p>We propose HITNet, a framework for neural network based depth estimation which overcomes the computational disadvantages of operating on a 3D volume by integrating image warping, spatial propagation and a fast high resolution initialization step into the network architecture, while keeping the flexibility of a learned representation by allowing features to flow through the network. The main idea of our approach is to represent image tiles as planar patches which have a learned compact feature descriptor attached to them. The basic principle of our approach is to fuse information from the high resolution initialization and the current hypotheses using spatial propagation. The propagation is implemented via a convolutional neural network module that updates the estimate of the planar patches and their attached features. In order for the network to iteratively increase the accuracy of the disparity predictions, we provide the network a local cost volume in a narrow band (?1 disparity) around the planar patch using in-network image warping allowing the network to minimize image dissimilarity. To reconstruct fine details while also capturing large texture-less areas we start at low resolution and hierarchically upsample predictions to higher resolution. A critical feature of our architecture is that at each resolution, matches from the initialization module are provided to facilitate recovery of thin structures that cannot be represented at low resolution. An example output of our method shows how our network recovers very accurate boundaries, fine detail and thin structures in <ref type="figure">Fig. 1</ref>.</p><p>To summarize, our main contributions are:</p><p>? A fast multi-resolution initialization step that computes high resolution matches using learned features.</p><p>? An efficient 2D disparity propagation that makes use of slanted support windows with learned descriptors.</p><p>? State-of-art results in popular benchmarks using a fraction of the computation compared to other methods.</p><p>A key component in modern architectures is a cost volume layer <ref type="bibr" target="#b23">[24]</ref> (or correlation layer <ref type="bibr" target="#b22">[23]</ref>), allowing the network to run per-pixel feature matching. To speed up computation, cascaded models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr">61]</ref> have been proposed to search in disparity space in a coarseto-fine fashion. In particular, <ref type="bibr" target="#b39">[40]</ref> uses multiple residual blocks to improve the current disparity estimate. The recent work <ref type="bibr" target="#b53">[54]</ref> relies on a hierarchical cost volume, allowing the method to be trained on high resolution images and generate different resolutions on demand. All these methods rely on expensive cost-volume filtering operations using 3D convolutions <ref type="bibr" target="#b53">[54]</ref> or multiple refinement layers <ref type="bibr" target="#b39">[40]</ref>, preventing real-time performance. Fast approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">62]</ref> downsample the cost volume in spatial and disparity space and attempt to recover fine details by edge-aware upsampling layers. These methods show real-time performance but sacrifice accuracy especially for thin structures and edges since they are missing in the low-res initialization.</p><p>Our method is inspired by classical stereo matching methods, which aim at propagating good sparse matches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b51">52]</ref>. In particular, Tankovich et al. <ref type="bibr" target="#b51">[52]</ref> proposed a hierarchical algorithm that makes use of slanted support windows to amortize the matching cost computation in tiles. Inspired by this work, we propose an end-to-end learning approach that overcomes the issues of hand-crafted algorithms, while maintaining computational efficiency.</p><p>PWC-Net <ref type="bibr" target="#b49">[50]</ref>, although designed for optical flow estimation, is related to our approach. The method uses a low resolution cost volume with multiple refinement stages via image warps and local matching cost computations. Thereby following the classical pyramidal matching approach where a low resolution result gets hierarchically upsampled and refined by initializing the current level with the previous level's solution. In contrast we propose a fast, multi-scale, high resolution initialization which is able to recover fine details that cannot be represented at low resolution. Finally, our refinement steps produce local slanted plane approximations, which are used to predict the final disparities, as opposed to standard bilinear warping and interpolation employed in <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The design of HITNet, follows the principles of traditional stereo matching methods <ref type="bibr" target="#b43">[44]</ref>. In particular, we observe that recent efficient methods rely on the three following steps: 1 compact feature representations are extracted <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>; 2 a high resolution disparity initialization step utilizes these features to retrieve feasible hypotheses; 3 an efficient propagation step refines the estimates using slanted support windows <ref type="bibr" target="#b51">[52]</ref>. Motivated by these observations, we represent the disparity map as planar tiles at various resolutions and attach a learnable feature vector to each tile hypothesis (Sec. 3.1). This allows our network to learn which information about a small part of the disparity map is relevant to further improving the result. This can be interpreted as an efficient and sparse version of the learnable 3D cost volumes that have shown to be beneficial <ref type="bibr" target="#b23">[24]</ref>.</p><p>The overall method is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. Our feature extraction module relies on a very small U-Net <ref type="bibr" target="#b41">[42]</ref>, where the multi-resolution features of the decoder are used by the rest of the pipelines. These features encode multi-scale details of the image, similar to <ref type="bibr" target="#b6">[7]</ref> (Sec. 3.2). Once the features are extracted, we initialize disparity maps as fronto parallel tiles at multiple resolutions. To do so, a matcher evaluates multiple hypotheses and selects the one with the lowest 1 distance between left and right view feature. Additionally, a compact per-tile descriptor is computed using a small network (Sec. 3.3). The output of the initialization is then passed to a propagation stage, which acts similarly to the approximated Conditional Random Field solution used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52]</ref>. This stage hierarchically refines the tile hypotheses in an iterative fashion (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tile Hypothesis</head><p>We define a tile hypothesis as a planar patch with a learnable feature attached to it. Concretely, it consists of a geometric part describing a slanted plane with the disparity d and the gradient of disparity in x and y directions (d x , d y ), and a learnable part p which we call tile feature descriptor. The hypothesis is therefore described as a vector which encodes a slanted 3D plane,</p><formula xml:id="formula_0">h = [d, d x , d y plane , p descriptor ]<label>(1)</label></formula><p>The tile feature descriptor is a learned representation of the tile which allows the network to attach additional information to the tile. This could for example be matching quality or local surface properties such as how planar the geometry actually is. We do not constrain this information and learned it end-to-end from the data instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extractor</head><p>The feature extractor provides a set of multi-scale feature maps E = {e 0 , . . . e M } that are used for initial matching and for warping in the propagation stage. We denote a feature map as e l and an embedding vector e l,x,y for locations x, y at resolution l ? 0, . . . , M , 0 being the original image resolution and M denoting a 2 M ? 2 M downsampled resolution. A single embedding vector e l,x,y is composed of multiple feature channels. We implement the feature extractor E = F(I; ? F ) as a U-Net like architecture <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b33">34]</ref>, i.e. an encoder-decoder with skip connections, with learnable parameters ? F . The network is composed of strided convolutions and transposed convolutions with leaky ReLUs as non-linearities. The set of feature maps E that we use in the remainder of the network are the outputs of the upsampling part of the U-Net at all resolutions. This means that even the high resolution features do contain some amount of spatial context. In more details, one down-sampling block of the U-Net has a single 3 ? 3 convolution followed by a 2 ? 2 convolution with stride 2. One up-sampling block applies 2 ? 2 stride 2 transpose convolutions to up-sample results of coarser U-Net resolution. Features are concatenated with skip-connection, and a 1 ? 1 convolution followed by a 3 ? 3 convolution are applied to merge the skipped and upsampled feature for the current resolution. Each upsampling block generates a feature map e l , which is then used for downstream tasks and also further upsampled in the U-Net to generate a higher resolution feature map. We run the feature extractor on the left and the right image and obtain two multi-scale representations E L and E R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Initialization</head><p>The goal of the initialization is to extract an initial disparity d init and a feature vector p init for each tile at various resolutions. The output of the initialization is fronto-parallel tile hypotheses of the form h init = [d init , 0, 0, p init ].</p><p>Tile Disparity. In order to keep the initial disparity resolution high we use overlapping tiles along the x direction, i.e. the width, in the right (secondary) image but we still use non-overlapping tiles in the left (reference) image for efficient matching. To extract the tile features we run a 4 ? 4 convolution on each extracted feature map e l . The strides for the left (reference) image and the right (secondary) image are different to facilitate the aforementioned overlapping tiles. For the left image we use strides of 4 ? 4 and for the right image we use strides of 4 ? 1, which is crucial to maintain the full disparity resolution to maximize accuracy. This convolution is followed by a leaky ReLU and an MLP.</p><p>The output of this step will be a new set of feature maps E = {? 0 , . . . ,? M } with per tile features? l,x,y . Note that the width of the feature maps in? L and? R are now different. The per-tile features are explicitly matched along the scan lines. We define the matching cost at location (x, y) and resolution l with disparity d as:</p><formula xml:id="formula_1">(l, x, y, d) = ? L l,x,y ?? R l,4x?d,y 1<label>(2)</label></formula><p>The initial disparities are then computed as:</p><formula xml:id="formula_2">d init l,x,y = argmin d?[0,D] (l, x, y, d)<label>(3)</label></formula><p>for each (x, y) location and resolution l, where D is the maximal disparity that is considered. Note that despite the fact that the initialization stage exhaustively computes matches for all disparities there is no need to ever store the whole cost volume. At test time only the location of the best match needs to be extracted, which can be done very efficiently utilizing fast memory, e.g. shared memory on GPUs and a fused implementation in a single Op. Hence, there is no need to store and process a 3D cost volume.</p><p>Tile Feature Descriptor. The initialization stage also predicts a feature description p init l,x,y for each (x, y) location and resolution l:</p><formula xml:id="formula_3">p init l,x,y = D( (d init l,x,y ),? L l,x,y ; ? D l ).<label>(4)</label></formula><p>The features are based on the embedding vector of the reference image? L l,x,y and the costs of the best matching disparity d init . We utilize a perceptron D, with learnable weights ? D , which is implemented with a 1 ? 1 convolution followed by a leaky ReLU. The input to the tile feature descriptor includes the matching costs (?), which allows the network to get a sense of the confidence of the match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Propagation</head><p>The propagation step takes tile hypotheses as input and outputs refined tile hypotheses based on spatial propagation of information and fusion of information. It internally warps the features from the feature extraction stage from the right image (secondary) to the left image (reference) in order to predict highly accurate offsets to the input tiles. An additional confidence is predicted which allows for effective fusion between hypotheses coming from earlier propagation layers and from the initialization stage.</p><p>Warping. The warping step computes the matching costs between the feature maps e L l and e R l at the feature resolution l associated to the tiles. This step is used to build a local cost volume around the current hypothesis. Each tile hypothesis is converted into a planar patch of size 4 ? 4 that it originally covered in the feature map. We denote the corresponding 4 ? 4 local disparity map as d with</p><formula xml:id="formula_4">d i,j = d + (i ? 1.5)d x + (j ? 1.5)d y ,<label>(5)</label></formula><p>for patch coordinates i, j ? {0, ? ? ? , 3}. The local disparities are then used to warp the features e R l from the right (secondary) image to the left (reference) image using linear interpolation along the scan lines. This results in a warped feature representation e R l which should be very similar to the corresponding features of the left (reference) image e L if the local disparity maps d are accurate. Comparing the features of the reference (x, y) tile with the warped secondary tile we define the cost vector ? ? ?(e, d ) ? R 16 as:</p><formula xml:id="formula_5">? ? ?(e l , d ) = [c 0,0 , c 0,1 , . . . , c 0,3 , c 1,0 . . . c 3,3 ],<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">c i,j = e L l,4x+i,4y+j ? e R l,4x+i?d i,j ,4y+j 1 .</formula><p>Tile Update Prediction. This step takes n tile hypotheses as input and predicts deltas for the tile hypotheses plus a scalar value w for each tile indicating how likely this tile is to be correct, i.e. a confidence measure. This mechanism is implemented as a CNN module U, the convolutional architecture allows the network to see the tile hypotheses in a spatial neighborhood and hence is able to spatially propagate information. A key part of this step is that we augment the tile hypothesis with the matching costs ? ? ? from the warping step. By doing this for a small neighborhood in disparity space we build up a local cost volume which allows the network to refine the tile hypotheses effectively. Concretely, we displace all the disparities in a tile by a constant offset of one disparity 1 in the positive and negative directions and compute the cost three times. Using this let a be the augmented tile hypothesis map for input tile map h:</p><formula xml:id="formula_7">a l,x,y = [h l,x,y , ? ? ?(e l , d ? 1), ? ? ?(e l , d ), ? ? ?(e l , d + 1) local cost volume ],<label>(7)</label></formula><p>for a location (x, y) and resolution l, The CNN module U l then predicts updates for each of the n tile hypothesis maps and additionally w i ? R which represent the confidence of the tile hypotheses:</p><formula xml:id="formula_8">(?h 1 l , w 1 , . . . , ?h n l , w n hypotheses updates ) = U l (a 1 l , . . . , a n l ; ? U l ). (8)</formula><p>The architecture of U is implemented with residual blocks <ref type="bibr" target="#b19">[20]</ref> but without batch normalization. Following <ref type="bibr" target="#b24">[25]</ref> we use dilated convolutions to increase the receptive field. Before running a sequence of residual blocks with varying dilation factors we run a 1?1 convolution followed by a leaky ReLU to decrease the number of feature channels. The update module is applied in a hierarchical iterative fashion (see <ref type="figure" target="#fig_0">Fig. 2</ref>). At the lowest resolution l = M we only have 1 tile hypothesis per location from the initialization stage, hence n = 1. We apply the tile updates by summing the input tile hypotheses and the deltas and upsample the tiles by a factor of 2 in each direction. Thereby, the disparity d is upsampled using the plane equation of the tile and the remaining parts of the tile hypothesis d x , d y and p are upsampled using nearest neighbor sampling. At the next resolution M ?1 we now have two hypotheses: the one from the initialization stage and the upsampled hypotheses from the lower resolution, hence n = 2. We utilize the w i to select the updated tile hypothesis with highest confidence for each location. We iterate this procedure until we reach the resolution 0, which corresponds to tile size 4 ? 4 and full disparity resolution in all our experiments. To further refine the disparity map we use the winning hypothesis for the 4 ? 4 tiles and apply propagation module 3 times: for 4 ? 4, 2 ? 2, 1 ? 1 resolutions, using n = 1. The output at tile size 1 ? 1 is our final prediction. More details about the network architecture are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Loss Functions</head><p>Our network is trained end-to-end with ground truth disparities d gt utilizing the losses described in the remainder of this section. The final loss is a sum of the losses over all the scales and pixels: l,x,y L init</p><formula xml:id="formula_9">l + L prop l + L slant l + L w l .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initialization Loss</head><p>Ground truth disparities are given with subpixel precision, however matching in initialization happens with integer disparities. Therefore we compute the matching cost for subpixel disparities using linear interpolation. The cost for subpixel disparities is then given as</p><formula xml:id="formula_10">?(d) = (d ? d ) ( d + 1) + ( d + 1 ? d) ( d ), (9)</formula><p>where we dropped the l, x, y subscripts for clarity. To compute them at multiple resolutions we maxpool the ground truth disparity maps to downsample them to the required resolution. We aim at training the features E to be such that the matching cost ? is smallest at the ground truth disparity and larger everywhere else. To achieve this, we impose an</p><formula xml:id="formula_11">1 contrastive loss [18] L init (d gt , d nm ) = ?(d gt ) + max(? ? ?(d nm ), 0),<label>(10)</label></formula><p>where ? &gt; 0 is a margin, d gt the ground truth disparity for a specific location and</p><formula xml:id="formula_12">d nm = argmin d?[0,D]/{d:d?[d gt ?1.5,d gt +1.5]} (d) (11)</formula><p>the disparity of the lowest cost non match for the same location. This cost pushes the ground truth cost toward 0 as well as the lowest cost non match toward a certain margin. In all our experiments we set the margin to ? = 1. Similar contrastive losses have been used to learn the matching score in earlier deep learning based approaches to stereo matching <ref type="bibr">[60,</ref><ref type="bibr" target="#b35">36]</ref>. However, they either used a random non-matching location as negative sample or used all the non matching locations as negative samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Propagation Loss</head><p>During propagation we impose a loss on the tile geometry d, d x , d y and the tile confidence w. We use the ground truth disparity d gt and ground truth disparity gradients d gt x and d gt y , which we compute by robustly fitting a plane to d gt in a 9?9 window centered at the pixel. In order to apply the loss on the tile geometry we first expand the tiles to a full resolution disparitiesd using the plane equation (d, d x , d y ) analogously to Eq. 5. The slant portion is also up-sampled to full resolution using nearest neighbor approach before slant loss is applied. We use the general robust loss function ?(?) from <ref type="bibr" target="#b1">[2]</ref> which resembles a smooth 1 loss, i.e., Huber loss. Additionally, we apply a truncation to the loss with threshold A</p><formula xml:id="formula_13">L prop (d, dx, dy) = ?(min(|d diff |, A), ?, c),<label>(12)</label></formula><p>where d diff = d gt ?d. Further we impose a loss on the surface slant, as</p><formula xml:id="formula_14">L slant (d x , d y ) = d gt x ? d x d gt y ? d y 1 ? |d diff |&lt;B ,<label>(13)</label></formula><p>where ? is an indicator function which evaluates to 1 when the condition is satisfied and 0 otherwise. To supervise the confidence w we impose a loss which increases the confidence if the predicted hypothesis is closer than a threshold C 1 from the ground truth and decrease the confidence if the predicted hypothesis is further than a threshold C 2 away from the ground truth.</p><formula xml:id="formula_15">L w (w) = max(1?w, 0)? |d diff |&lt;C1 +max(w, 0)? |d diff |&gt;C2 (14) For all our experiments A = B = C 1 = 1; C 2 = 1.5.</formula><p>For the last several levels, when only a single hypotheses is available, loss is applied to all pixels ( A = ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the proposed approach on popular benchmarks showing competitive results at a fraction of the computational time compared to other methods. We consider the following datasets: SceneFlow <ref type="bibr" target="#b37">[38]</ref>, KITTI 2012 <ref type="bibr" target="#b14">[15]</ref>, KITTI 2015 <ref type="bibr" target="#b38">[39]</ref>, ETH3D <ref type="bibr" target="#b44">[45]</ref>, Middlebury dataset V3 <ref type="bibr" target="#b42">[43]</ref>. Following the standard evaluation settings we consider the two popular metrics: the End-Point-Error (EPE), which is the absolute distance in disparity space between the predicted output and the groundtruth; the x-pixels error, which is the percentage of pixels with disparity error greater than x. For the EPE computation on SceneFlow we adopt the same methodology of PSMNet <ref type="bibr" target="#b6">[7]</ref>, which excludes all the pixel with ground truth disparity bigger than 192 from the evaluation. Unless stated otherwise we use a HITNet with 5 levels, i.e. M = 4.</p><p>In this section we focus on comparisons with stateof-art on popular benchmarks, detailed ablation studies, run-time breakdown, cross-domain generalization and additional evaluations, are provided in the supplementary material.</p><p>The trained models used for submission to benchmarks and evaluation scripts can be found at https://github.com/google-research/googleresearch/tree/master/hitnet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparisons with State-of-the-art</head><p>SceneFlow. On the synthetic dataset SceneFlow "finalpass" we achieve the remarkable End-Point-Error (EPE) of 0.36, which is 2X better than state-of-art at time of writing (see supplementary materials for details of L and XL versions). Representative competitors are reported in Tab. 1. The PSMNet algorithm <ref type="bibr" target="#b6">[7]</ref> performs multi-scale feature extraction similarly to our method, but in contrast they use a more sophisticated pooling layer. Here we show that   our architecture is more effective. Compared to GA-Net [61], we do not need complex message passing steps such as SGM. The results we obtain show that our strategy is also achieving a very similar inference. Finally, a representative fast method, StereoNet <ref type="bibr" target="#b24">[25]</ref> is considered, which we consistently outperform. As result our method achieves the lowest EPE while still maintaining real-time performance. See <ref type="figure" target="#fig_1">Figure 3</ref> for qualitative results.</p><p>Middlebury Stereo Dataset v3. We evaluated our method with multiple state-of-art approaches on the Middlebury stereo dataset v3, see <ref type="table">Table 4</ref> and the official benchmark website. <ref type="bibr" target="#b1">2</ref> . As we can observe we outperform all the other end-to-end learning based approaches on most of the metrics, we rank among the top 10 when considering also hand-crafted approaches and in particular we rank first for bad 0.5 and A50, second for bad 1 and avgerr. In addition, we note that our average error is impacted by specifically one image, DjembL, which is due to the fact that we do not explicitly handle harsh lighting variations between input pairs. For visual results on the Middlebury datasets and details regarding the training procedure we refer the reader to the supplementary material.</p><p>ETH3D two view stereo. We evaluated our method with multiple state-of-art approaches on the ETH3D dataset, see Tab. 2. At time of submission to benchmark, HITNet ranks 1 st -4 rd on all the metrics published on the website. In particular, our method ranks 1 nd on the following metrics: bad 0.5, bad 4, average error, rms error, 50% quantile, 90% quantile: this shows that HITNet is resilient to the particular measurement chosen, whereas competitive approaches exhibits substantial differences when different metrics are selected. See the submission website for details. <ref type="bibr" target="#b2">3</ref>   compares favorably to GC-Net <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b39">[40]</ref> and many others. Recent methods such as GA-Net [61] and HSM <ref type="bibr" target="#b53">[54]</ref> are obtaining slightly better metrics, although they require 1.8 and 0.15 seconds respectively. Note also that HSM <ref type="bibr" target="#b53">[54]</ref> has been trained with additional external high resolution data. Similarly, GA-Net [61] is pre-trained on SceneFlow and fine-tuned on KITTI benchmarks, whereas our approach is fully trained on the small data available on KITTI. Compared to fast methods such as StereoNet <ref type="bibr" target="#b24">[25]</ref> and RTSNet <ref type="bibr" target="#b28">[29]</ref>, our method consistently outperforms them by a considerable margin, showing that it can be employed in latency critical scenarios without sacrificing accuracy. 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented HITNet, a real-time end-to-end architecture for accurate stereo matching. We presented a fast initialization step that is able to compute high resolution matches using learned features very efficiently. These tile initializations are then fused using propagation and fusion steps. The use of slanted support windows with learned descriptors provides additional accuracy. We presented state-of-the art accuracy on multiple commonly used benchmarks. A limitation of our algorithm is that it needs to be trained on a dataset with ground truth depth. To address this in the future we are planning to investigate self-supervised methods and self-distillation methods to further increase the accuracy and decrease the amount of training data that is required. A limitation of our experiments is that different datasets are trained on separately and use slightly different model architectures. To address this in the future, a single experiment is required that aligns with Robust Vision Challenge requirements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>In this section we add additional details regarding the training procedure and discuss difference among datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training Setup</head><p>The SceneFlow dataset consists of 3 components (Flyingthings, Driving and Monkaa) and comes with a predefined train and test split with ground truth for all examples. Following the standard practice with this dataset we use the predefined train and test split for all experiments. We also used only FlyingThings part of the dataset, as Driving and Monkaa don't have corresponding TEST sets and including them into training hurts accuracy for both Sceneflow and when it's used to pre-train for Middlebury. When all 35k images are used to train a model, the PSM EPE of XL model is 0.41 on "finalpass". We considered random crops of 320 ? 960 and a batch size of 8, and a maximum disparity of 320. We trained for 1.42M iterations using the Adam optimizer, starting from a learning rate of 4e ?4 , dropping it to 1e ?4 , then to 4e ?5 , then to 1e ?5 after 1M, 1.3M, 1.4M iterations respectively. The general robust loss for Scene-Flow experiments was applied with, ? = 0.9, c = 0.1. For all other experiments, ? = 0.8, c = 0.5.</p><p>For real world datasets such as KITTI 2012 and 2015 a training set with ground truth and a test set where the ground truth is not available is provided. For the benchmark submission we trained the network on all 394 images available from both datasets. For ablation studies on the KITTI dataset we split training set into a train and validation set with 75% of the data in the training set and 25% of the data in the validation set. We trained with data augmentation, batch-size of 4 and random crops of 311?1178 and a maximum disparity of 256. The training schedule followed the following step: 400k iterations with learning rate 4e ?4 , followed by 8k iterations with learning rate 1e ?4 , followed by 2k iterations with learning rate 4e ?5 . Note that the network is not pre-trained on any other datasets as in <ref type="bibr" target="#b53">[54]</ref>, and a small training set is sufficient for our method to achieve good performance.</p><p>Indeed, empirically we found that using a small initial learning rate 1e ?4 and training for longer achieves the best results on multiple datasets without showing sign of overfitting. In <ref type="figure" target="#fig_3">Figure 5</ref> we show the evolution of the training HITNet L for more than 200 epochs (learning rate change to 1e ?5 after 200 epochs) on the SceneFlow cleanpass dataset. We also compared this scheme with using a higher starting learning rate (1e ?3 ): after 10 epochs we observed EPE of 0.85 for 1e ?4 and 0.66 for 1e ?3 . Although 1e ?3 achieved smaller error within a few epochs, our experiments confirm that longer training with a small learning rate is beneficial to achieve higher quality results without overfitting. See also generalization experiment showing that the method has very good cross-dataset performance. The training set for the real world ETH3D stereo dataset <ref type="bibr" target="#b44">[45]</ref> contains just a few stereo pairs, so additional data is needed to avoid overfitting. For the benchmark submission we trained the network on all 394 images from both KITTI datasets, as well as all half and quarter resolution training images from Middlebury dataset V3 <ref type="bibr" target="#b42">[43]</ref> and training images from ETH3D dataset. We used the same training parameters as for KITTI submission and stopped training after 115k iterations, which was picked using 4 fold crossvalidation on ETH3D training set. Note that there is no additional training, pre-training, finetuning.</p><p>Similarly, the Middlebury dataset <ref type="bibr" target="#b42">[43]</ref> contains a limited training set. To avoid overfitting, we pre-trained the model on SceneFlow's FlyingThings TRAIN set with data augmentation, then fine-tuned on the 23 Middlebury14-perfectH training images, while keeping all data augmentations on. Specifically, we used a HITNet Large model with initialization at 6 scales (M=5), pre-trained it for 445k iterations, using batch size of 8 and random crops of 512 ? 960. We initialize the learning rate to 4e ?4 , then gradually drop it to 1e ?4 , 4e ?5 and 1e ?5 after 300K, 400K, 435K iterations respectively. Finally, we fine-tuned the model for 5K iterations at 1e ?5 learning rate. These parameters were selected by using 4-fold cross validation on Middlebury training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Data Augmentation</head><p>The training data available may not be fully representative of the actual test sets for small real world datasets such as KITTI, ETH3D and Middlebury. Indeed, we often observed substantial differences at test time, such as changes in brightness, unexpected reflections and mis-calibrations. In order to improve the network robustness we performed the following augmentations. We first perturb the brightness and contrast of left and right images by using random symmetric and asymmetric multiplicative adjustments. Symmetric adjustments are sampled within [0.8, 1.2] interval and asymmetric between [0.95, 1.05]. Similar to <ref type="bibr" target="#b54">[55]</ref>, We then replace random areas of the right image with random crops taken from another portion of the right image: this helps the network to deal with occluded areas and encourages a better "inpainting". The crop size to be replaced is randomly sampled between <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b49">50]</ref> and <ref type="bibr">[180,</ref><ref type="bibr">250]</ref>.</p><p>Finally, the Middlebury images contains a substantially different color distribution compared to other datasets. To mitigate this we used the approach from <ref type="bibr" target="#b47">[48]</ref> that brings color distribution of training images closer to that of Middlebury set and during test time we normalize color distribution between left and right images of a stereopair. Additionally, similar to <ref type="bibr" target="#b54">[55]</ref>, in order to deal with miscalibrated pairs of this dataset, we augmented the training data with random y offset between [?2, 2] pixels. The random values for y offset are generated at a low resolution [H/64, W/64], and then bilineraly up-sampled to full resolution [H, W] of the input image. To simulate different noise levels images with different exposure contain, we add Gaussian random noise with variance sampled between [0 and 5] intensity levels once for the whole image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Evaluations</head><p>In this section we show additional qualitative results on real-world datasets. In <ref type="figure" target="#fig_2">Figure 4</ref> we show comparisons of our method with other approaches. We consider multiple representative competitors such as: GC-Net <ref type="bibr" target="#b23">[24]</ref>, which uses the full cost volume and 3D convolutions to infer context, RTS-Net <ref type="bibr" target="#b28">[29]</ref> that has similar inference time than HIT-Net, and finally GA-Net [61], as one of the best performing methods in terms of accuracy.</p><p>Our method compares very favorably to other approaches such as GC-Net and fast methods like RTSNet and is on par with the state-of-the-art approaches, e.g. GA-Net [61]. Note how our method retrieves fine structures and crisp edges, while only training on the KITTI datasets, which exhibit significant edge fattening artifacts.</p><p>Similarly, in <ref type="figure">Figure 7</ref> we show qualitative results on the Middlebury dataset <ref type="bibr" target="#b42">[43]</ref>. For each image, we compare HIT-Net with the best performing competitor on the Bad 0.5 metric. Note how our method is able to produce crisp edges, correct occlusions and thin structures in all the considered cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Intermediate Outputs</head><p>We show intermediate outputs from within our network in <ref type="figure" target="#fig_4">Fig 6.</ref> We observe that with increasing resolution the disparity gets more fine grained and the details from the higher resolution initialization gets merged into the global context that is coming from the lower resolutions. Note that our results on the KITTI 2015 dataset are only trained on the KITTI datasets from scratch without any pre-training on other data sources. This means the network has not been supervised on the top one third of the image as these datasets do only provide ground truth for the bottom two thirds of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Generalization.</head><p>We finally demonstrate the cross-domain adaptation capabilities of our method. Following the protocol in <ref type="bibr" target="#b48">[49]</ref>, we trained HITNet on SceneFlow with data augmentations and tested on KITTI 2012 and KITTI 2015 respectively. We also considered multiple competitors as in <ref type="bibr" target="#b48">[49]</ref> and report the results in Tab. 5: note how our method shows superior generalization results compared to all the other state-of-theart approaches. This shows that our method is able to effectively generalize to unseen dataset even without explicit fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Ablation Study</head><p>We analyze the importance of our proposed components. The full HITNet is considered as baseline and compared with a version where features are removed. The ablation study is performed on the SceneFlow "finalpass" data and KITTI 2012. See <ref type="figure" target="#fig_6">Figure 8</ref> for a qualitative evaluation.</p><p>Multi Scale Prediction. The multi-scale feature affects both initialization and propagation stages. In Tab. 6, we report the results for the full model (HITNet) on KITTI 2012, with 5 scales, results for 4 scales and finally we removed the multi-resolution prediction completely. When we evaluated the same settings on the synthetic SceneFlow dataset we did not find a substantial differences between a single scale or multiple ones: clearly the synthetic dataset contains much more textured regions that do not benefit of additional context during propagation, whereas real world scenarios are full of textureless scenes (e.g. walls), where the multiresolution approach is naturally performing better.</p><p>4x4x4 Downsampled. Initialization at full disparity resolution provides a compelling starting point to the network, which can focus mostly on refining the prediction. In Tab. 6 we show that using tile resolution for disparity (cost volume  16x16x8 Downsampled. Decreasing the resolution of the cost volume for all dimensions similar to <ref type="bibr" target="#b49">[50]</ref> degrades accuracy (16X downsampled in H and W, 8X in D).</p><p>16x16x1 Downsampled. Using larger tiles, while maintaining disparity resolution degrades accuracy even more, as the network is not able to reason about precise disparity at low spatial resolution during initialization.</p><p>Slant Prediction. In this experiment, we forced tile hypotheses to always be fronto parallel by setting d x and d y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>HITNet CRL <ref type="bibr" target="#b39">[40]</ref> iResNet <ref type="bibr" target="#b31">[32]</ref>    <ref type="table">Table 6</ref>: Ablation study of the proposed HITNet on SceneFlow <ref type="bibr" target="#b37">[38]</ref> and KITTI 2012 <ref type="bibr" target="#b14">[15]</ref> datasets. Lower is better.</p><p>to 0 and using bilinear interpolation for upsampling. As showed in Tab. 6, removing the slant prediction leads to a substantial drop in precision for both SceneFlow and KITTI 2012. Moreover the network loses its inherent capability of predicting some notion of surface normals that can be useful for many applications such as plane detection.</p><p>Tile Features. Here we removed the additional features predicted on each tile during the initialization and propagation steps. This turns out to be a useful component and without it we observe a decrease in accuracy for both datasets.</p><p>Warping. The image warps are used to compute the matching cost during the propagation. Removing this step hurts the subpixel precision as demonstrated in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Size. Finally, we tested if an increase in the model size is beneficial or not. In particular we double the channels in the feature extractor, and use 32 channels and 6 residual blocks for the last 3 propagation steps, this resorts to a run-time increase to 54ms. As expected this has an improvement on SceneFlow as reported in Tab. 6, HITNet Large; however for the small KITTI datasets this did not improve performance due to over-fitting. Further increasing model size by using 64 channels for the last 3 propagation steps improved SceneFlow results, increased runtime to 114ms, and increased over-fitting on a smaller dataset. We don't see a reason to explore larger model sizes on a synthetic dataset as it will add to over-fitting on smaller real datasets that are publicly available.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Runing Time Details</head><p>The HITNet architecture used for ETH3d and KITTI experiments runs at 19ms per frame on a Titan V GPU for 0.5Mpixel (KITTI resolution) input images. The majority of the time is spent during the last 3 propagation steps (7.5 ms) that operate on higher resolutions. The multi-scale propagation steps use down-sampled data and contribute less than 5ms. Efficient implementation of initialization using a single fused Op generates initial disparity estimates across all resolutions in 0.25ms, with feature extractor contributing 6ms. For Middlebury experiments the model has a run-time of approximately 107.5ms per Mpix of input resolution using custom CUDA operations for initialization and warping, when maximum disparity is 160 and the run-time scales linearly with resolution. The run-time has a small increase with disparity range, and is about 109ms per Mpix for a maximum disparity of 1024. Without custom CUDA operations the run-time is increased by a factor of 3, as a single warping operation contains more than a hundred simple operations over large tensors, and while it's trivial to fuse them together, not doing so results in most of the time spent on global memory access. When tested on an 18-core Xeon 6154 CPU, the default tensorflow runtime runs 3.3s per Mpix, which would translate to about 60s for a single threaded runtime, which compares favourably to other CPU methods. The CPU tensorflow runtime does make use of SIMD instruction set, which other methods may not utilize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Number of Parameters</head><p>An important aspect of efficient neural network architectures is the number of parameters they have. This will influence the amount of compute required and the amount   <ref type="table">Table 7</ref>: Comparisons of number of parameters and GMacs (Giga Multiply-accumulate operations) with other methods on Scene Flow "finalpass" dataset. The numbers were partially adopted from the papers cited in the table. The lower the better.</p><p>of memory needed to store them. Moreover, being able to achieve good performance with fewer numbers of parameters makes the network less susceptible to over-fitting. In Tab. 7 we show that our network is able to achieve better results than other approaches with a significantly lower number of parameters and compute.   <ref type="figure">Figure 11</ref>: ResNet blocks. First 3x3 convolution generates a local state for the tile, which is incrementally updated by each block. The final state generates tile updates. Each block may use dilated convolutions to increase the speed of diffusion.</p><p>Having less parameters also increases the generalization capabilities of the proposed method: indeed less learnable weights implies that the network is less prone to overfitting -our approach is able to outperform multiple state-of-theart baselines when trained on synthetic data and tested in real-world scenarios. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed framework. (Top) A U-Net is used to extract features at multiple scales from left and right images. The initialization step is run on each scale of the extracted features. This step operates on tiles of 4 ? 4 feature regions and evaluates multiple disparity hypotheses. The disparity with the minimum cost is selected.(Bottom)The output of the initialization is then used at propagation stage to refine the predicted disparity hypotheses using slanted support windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on SceneFlow and KITTI 2012. Note how the model is able to recover fine details, textureless regions and crisp edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative Results on KITTI 2012 and 2015. Note how HITNet is able to recover fine structures and crisp edges using a fraction of the computational cost required by other competitors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>We show the evolution of the training reporting the EPE on training and test set respectively. Note how the scheme reduces the error on both training and test set without showing signs of overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Intermediate results of our network on the left side we show the disparity maps that the matching of the initialization stage provides. On the right hand side we show the final disparity and normals for each resolution. The final two resolutions are 2x2 and 1x1 tiles of the highest resolution feature map, while the initialization is always computed on 4x4 tiles of the feature maps. is 4X downsampled in H, W and D dimensions), the accuracy substantially drops. This demonstrates the importance of our proposed fast high resolution initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 9</head><label>9</label><figDesc>provides more details for the initialization module described in the main paper. Similarly Fig 10 and Fig 11 show how resblocks are integrated into propagation logic. Finally, Fig 12 depicts differences between propagation steps when a single hypothesis or multiple hypotheses are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Ablation study, qualitative evaluation. Note how our HITNet model relies on all proposed design choices in order to achieve the best results on fine details, edges and occluded regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Initialization: The features extracted by the feature extractor are matched and inital tile features are computed. The number of feature channels C depends on feature extractor architecture and the current level (see Sec. C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Propagation, for the single hypothesis case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>h init +h' init , w init &gt;w coarse h out = h coarse +h' coarse , w init &lt;=w coarseRight featuresLeft features Propagation with multiple hypotheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparisons with state-of-the-art methods on ETH3D stereo dataset. For all metrics lower is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">KITTI 2012 [15]</cell><cell></cell><cell></cell><cell cols="3">KITTI 2015 [39]</cell></row><row><cell>Method</cell><cell cols="5">2-noc 2-all 3-noc 3-all EPE noc</cell><cell>EPE all</cell><cell cols="4">D1-bg D1-fg D1-all Run-time</cell></row><row><cell>HITNet (ours)</cell><cell>2.00</cell><cell>2.65</cell><cell>1.41</cell><cell cols="3">1.89 0.4 0.5</cell><cell>1.74</cell><cell>3.20</cell><cell>1.98</cell><cell>0.02s</cell></row><row><cell>LEAStereo [8]</cell><cell>1.90</cell><cell>2.39</cell><cell>1.13</cell><cell cols="3">1.45 0.4 0.5</cell><cell>1.40</cell><cell>2.91</cell><cell>1.65</cell><cell>0.3s</cell></row><row><cell>GANet-deep [61]</cell><cell>1.89</cell><cell>2.50</cell><cell>1.19</cell><cell>1.6</cell><cell cols="2">0.4 0.5</cell><cell>1.48</cell><cell>3.46</cell><cell>1.81</cell><cell>1.8s</cell></row><row><cell>EdgeStereo-V2 [49]</cell><cell>2.32</cell><cell>2.88</cell><cell>1.46</cell><cell cols="3">1.83 0.4 0.5</cell><cell>1.84</cell><cell>3.30</cell><cell>2.08</cell><cell>0.32s</cell></row><row><cell>GC-Net [24]</cell><cell>2.71</cell><cell>3.46</cell><cell>1.77</cell><cell cols="3">2.30 0.6 0.7</cell><cell>2.21</cell><cell>6.16</cell><cell>2.87</cell><cell>0.9s</cell></row><row><cell>SGM-Net [46]</cell><cell>3.60</cell><cell>5.15</cell><cell>2.29</cell><cell cols="3">3.50 0.7 0.9</cell><cell>2.66</cell><cell>8.64</cell><cell>3.66</cell><cell>67s</cell></row><row><cell>ESMNet [17]</cell><cell>3.65</cell><cell>4.30</cell><cell>2.08</cell><cell cols="3">2.53 0.6 0.7</cell><cell>2.57</cell><cell>4.86</cell><cell>2.95</cell><cell>0.06s</cell></row><row><cell>MC-CNN-acrt [60]</cell><cell>3.90</cell><cell>5.45</cell><cell>2.09</cell><cell cols="3">3.22 0.6 0.7</cell><cell>2.89</cell><cell>8.88</cell><cell>3.89</cell><cell>67s</cell></row><row><cell>RTSNet [29]</cell><cell>3.98</cell><cell>4.61</cell><cell>2.43</cell><cell cols="3">2.90 0.7 0.7</cell><cell>2.86</cell><cell>6.19</cell><cell>3.41</cell><cell>0.02s</cell></row><row><cell>Fast DS-CS [56]</cell><cell>4.54</cell><cell>5.34</cell><cell>2.61</cell><cell cols="3">3.20 0.7 0.8</cell><cell>2.83</cell><cell>4.31</cell><cell>3.08</cell><cell>0.02s</cell></row><row><cell>StereoNet [25]</cell><cell>4.91</cell><cell>6.02</cell><cell>-</cell><cell>-</cell><cell cols="2">0.8 0.9</cell><cell>4.30</cell><cell>7.45</cell><cell>4.83</cell><cell>0.015s</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">KITTI 2012 and 2015. At time of writing, among the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">published methods faster than 100ms, HITNet ranks #1 on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">KITTI 2012 and 2015 benchmarks. Compared to other</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">state-of-the-art stereo matchers (see Tab. 3), our approach</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :Table 4 :</head><label>34</label><figDesc>Quantitative evaluation on KITTI 2012 and KITTI 2015. For KITTI 2012 we report the percentage of pixels with error bigger than x disparities in both non-occluded (x-noc) and all regions (x-all), as well as the overall EPE in both non</figDesc><table><row><cell>Method</cell><cell cols="7">RMS AvgErr Bad 0.5 Bad 1.0 Bad 2.0 Bad 4.0 A50</cell><cell>Run-time</cell></row><row><cell>HITNet (ours)</cell><cell>9.97</cell><cell>1.71</cell><cell>34.2</cell><cell>13.3</cell><cell>6.46</cell><cell>3.81</cell><cell>0.40</cell><cell>0.14 s</cell></row><row><cell>LEAStereo [8]</cell><cell>8.11</cell><cell>1.43</cell><cell>49.5</cell><cell>20.8</cell><cell>7.15</cell><cell>2.75</cell><cell>0.53</cell><cell>2.9 s</cell></row><row><cell cols="2">NOSS-ROB [30] 12.2</cell><cell>2.08</cell><cell>38.2</cell><cell>13.2</cell><cell>5.01</cell><cell>3.46</cell><cell cols="2">0.42 662s (CPU)</cell></row><row><cell>LocalExp [51]</cell><cell>13.4</cell><cell>2.24</cell><cell>38.7</cell><cell>13.9</cell><cell>5.43</cell><cell>3.69</cell><cell cols="2">0.43 881s (CPU)</cell></row><row><cell>CRLE [53]</cell><cell>13.6</cell><cell>2.25</cell><cell>38.1</cell><cell>13.4</cell><cell>5.75</cell><cell>3.90</cell><cell cols="2">0.42 1589s (CPU)</cell></row><row><cell>HSM [54]</cell><cell>10.3</cell><cell>2.07</cell><cell>50.7</cell><cell>24.6</cell><cell>10.2</cell><cell>4.83</cell><cell>0.56</cell><cell>0.51 s</cell></row><row><cell>MC-CNN [60]</cell><cell>21.3</cell><cell>3.82</cell><cell>40.7</cell><cell>17.1</cell><cell>8.08</cell><cell>4.91</cell><cell>0.45</cell><cell>150 s</cell></row><row><cell>EdgeStereo [49]</cell><cell>9.84</cell><cell>2.67</cell><cell>55.6</cell><cell>32.4</cell><cell>18.7</cell><cell>10.8</cell><cell>0.72</cell><cell>0.35 s</cell></row></table><note>occluded (EPE-noc) and all the pixels (EPE-all). For KITTI 2015 We report the percentage of pixels with error bigger than 1 disparity in background regions (bg), foreground areas (fg), and all.Comparisons with state-of-the-art methods on Middlebury V3 dataset. For all metrics lower is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>[ 58 ]</head><label>58</label><figDesc>Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 2 [59] Jure Zbontar and Yann LeCun. Computing the stereo matching cost with a convolutional neural network. In IEEE conference on computer vision and pattern recognition (CVPR),</figDesc><table><row><cell>2015. 2</cell></row><row><cell>[60] Jure Zbontar and Yann LeCun. Stereo matching by training</cell></row><row><cell>a convolutional neural network to compare image patches.</cell></row><row><cell>Journal of Machine Learning Research (JMLR), 2016. 2, 6,</cell></row><row><cell>8</cell></row><row><cell>[61] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and</cell></row><row><cell>Philip HS Torr. Ga-net: Guided aggregation net for end-</cell></row><row><cell>to-end stereo matching. In IEEE Conference on Computer</cell></row><row><cell>Vision and Pattern Recognition (CVPR), 2019. 3, 7, 8, 13,</cell></row><row><cell>17</cell></row><row><cell>[62] Yinda Zhang, Sameh Khamis, Christoph Rhemann, Julien</cell></row><row><cell>Valentin, Adarsh Kowdle, Vladimir Tankovich, Michael</cell></row><row><cell>Schoenberg, Shahram Izadi, Thomas Funkhouser, and Sean</cell></row><row><cell>Fanello. ActiveStereoNet: End-to-end self-supervised learn-</cell></row><row><cell>ing for active stereo systems. European Conference on Com-</cell></row><row><cell>puter Vision (ECCV), 2018. 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Generalization Experiment. We trained each method on SceneFlow with data augmentation and tested on KITTI 2012 and 2015. Note how our method outperforms the others.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SceneFlow finalpass [38]</cell><cell></cell><cell cols="2">KITTI 2012 [15]</cell><cell></cell></row><row><cell>Model</cell><cell>EPE</cell><cell>0.1 px</cell><cell>1 px</cell><cell>3 px</cell><cell>EPE</cell><cell>2 px</cell><cell>3 px</cell></row><row><cell>HITNet</cell><cell cols="7">0.529 px 24.0 % 5.52 % 3.00 % 0.484 px 2.91 % 2.00 %</cell></row><row><cell>4 Scales</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.507 px 3.10 % 2.20 %</cell></row><row><cell>No Multi-scale</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.747 px 4.76 % 3.62 %</cell></row><row><cell>4x4x4 Downsampled</cell><cell cols="7">0.561 px 26.4 % 5.88 % 3.15 % 0.526 px 3.16 % 2.19 %</cell></row><row><cell cols="8">16x16x8 Downsampled 0.615 px 27.5 % 6.36 % 3.39 % 0.536 px 3.35 % 2.30 %</cell></row><row><cell cols="8">16x16x1 Downsampled 0.651 px 31.9 % 7.28 % 3.62 % 0.554 px 3.60 % 2.51 %</cell></row><row><cell>No Warping</cell><cell cols="7">0.588 px 31.6 % 5.88 % 3.10 % 0.602 px 3.72 % 2.54 %</cell></row><row><cell>No Slant Prediction</cell><cell cols="7">0.548 px 25.2 % 5.74 % 3.08 % 0.513 px 3.23 % 2.18 %</cell></row><row><cell>No Tile Features</cell><cell cols="7">0.538 px 24.7 % 5.64 % 3.01 % 0.488 px 3.04 % 2.06 %</cell></row><row><cell>HITNet L</cell><cell cols="7">0.43 px 20.7 % 4.70 % 2.57 % 0.490 px 2.98 % 2.13 %</cell></row><row><cell>HITNet XL</cell><cell cols="7">0.36 px 18.2 % 4.09 % 2.21 % 0.492 px 3.11 % 2.20 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>-scale feature extractor with<ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32</ref> channels at corresponding resolutions. During Initialization step the first convolution over 4 ? 4 tiles outputs 16 channels, followed by 2-layer MLP with 32 and 16 channels and ReLU non-linearities. Tile descriptor has 13 channels by default, residual blocks use 32 channels, unless mentioned otherwise. Each intermediate propagation steps use 2 residual blocks without dilations. At each spatial resolutions, the propagation module uses feature maps from appropriate scale: full-resolution feature maps for 4 ? 4 tiles, 2X downsampled for coarser tiles that have size 8 ? 8 in full resolution, but sample 4 ? 4 pixels in coarser feature map, etc till 16X downsampled and 64 ? 64 tiles in original resolution. The last 3 levels of propagation start at 4?4 tiles and progressively in-paint and refine strong correct disparity at the edges over larger regions. To achieve that, they operate on coarse feature maps: the 4 ? 4 tiles use 4X downsampled features for warping, the 2 ? 2 tiles use 2X downsampled features for warping, the 1 ? 1 tiles use full-resolution features for warping.In HITNet model used in KITTI and ETH3d experi-Figure 7: Qualitative comparisons on Middlebury dataset. For each image we compare our method with the best performing competitor following the Bad 0.5 metric. Note how our method is able to produce crisp edges, correct occlusions and thin structures in all the considered cases. Sceneflow experiments uses 16, 16, 24, 24, 32 channels for feature extractor. A single initialization at 4x4 tiles. Last 3 propagation steps use 6, 6, 6 residual blocks with 32, 32, 16 channels and 1, 2, 4, 8, 1, 1 dilations. HITNetL model used in Sceneflow experiments uses 32, 40, 48, 56, 64 channels for feature extractor. A single initialization at 4x4 tiles. Last 3 propagation steps use 6, 6, 6 residual blocks with 32, 32, 32 channels and 1, 2, 4, 8, 1, 1 dilations. HITNetXL model used in Sceneflow experiments uses 32, 40, 48, 56, 64 channels for feature extractor. A single initialization at 4x4 tiles. Last 3 propagation steps use 6, 6, 6 residual blocks with 64, 64, 64 channels and 1, 2, 4, 8, 1, 1 dilations. HITNet model used in Middlebury experiments uses 32, 40, 48, 56, 64 channels for feature extractor. Last 3 propagation steps use 6, 6, 6 residual blocks with 32, 32, 32 channels and 1, 2, 4, 8, 1, 1 dilations.</figDesc><table><row><cell>The metrics on "clean-pass" for XL version are: 0.31 epe, 15.6 bad 0.1, 3.67 bad 1.0, 1.99 bad 3.0. C. Model Architecture Details By default, the HITNet architecture is implemented with a 5ments last 3 propagation steps use 4, 4, 2 residual blocks with 32, 32, 16 channels and 1, 3, 1, 1; 1, 3, 1, 1; 1, 1 dila-tions. scripts to run them are available at https://github.com/google-research/google-HITNet model used in The models used for submission for benchmarks and research/tree/master/hitnet</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Additional approaches faster than 100ms are on the leaderboard but the algorithms are unpublished and hence it is unknown how the results were achieved.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See "HITNet" entry on the official dataset website.<ref type="bibr" target="#b2">3</ref> See the ETH3D Website at https://www.eth3d.net/low res two view for the complete metrics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">See the KITTI Website at http://www.cvlibs.net/datasets/kitti/eval stereo.php for the complete metrics.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Shahram Izadi for support and enabling of this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general and adaptive robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pmbp: Patchmatch belief propagation for correspondence field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple but effective tree structures for dynamic programming-based stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications (VISAPP)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Patchmatch stereo-stereo matching with slanted support windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">StereoDRNet: Dilated residual stereonet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeppruner: Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4383" to="4392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to be a depth camera for close-range human capture and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transaction On Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperdepth: Learning depth from structured light without matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low compute and fully parallel computer vision with hashmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Sean Ryan Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ciliberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inernational Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ultrastereo: Efficient learning-based matching for active stereo systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Sean Ryan Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient belief propagation for early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detect, replace, refine: Deep structured prediction for pixel wise labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning efficient stereo matching network with depth discontinuity aware super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Literature survey on stereo vision disparity map algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Affendi</forename><surname>Rostam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidi</forename><surname>Hamzah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sensors</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmaa</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">StereoNet: Guided hierarchical refinement for edge-aware depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Karner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computing visual correspondence with occlusions using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Valentin, and Shahram Izadi. The need 4 speed in real-time dense visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transaction On Graphics (TOG)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time stereo matching network with high accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Superpixel alphaexpansion and normal adjustment for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penglei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinguo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAD/Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SPM-BP: Sped-up patchmatch belief propagation for continuous mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Patch match filter: Efficient edge-aware filtering meets randomized search for fast correspondence field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cooperative computation of stereo disparity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Workshop on Image Sequence Analysis (ISA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Js Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision-Workshop on Geometry Meets Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Nesic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with highresolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SGM-Nets: Semi-global matching with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved stereo matching with constant highway networks and reflective confidence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adastereo: A simple and efficient approach for adaptive stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04627</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Edgestereo: An effective multi-task learning network for stereo matching and edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PWC-Net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Continuous 3D Label Stereo Matching using Local Expansion Moves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2725" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sos: Stereo matching in o(1) with slanted support windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schoenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">Ryan</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Dzitsiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Crosspatch-based rolling label expansion for dense stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="63470" to="63481" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hierarchical deep stereo matching on highresolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Manela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hierarchical deep stereo matching on highresolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Manela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5510" to="5519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast deep stereo with 2d convolutional processing of cost signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Locally adaptive supportweight approach for visual correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Kuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-So</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
