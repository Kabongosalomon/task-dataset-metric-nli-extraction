<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QMagFace: Simple and Accurate Quality-Aware Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terh?rst</surname></persName>
							<email>philipp.terhoerst@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ihlefeld</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Darmstadt</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Huber</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research IGD</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Raja</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<settlement>Gj?vik</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
						</author>
						<title level="a" type="main">QMagFace: Simple and Accurate Quality-Aware Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose QMagFace, a simple and effective face recognition solution (QMagFace) that combines a quality-aware comparison score with a recognition model based on a magnitude-aware angular margin loss. The proposed approach includes model-specific face image qualities in the comparison process to enhance the recognition performance under unconstrained circumstances. Exploiting the linearity between the qualities and their comparison scores induced by the utilized loss, our quality-aware comparison function is simple and highly generalizable. The experiments conducted on several face recognition databases and benchmarks demonstrate that the introduced qualityawareness leads to consistent improvements in the recognition performance. Moreover, the proposed QMagFace approach performs especially well under challenging circumstances, such as cross-pose, cross-age, or cross-quality. Consequently, it leads to state-of-the-art performances on several face recognition benchmarks, such as 98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition systems are spreading worldwide and are increasingly involved in unconstrained environments <ref type="bibr" target="#b28">[29]</ref>. In these environments, these systems have to deal with large variabilities, such as challenging illuminations, poses, and expressions, that might result in incorrect matching decisions <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b18">[19]</ref>. The face image quality of a sample is defined as its utility for recognition <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b33">[34]</ref> [5] <ref type="bibr" target="#b41">[42]</ref> and measures the impact of these variabilities on the face recognition performance. Previous works either do not employ face image quality information during comparison <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b43">[44]</ref> [9] <ref type="bibr" target="#b17">[18]</ref> or include quality estimates in comparison process 1 https://github.com/pterhoer/QMagFace <ref type="figure">Figure 1</ref>. Visualisation of the proposed quality-aware comparison score. The genuine (blue) and imposter (red) distributions are shown with respect to the comparison scores and their face image qualities. While the quality distributions (right) are very similar, the score distributions (top) are strongly overlapping. The proposed quality-aware comparison score is shown via black isolines. Dashed lines indicate negative scores and solid lines positive. The quality-awareness increases the separability of both distributions. that are not inherently suitable for such a task <ref type="bibr" target="#b39">[40]</ref>  <ref type="bibr" target="#b25">[26]</ref>. While the first case results in a loss of valuable information for the comparison, in the second case, the limiting factor lies in the utilized quality estimates.</p><p>In this work, we propose QMagFace, a solution that combines a quality-aware comparison function with a face recognition model trained with a magnitude-aware angular margin (MagFace) loss. Incorporating model-specific face image qualities in the comparison process aims at enabling an improved face recognition performance even under challenging circumstances. Exploiting the linear relationship between the qualities and their comparison scores that is induced by the MagFace loss, our quality-aware comparison function is simple but effective.</p><p>In <ref type="figure">Figure 1</ref>, the effect of the proposed quality-aware scoring function is visualized. Using the standard comparison score, the genuine (same person) and imposter (different person) distributions are strongly overlapping (see top plot). Even if their respective quality distributions are very similar (right plot), combining both information with the proposed quality-aware scoring function increases the separability leading to more reliable comparison scores for matching. Especially for lower comparisons and quality scores, the proposed approach adapts the scores more strongly to increase the accuracy. For higher qualities and comparison scores, the proposed solution does not alter the score since high comparison scores imply face pairs of high quality. Consequently, QMagFace is especially effective when dealing with challenging circumstances such as crosspose or cross-age.</p><p>The experiments were conducted on four face recognition databases and six benchmarks. The results demonstrate (a) a constant improvement in the face recognition performance compared to standard comparison scores over a wide range of false match rates, (b) the suitability of the used linear function for quality weighting, and (c) a high generalizability of the proposed approach despite changes in backbone architecture, training databases, and evaluation benchmarks. Moreover, the QMagFace consistently reaches high performances in video-based recognition tasks and achieves state-of-the-art results on three of the four image-based face recognition benchmarks. Especially under challenging circumstances, such as cross-pose, cross-age, or cross-quality, QMagFace achieved high performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face Image Quality Assessment</head><p>Driven by the international standards, such as ISO/IEC 19794-5 <ref type="bibr" target="#b19">[20]</ref> and ICAO 9303 <ref type="bibr" target="#b18">[19]</ref>, the first generation of face image quality assessment (FIQA) approaches are built on human perceptive image quality factors <ref type="bibr" target="#b13">[14]</ref>   <ref type="bibr" target="#b47">[48]</ref> consists of supervised learning algorithms based on human or artificially constructed quality labels. However, humans may not know the best characteristics for face recognition systems and artificially labelled quality values, derived from comparison scores, rely on error-prone labelling mechanisms.</p><p>The third generation of FIQA approaches <ref type="bibr" target="#b41">[42]</ref> completely avoids the use of quality labels by utilizing the face recognition networks themselves. In 2020, Terh?rst et al. <ref type="bibr" target="#b41">[42]</ref> proposed stochastic embedding robustness for FIQA (SER-FIQ). This concept measures the robustness of a face representation against dropout variations and uses this measure to determine the quality of a face. It avoids the need for training and takes into account the decision patterns of the deployed face recognition model. In 2021, Meng et al. <ref type="bibr" target="#b30">[31]</ref> proposed a class of loss functions that include magnitude-aware angular margins encoding the quality into the face representation. Training with this loss results in a face recognition model that produces embeddings whose magnitudes can measure the FIQ of their faces.</p><p>While the first two generations of FIQA methods aim to assess the utility of an image for face recognition in general, the third generation aims at determining modelspecific quality values. Therefore, we assert that these methods, SER-FIQ <ref type="bibr" target="#b41">[42]</ref> and MagFace <ref type="bibr" target="#b30">[31]</ref>, have the highest potential for improving the recognition performance. SER-FIQ can be applied to arbitrary face recognition networks but produce a complex relation between FIQ and comparison scores. On the other hand, MagFace produces a linear relationship between the qualities and the comparison scores, as we will show in Section 5.4, and thus, it is more suitable for a generalisable enhancement of the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Face Recognition</head><p>In recent years, face recognition is strongly driven by advances in deep representation learning. Early works rely on metric-learning based losses <ref type="bibr" target="#b30">[31]</ref>, such as contrastive loss <ref type="bibr" target="#b7">[8]</ref>, triplet loss <ref type="bibr" target="#b36">[37]</ref>, or penalty angular-margin losses <ref type="bibr" target="#b44">[45]</ref>  <ref type="bibr" target="#b8">[9]</ref>. However, due to the combinatorial explosion in the number of face triplets needed for training, the research focus shifted to classification-based approaches. These approaches are typically based on softmax and aim at classifying on a closed-set of identities during training and utilize the previous layer as a feature extractor for unseen faces. Combining the softmax activation with cross-entropy loss, most face recognition losses L are of the form</p><formula xml:id="formula_0">L = ? 1 N N i=1 log (L i ) with (1) L i = e r cos(m1?y i +m2)?m3 e r cos(m1?y i +m2)?m3 + N j=1,j =yi e r cos(?y i ) .<label>(2)</label></formula><p>Here, the training set contains N samples and ? yi refers to the angle between between last layer weight-vector and the normalized feature vector x i (with x i 2 = r). For training without margins m 1 = 1, m 2 = m 3 = 0, this refers to a simplified softmax loss. In SphereFace <ref type="bibr" target="#b26">[27]</ref>, a multiplicative angular margin is deployed with m 1 = ? and m 2 = m 3 = 0. For keeping the cosine margin penalty m 2 = ? (m 1 = 1 and m 3 = 0), this refers to CosFace <ref type="bibr" target="#b43">[44]</ref> and for penalizing an angular margin m 2 = ? (m 1 = 1 and m 3 = 0), this refers to the ArcFace <ref type="bibr" target="#b8">[9]</ref> loss. However, these losses select a fixed margin ? assuming that the samples are equally distributed in the embedding space around the class centers, which is not true when dealing with largely intraclass variations. To solve this problem, solutions based on variable margins are proposed. In <ref type="bibr" target="#b5">[6]</ref>, Boutros et al. proposed ElasticFace in which random margins are drawn from a Gaussian distribution N in each training iteration. This allows ElasticFace to extract and retract the margins individually for each class (e.g. m 1 = 1 and m 2 ? N ). Similarly, CurricularFace <ref type="bibr" target="#b17">[18]</ref> addresses easy samples in the early training stage and hard ones in the later stage adapting m 2 while keeping m 1 = 1 and m 3 = 0. In MagFace <ref type="bibr" target="#b30">[31]</ref>, a magnitude-aware angular margin m(r) (with m 1 = 1 and m 3 = 0) with a regularization g(r) is proposed that aims at including the utility (face image quality) of a sample in the margin. While the regularization g(r) rewards samples with large magnitudes r, m(r) is a simple linear function that aims at concentrating high-quality samples in a small region around the class centers. This results in more suitable margins that are based on the utility of the samples and are encoded in the magnitude of the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Quality-Aware Face Recognition</head><p>The qualities of face images are often used in video-tovideo recognition tasks where a set of images from one person is matched to a set of images from another <ref type="bibr" target="#b27">[28]</ref> [33] <ref type="bibr" target="#b47">[48]</ref> [36] <ref type="bibr" target="#b48">[49]</ref>. There, the quality of each image is used for a weighted aggregation of information. For recognition tasks based on single images, only a few works included the face image quality to enhance the face recognition performance. In EQFace <ref type="bibr" target="#b25">[26]</ref>, Liu et al. attached a quality-prediction network on a face recognition model to include the qualities in the training process. However, this method is limited by computationally-expensive training that is not end-to-end.</p><p>In <ref type="bibr" target="#b39">[40]</ref>, Shi and Jain proposed probabilistic face embeddings (PFE). Representing face images as Gaussian distributions in the embedding space, the variance of each feature is interpreted as its uncertainty and thus, as its quality. For comparison, they make use of a mutual-likelihood score to include the quality in the comparison score. However, performance is determined by the quality estimate that is limited by (a) the used uncertainty estimation module which is trained separately from the face recognition network and (b) the assumption that each feature can be independently represented as a Gaussian Process.</p><p>In contrast to previous works, we make use of modelspecific quality estimates that were linearly included in an end-to-end fashion. Consequently, this allows our proposed solution to work simplistically and more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The main contribution of this work, QMagFace, combines a quality-aware comparison function with a face recognition model trained with MagFace loss <ref type="bibr" target="#b30">[31]</ref>. Including the model-specific face image quality in the compari-son process aims to consistently improve the face recognition performance, especially under challenging conditions such as cross-pose, cross-age, or cross-quality. Moreover, the proposed quality-aware scoring function can be robustly trained on any face recognition network based on the Mag-Face loss as it will be shown in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quality-Aware Comparison Scoring</head><p>In face biometrics, a comparison score reflects the identity similarity of two face images. This reflection of similarity is more accurate when the compared samples are of high quality <ref type="bibr" target="#b15">[16]</ref>. The biometric quality of a face image is defined as its utility for recognition <ref type="bibr" target="#b15">[16]</ref> [34] [5] <ref type="bibr" target="#b41">[42]</ref>. In <ref type="bibr" target="#b40">[41]</ref>, it was shown that model-specific quality assessment reflects challenging situations for the face recognition model, such as comparisons with strong variations in pose or age. Similar to SER-FIQ <ref type="bibr" target="#b41">[42]</ref>, the qualities of MagFace networks <ref type="bibr" target="#b30">[31]</ref> utilize the deployed face recognition system and thus, strongly reflect the decision patterns and model biases. Consequently, we propose a simple, but effective, comparison function that includes these modelspecific quality values to enhance the accuracy and robustness of the face recognition system.</p><p>Given a face recognition model M trained with Mag-Face loss and two face images I 1 and I 2 , their embeddings are given by e 1 = M(I 1 ) and e 2 = M(I 2 ) together with the corresponding face image qualities q 1 = e 1 2 and q 2 = e 2 2 encoded through the vector length. The standard comparison score s = cos(e 1 , e 2 ) is defined over cosine similarity of these templates and thus, represents the angular similarity between both templates. However, comparisons with low-quality images affect the comparison scores and thus, needs to be adjusted. The proposed quality-aware comparison score? is defined a?</p><formula xml:id="formula_1">s(s, q 1 , q 2 ) = ?(s) * min{q 1 , q 2 } + s<label>(3)</label></formula><p>with a quality-weighting function</p><formula xml:id="formula_2">?(s) = min {0, ? * s ? ?} .<label>(4)</label></formula><p>This comparison function consists of only two trainable parameters (? and ?) and thus, can be robustly trained.</p><p>Since the biometric sample quality is included linearly in the MagFace loss (through m(r)), we assume a linear relationship between the face image qualities and their comparison scores. In Section 5.4, we will demonstrate the suitability of a linear function for the quality weighting. We further assume that the score-adjustment is dependent on the lowest quality of the comparison and that only comparisons with at least one low-quality sample needs to be adjusted. Please note that the qualities q 1 and q 2 are not easily exchangeable with other FIQ methods since these need to be model-specific with regard to the FR model and require a linear relationship between the quality estimates and its comparison scores.</p><p>A high similarity score s can only be achieved through the comparison of two high-quality samples. In this case, the similarity is well reflected in the comparison score s and thus, no quality-based score adjustments (? = 0) is needed. A lower comparison score might result from the degradation of a pair with at least one low-quality sample. In this case, the similarity is altered by the sample quality and our proposed function adjusts the score based on the quality. Consequently, if two comparisons result in similar comparison scores s 1 ? s 2 (with ?(s) &lt; o) but have different minimum qualities q min 1 q min 2 , the score with the higher quality undergoes a stronger adjustment and thus, results in a lower quality-aware comparison score (? 1 &lt;? 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training the Quality-Weighting Function</head><p>For training, the comparison scores of the training set S = S G + S I are separated into genuine and imposter comparisons with the corresponding minimal qualities Q min of the sample pairs. The training process is divided into three steps and aims at learning the quality weighting function ?(s) as shown in Algorithm 1.</p><p>In the first step, we define the optimal quality weight w opt (t) for a given threshold t. This is given through</p><formula xml:id="formula_3">w opt (t) = argmin ? 1 |S G | s?S G ?(t ?s(?, s))<label>(5)</label></formula><p>where ?(?) describes the Heaviside function and</p><formula xml:id="formula_4">s(?, s) = ?(? * Q min (s) + s).<label>(6)</label></formula><p>is a quality-aware scoring function given by ? and s that is scaled to the range of [0, 1], similar to the range of comparison scores, with a sigmoid function. This optimization aims at minimizing the FNMR at a threshold t through including face image quality information.</p><p>In the second step, a relevant threshold range T needs to be defined that represents the target FMR range. In this work, we choose the range from FMR max = 10 ?2 to FMR min = 10 ?5 to cover a wide variety of potential applications and due to the amount of training data available (in the order of 10 5 images). The relevant threshold range T = [t(FMR max ), t(FMR min )] is determined by finding the threshold t that corresponds to the required FMR on the quality-aware scores on the training data. This can be determined by</p><formula xml:id="formula_5">t(FMR) = argmin t FMR ? 1 |S I | s?S I ?(s(? opt (t), s) ? t) .</formula><p>The third step aims to learn the quality weighting function ?(s). Since the quality is included linearly in the Mag-Face loss, a linear relationship between the importance of the quality q(s) and its comparison score s is assumed. Therefore, we model the quality weights through a simple linear function ?(s) = ? * s ? ?. The parameters ? and ? can be learned by solving the following optimization</p><formula xml:id="formula_6">min ?,? t?T ? opt (t) + ? ? ? * t 2 ,<label>(7)</label></formula><p>resulting in the optimal parameter?</p><formula xml:id="formula_7">? = t?T (t ?t)(? opt (t) ?? opt ) t?T (t ?t) 2 (8) ? =? * t ?? opt with (9) ? opt = 1 |T | t?T ? opt (t) andt = 1 |T | t?T t.<label>(10)</label></formula><p>Algorithm 1: Quality-weighting function </p><formula xml:id="formula_8">Input: S G , S I , Q min , FMR min , FMR max Output: ?(s) / * Determine optimal weights * / 1 for t ? T ? [t(FMR max ), t(FMR min )] do 2 w opt (t) ? argmin ? 1 |S G | s?S G ?(t ? (? * Q min (s) + s)) / * Learns quality weighting function parameters * / 3? opt ? 1 |T | t?T ? opt (t) 4t ? 1 |T | t?T t 5? = t?T (t ?t)(? opt (t) ?? opt ) t?T (t ?t) 2 6? =? * t ??</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Databases and Benchmarks</head><p>To compare the performance of the proposed QMagFace approach with ten recent state-of-the-art approaches six face recognition benchmarks are used, LFW <ref type="bibr" target="#b16">[17]</ref>, AgeDB-30 <ref type="bibr" target="#b31">[32]</ref>, CFP-FP <ref type="bibr" target="#b38">[39]</ref>, XQLFW <ref type="bibr" target="#b24">[25]</ref>, IJB-B <ref type="bibr" target="#b45">[46]</ref>, and IJB-C <ref type="bibr" target="#b29">[30]</ref>.</p><p>LFW <ref type="bibr" target="#b16">[17]</ref> is a face verification benchmark containing 13k images of over 5k identities. In the benchmark experiments, we followed the standard protocol <ref type="bibr" target="#b16">[17]</ref> using the 6k predefined comparison pairs. Moreover, we conducted the experiments on three more challenging benchmarks representing the issues of cross-age (AgeDB <ref type="bibr" target="#b31">[32]</ref>), cross-pose (CFP-FP <ref type="bibr" target="#b38">[39]</ref>), and cross-quality (XQLFW <ref type="bibr" target="#b24">[25]</ref>). AgeDB <ref type="bibr" target="#b31">[32]</ref> is unconstrained face recognition benchmark for ageinvariant face verification. It contains over 16k images of over 5k identities. In the experiments, we follow the protocol of AgeDB-30 since it is the most reported and challenging one for AgeDB consisting of age gaps of over 30 years. CFP-FP <ref type="bibr" target="#b38">[39]</ref> is a face recognition benchmark that addresses the issue of comparing frontal to profile face images. In our experiments, we followed the evaluation protocol of <ref type="bibr" target="#b38">[39]</ref> containing 3500 genuine pairs and 3500 imposter pairs. XQLFW <ref type="bibr" target="#b24">[25]</ref> is a benchmark that addresses the problem of cross-quality comparisons in face recognition. The protocol defines 6k face image pairs based on the LFW database. However, for each pair, one face image is of much lower quality than the other face. The IARPA Janus Benchmark-B (IJB-B) <ref type="bibr" target="#b45">[46]</ref> contains around 21k images and 55k frames from over 7k videos of 1,845 identities. In the experiment, we follow the standard evaluation protocol using around 10k genuine and 8M impostor comparisons. The IARPA Janus Benchmark-C (IJB-C) <ref type="bibr" target="#b29">[30]</ref> extends on the IJB-B by adding more identities. In total, it consists of 31k images with over 117k frames of over 11k videos from 3531 identities. The verification protocol considers over 19k genuine and 16M imposter comparisons. In contrast to IJB-B, IJB-C focuses more on occlusion and diversity of subject occupation to improve the representation of the global population.</p><p>Besides reporting the verification accuracy based on benchmarks, we make use of four face recognition datasets to cover a much wider range of possible decision thresholds and thus, to cover more potential applications. Morph <ref type="bibr" target="#b21">[22]</ref> consists of 55k face images from over 13k subjects. The images are frontal and of high quality. LFW <ref type="bibr" target="#b16">[17]</ref> contains 13k images of over 5k identities and the ColorFeret database <ref type="bibr" target="#b34">[35]</ref> consists of 14k high-resolution face images from over 1k different individuals. The data possess a variety of poses (from frontal to profile) and facial expressions under well-controlled conditions. The Adience dataset <ref type="bibr" target="#b11">[12]</ref> consists of 26k images from over 2k different subjects. The images of the Adience dataset possess a wide range in terms of image quality. In the supplementary material, we included a more detailed discussion, such as on the licenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Following the international standard for biometric verification evaluation <ref type="bibr" target="#b20">[21]</ref>, we report the face verification error in terms of false non-match rate (FNMR) at fixed false match rate (FMR). Moreover, we report the equal error rate (EER) and the area under curve (AUC) of the receiver operating characteristic (ROC) curve. The EER equals the FMR at the threshold where FMR = FNMR and is well known In our experiments, we report the face verification performance over a wide range of FMRs to cover a variety of potential applications. On the benchmarks, we follow the mentioned protocols and report the verification accuracy to be comparable with previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face Recognition Models</head><p>In the experiments, the proposed QMagFace approach is built on three pre-trained models 2 based on MagFace loss released by the authors <ref type="bibr" target="#b30">[31]</ref>. These were trained on the MS1MV2 database <ref type="bibr" target="#b14">[15]</ref> and are based on iResNet-18, iResNet-50, and iResNet-100 backbones <ref type="bibr" target="#b9">[10]</ref>. In the following, we use the name of the loss function and the model trained with it interchangeably to keep this work easily comprehensible.</p><p>The parameters ? and ? needed for the proposed QMag-Face approach are trained on the Adience dataset <ref type="bibr" target="#b11">[12]</ref> due to the large quality variance in its samples and to create a generalizable approach by using the estimated parameters on this dataset. However, the training process is robust and thus, the choice of the training database only affect the performance minimally as we will show in Section 5.4. Due to the simplicity of the proposed approach, the learned parameters are shown in <ref type="table" target="#tab_1">Table 1</ref>. To extract a face embedding from a given face image, the image is aligned, scaled, and cropped as described in <ref type="bibr" target="#b30">[31]</ref>. Then, the preprocessed image is passed to the face recognition models to extract the feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Investigations</head><p>The proposed approach is analysed in three steps. First, we report the performance of QMagFace on six face recognition benchmarks against ten recent state-of-the-art methods in image-and video-based recognition tasks to provide a comprehensive comparison with state-of-the-art. Second, we investigate the face recognition performance of QMagFace over a wide FMR range to show its suitability for a wide variety of applications and to demonstrate that the quality-aware comparison score constantly enhances the recognition performance. Third, we analyse the optimal quality weight over a wide threshold range to demonstrate the robustness of the training process and the generalizability of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance on Single-Image Benchmarks</head><p>To demonstrate that the proposed QMagFace approach achieves state-of-the-art performance in image-to-image face recognition tasks, the proposed method is compared against ten recent face recognition models on four benchmarks. For PFE 3 <ref type="bibr" target="#b39">[40]</ref>, ArcFace 4 <ref type="bibr" target="#b8">[9]</ref>, and the MagFace variants <ref type="bibr" target="#b30">[31]</ref>, we used the implementations released by the authors. The remaining benchmark results are taken from <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b25">[26]</ref>. In <ref type="table">Table 2</ref>, the face recognition performances of these are shown. On the LFW benchmark, the proposed QMagFace approach based on the iResNet-100 backbone achieved a performance of 99.83%, which is close to the state-of-the-art performance of 99.85%. On more challenging and less-saturated benchmarks, the proposed approach achieves state-of-the-art performance. This includes 98.50% on cross-age face recognition (AgeDB), 98.74% on cross-pose face recognition (CFP-FP), and 83.95% on cross-quality face recognition (XQLFW). Since the FIQ captures these challenging conditions and the quality values represent the utility of the images for our specific network, the proposed quality-aware comparison score can specifically address the circumstance and their effect on the network. Consequently, it performs highly accurate in the cross-age, cross-pose, and cross-quality scenarios and achieves state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance on Video-Based Benchmarks</head><p>In <ref type="table">Table 3</ref>, the video-based face recognition performance is analysed based on IJB-B and IJB-C. The FNMR is investigated over a wide range of FMRs. The performances of most state-of-the-art approaches are taken from the respective works. Since some original works did not investigate the performance on IJB-B/C, the remaining performances are taken from <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b5">[6]</ref>. For creating an embedding with the corresponding quality-value for a video, the unitsized embeddings and qualities per frame are aggregated by a quality-weighted sum. Introducing quality-awareness to the MagFace-100 model generally reduces the recognition error. Especially for FMRs up to 10 ?3 , QMagFace achieves state-of-the-art performance. Despite the effectiveness of QMagFace for image-to-image face recognition tasks, the quality of a video-embedding does currently not well represent its utility for recognition. While the qual- <ref type="table">Table 2</ref>. Image-to-image face recognition performance on four benchmarks reported in terms of benchmark accuracy (%). The highest performance is marked bold. The proposed approach, QMagFace-100, achieves state-of-the-art face recognition performance, especially in cross-age (AgeDB), cross-pose (CFP-FP), and cross-quality (XQLFW) scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>AgeDB CFP-FP LFW XQLFW SphereFace <ref type="bibr" target="#b26">[27]</ref> 98.17 86.84 99.67 -CosFace <ref type="bibr" target="#b43">[44]</ref> 98.17 98.26 99.78 -PFE <ref type="bibr" target="#b39">[40]</ref> 96.90 97.49 99.80 79.80 ArcFace <ref type="bibr" target="#b8">[9]</ref> 98.07 97.31 99.77 79.73 GroupFace <ref type="bibr" target="#b23">[24]</ref> 98.28 98.63 99.85 -CurricularFace <ref type="bibr" target="#b17">[18]</ref> 98.32 98.37 99.80 -ElasticFace-Arc <ref type="bibr" target="#b5">[6]</ref> 98. <ref type="bibr" target="#b34">35</ref>  ity corresponding to an embedding for a single frame has a high correlation with the true utility of this frame, the same does not apply for the (weighted-sum) aggregated embedding and thus for the quality of the video. This needs to be addressed by future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Full Performance Analysis</head><p>In <ref type="table">Table 2</ref>, it was already shown that proposed qualityaware face recognition approach leads to stable improvements in the recognition performance. In this section, we will demonstrate these improvements for a wide FMR range. <ref type="table">Table 4</ref> shows the recognition performance of Mag-Face and QMagFace variants. To cover a wide range of potential applications, the performance is analysed over a wide range of decision thresholds (ranging from FMR of 10 ?1 to 10 ?5 ) for three databases. The analysis involved over 300k/160k/1.1M comparisons on the Color-Feret/LFW/Morph database. For three backbones (iResNet-18/50/100), the performance of MagFace and the proposed QMagFace approach is compared.</p><p>For QMagFace-18, 19 out of 21 scenarios showed an improved recognition performance while for QMagFace-50, 20 out of 21 scenarios showed a performance enhancement. The three cases with a decreased performance took place on ColorFeret, which involves many challenging frontal to profile face comparisons. Since the recognition performance of <ref type="table">Table 3</ref>. Video-based face recognition performance. The performance [%] is reported in terms of FNMR at different FMRs. The best is marked bold. The asterisk (*) denotes a method that is optimized for video-based recognition. In nearly all cases, the quality-awareness increases the performance of MagFace. For FMRs up to 10 ?3 , the proposed quality-aware solution performs best despite that aggregated quality does not reflect well the combined embeddings per video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IJB-B</head><p>IJB-C FNMR at FMR of 10 ?2 10 ?3 10 ?4 10 ?5 10 ?2 10 ?3 10 ?4 10 ?5 MagFace-18 and MagFace-50 is much lower than the models based on iResNet-100, the performance in estimating the model-specific face image quality correctly is lower as well. Therefore, the QMagFace approaches that make use of these quality estimates become less accurate when the quality estimate is failed by a large degree. For QMagFace-100, the performance, and thus the quality estimation, is higher. Consequently, the proposed QMagFace-100 approach leads to strong performance improvements in all investigated cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Robustness Analysis</head><p>Lastly, we demonstrate (a) the suitability of choosing a linear quality-weighting function and (b) the generalizability of the QMagFace solution.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, the correlation between the optimal quality weight and different decision thresholds are shown for different MagFace models and databases. For each of the four databases, the optimal quality weight ? opt is computed with Eq. 5 for several thresholds t ? T . These weights show the optimal solutions for a given database that can be achieved by using the quality-aware score function?(s, q 1 , q 2 ) from Eq. 3. Moreover, a linear function is fitted through these points and shown in the same color.</p><p>For all three models <ref type="figure" target="#fig_1">(Figure 2a, 2b, 2c)</ref>, two observations are made. First, the optimal quality weights ? opt follow a linear function with respect to the decision threshold t. This is observed for all MagFace models and on each database, proving the suitability of our linear quality weight function ?(s) from Eq. 4. Second, for each model, the optimal quality weight functions are similar. The only exception is the quality weight function for MagFace-100 that is optimized on LFW. In this case, the database turns out to be too easy to train the model effectively since even for an FMR of 10 ?5 the decision threshold is below 0.5. However, in all the other investigated cases, the relation between the optimal quality weights and the decision thresholds is similarly independent of the analysed database. Utilising these databases for training, QMagFace will lead to similar matching decisions demonstrating the robustness of the QMagFace training process. Moreover, it indicates a high generalizability since the learned function on one database is very similar to the optimal functions of the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Ethical Considerations</head><p>Despite the high generalizability and the effectiveness of QMagFace for unconstrained face recognition, the approach has to deal with two limiting factors. First, the additional quality information is most beneficial for images of lower quality and thus, the performance improvements of QMag-Face decrease for very low FMRs, such as 10 ?7 . Second, for more effective video-based recognition, a more suitable quality aggregation is needed. For single images, the quality of an embedding well reflects its utility for recognition. However, this does not apply when fusing the frames of a video with the corresponding qualities. To more efficiently exploit the quality information in video-based recognition with QMagFace, future works need to focus on more advanced quality-based fusion techniques for video frames.</p><p>While the proposed quality-awareness approach might strongly improve unconstrained face recognition for the sake of higher security or convenience, we want to point out the importance of unbiased quality estimates for fair face recognition. The use of biased quality estimates might lead <ref type="table">Table 4</ref>. Face recognition performance reported in terms of FNMR [%] over a wide range of FMRs. The MagFace and the proposed QMagFace approach are compared for three backbones on three databases. The better values between both approaches are highlighted in bold. In general, the proposed quality-aware solutions constantly improve the performance, often by a large margin. This is especially true for QMagFace based on the iResNet-100 backbone.  to unfair, and thus discriminatory, matching decisions depending on demographic and non-demographic factors of their users <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FNMR at FMR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>For recognition in unconstrained environments, FR systems have to deal with challenging situations, such as different illuminations, poses, and expressions. Previous works either focused on learning margin-based approaches while not considering FIQ information or included noninherently fit quality estimates. In this work, we proposed QMagFace, a simple and robust quality-aware FR approach. It integrates model-specific FIQ information in the comparison process to allow a more accurate performance under challenging situations, such as cross-pose or cross-age. The experiments were conducted on ten FR databases and benchmarks. The results demonstrated that including the quality-awareness consistently increases the FR performance. Moreover, it was shown that QMag-Face reaches competitive recognition results with stateof-the-art solutions. For challenging circumstances, such as cross-pose, cross-age, or cross-quality, QMagFace constantly beat state-of-the-art approaches. Additional experiments indicated a high generalizability of the proposed approach demonstrated the suitability of a linear function for the quality weighting. More details can be found in <ref type="bibr" target="#b34">[35]</ref> and https : / / www.nist.gov/itl/products-and-services/ color-feret-database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Experiments on Various Training Data</head><p>After finalizing the discussion on the databases, this section aims to emphasize the high generalizability of the proposed approach against various training data. In Section 5.4, this was already demonstrated indirectly by comparing the optimal quality-weighting functions for each database. In this section, we show this in a more direct way by iteratively using the different FR databases for training.</p><p>In <ref type="table">Table 5</ref>, the effect of the different training databases on the single-image FR benchmarks are shown. For QMagFace-18, the performance does not improve in all cases due to the limited quality estimation performance of MagFace-18 as discussed above. In contrast, adding the quality-awareness to the MagFace-100 model improves the recognition performance independent of the utilized training data. Moreover, it seems that the choice in the paper to use Adience for training was wrong since the performance when using the other databases for training is higher. However, the choice for Adience was done to its large variety in quality-decreasing factors, such as occlusions, head poses, illuminations, and image qualities. When it comes to smaller FMRs, these factors become more important and Adience might be the better choice for stable improvements in the recognition performance.</p><p>In <ref type="table" target="#tab_5">Tables 6, 7</ref>, 8, and 9, the effect of different training databases is analysed over a wide range of FMRs. For low FMRs, such as 10 ?5 , a larger variety of quality factors play important roles in enhancing the recognition performance and thus, using Adience as the training database leads to very stable performance improvements in all cases. However, including the quality-awareness leads to strong performance improvements for most FMRs and the different training datasets generally leads to similar performances demonstrating the high generalizability of the proposed QMagFace approach. <ref type="table">Table 5</ref>. The effect of the different training databases on the single-image FR benchmarks. The performance is reported in terms of benchmark accuracy (%). For comparison, the performance of the QMagFace variants is shown against the MagFace models without quality-awareness. It turns out that choosing Adience, as done in the paper, leads to the weakest performance on these benchmarks. Consequently, the proposed approach, QMagFace-100, achieves state-of-the-art face recognition performance independent of the training data, especially in cross-age (AgeDB), cross-pose (CFP-FP), and cross-quality (XQLFW) scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trained on Model</head><p>AgeDB </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>opt / * Define the quality function * / 7 ?(s) = min 0,? * s ?? 8 return ?(s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Optimal quality weight for different decision thresholds on four databases. The threshold range reflect FMRs from 10 ?2 to 10 ?5 . Training on different databases lead to similar linear solutions for ?(s). The results demonstrate that (a) the choice of a linear function is justified and (b) that the learned models have a high generalizability since the weighting function ?(s) trained on one database is very similar to the optimal functions of the others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Quality distributions for the four FR datasets for the three MagFace backbones based on iResNet-18/50/100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Learned parameters</figDesc><table><row><cell></cell><cell cols="2">Learned parameters</cell></row><row><cell>Model</cell><cell>?</cell><cell>?</cell></row><row><cell>QMagFace-18</cell><cell cols="2">0.092861 0.135311</cell></row><row><cell>QMagFace-50</cell><cell cols="2">0.065984 0.103799</cell></row><row><cell cols="3">QMagFace-100 0.077428 0.125926</cell></row><row><cell cols="3">as a single-value indicator of the verification performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Evaluation on Adience based on different training datasets -The performance [%] is reported in terms of FNMR at different FMRs and EER. Three MagFace variants are compared against QMagFace variants that are trained on different training sources. MagFace-18 4.505 2.665 10.639 28.935 49.982 74.662 MagFace-50 2.432 1.334 3.463 8.818 18.396 44.821 MagFace-100 2.291 1.395 2.926 5.478 11.211 30.331 ColorFeret QMagFace-18 3.798 2.110 8.466 26.547 49.403 74.881 QMagFace-50 2.371 1.276 3.310 8.604 18.386 44.232 QMagFace-100 2.255 1.368 2.818 5.336 11.098 30.188 LFW QMagFace-18 3.794 2.099 8.360 25.556 48.337 73.807 QMagFace-50 2.369 1.286 3.280 8.479 19.001 45.802 QMagFace-100 2.266 1.381 2.810 5.277 11.765 30.967 Morph QMagFace-18 3.792 2.120 8.396 26.397 49.208 74.669 QMagFace-50 2.368 1.289 3.282 8.412 19.405 45.381 QMagFace-100 2.264 1.369 2.817 5.288 11.443 30.704 Table 7. Evaluation on ColorFeret based on different training datasets -The performance [%] is reported in terms of FNMR at different FMRs and EER. Three MagFace variants are compared against QMagFace variants that are trained on different training sources .232 3.068 6.902 22.951 82.531 97.723 QMagFace-50 2.941 1.464 4.173 6.832 12.247 23.426 QMagFace-100 2.060 0.950 2.616 4.409 7.145 16.454 LFW QMagFace-18 4.282 3.082 6.917 24.382 82.466 96.127 QMagFace-50 2.961 1.572 4.194 6.815 12.045 22.677 QMagFace-100 2.031 1.033 2.565 4.275 7.144 18.318 Morph QMagFace-18 4.245 3.085 6.901 23.900 80.080 94.926 QMagFace-50 2.939 1.566 4.141 6.733 12.130 24.999 QMagFace-100 2.051 1.055 2.596 4.302 7.109 17.213 Table 8. Evaluation on LFW based on different training datasets -The performance [%] is reported in terms of FNMR at different FMRs and EER. Three MagFace variants are compared against QMagFace variants that are trained on different training sources</figDesc><table><row><cell></cell><cell></cell><cell cols="4">CFP-FP LFW XQLFW</cell></row><row><cell></cell><cell>MagFace-18</cell><cell>93.37</cell><cell cols="2">93.11 99.22</cell><cell>69.55</cell></row><row><cell></cell><cell>MagFace-50</cell><cell>97.60</cell><cell cols="2">97.33 99.72</cell><cell>80.60</cell></row><row><cell></cell><cell>MagFace-100</cell><cell>98.18</cell><cell cols="2">98.36 99.73</cell><cell>83.90</cell></row><row><cell>Adience</cell><cell>QMagFace-18</cell><cell>92.98</cell><cell cols="2">94.00 99.30</cell><cell>68.60</cell></row><row><cell></cell><cell>QMagFace-50</cell><cell>97.88</cell><cell cols="2">97.74 99.73</cell><cell>80.63</cell></row><row><cell></cell><cell cols="2">QMagFace-100 98.50</cell><cell cols="2">98.74 99.80</cell><cell>83.97</cell></row><row><cell cols="2">ColorFeret QMagFace-18</cell><cell>92.90</cell><cell cols="2">94.03 99.33</cell><cell>68.68</cell></row><row><cell></cell><cell>QMagFace-50</cell><cell>97.88</cell><cell cols="2">97.80 99.73</cell><cell>80.63</cell></row><row><cell></cell><cell cols="2">QMagFace-100 98.48</cell><cell cols="2">98.76 99.80</cell><cell>84.03</cell></row><row><cell>Morph</cell><cell>QMagFace-18</cell><cell>93.02</cell><cell cols="2">94.06 99.33</cell><cell>68.67</cell></row><row><cell></cell><cell>QMagFace-50</cell><cell>97.95</cell><cell cols="2">97.86 99.73</cell><cell>80.57</cell></row><row><cell></cell><cell cols="2">QMagFace-100 98.55</cell><cell cols="2">98.77 99.82</cell><cell>83.82</cell></row><row><cell>LFW</cell><cell>QMagFace-18</cell><cell>92.92</cell><cell cols="2">94.07 99.35</cell><cell>68.72</cell></row><row><cell></cell><cell>QMagFace-50</cell><cell>97.88</cell><cell cols="2">97.86 99.72</cell><cell>80.58</cell></row><row><cell></cell><cell cols="2">QMagFace-100 98.60</cell><cell cols="2">98.77 99.83</cell><cell>83.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">FNMR at FMR</cell></row><row><cell cols="2">Training database Model</cell><cell>EER 10 ?1</cell><cell>10 ?2</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>10 ?5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/IrvingMeng/MagFace (Apache License 2.0)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https : / / github . com / seasonSH / Probabilistic -Face-Embeddings (MIT License) 4 https://github.com/deepinsight/insightface (MIT License)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head><p>In the main paper, we limited information on the databases to essential parts. At this point, we provide additional information in the context of the used databases to enhance the understanding of this work. More precisely, this supplementary material consists of three parts. First, we provide information on the quality distributions of the databases to better understand the choice of the training dataset. Second, we provide additional information on the database licenses and their creation processes. Third, demonstrate the effect of using different training databases for QMagFace to support the reasoning concerning the generalizability of QMagFace from Section 5.4 in a more direct manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Distributions of MagFace</head><p>The proposed QMagFace approach makes use of Mag-Face qualities and includes these in the decision process. To get a better understanding of the quality distributions of the different used databases, <ref type="figure">Figure 3</ref> shows these distributions for the three MagFace backbones. For all backbones, LFW and Morph consist of the highest FIQ values and share a similar distribution due to the fact that both databases consist of mostly frontal and well-illuminated images with high image quality. For MagFace-50 and MagFace-100, the quality distribution of ColorFeret shows the widest range of FIQ values. ColorFeret consists of high-quality images that were taken under controlled capturing conditions. The high variety of FIQ values origin from the head pose variations and the lowest FIQ values come from the profile face images since these prove to have a very low utility for recognition <ref type="bibr" target="#b40">[41]</ref>. The Adience database consists of face images with a wide variety of quality-decreasing factors such as variations in image quality, occlusions, expressions, and head poses. However, it does not contain many full profile images and thus, ColorFeret consists of images with lower FIQ values. It should be noted that the quality estimation performance of MagFace is dependent on its FR performance. Consequently, for MagFace-18, this leads to many wrongly assessed qualities and thus, to a lower performance of QMagFace-18. This also explains why for MagFace-18 the quality distributions are similar while for MagFace-50 and MagFace-100 the distributions show strong differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Information on the Utilized Databases</head><p>After discussing the additional properties of the databases themselves, this section provides additional information about the licenses and the creation process of these databases. Since the amount of information about the used datasets is restricted by the page limit, the paper focuses on the most important aspects to make the experiments understandable and reproducible.</p><p>LFW <ref type="bibr" target="#b16">[17]</ref> is licensed under CC-BY-4.0. It is based on the Faces in the Wild database <ref type="bibr" target="#b3">[4]</ref> collected by Tamara Berg at Berkeley and consists of face captioned from news images. More details can be found in <ref type="bibr" target="#b16">[17]</ref> or under http: //vis-www.cs.umass.edu/lfw/index.html.</p><p>AgeDB <ref type="bibr" target="#b31">[32]</ref> is available for non-commercial research purposes only and consists of images manually collected from the internet. More details on the collection process can be found in <ref type="bibr" target="#b31">[32]</ref> and the details on the license are presented in https://ibug.doc.ic.ac.uk/resources/ agedb/.</p><p>CFP-FP <ref type="bibr" target="#b38">[39]</ref> consist of manually collected images of celebrities in frontal and profile views. More information can be found in <ref type="bibr" target="#b38">[39]</ref> and http://www.cfpw.io/. To get more information on license and consent, we reached out to the first author via mail.</p><p>XQLFW <ref type="bibr" target="#b24">[25]</ref> is licensed under the MIT License and is based on the modified images of the LFW dataset <ref type="bibr" target="#b16">[17]</ref> (CC-BY-4.0). Detailed information can be found in <ref type="bibr" target="#b24">[25]</ref>, https://martlgap.github.io/xqlfw/ pages/citation.html, and https://github. com/Martlgap/xqlfw.</p><p>The images of the IJB-B <ref type="bibr" target="#b45">[46]</ref> and the IJB-C [30] databases from the National Institute for Standards and Technology (NIST) are made available under different Creative Commons license variants. Details on the collection process and corresponding informationcan be found in <ref type="bibr" target="#b45">[46]</ref> and IJB-C <ref type="bibr" target="#b29">[30]</ref>. More information on the license are shown under https://nigos.nist.gov/ datasets/ijbc/request and https://nigos. nist.gov/facechallenges/data/IJBC/IJBC_ LICENSES.TXT.</p><p>Adience <ref type="bibr" target="#b11">[12]</ref> is a database that includes a compilation of individual images which were uploaded to the internet and tagged as publicly available by the original author. It is limited to research purposes only. More information can be found in <ref type="bibr" target="#b11">[12]</ref> and under https : / / talhassner . github . io / home / projects / Adience/LICENSE.txt.</p><p>In this work, the academic version of the Morph dataset <ref type="bibr" target="#b21">[22]</ref> is used. This is restricted to for research purposes only. The legacy photographs associated with these records were taken 1962 and 1998. Digital scans of these photographs were collected with legal considerations and IRB approval. More information can be found in <ref type="bibr" target="#b21">[22]</ref> and under https: //uncw.edu/oic/tech/morph.html.</p><p>ColorFeret <ref type="bibr" target="#b34">[35]</ref> database is restricted to face recognition research. During the data collection, the different subjects were photographed in 15 sessions over three years under controlled conditions. Detailed license information can be found under https://www. nist . gov / system / files / documents / 2019 / 11/25/colorferet_release_agreement.pdf.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quality metrics for practical face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bourlai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)</title>
		<meeting>the 21st International Conference on Pattern Recognition (ICPR2012)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3103" to="3107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design and evaluation of photometric image quality measures for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bourlai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="314" to="324" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting performance of face recognition systems: An image characterization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soma</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Names and faces in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaety</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2004), with CD-ROM</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004-06-27" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning face image quality from human assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elasticface: Elastic margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadi</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>CoRR, abs/2109.09416, 2021. 3, 6</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face image quality assessment based on learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="94" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved residual networks for image and video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ionut Cosmin Duta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9415" to="9422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A bayesian model for predicting face recognition performance using image quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luuk</forename><forename type="middle">J</forename><surname>Veldhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spreeuwers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics</title>
		<meeting><address><addrLine>Clearwater, IJCB 2014, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-02" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face image conformance to iso/icao standards in machine readable travel documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1204" to="1213" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Standardization of face image sample quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiufeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiren</forename><surname>Zhang</surname></persName>
		</author>
		<idno>berg. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Biometrics</title>
		<editor>Seong-Whan Lee and Stan Z. Li</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faceqnet: Quality assessment for face recognition based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hernandez-Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fi?rrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Haraksim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Beslay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Biometrics</title>
		<meeting><address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Curricularface: Adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2020. 1, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Machine Readable Travel Documents. Standard, International Civil Aviation Organization</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Information technology -Biometric data interchange formats -Part 5: Face image data. Standard, International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iso/Iec</surname></persName>
		</author>
		<title level="m">Information technology -Biometric performance testing and reporting. Standard, International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MORPH: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ricanek</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamirat</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Conference on Automatic Face and Gesture Recognition (FGR 2006)</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006-04" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face image assessment learned with objective and relative face image qualities for improved face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4027" to="4031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Groupface: Learning latent groups and constructing group-based representations for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Cheol</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongju</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cross-quality LFW: A database for analyzing crossresolution image face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Knoche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno>abs/2108.10290, 2021. 4</idno>
		<imprint>
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eqface: A simple explicit quality network for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2021. 1, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep face recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st SIBGRAPI Conference on Graphics, Patterns and Images, SIBGRAPI 2018</title>
		<meeting><address><addrLine>Paran?, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-11-01" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">IARPA janus benchmark -C: face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Tyler</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics</title>
		<meeting><address><addrLine>Gold Coast, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-02-20" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Magface: A universal representation for face recognition and quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2021. 2, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Agedb: The first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sdd-fiqa: Unsupervised face image quality assessment with similarity distribution distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Fu-Zhao Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Gen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the existence of face quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Teli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjoon</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Crystal loss and quality pooling for unconstrained face verification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Domingo</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1804.01159</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image-quality-based adaptive face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sellahewa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Jassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="805" to="813" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jacobs. Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Domingo</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016</title>
		<meeting><address><addrLine>Lake Placid, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6901" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face quality estimation and its correlation to demographic and non-demographic bias in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terh?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Niklas</forename><surname>Kolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Joint Conference on Biometrics</title>
		<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-10-01" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SER-FIQ: unsupervised estimation of face image quality based on stochastic embedding robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terh?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Niklas</forename><surname>Kolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A comprehensive study on face recognition biases beyond demographics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terh?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Niklas</forename><surname>Kolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aythami</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fi?rrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
		<idno>abs/2103.01592</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">IARPA janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Patch-based probabilistic image quality assessment for face selection and improved video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011 WORKSHOPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Inducing predictive uncertainty estimation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/2009.00603</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5216" to="5225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evaluation on Morph based on different training datasets -The performance [%] is reported in terms of FNMR at different FMRs and EER</title>
	</analytic>
	<monogr>
		<title level="m">Table 9</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
