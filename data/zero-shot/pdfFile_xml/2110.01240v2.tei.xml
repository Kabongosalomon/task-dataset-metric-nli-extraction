<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A FREE LUNCH FROM VIT: ADAPTIVE ATTENTION MULTI-SCALE FUSION TRANSFORMER FOR FINE-GRAINED VISUAL RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangcheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A FREE LUNCH FROM VIT: ADAPTIVE ATTENTION MULTI-SCALE FUSION TRANSFORMER FOR FINE-GRAINED VISUAL RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-FGVR</term>
					<term>vision transformer</term>
					<term>adaptive at- tention</term>
					<term>multi-scale fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AF-Trans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Compared with generic object recognition, fine-grained visual recognition is a challenging task, with large intra-class variance and small inter-class variance. Fine-grained objects (birds <ref type="bibr" target="#b0">[1]</ref> and dogs <ref type="bibr" target="#b1">[2]</ref>) are similar at a cursory glance, while they can be recognized by details in discriminative local parts.</p><p>Methods for FGVR can be classified into three categories: feature-encoding methods, localization-based methods and attention-based methods. Compared with feature-encoding methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, the localization methods can explicitly tell the subtle differences among different sub-classes and usually yields better results. Recent localization works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> integrate region proposal networks (RPN) to propose bounding boxes containing details, while the early <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> rely on the annotations of parts where extra labor is necessary. However, RPN-based methods ignore the relationships among regions they selected. Attention-based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> methods select region attentions in images by exploiting the attention properties of the feature maps of CNN itself , releasing the reliance on manually annotation. When it comes to attention mechanisms, transformer comes to our mind.</p><p>Transformer in CV field has become a research hotspot, with its application on image classification <ref type="bibr" target="#b13">[14]</ref> and image detection <ref type="bibr" target="#b14">[15]</ref>. The innovation of image sequentialization in vision transformer, where ViT flattens the image patches and transforms them into patch tokens, inspires the community to utilize the innate attention mechanism. Nonetheless, native vision transformer cannot play to its strengths on FGVR directly. For example, the receptive field of ViT cannot be effectively extended, since the length of patch tokens does not change as its encoder blocks increases. Besides, the model may not effectively capture the region attention carried in the patch tokens. To tackle above problems, TransFG <ref type="bibr" target="#b15">[16]</ref> proposed to exclude immaterial inputs of the final transformer layer with ViT inherent attention weights to choose which tokens to stay. However, when generating the attention map for picking up tokens, TransFG cannot use all tranformer layers attention completely. As shown in the Fig1, better accuracy can be achieved with attention in single transform layer (e.g. L3) compared with PSM in TransFG, while SACM achieves the best with adaptive integration. We prove that attention weights in each layer do not play the equal role in terminating the fused attention map, which aims to correspond to the relative importance of input tokens. Besides, TransFG over- looks the supervise for global loss and lacks the combination of multi-scale and multi-branch.</p><p>To this end, we launch the adaptive attention multi-scale fusion transformer, which leverages the transformer innate attention mechanism to strengthen locality in a multi-scale pipeline, which is shown in the Fig2. All told, how to digest and absorb 'the free lunch' from ViT better is our work focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Raw Materials: Attention in Vision Transformer</head><p>Vision transformer successfully applies transformer to the visual field by introducing patch embedding. For the sequence patch embedding input of changing the image into the corresponding 1D, 2D image ? ? ? ? , is divided into patches with size ? , ? ? ?( ? ? ) . Patches are passed through convolution layer with kernel size ? , and then added with the randomly initialized learnable position embedding to obtain the 1D sequence. The formula is as follows:</p><formula xml:id="formula_0">= [ ( 1 , 1 , ? , )] + (1)</formula><p>Where is the embedding value before entering the encoder, is patch embedding, and is position embedding. Position embedding can bring corresponding location information to patches without location information. It should be noted that 1 is a randomly initialized learnable class token. Transformer encoder is composed of multihead self-attention (MSA) and multi-layer perceptron (MLP) blocks. The output of -th layer is shown below: ? = ( * ) *</p><formula xml:id="formula_1">= ? * + ?<label>(2)</label></formula><p>Where , , are Query, Key and Value vectors respectively. ? is the output passing through MSA but not MLP yet, and is the output of the -th layer. Vision transformer selects the first output generated from the class token for classification tasks. This is because class token has more global rich information than other patches. The formula of classification network is as follows:</p><formula xml:id="formula_3">= ( ? ) (4)</formula><p>Where is the output of the classification network, is the weight of net, and is the output of the encoder corresponding to class token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cooking Method: AFTrans Model</head><p>The Vision Transformer can be for fine-grained classification, while it does not well capture the local information required for FGVR. We propose the adaptive attention multi-scale fusion transformer (AFTrans) to cope with the above problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Selective Attention Collection Module</head><p>To capture the local discriminative part 2 of the raw image 1 , the SACM collects multi-head self-attention weights of each transformer layer, and adaptively finetunes their perception contribution to attention map. We first take out the attention weights of each transformer layer as:</p><formula xml:id="formula_4">= ( 1 2 ) = [ 0 , 1 , ? , ?1 ]<label>(5)</label></formula><p>is the dimension of embedding space. We argue that effective attention features can be gradually accumulated and amplified in each layer by the Hadamard product, so we integrate each layer's attention weights of multi-heads as:</p><formula xml:id="formula_5">= ? ?1 =0 ? (0,1, ? , ? 1)<label>(6)</label></formula><p>= [ 0 , 1 , ? , ?1 ] ?? (0,1, ? , ? 1)</p><p>Then we concatenate attention weights of layers as . The core of SACM is to generate 'attention in attention'. We first adopt global average-pooling to squeeze the token dimension of the input attention weights. Descriptors are forwarded to a network to produce our layer attention ? ? ?(1?1? ) . The network is composed of multi-layer perceptron (MLP) with one hidden layer, whose size is set to ? / ?1?1 , where is the reduction ratio. Then we employ a gating mechanism with sigmoid activation . The fused attention map is computed as:</p><formula xml:id="formula_7">= * (8) = ?( ( (? ?1 =0 )))<label>(9)</label></formula><formula xml:id="formula_8">= ( 2 ( 1 * )</formula><p>Where is ReLU, 1 ? ? / ? and 2 ? ? ? / . The matrix reflects the correlation between image tokens. In order to select effective tokens for classification, we choose the most relevant ? ? tokens with class token for classification as ?, and map them into coordinates in 1 . Where is the number of patches and is the threshold of cropping. Then we employ the max connected region search algorithm to extract the largest connected component of ? for localizing and zooming region attention in the raw image 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Multi-scale Fusion Training Pipeline</head><p>To improve classification ability and robustness for images of different scales, we take multi-granularity as our general framework, which is inspired by MMAL and RAMS-Trans <ref type="bibr" target="#b31">[30]</ref>. First, send the raw input image 1 into ViT and get global loss. Then, get the local image 2 cropped from 1 and resized 224 ? 224 by bilinear interpolation, and feed it into the enocder with its own cls tokens to get local loss. The two branches share one ViT to avoid more computation. During training, our loss function is a multi-task loss with global and local loss, supervised by cross entropy loss respectively: = + <ref type="bibr" target="#b9">(10)</ref> and are the coefficients complementary to each other. represents the classification loss of general scale and is the guided loss designed to guide the SACM to select the more discriminative parts. It enables the convergent model to make predictions based on overall structural characteristics of the object and the characteristics of the region part. When the model goes through the inference phase, we prefer taking global logits to decide the classification result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS CONFIGURATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluate our proposed AFTrans on widely recognized fine-grained benchmarks, namely CUB-200-2011, Stanford Cars, Stanford Dogs and iNat2017 <ref type="bibr" target="#b30">[29]</ref>. We only utilize the image labels provided by these datasets with no extra annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>In all experiments, we first resize raw images to size 448?448 for global branch and part images to size 224?224 for local branch. We load weights from the official ViT-B_16 model pre-trained on ImageNet21k. We split images to patches of size 16?16 and the step size of sliding window is set to be 12. We employ the SGD optimizer to optimize with a mini-batch size of 12. We use weight decay 0. We take cosine annealing to adjust the learning rate and the first 500 steps are warm-up. The coefficients in Eq13 are set to be 1 and 1. The hyperparameter is chosen to be 0.4. All the experiments are performed on four Quadro RTX 8000 GPUs with Pytorch as our code-base. APEX with FP16 training is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND VISUALIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance Comparison</head><p>We compare AFTrans with SOTA works on above mentioned fine-grained benchmarks. The experiment results on CUB-200-2011, Stanford Cars and Stanford Dogs are shown in tab1. From the results, our method AFTrans outperforms the strong baseline with a margin on CUB dataset, Stanford Dogs and achieves competitive performance on Stanford Cars in Transformer series.  <ref type="table">Table 2</ref>. Comparison of SOTA methods on iNaturalist2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone Acc.(%)</head><p>ResNet-152 <ref type="bibr" target="#b16">[17]</ref> ResNet-152 59.0 SSN <ref type="bibr" target="#b26">[25]</ref> ResNet-101 65.2 IARG <ref type="bibr" target="#b27">[26]</ref> ResNet-101 66.8 IncResNetV2 <ref type="bibr" target="#b28">[27]</ref> IncResNetV2 67.3 TASN <ref type="bibr" target="#b4">[5]</ref> ResNet-101 68.2 ViT <ref type="bibr" target="#b13">[14]</ref> ViT-B_16 68.0 TransFG &amp; PSM <ref type="bibr" target="#b15">[16]</ref> ViT-B_16</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="67.4">AFTrans (ours)</head><p>ViT-B_16 68.9</p><p>For CUB-200-2011, almost all the FGVR methods evaluate their model on it. Compared to the best result TransFG so far, AFTrans achieves 0.6% improvement and outperforms all CNN-based methods. While base framework ViT achieves good performance on CUB, with the addition of our SACM and multi-branch, it achieves a further 1.3% improvement.</p><p>Our method achieves well-matched result in Transformer series and gets 1.5% improvement compared to baseline ViT on Stanford Cars. Due to less necessary work on locatization with simpler backgrounds, AFTrans is lightly worse than PMG and API-Net. For Stanford Dogs, our approach is better than TransFG with its PSM by 1.2%. AFTrans gets 0.4% improvement compared to ViT, attributed to its adaptive attention mechanisms catching subtle differences between species.</p><p>Tab2 shows our evaluation results on iNat2017, which is a large-scale FGVR dataset. Few methods tested on iNat2017 because of the complicate background and computational complexity. Notably, baseline ViT is born for large scale datasets, and outperformes ResNet152 by 9.0%. Based on ViT, our method AFTrans is can achieve an improvement of 0.9% and outperforms all the SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>We conduct ablation studies to understand variants in AF-Trans architecture, which are done on CUB-200-2011 dataset.</p><p>Impact of Selective Attention Collection Module. Shown in tab3, the SACM making a better performance than ViT by proposing subtle parts, which improves by 1.1%. Combined with Fig1, we argue that SACM adaptively fuses perception of attention contribution and forces the attention map to learn the relative importance of input patches. Impact of branch logits in multi-scale framework. There are global branch and local branch in AFTrans, so whose logits decide the classification result needs confirmation. Shown in the tab4, decided by the global logits gets the best result, which observes the object in a holistic perspective and is the resource of attention weights. Impact of hyperparameters . controls SACM to accept which tokens are effective contribution to generate the coordinate. The best accuracy occurs when is set to 0.4. We conclude that when is small, the SACM will crop fewer in the raw images, losing many critical regions to some extent. While when is large, the SACM will crop more in the raw images, resulting in redundant regions fed into model again. Impact of the manner of generating local coordinate. How to convert ? ? coordinates to the last coordinate needs discussion. One is choosing extreme values of coordinates converted as the boundary points, the other is extracting the largest connected component (LCC) of coordinates. Tab6 confirms that LCC method achieves better performance, while it ignores immaterial coordinates forwardly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualization Analysis</head><p>Fig3 conveys that local regions the AFTrans proposed do contain richer fine grained information. The bounding boxes are obtained from attention map fused by SACM. We can clearly see that AFTrans has captured the most discriminative regions of birds, such as head, wings and tail. For Cars, headlights and radiator grille may be the key points, while for dogs, head and ears are critical. In complex dataset iNat2017, AF-Trans still can select local parts for an object, i,e., heads for Aves, horns and head for Mammalia; fins for Actinopterygii.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we propose a novel method named adaptive attention multi-scale fusion transformer (AFTrans) to learn subtle region attention without box annotations and compensate for ViT shortcomings in FGVR. The multi-scale structure can make full use of the images obtained by SACM to achieve excellent performance. Our algorithm can achieve SOTA performance on three fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017. The future work is how to locate region attention more precisely to further improve the classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The performance comparison on CUB-200-2011 with single or multiple fusion attention weights in transformer layers, utilized for proposing the critical parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The Architecture of AFTrans. Images are split into patches and sent into ViT. Collected from attention weights processed by Hadamard product in each transform layer, the Selective Attention Collection Module (SACM) combines with critical tokens and converts it into the coordinates of local regions like the red bounding box. Notably, the linear projection, the transformer layers, and linear layer are parameter-sharing, while classification tokens and position embeddings do not. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different SOTA methods on CUB-200-2011, Stanford Cars and Stanford Dogs (%).</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">CUB CAR</cell><cell>DOG</cell></row><row><cell>RA-CNN [18]</cell><cell>VGG-19</cell><cell>85.3</cell><cell>92.5</cell><cell>87.3</cell></row><row><cell>MaxEnt [19]</cell><cell>DenseNet-161</cell><cell>86.6</cell><cell>93.0</cell><cell>83.6</cell></row><row><cell>DFL-CNN [20]</cell><cell>ResNet-50</cell><cell>87.4</cell><cell>93.1</cell><cell>84.9</cell></row><row><cell>Cross-X [21]</cell><cell>ResNet-50</cell><cell>87.7</cell><cell>94.6</cell><cell>88.9</cell></row><row><cell>SEF [28]</cell><cell>ResNet-50</cell><cell>87.3</cell><cell>94.0</cell><cell>88.8</cell></row><row><cell>FDL [24]</cell><cell>DenseNet-161</cell><cell>89.1</cell><cell>94.2</cell><cell>84.9</cell></row><row><cell>PMG [22]</cell><cell>ResNet-50</cell><cell>89.6</cell><cell>95.1</cell><cell>-</cell></row><row><cell>MMAL [16]</cell><cell>ResNet-50</cell><cell>89.6</cell><cell>95.0</cell><cell>-</cell></row><row><cell>API-Net [23]</cell><cell>DenseNet-161</cell><cell>90.0</cell><cell>95.3</cell><cell>90.3</cell></row><row><cell>StackedLSTM [7]</cell><cell>GoogleNet</cell><cell>90.4</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT [14]</cell><cell>ViT-B_16</cell><cell>90.2</cell><cell>93.5</cell><cell>91.2</cell></row><row><cell>TransFG [16]</cell><cell>ViT-B_16</cell><cell>90.9</cell><cell>94.1</cell><cell>90.4</cell></row><row><cell>AFTrans (ours)</cell><cell>ViT-B_16</cell><cell>91.5</cell><cell>95.0</cell><cell>91.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiment on attention weights fusion method.</figDesc><table><row><cell>Methods</cell><cell>Acc.(%)</cell></row><row><cell>None(ViT)</cell><cell>90.2</cell></row><row><cell>PSM(TransFG)</cell><cell>90.9</cell></row><row><cell>SACM(AFTrans)</cell><cell>91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation experiment on branch logits ( = 0.4).</figDesc><table><row><cell>Global Logits</cell><cell>Local Logits</cell><cell>Acc.(%)</cell></row><row><cell></cell><cell>?</cell><cell>90.6</cell></row><row><cell>?</cell><cell></cell><cell>91.5</cell></row><row><cell>?</cell><cell>?</cell><cell>91.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation experiment on thresh . Visualization of discriminative local parts learned by the SACM on four FGVR datasets. Best viewed in color.</figDesc><table><row><cell>Value of</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>Acc.(%)</cell><cell>90.6</cell><cell>90.9</cell><cell>91.3</cell><cell>91.5</cell><cell>91.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation experiment on generating the coordinate of local regions</figDesc><table><row><cell>Methods</cell><cell>Acc.(%)</cell></row><row><cell>Extreme Values</cell><cell>90.9</cell></row><row><cell>LCC</cell><cell>91.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Deep Bilinear Transformation for Fine-grained Image Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Kernel Pooling for Convolutional Neural Networks. In2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Selective sparse sampling for fine-grained image recognition. InProceedings of the IEEE/CVF International Con-ference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin-Jiao</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bird Species Categorization Using Pose Normalized Deep Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuiyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guisheng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InMultiMedia Modeling -27th International Conference, MMM 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TransFG: A Transformer Architecture for Fine-grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N.Cesa-Bianchi, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xitong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lingyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to zoom: a saliency based sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpretable and accurate finegrained recognition via region grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8662" to="8672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning semantically enhanced feature for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1545" to="1549" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The INaturalist Species Classification and Detection Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on-Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
