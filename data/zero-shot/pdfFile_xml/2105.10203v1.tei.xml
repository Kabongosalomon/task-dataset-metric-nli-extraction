<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
							<email>yyqu@xmu.edu.cnyxie@cs.ecnu.edu.cnma-lz@cs.sjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Fujian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hidden features in neural network usually fail to learn informative representation for 3D segmentation as supervisions are only given on output prediction, while this can be solved by omni-scale supervision on intermediate layers. In this paper, we bring the first omni-scale supervision method to point cloud segmentation via the proposed gradual Receptive Field Component Reasoning (RFCR), where target Receptive Field Component Codes (RFCCs) are designed to record categories within receptive fields for hidden units in the encoder. Then, target RFCCs will supervise the decoder to gradually infer the RFCCs in a coarse-tofine categories reasoning manner, and finally obtain the semantic labels. Because many hidden features are inactive with tiny magnitude and make minor contributions to RFCC prediction, we propose a Feature Densification with a centrifugal potential to obtain more unambiguous features, and it is in effect equivalent to entropy regularization over features. More active features can further unleash the potential of our omni-supervision method. We embed our method into four prevailing backbones and test on three challenging benchmarks. Our method can significantly improve the backbones in all three datasets. Specifically, our method brings new state-of-the-art performances for S3DIS as well as Semantic3D and ranks the 1st in the ScanNet benchmark among all the point-based methods. Code will be publicly available at https://github.com/azuki-miho/RFCR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation of point cloud in which we need to infer the point-level labels is a typical but still challenging The Receptive Field Component Code (RFCC) indicates the category components in the receptive field. In the decoding stage, the segmentation problem is decomposed into a much easier global context recognition problem (predicting the global RFCCs, see the top of figure) and a series of receptive field component reasoning problems. During reasoning, the target RFCCs generated in the encoder are used as the groundtruth in the decoder to guide the network to gradually reason the RFCCs in a coarse-to-fine manner, and finally obtain the semantic labels. task in 3D vision. Meanwhile, this technique can be widely used in many applications like robotics, autonomous driving, and virtual/augmented reality.</p><p>To handle point cloud segmentation, previous works usu-ally introduced well-designed encoder-decoder architecture to hierarchically extract global context features in the encoding stage, and distribute contextual features to points in the decoding stage to achieve point-wise labeling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. However, in the typical encoder-decoder framework, network is merely supervised by labels of points in the final layer <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9]</ref>, while ignoring a critical fact that, hidden units in other layers lack direct supervision to extract features with informative representation. In other words, multi-scale/omni-scale supervision is indeed necessary. In 2D vision, CVAE <ref type="bibr" target="#b27">[28]</ref> attempted to give a multi-scale prediction and supervision to extract useful features in segmentation task. CPM <ref type="bibr" target="#b34">[35]</ref> and MSS-net <ref type="bibr" target="#b13">[14]</ref> tried to add intermediate supervision periodically and layer-wise loss, respectively. PointRend <ref type="bibr" target="#b15">[16]</ref> proposed to segment image in low-resolution, and iteratively up-sample the coarse prediction and fine-tune it to obtain final result, thus prediction at different scales can be supervised together.</p><p>However, so far, no one succeed in applying multi-scale, let alone omni-scale supervision to 3D semantic segmentation, due to the irregularity of point cloud. Unlike in image domain, it is hard to up-sample the hidden features to the original resolution through simple tiling or interpolation, because there is no fixed mapping relationship between sampled point cloud and original point cloud especially when the sampling is random <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9]</ref>. Additionally, the common up-sampling methods using nearest neighbors cannot trace the encoding relationship, thus introducing improper supervisions to the intermediate features (referring Sec 4.4 for discussion). More recently, SceneEncoder <ref type="bibr" target="#b36">[37]</ref> provided a method to supervise the center-most layer to extract meaningful global features, but lots of other layers remain unhandled.</p><p>To solve this problem, we propose an omni-scale supervision method via gradual Receptive Field Component Reasoning. Instead of up-sampling the hidden features to the original resolution, we design a Receptive Field Component Code (RFCC) to effectively trace the encoding relationship and represent the categories within receptive field for each hidden unit. Based upon this, we generate the target RFCCs at different layers from semantic labels in the encoding stage to supervise the network at all scales. Specifically, in the decoding stage, the target RFCCs will supervise the network to predict the RFCCs at different scales, and the features (hints) from skip link can help further deduce RFCCs within more local and specific receptive fields. In this way, the decoding stage is transferred into a gradual reasoning procedure, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Inspired by SceneEncoder <ref type="bibr" target="#b36">[37]</ref>, for each sampled point in any layer of encoder, according to the existence of categories in its receptive field, a multi-hot binary code can be built, designated as target Receptive Field Component Code (RFCC). The target RFCCs at different layers are gen-erated alongside the convolution and down-sampling, thus they can precisely record the existing categories in corresponding receptive fields without any extra annotations. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show the target RFCCs at various layers for a point cloud in the decoding stage, where the network will first recognize the global context (inferring the categories of objects existing in the whole point cloud). Then, contextual features will be up-sampled iteratively to gradually reason the RFCCs in a coarse-to-fine manner. By comparing the target RFCCs and the predicted RFCCs, the omni-scale supervision can be realized. It is noteworthy that even the network reasons the RFCCs gradually, the training and inference of network is implemented in a end-to-end manner.</p><p>Additionally, to further unleash the potential of omniscale supervision, more active features (features with large magnitude) are required to make unambiguous contribution to the RFCC prediction. Contrarily, in traditional networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>, lots of units are inactive with tiny magnitude, such that having minor contribution to the final prediction. The principle underlying the above observations comes from entropy regularization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> over features, where greater number of active dimensionalities would bring low-density separation between positive features and negative features, generating more unambiguous features with certain signals. Consequently, in point cloud scenario, more certainty in features can help the training of the network to better reason the RFCCs at various scales and finally predict the semantic labels. Motivated by this, we proposed a Feature Densification method with a welldeigned potential function to push hidden features away from 0. Moreover, this potential is in effect equivalent to a entropy loss over features (detailed deduction is shown in Sec 3.4), leading to a simple but highly effective regularization for intermediate features.</p><p>To evaluate the performance and versatility of our method in point cloud semantic segmentation task, we embed our method into four prevailing backbones (deformable KPConv, rigid KPConv <ref type="bibr" target="#b30">[31]</ref>, RandLA <ref type="bibr" target="#b8">[9]</ref>, and SceneEncoder <ref type="bibr" target="#b36">[37]</ref>), and test on three challenging point cloud datasets (ScanNet v2 <ref type="bibr" target="#b1">[2]</ref> for indoor cluttered rooms, S3DIS <ref type="bibr" target="#b0">[1]</ref> for large indoor space, and Semantic3D <ref type="bibr" target="#b6">[7]</ref> for large-scale outdoor space). In all the three datasets, we outperform the backbone methods and almost all the state-ofthe-art point-based competitors. What's more, we also push the state-of-the-art of S3DIS <ref type="bibr" target="#b0">[1]</ref> and Semantic3D <ref type="bibr" target="#b6">[7]</ref> ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Point Cloud Semantic Segmentation. PointNet <ref type="bibr" target="#b24">[25]</ref> proposed to directly concatenate global features to pointwise features before several Multi-Layer Perceptrons (MLPs) to finish the semantic segmentation. Later, Point-Net++ <ref type="bibr" target="#b25">[26]</ref>, SubSparseConv <ref type="bibr" target="#b4">[5]</ref> and KPConv <ref type="bibr" target="#b30">[31]</ref> utilized an encoder-decoder architecture with skip links for better fusion of local and global information. Joint tasks like instance segmentation and edge detection are also introduced to enhance the performance of semantic segmentation through additional supervision <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10]</ref>. SceneEncoder <ref type="bibr" target="#b36">[37]</ref> designed a meaningful global scene descriptor to guide the global feature extraction. These methods directly utilized semantic labels to supervise the output features or features in the center-most layer.</p><p>Compared with previous works, we propose an omniscale supervision method for point cloud semantic segmentation via a gradual Receptive Field Component Reasoning.</p><p>Multi-scale Supervision. In 2D Vision, CVAE <ref type="bibr" target="#b27">[28]</ref> proposed to give multi-scale prediction in the segmentation task. RMI <ref type="bibr" target="#b43">[44]</ref> proposed to predict and supervise the neighborhood of each pixel rather than the pixel itself. PointRend <ref type="bibr" target="#b15">[16]</ref> segmented the images in a coarse-to-fine fashion, i.e. give low-resolution prediction, and iteratively up-sample and fine-tune it to obtain the original-resolution prediction. CPM <ref type="bibr" target="#b34">[35]</ref> and MSS-net <ref type="bibr" target="#b13">[14]</ref> added intermediate supervision periodically and layer-wise loss, respectively.</p><p>Compared with these methods, we design a Receptive Field Component Code (RFCC) to represent receptive field component and dynamically generate target RFCCs to give omni-scale supervision to the network rather than simply up-sample the features to the original resolution or downsample the ground truth. Thanks to the omni-scale supervision, the network can infer the RFCCs gradually and finally obtain RFCCs in the original resolution which is also the semantic labels.</p><p>Entropy Regularization. Entropy Regularization <ref type="bibr" target="#b5">[6]</ref> minimized the prediction entropy in semi-supervised classification task to obtain unambiguous final features. This idea is introduced into the deep neural network for self-training by <ref type="bibr" target="#b17">[18]</ref>, and the final features with tiny magnitude will be pushed away from 0 to make deterministic contribution to the final prediction. In these methods, final features with positive values will be greater and negative features will be smaller due to the entropy loss.</p><p>Compared with their methods, our Feature Densification introduce the entropy regularization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> into the hidden features rather than just the final features to obtain more active hidden features which can directly contribute to the RFCC prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In the following parts, we will first give an overview of our method in Sec 3.1. Then, we will introduce the Receptive Field Component Codes (RFCCs) and the target RFCCs that we generate at various layers in Sec 3.2. In Sec 3.3, how to use these target RFCCs to supervise the network, and make the gradual Receptive Field Component Reasoning, would be explained. At last, we will show the strategy of Feature Densification for more active features in Sec 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The framework of our gradual Receptive Field Component Reasoning (RFCR) is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In our method, we generate target Receptive Field Component Codes (RFCCs) at different layers alongside the convolution and sampling of features <ref type="figure" target="#fig_1">(Figure 2 (a)</ref>) in the encoding stage. In the decoding stage, the network will reason the RFCCs at different layers, and the corresponding target RFCCs will give omni-scale supervision on the predicted RFCCs <ref type="figure" target="#fig_1">(Figure 2 (b)</ref>). Consequently, the semantic segmentation task can be treated as a coarse-to-fine receptive field component reasoning procedure after recognizing the global context (predicting categories of objects existing in the point cloud). Additionally, we introduce Feature Densification through a centrifugal potential to obtain more active features for omni-scale RFCC prediction <ref type="figure" target="#fig_1">(Figure 2</ref> (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Receptive Field Component Code</head><p>For a point cloud, it is easy to define the label of a point in the original point cloud. Nevertheless, it is non-trivial to give a label to a point in any down-sampled point cloud which receives information from points inside its receptive field. In our method, we design a Receptive Field Component Code (RFCC) to represent all categories within the receptive field of sampled points in the encoder. The target RFCCs are generated alongside the convolution and sampling of features in the encoding stage. In other words, sharing sampling is used between the encoding stage (left part of top branch in <ref type="figure" target="#fig_1">Figure 2</ref>) and RFCC generation (Figure 2 (a)), thus the generated target RFCCs can precisely record the category components in the receptive fields, even though the sampling of point cloud is a random process.</p><p>Implementation. Our RFCC is designed to be a multihot label for every point in any layer of encoder. Specifically, in the semantic segmentation task where we need to classify each point into C categories, the RFCC will be a 1 ? C binary vector. Given the i-th point in the l-th layer of the encoder p l i , the target RFCC g l i represents the categories of objects existing in the receptive field of p l i , and each element g l i [k] indicates the existence of category k. Based upon this definition, we can first assign the one-hot label of input point p i to the RFCC g 1 i in the input layer, because the receptive field of point p i only contains p i itself:</p><formula xml:id="formula_0">g 1 i = one-hot(y i ),<label>(1)</label></formula><p>where y i is the label of point p i in the original point cloud. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (a), we can obtain g l i from the indicates the network will predict the RFCCs in a coarse-to-fine manner. (c) represents the centrifugal potential which pushes hidden features away from 0. In our network, the target RFCCs will supervise the RFCC predictions, and the learnt feature can reason RFCCs in more local and specific receptive fields as more and more local features (clues) are provided through skip links. The prediction activation function will be Softmax for the final layer and Sigmoid otherwise.</p><p>RFCCs in the previous layer g l?1 i alongside the 3D Convs:</p><formula xml:id="formula_1">g l i [k] = ? j?N (i) {g l?1 j [k]}<label>(2)</label></formula><p>where k ? [1, C] indicates the channel index, and j is the index of point in p l i 's receptive field at the (l?1)-th layer. That is to say, p l i receives features from p l?1 j in the 3D Convs thanks to the sharing sampling. ? represents the logical OR (disjunction) operation. It is noteworthy that the generation of RFCCs only occurs in the encoder, rather than the decoder. The generation of RFCCs is iterated until reaching the center-most layer L. Typically, the scene descriptor is only a naturally deduced global supervisor when the center-most layer contains only one point <ref type="bibr" target="#b36">[37]</ref>. Besides, g 2 i can also be treated as a simplified version of neighborhood multi-dimension distribution in RMI <ref type="bibr" target="#b43">[44]</ref>, which exploits the semantic relationship among neighboring points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RFCC Reasoning</head><p>The decoder of network is to infer the category of each input point in the task of semantic segmentation. In our method, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b), we decompose this complex problem into a much easier global context recognition problem (predicting g L i ) and a series of gradual receptive field component reasoning problem (reasoning g l?1 i from g l i gradually with additional features ? l i from skip link and finally obtain the semantic labels g 1 i ). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, ? l i is the features of sampled point p l i in decoder. For each layer of decoder except the last one, we apply a shared Multi-Layer Perceptron (MLP) M l and a sigmoid function ? to ? l i to predict the RFCCsg l i :</p><formula xml:id="formula_2">g l i = ?(M l (? l i )).<label>(3)</label></formula><p>Then, the target RFCC g l i generated in the encoding stage is directly used to guideg l i prediction through layer-wise supervision L l R :</p><formula xml:id="formula_3">L l R = ? 1 C|P l | |P l | i=1 C k=1 L l R (i, k),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">L l R (i, k) = g l i [k]log(g l i [k])+(1?g l i [k])log(1?g l i [k]),<label>(5)</label></formula><p>P l denotes the sampled point cloud in the l-th layer of encoder, and |P l | corresponds the number of points in P l . According to Eq. (3), the center-most features ? L i which contain global information will learn to recognize the global context, i.e., predictg L i with largest receptive field. Meanwhile, g L i will be used to regularize this prediction to help ? L i learn a better representation. Then, for the following layer of decoder, ? L which learns informative representation to predictg L i will be up-sampled and concatenated with ? L?1 i from the skip link. After that, the concatenated features will be used to extract more distinguishable ? L?1 i via 3D Convs, and the extracted features ? L?1 i will be used to reason the RFCCsg L?1 i of more local and specific receptive field. This procedure is iterated until l = 2. The whole RFCC reasoning loss can be simply expressed by</p><formula xml:id="formula_5">L R = 1 L ? 1 L l=2 L l R .<label>(6)</label></formula><p>In the last layer, we can simply utilize the MLPs and softmax to predict theg 1 i , and cross entropy loss is used to supervise the output features in the original scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature Densification</head><p>Due to the large amounts of supervision introduced by the gradual Receptive Field Component Reasoning, more active features with unambiguous signals are required. However, there are many inactive hidden units with tiny magnitude in the traditional network (detailed experiment is shown in Sec 4.4). Therefore, we introduce a centrifugal potential to bring low-density separation between positive features and negative features (i.e. push features away from 0) as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c):</p><formula xml:id="formula_6">?(?) = ?log 1 1 + e ?|?| ,<label>(7)</label></formula><p>where? = a(?) and a can be an identity function or a simple perceptron. We can see the negative gradient of potential function over feature is:</p><formula xml:id="formula_7">? ??(?) ?? = sign(?) e ?|?| 1 + e ?|?|<label>(8)</label></formula><p>which have the same sign as the feature. This indicates positive features will become greater and negative features will be smaller given this potential. Additionally, features with smaller absolute value will receive larger gradient according to this formula. Meanwhile, this centrifugal potential can be implemented by a simple entropy loss:</p><formula xml:id="formula_8">L l F (i, k) = ?(? l i,k ) = ?log 1 (1+e ?|? l i,k | ) = ?log(?(? l i,k ))? l i,k ? 0 ?log(1 ? ?(? l i,k ))? l i,k &lt; 0 ,<label>(9)</label></formula><p>where? l i,k is the k-th channel of? l i . If we take the following notation:</p><formula xml:id="formula_9">t l i,k = ?(? l i,k ) t l i,k = 1 if? l i,k ? 0, 0 if? l i,k &lt; 0,<label>(10)</label></formula><p>we can reformulate Eq. (9) into</p><formula xml:id="formula_10">L l F (i, k) = ?[t l i,k log(t l i,k ) + (1 ? t l i,k )log(1 ?t l i,k )].<label>(11)</label></formula><p>So, our centrifugal potential can be treated as entropy regularization <ref type="bibr" target="#b17">[18]</ref> over hidden features which can decrease ambiguity of features in the intermediate layers. On the other side, our omni-scale supervision can directly benefit from more active features with certain signal introduced by the Feature Densification. That is because more unambiguous features can participate into the RFCC predictions and help learning better representation of hidden layer, improving the semantic segmentation performance.</p><p>The total loss for Feature Densification can be summarized by</p><formula xml:id="formula_11">L F = 1 L ? 1 L l=2 1 |P l |K l |P l | i=1 K l k=1 L l F (i, k),<label>(12)</label></formula><p>and K l represents the number of features' channel in? l i . In a nutshell, all the supervision can be concluded by</p><formula xml:id="formula_12">L = L S + ? 1 L R + ? 2 L F .<label>(13)</label></formula><p>where ? 1 and ? 2 are two adjustable hyper-parameters while L S represents the common cross entropy loss for semantic segmentation. In our experiment, we simply set ? 1 and ? 2 to 1, and we find it can perform well in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the effectiveness of our method and prove our claims, we embed our method into four prevailing methods (deformable KPConv, rigid KPConv <ref type="bibr" target="#b30">[31]</ref>, RandLA <ref type="bibr" target="#b8">[9]</ref> and SceneEncoder <ref type="bibr" target="#b36">[37]</ref>), and conduct experiments on three popular point cloud segmentation datasets (ScanNet v2 <ref type="bibr" target="#b1">[2]</ref> for cluttered indoor scenes, S3DIS <ref type="bibr" target="#b0">[1]</ref> for large-scale indoor rooms and Semantic3D <ref type="bibr" target="#b6">[7]</ref> for large outdoor spaces). First, we introduce these three datasets in Sec 4.1. Next, implementation details and hyper-parameters used in our experiments are described in Sec 4.2. Then, we give the metric used to evaluate the performance as well as the quantitative and qualitative results in Sec 4.3. Finally, we conduct more ablation studies to prove our claims in Sec 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ScanNet v2. In the task of ScanNet v2 <ref type="bibr" target="#b1">[2]</ref>, we need to classify all the points into 20 different semantic categories. This dataset provides 1, 513 scanned scenes with point-level annotations, 1, 201 scanned scenes for training, and 312 scanned scenes for validation. Another 100 scanned scenes are published without any annotations for testing. We need to make prediction on the test set and submit our final result to ScanNet server for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3DIS. S3DIS [1] provides point clouds of 271 rooms</head><p>with comprehensive annotations in 6 large-scale indoor areas from 3 different buildings. There are 273 million points in total, and all these points are categorized into 13 classes. Following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, we take Area 5 as the test set and rooms in the remaining areas for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>All the experiments can be conducted on a single GTX 1080Ti with 3700X CPU and 64 GB RAM. We apply our method to a common backbone deformable KPConv <ref type="bibr" target="#b30">[31]</ref> and evaluate the performance on all three datasets. To show the versatility of our method, we also embed our method into three other backbones (one for each dataset).</p><p>ScanNet. We separately choose deformable KPConv <ref type="bibr" target="#b30">[31]</ref> and SceneEncoder <ref type="bibr" target="#b36">[37]</ref> as our backbones and apply our method. When we take deformable KPConv as our backbone, we randomly sample spheres with radius equal to 2 meters from scenes in the training set during training procedure, and the batch size is set to 10. When we take SceneEncoder as our backbone and train our model, we randomly sample 8 3m?1.5m?1.5m cubes from training scenes for every batch like SceneEncoder <ref type="bibr" target="#b36">[37]</ref>. After training, we separately predict the results of the test set using these two trained models and submit them to the online benchmark server for testing <ref type="bibr" target="#b1">[2]</ref>.</p><p>S3DIS. We insert our methods into deformable KP-Conv <ref type="bibr" target="#b30">[31]</ref> and RandLA <ref type="bibr" target="#b8">[9]</ref> respectively and treat them as our backbones. When we take deformable KPConv as our backbone, we randomly sample spheres with 2m radius from original point clouds, and the batch size is set to 5. We randomly sample 40, 960 points from entire rooms for each training sample and set the batch size to be 6 when taking RandLA <ref type="bibr" target="#b8">[9]</ref> as the backbone. Rooms in Area-1,2,3,4,6 are used for training. After training, we test the model on the whole S3DIS Area-5 set.</p><p>Semantic3D. Deformable KPConv and rigid KPConv proposed in <ref type="bibr" target="#b30">[31]</ref> are taken as our backbones to evaluate our method on Semantic3D reduced-8 task <ref type="bibr" target="#b6">[7]</ref>. Because Semantic3D is a large-scale outdoor space dataset, point cloud is randomly sampled into a sphere with 3m radius for deformable KPConv backbone and 4m radius for rigid KPConv backbone. Every time, 10 samples are fed into the network for training and testing. We need to submit the final predictions to the Semantic3D server for testing <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU(%)</head><p>PointNet++ (NIPS'17) <ref type="bibr" target="#b25">[26]</ref> 33.9 PointCNN (NIPS'18) <ref type="bibr" target="#b20">[21]</ref> 45.8 3DMV (ECCV'18) <ref type="bibr" target="#b2">[3]</ref> 48.4 PointConv (CVPR'19) <ref type="bibr" target="#b35">[36]</ref> 55.6 TextureNet (CVPR'19) <ref type="bibr" target="#b10">[11]</ref> 56.6 HPEIN (ICCV'19) <ref type="bibr" target="#b12">[13]</ref> 61.8 SPH3D-GCN (TPAMI'20) <ref type="bibr" target="#b19">[20]</ref> 61.0 FusionAwareConv (CVPR'20) <ref type="bibr" target="#b40">[41]</ref> 63.0 FPConv (CVPR'20) <ref type="bibr" target="#b21">[22]</ref> 63.9 DCM-Net (CVPR'20) <ref type="bibr" target="#b26">[27]</ref> 65.8 PointASNL (CVPR'20) <ref type="bibr" target="#b37">[38]</ref> 66.6 FusionNet (ECCV'20) <ref type="bibr" target="#b39">[40]</ref> 68. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Metric and Results</head><p>Metric. For better evaluation of segmentation performance, we take mean Intersection over Union (mIoU) among categories as our metric like many previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The results of semantic segmentation on ScanNet v2 <ref type="bibr" target="#b1">[2]</ref> are reported in <ref type="table">Table 1</ref>, where we achieve 70.2% mIoU and rank first in this benchmark among all point-based methods. Here, we take deformable KPConv as our baseline and 1.8% improvement is achieved in mIoU. To show the generalization ability of our method, we also apply our method to SceneEncoder <ref type="bibr" target="#b36">[37]</ref>. As shown in <ref type="table">Table 1</ref>, 3.1% improvement in mIoU is achieved. Additionally, we provide the qualitative results of our baseline (deformable KPConv) and our method in <ref type="figure">Figure 3</ref>. The red dashed circles indicate the obvious qualitative improvements.</p><p>We report the segmentation results on S3DIS Area-5 <ref type="bibr" target="#b0">[1]</ref> in <ref type="table" target="#tab_2">Table 2</ref>. In this dataset, we also take deformable KP-Conv as our backbone and achieve 68.73% mIoU in S3DIS Area-5 task which pushes the state-of-the-art performance ahead. Deformable KPConv is also treated as our baseline for its good performance. Meanwhile, we also apply our method to RandLA and the improvement over these backbones is also obvious (i.e., 2.67% mIoU). <ref type="figure">Figure 4</ref> gives the visualization results of our method and the qualitative improvement over the baseline (deformable KPConv).</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, we show the results of our method and other prevailing methods on Semantic3D <ref type="bibr" target="#b6">[7]</ref>. In this task, we achieve 77.8% in mIoU, outperforming all the state-of-the-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Ground Truth</head><p>KPConv deform (Baseline) Ours <ref type="figure">Figure 3</ref>: Visualization results on the validation dataset of ScanNet v2. The images from the left to right are input point clouds, semantic labels, predictions given by our baseline and our method, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU(%)</head><p>PointNet (CVPR'17) <ref type="bibr" target="#b24">[25]</ref> 41.09 RSNet (CVPR'18) <ref type="bibr" target="#b11">[12]</ref> 51.93 PointCNN (NIPS'18) <ref type="bibr" target="#b20">[21]</ref> 57.26 ASIS (CVPR'19) <ref type="bibr" target="#b33">[34]</ref> 54.48 ELGS (NIPS'19) <ref type="bibr" target="#b32">[33]</ref> 60.06 PAT (CVPR'19) <ref type="bibr" target="#b38">[39]</ref> 60.07 SPH3D-GCN (TPAMI'20) <ref type="bibr" target="#b19">[20]</ref> 59.5 PointASNL (CVPR'20) <ref type="bibr" target="#b37">[38]</ref> 62.6 FPConv (CVPR'20) <ref type="bibr" target="#b21">[22]</ref> 62.8 Point2Node (AAAI'20) <ref type="bibr" target="#b7">[8]</ref> 62.96 SegGCN (CVPR'20) <ref type="bibr" target="#b18">[19]</ref> 63.6 DCM-Net (CVPR'20) <ref type="bibr" target="#b26">[27]</ref> 64.0 FusionNet (ECCV'20) <ref type="bibr" target="#b39">[40]</ref> 67.  Then we take rigid KPConv as our backbone, and our method can also bring 3.0% improvement in mIoU. We present the visual results of our method and the baseline (deformable KPConv) on the validation set of Semantic3D in <ref type="figure" target="#fig_3">Figure 5</ref>. The dark blue dashed circles indicate the qualitative improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we conduct more experiments to evaluate the effectiveness of the proposed gradual Receptive Field Component Reasoning (RFCR) method from different as-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth KPConv deform (Baseline) Ours <ref type="figure">Figure 4</ref>: Visualization results on the test dataset of the S3DIS Area-5. The left-most images are input point clouds and the following images are segmentation ground truth, predictions of baseline and our method separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU(%)</head><p>SegCloud (3DV'17) <ref type="bibr" target="#b28">[29]</ref> 61.3 RF MSSF (3DV'18) <ref type="bibr" target="#b29">[30]</ref> 62.7 SPG (CVPR'18) <ref type="bibr" target="#b16">[17]</ref> 73.2 ShellNet (ICCV'19) <ref type="bibr" target="#b41">[42]</ref> 69.4 GACNet (CVPR'19) <ref type="bibr" target="#b31">[32]</ref> 70.8 FGCN (CVPR'20) <ref type="bibr" target="#b14">[15]</ref> 62.4 PointGCR (WACV'20) <ref type="bibr" target="#b22">[23]</ref> 69.5 RandLA (CVPR'20) <ref type="bibr" target="#b8">[9]</ref> 77.4   Inputs, semantic labels, results of our baseline and our method are presented separately from the left to the right.</p><p>pects. Without loss of generality, our ablation studies are mainly conducted on the task of Semantic3D reduced-8 and deformable KPConv <ref type="bibr" target="#b30">[31]</ref> is chosen as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU</head><p>KPConv deform 73.1 + RFCR 76.0 + FD 77.8 KPConv deform + RFCR + FD 77.8 <ref type="table">Table 5</ref>: Ablation study on omni-scale supervision strategy.</p><p>Gradual Receptive Field Component Reasoning. To conduct ablation studies on different parts of gradual Receptive Field Component Reasoning in the semantic segmentation, we firstly only give the omni-supervision in the decoding procedure to guide the network reason Receptive Field Component Codes (RFCCs) gradually without the loss for Feature Densification (FD). Then, we add the centrifugal potential to obtain more active features for RFCC prediction, and the results are reported in <ref type="table" target="#tab_5">Table 4</ref>. The results indicate the Receptive Field Component Reasoning can improve the segmentation performance by 2.9% alone, and FD can further bring 1.8% improvement. We also conduct ablation studies on the effects of supervisions at different scales and provide the details in supplementary materials.</p><p>Omni-supervision via Up-sampling. Multi-scale supervision is usually used in 2D segmentation via up-sampling the low-resolution prediction. Even we cannot up-sample the point cloud through simple tiling or interpolation, we attempt to up-sample the intermediate predictions iteratively using the nearest neighbors. Then, semantic labels are used to supervise all the up-sampled predictions. Same as our method, all scales are supervised and Feature Densification is also used to provide more unambiguous features for intermediate prediction. We report the result of Omnisupervision via Up-sampling (OvU) in <ref type="table">Table 5</ref> and compare it with our method. It shows inferior performance (76.2%) because the up-sampling method using nearest neighbors cannot trace the proper encoding relationship.</p><p>One-hot RFCC. In previous works like PointRend <ref type="bibr" target="#b15">[16]</ref>, they give one-hot predictions at low resolutions, and these predictions will be up-sampled to be supervised by the onehot labels at original resolution. So, it is intuitive to take an one-hot RFCC for the major category in the receptive field to supervise the prediction. However, the category information of some points will be ignored in this way. Compared with this method, we take a multi-hot label for every sampled point at all the scales, and no labels will be ignored in the supervision of down-sampled points. In order to show the benefit of multi-hot labels, we replace the multi-hot labels with one-hot labels which represent the majority of categories in the receptive fields, and all other settings remain the same. We report the results in <ref type="table">Table 5</ref>. We can see onehot RFCC which ignores the minor category cannot fully represent the information in the receptive field, thus having sub-optimal performance (76.4%) in the segmentation which is 1.4% lower than multi-hot RFCC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a gradual Receptive Field Component Reasoning method for omni-supervised point cloud segmentation which decomposes the hard segmentation problem into a global context recognition task and a series of gradual Receptive Field Component Code reasoning steps. Additionally, we propose a complementary Feature Densification method to provide more active features for RFCC prediction. We evaluate our method with four pre-vailing backbones on three popular benchmarks and outperform almost all the state-of-the-art point-based competitors. Furthermore, our method brings new state-of-the-art performance for Semantic3D and S3DIS benchmarks. Even our method brings large improvements to many backbones for point cloud segmentation, it is more suitable for networks with encoder-decoder architecture.  <ref type="table">Table 6</ref>: Ablation study on significance of supervisions at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervisions at Different Layers</head><p>We design an omni-scale supervision method for point cloud segmentation via the proposed gradual Receptive Field Component Reasoning in the main paper. All scales are supervised in the decoding stage to learn informative representation for semantic segmentation. In this section, we attempt to analyze the significance of supervisions at different scales. In this ablation study, deformable KP-Conv <ref type="bibr" target="#b30">[31]</ref> is also taken as the backbone and performance is evaluated on the Semantic3D reduced-8 task. In the architecture of deformable KPConv network, there are 5 different scales as shown in <ref type="figure" target="#fig_2">Figure 7</ref>. So, we separately remove the supervisions for l = 2, 3, 4, 5. It is noteworthy that we always keep the supervision for the final layer (l = 1) because it directly guides the semantic label prediction, otherwise the network will give random prediction. The results is reported in <ref type="table">Table 6</ref>. The results indicates supervision in the center-most layer (l = 5) plays an important role in the omni-scale supervision. That is because it can help the encoder to obtain representative global features which is quite important for the following reasoning. Meanwhile, the supervision before the final prediction l = 2 also contributes a lot because it can directly provide semantic informative features to the final segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization of intermediate RFCC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supervision on Decoder vs. Encoder</head><p>In our implementation, all the supervisions are added in the decoder even the target RFCCs are generated according to the receptive fields of features in the encoder. That is   <ref type="table">Table 7</ref>: More ablation study on the strategy of omni-scale supervision.</p><formula xml:id="formula_13">" # $ % &amp; &amp; % $ # " Input " # $ % &amp; ) &amp; ) % ) $ ) # ) " Label</formula><p>because the features in the encoder can also be supervised through the skip links. In order to show the advantage of our strategy, we attempt to supervise the features in the encoder rather than the decoder according to the RFCCs, and Feature Densification is also applied on the corresponding features in the encoder. Compared with supervision in the decoding stage, guiding the feature extraction using RFCCs in the encoder is not able to effectively extract informative representation from global and local features in the decoding stage, such obtaining inferior result as reported in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization Results</head><p>In this section, we present more visualization results of our method on the three datasets described in the main paper. We present more visualization results of our baseline and our methods on the validation set of ScanNet v2 <ref type="bibr" target="#b1">[2]</ref> in Input Ground Truth KPConv deform (Baseline) Ours <ref type="figure">Figure 9</ref>: More visualization results on the validation dataset of ScanNet v2. The images from the left to right are input point clouds, semantic labels, predictions given by our baseline and our method, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Ground Truth</head><p>KPConv deform (Baseline) Ours <ref type="figure" target="#fig_0">Figure 10</ref>: More visualization results on the test dataset of the S3DIS Area-5. The left-most images are inputs and the following images are segmentation ground truth, predictions of baseline and our method separately. <ref type="figure">Figure 9</ref>. In <ref type="figure" target="#fig_0">Figure 10</ref>, we provide additional visualization results to show the qualitative improvement over the baseline in S3DIS Area 5. We also visualize more scenes in the validation set of Semantic3D in <ref type="figure" target="#fig_0">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Detailed Experimental Results</head><p>In this section, we provide more quantitative details about our experimental results for better comparison with other competitors. In <ref type="table" target="#tab_9">Table 8</ref>, we present the mean IoU (mIoU) over categories and the IoUs for different classes for ScanNet v2. We also list the category scores for S3DIS Area-5 in <ref type="table" target="#tab_10">Table 9</ref>. It's noteworthy that all the methods do not have good performance on the segmentation of beams</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth KPConv deform (Baseline) Ours <ref type="figure" target="#fig_0">Figure 11</ref>: More visualization results on the validation dataset of Semantic3D. Input point clouds, semantic labels, results of our baseline and our method are presented respectively from left to right.</p><p>in Area 5 because there is a large difference between the beams in Area 5 (test set) and those in Area 1, 2, 3, 4, and 6 (training set). Finally, <ref type="table" target="#tab_11">Table 10</ref> shows the IoUs of various classes for Semantic3D reduced-8 task.</p><p>SceneEncoder (IJCAI'20) <ref type="bibr" target="#b36">[37]</ref> 62.8 -    </p><formula xml:id="formula_14">- - - - - - - - - - - - - - - - - - -</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of Receptive Field Component Reasoning for a point cloud in ScanNet v2 from top to bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of gradual Receptive Field Component Reasoning. (a) shows the target Receptive Field Component Codes (RFCCs) is generated alongside the common encoding procedure. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Semantic3D. Semantic3D [ 7 ]</head><label>7</label><figDesc>is a large-scale outdoor point cloud dataset with online benchmark. It contains more than 4 billion points from diverse urban scenes, and all the points are classified into 8 categories. The whole dataset includes 15 point clouds for training and another 15 point clouds for testing. For easy evaluation, Semantic3D provides the task of Semantic3D reduced-8, where 15 largescale point clouds are used for training and 4 down-sampled point clouds are used for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualizations on validation set of Semantic3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature Densification.Figure 6 :</head><label>6</label><figDesc>As stated in Sec 3.4, active features will be densified by centrifugal potential given the loss in Eq. (12). The distribution of features' magnitude after training can be visualized by the bar chart shown in Figure 6. As indicated in this figure, features are pushed away from 0 and more unambiguous features are available for the Receptive Field Component Reasoning, thus improving the segmentation performance (Table 4). 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2Visualization of features' magnitude in the decoding layers. The green chart bars represent the distribution of features' absolute value after adding Feature Densification while the red chart bars represent the distribution of features' absolute value in the original network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>We visualize the RFCC reasoning process and our predicted RFCCs in intermediate layers to implicitly show the intermediate feature learning inFigure 8. Meanwhile, the OA of RFCC prediction is 97.34% on the validation set of ScanNet v2, demonstrating good representation learning of intermediate features to some extent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of framework using deformable KP-Conv as the backbone. In our method, all the five scales are supervised by the target RFCCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of indoor scene semantic segmentation on S3DIS Area-5. art competitors. When taking deformable KPConv as our backbone, our method improves it by 4.7%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of outdoor space semantic segmentation on Semantic3D (reduced-8).</figDesc><table><row><cell>Input</cell><cell>Ground Truth</cell><cell>KPConv deform (Baseline)</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on impact of different parts of gradual Receptive Field Component Reasoning.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell>KPConv deform</cell><cell>73.1</cell></row><row><cell>KPConv deform + OvU + FD</cell><cell>76.2</cell></row><row><cell cols="2">KPConv deform + RFCR[one-hot] + FD 76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>SceneEncoder + Ours 65.9 69.1 72.4 69.6 63.2 81.5 47.7 75.4 64.6 50.9 95.2 42.8 28.4 56.6 76.1 62.6 71.1 61.0 88.9 79.3 61.0 KPConv deform (ICCV'19) [31] 68.4 84.7 75.8 78.4 64.7 81.4 47.3 77.2 60.5 59.4 93.5 45.0 18.1 58.7 80.5 69.0 78.5 61.4 88.2 81.9 63.2 KPConv deform + Ours 70.2 88.9 74.5 81.3 67.2 81.8 49.3 81.5 62.3 61.0 94.7 47.0 24.9 59.4 84.8 70.5 77.9 64.6 89.2 82.3 61.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Semantic segmentation results on ScanNet v2.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>ceil.</cell><cell>floor</cell><cell>wall beam</cell><cell>col.</cell></row></table><note>wind. door chair table book. sofa board clut.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Results of indoor scene semantic segmentation on S3DIS Area-5.</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU man-made.</cell><cell>natural.</cell><cell>high veg.</cell><cell>low veg.</cell><cell cols="3">buildings hard scape scanning.</cell><cell>cars</cell></row><row><cell>SegCloud (3DV'17) [29]</cell><cell>61.3</cell><cell>83.9</cell><cell>66.0</cell><cell>86.0</cell><cell>40.5</cell><cell>91.1</cell><cell>30.9</cell><cell>27.5</cell><cell>64.3</cell></row><row><cell>RF MSSF (3DV'18) [30]</cell><cell>62.7</cell><cell>87.6</cell><cell>80.3</cell><cell>81.8</cell><cell>36.4</cell><cell>92.2</cell><cell>24.1</cell><cell>42.6</cell><cell>56.6</cell></row><row><cell>SPG (CVPR'18) [17]</cell><cell>73.2</cell><cell>97.4</cell><cell>92.6</cell><cell>87.9</cell><cell>44.0</cell><cell>93.2</cell><cell>31.0</cell><cell>63.5</cell><cell>76.2</cell></row><row><cell>ShellNet (ICCV'19) [42]</cell><cell>69.4</cell><cell>96.3</cell><cell>90.4</cell><cell>83.9</cell><cell>41.0</cell><cell>94.2</cell><cell>34.7</cell><cell>43.9</cell><cell>70.2</cell></row><row><cell>GACNet (CVPR'19) [32]</cell><cell>70.8</cell><cell>86.4</cell><cell>77.7</cell><cell>88.5</cell><cell>60.6</cell><cell>94.2</cell><cell>37.3</cell><cell>43.5</cell><cell>77.8</cell></row><row><cell>FGCN (CVPR'20) [15]</cell><cell>62.4</cell><cell>90.3</cell><cell>65.2</cell><cell>86.2</cell><cell>38.7</cell><cell>90.1</cell><cell>31.6</cell><cell>28.8</cell><cell>68.2</cell></row><row><cell>PointGCR (WACV'20) [23]</cell><cell>69.5</cell><cell>93.8</cell><cell>80.0</cell><cell>64.4</cell><cell>66.4</cell><cell>93.2</cell><cell>39.2</cell><cell>34.3</cell><cell>85.3</cell></row><row><cell>RandLA (CVPR'20) [9]</cell><cell>77.4</cell><cell>95.6</cell><cell>91.4</cell><cell>86.6</cell><cell>51.5</cell><cell>95.7</cell><cell>51.5</cell><cell>69.8</cell><cell>76.8</cell></row><row><cell>KPConv rigid (ICCV'19) [31]</cell><cell>74.6</cell><cell>90.9</cell><cell>82.2</cell><cell>84.2</cell><cell>47.9</cell><cell>94.9</cell><cell>40.0</cell><cell>77.3</cell><cell>79.7</cell></row><row><cell>KPConv deform + Ours</cell><cell>77.6</cell><cell>97.0</cell><cell>90.9</cell><cell>86.7</cell><cell>50.8</cell><cell>94.5</cell><cell>37.3</cell><cell>79.7</cell><cell>84.1</cell></row><row><cell>KPConv deform (ICCV'19) [31]</cell><cell>73.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KPConv deform + Ours</cell><cell>77.8</cell><cell>94.2</cell><cell>89.1</cell><cell>85.7</cell><cell>54.4</cell><cell>95.0</cell><cell>43.8</cell><cell>76.2</cell><cell>83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Semantic segmentation results on Semantic3D (reduced-8).</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint 3d-multiview prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boundary-aware geometric encoding for semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03847</idno>
		<title level="m">Semantic3d. net: A new large-scale point cloud classification benchmark</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Point2node: Correlation learning of dynamic-node for point cloud feature modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fourth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texturenet: Consistent local parametrizations for learning from highresolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fgcn: Deep feature-based graph convolutional network for semantic segmentation of urban 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Saqib Ali Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointrend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spherical kernel for efficient graph convolution on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fpconv: Learning local flattening for point convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global context reasoning for semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjian</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dualconvmesh-net: Joint geodesic and euclidean convolutions on 3d meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic classification of 3d point clouds with multiscale spherical neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Legall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting local and global structure for point cloud semantic segmentation with contextual point representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sceneencoder: Scene-aware semantic segmentation of point clouds with a learnable scene descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 29th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fusion-aware point convolution for online semantic 3d scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jsnet: Joint instance and semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12951" to="12958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Region mutual information loss for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Method mIoU bath. bed bksf. cab. chair ctr. curt. desk door floor oth. pic. ref. shw. sink sofa tab. toil</title>
		<imprint/>
	</monogr>
	<note>wall win</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seggcn</surname></persName>
		</author>
		<idno>CVPR&apos;20) [19] 58.9 83.3 73.1 53.9 51.4 78.9 44.8 46.7 57.3 48.4 93.6 39.6 6.1 50.1 50.7 59.4 70.0 56.3 87.4 77.1 49.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fusionawareconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fpconv</surname></persName>
		</author>
		<idno>CVPR&apos;20) [22] 63.9 78.5 76.0 71.3 60.3 79.8 39.2 53.4 60.3 52.4 94.8 45.7 25.0 53.8 72.3 59.8 69.6 61.4 87.2 79.9 56.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fusionnet</surname></persName>
		</author>
		<idno>ECCV&apos;20) [40] 68.8 70.4 74.1 75.4 65.6 82.9 50.1 74.1 60.9 54.8 95.0 52.2 37.1 63.3 75.6 71.5 77.1 62.3 86.1 81.4 65.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rsnet</surname></persName>
		</author>
		<idno>CVPR&apos;18) [12] 51.93 93.34 98.36 79.18 0.00 15.75 45.37 50.10 65.52 67.87 22.45 52.45 41.02 43.64</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seggcn</surname></persName>
		</author>
		<idno>CVPR&apos;20</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fusionnet</surname></persName>
		</author>
		<idno>ECCV&apos;20) [40] 67</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randla</surname></persName>
		</author>
		<idno>CVPR&apos;20) [9] 62.42 91.19 95.66 80.11 0.00 25.24 62.27 47.36 75.78 83.17 60.82 70.82 65.15 53.95</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">KPConv deform</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
