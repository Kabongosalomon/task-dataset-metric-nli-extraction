<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-Based Person Search with Limited Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
							<email>xiao.han@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">CVSSP University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>He</surname></persName>
							<email>sen.he@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">CVSSP University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<email>lizhangfd@fudan.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">CVSSP University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Text-Based Person Search with Limited Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>HAN ET AL.: TBPS WITH LIMITED DATA 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-based person search (TBPS) aims at retrieving a target person from an image gallery with a descriptive text query. Solving such a fine-grained cross-modal retrieval task is challenging, which is further hampered by the lack of large-scale datasets. In this paper, we present a framework with two novel components to handle the problems brought by limited data. Firstly, to fully utilize the existing small-scale benchmarking datasets for more discriminative feature learning, we introduce a cross-modal momentum contrastive learning framework to enrich the training data for a given mini-batch. Secondly, we propose to transfer knowledge learned from existing coarse-grained largescale datasets containing image-text pairs from drastically different problem domains to compensate for the lack of TBPS training data. A transfer learning method is designed so that useful information can be transferred despite the large domain gap. Armed with these components, our method achieves new state of the art on the CUHK-PEDES dataset with significant improvements over the prior art in terms of Rank-1 and mAP. Our code is available at https://github.com/BrandonHanx/TextReID.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-based person search (TBPS) <ref type="bibr" target="#b21">[21]</ref> is the problem of retrieving a target person from an image gallery with a descriptive text query. It is more flexible compared to image-based person search when the query image is difficult to obtain. It has thus gained increasing attention in the research community <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>. TBPS has various potential applications such as video surveillance and personal photo album search.</p><p>Despite the existing efforts, TBPS is still far from being solved. One of the reasons is that it is intrinsically challenging as a fine-grained cross-modal retrieval task, where all images belong to the same category, i.e., pedestrian. This contrasts with the more widely studied generic image-text retrieval task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>. The fine-grained nature dictates that more discriminative features must be learned to distinguish visual cues and textual attributes. This is thus the focus of existing TBPS methods. Specifically, prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32]</ref> typically use a two-stream architecture for fast inference, where both streams are initialized from backbones pre-trained on large-scale unimodal data, e.g., ResNet <ref type="bibr" target="#b15">[15]</ref> and BERT <ref type="bibr" target="#b8">[9]</ref>. For the purpose of learning more discriminative features, many methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43]</ref> also take advantage of multi-scale learning, where feature maps with different receptive fields and word/phrase/sentence embeddings are used for the visual and textual stream, respectively.</p><p>TBPS is also faced with a second challenge which has been largely ignored, that is, the lack of training data. Collecting a large-scale TBPS dataset and annotating it with highly fine-grained text descriptions is tedious and expensive. As a result, most existing TBPS dataset is orders of magnitude smaller than those coarse-grained generic image-text pair datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">22]</ref>. Having only limited data has clear negative effects on a TBPS model's ability to learn discriminative cross-modal features for fine-grained retrieval.</p><p>Existing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32]</ref> are ill-equipped to address this limited data problem. More specifically, most of their learning objectives require the training data to be organized into positive and negative pairs. However, previous works construct negative pairs merely from a certain mini-batch, which does not make full use of the available TBPS data. Pretraining on larger image-text pair datasets is also an obvious option to compensate for the lack of training data. Nevertheless, the visual and textual streams in previous works are initialized from models that are separately pre-trained on unimodal data rather than imagetext pairs. Information useful for cross-domain matching is thus not exploited. Cross-modal pre-training may have been attempted. However, as shown in this work, without a careful design, a naive pre-training then fine-tuning strategy would lead to negative transfer.</p><p>To overcome the learning with limited data problem, in this work, we propose a framework with two novel components for TBPS. Firstly, to fully utilize the existing small-scale benchmarking datasets for more discriminative feature learning, we introduce a cross-modal momentum contrastive learning (or CM-MoCo) framework to enrich the training data for a given mini-batch. CM-MoCo decouples the number of negative pairs with the minibatch size to obtain more negative cross-modal counterparts for each image or description. To implement such a framework, in addition to the two gradient-updated encoders (query encoders), we introduce another two momentum-updated encoders (key encoders) for two modalities and maintain three different queues to store visual features, textual features, and identities from previous batches. Further, a contrastive loss is formulated in a cross-modal manner, which treats the features from query encoders, key encoders and queues as anchors, positive samples and negative samples, respectively. Secondly, a cross-modal transfer learning method is proposed to benefit from large-scale coarse-grained image-text pair datasets. Instead of the commonly used pre-training + fine-tuning strategy, we propose to freeze the text encoder of the pre-trained model to embed each word and then adopt one bidirectional GRU layer (Bi-GRU) <ref type="bibr" target="#b6">[7]</ref> to contextualize words. Empirically, this transfer learning strategy can effectively prevent the negative transfer suffered by the naive full model transfer strategy (See <ref type="table" target="#tab_2">Table 2</ref> for experimental results).</p><p>The main contributions of this paper are: (1) A novel cross-modal momentum contrastive learning framework is proposed to better utilize the existing small-scale TBPS datasets. <ref type="bibr" target="#b1">(2)</ref> To effectively transfer the knowledge learned from large-scale generic image-text pairs, we propose to perform cross-modal pre-training, but for the text modality, only word embedding is transferred. (3) Extensive experiments are conducted to show that our proposed framework outperforms existing methods on CUHK-PEDES <ref type="bibr" target="#b21">[21]</ref> by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Text-based person search</head><p>Li et al. <ref type="bibr" target="#b21">[21]</ref> first propose TBPS with a challenging dataset CUHK-PEDES and a baseline built upon a recurrent neural network with gated neural attention. Following this, many single-scale methods are proposed for better investigating the intra-and inter-modal finegrained differences with the help of instance loss <ref type="bibr" target="#b44">[44]</ref>, cross-modal projection loss <ref type="bibr" target="#b42">[42]</ref>, adversarial loss <ref type="bibr" target="#b32">[32]</ref> and cross-modal knowledge adaption <ref type="bibr" target="#b4">[5]</ref>.</p><p>Besides, some multi-scale methods are proposed to learn the semantic relevance between specific image regions and phrases/words in descriptions. Many works implement such architecture by making local image features attend to corresponding noun phrases and words through a variety of attention mechanisms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43]</ref>. Additionally, some works adopt side information to help align two modalities, e.g., pose information <ref type="bibr" target="#b18">[18]</ref>, semantic segmentation maps <ref type="bibr" target="#b39">[39]</ref> and attribute labels <ref type="bibr" target="#b0">[1]</ref>. Most of these multi-scale architectures merely use global features during inference, because calculating the similarity between local features increases both the inference time and the offline features storage space.</p><p>For fast inference speed and less memory consumption, neither multi-scale architecture nor side information is involved in our method. Instead of manually designing more complicated network architectures or collecting more side information, from a more general perspective, we focus on a much more practical and under-studied problem in TBPS, i.e., the scarcity of data. Being orthogonal to all previous methods, our method can be easily extended or integrated. Many solid experiments show our method is comparable and even better with other more complex methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive learning</head><p>The basic idea of contrastive learning is to map the original data into a latent feature space where the similarities between positive/negative pairs are maximized/minimized <ref type="bibr" target="#b14">[14]</ref>. The instance discrimination is the most prevalent pretext task, whose positive pairs consist of two augmented views of the same instance, and the other pairs are defined to be negative. MoCo <ref type="bibr" target="#b16">[16]</ref> and SimCLR <ref type="bibr" target="#b1">[2]</ref> suggest that large quantities of data pairs are crucial to the performance of contrastive learning. Most recently, BYOL <ref type="bibr" target="#b12">[13]</ref> and SimSiam <ref type="bibr" target="#b2">[3]</ref> prove that negative pairs are unnecessary and the invariant observation of the same concept matters.</p><p>In this work, to fully exploit the available annotated dataset, we apply momentum contrastive learning to TBPS. There are two differences between our work and the classic instance discrimination contrastive learning framework: (1) Our task is identity-level rather than instance-level, because each identity has more than one image and description in the dataset. (2) In our task, the similarity is measured in a cross-modal manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Vision-language pre-training</head><p>With the advent of Transformer <ref type="bibr" target="#b37">[37]</ref> and BERT <ref type="bibr" target="#b8">[9]</ref>, there has been a surging interest in applying self-supervised learning to multimodal tasks. This is usually done by pre-training on large-scale generic image-text pairs and then fine-tuning on downstream tasks. ViLBERT <ref type="bibr" target="#b25">[25]</ref> and LXMERT <ref type="bibr" target="#b36">[36]</ref> introduce the two-stream architecture, where two Transformers are applied to images and text independently followed by another Transformer for cross-modal fusion. In addition to that, many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33]</ref> adopt the single-stream architecture and achieve much better performance. In such an architecture, a single Transformer is applied to both images and text. Although single stream models have achieved great success, its crucial component, crossmodal attention between two modalities, triggers the inevitable latency and significant computation during training and inference. To tackle this problem, CLIP <ref type="bibr" target="#b30">[30]</ref> and BriVL <ref type="bibr" target="#b17">[17]</ref> utilise larger datasets, larger batch size, and contrastive learning on the basis of two-stream architecture. LightningDOT <ref type="bibr" target="#b34">[34]</ref> adopts a faster two-stream model as the main inference model and another stronger single-stream model as the re-ranker, achieving a satisfactory balance between accuracy and efficiency.</p><p>In this work, we incorporate pre-trained two-stream models into TBPS. We study how to effectively transfer the knowledge pre-trained on large-scale coarse-grained image-text pairs for fine-grained TBPS in spite of the big domain gap between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Given a text query t, the goal of TBPS is to retrieve an image v that best matches the content in t from a gallery. The retrieval is successful if t and v share the same identity.</p><p>Our proposed framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of two query encoders f V q and f T q (Visual and Textual Q-Encoder) along with two key encoders f V k , f T k (Visual and Textual K-Encoder), parameterized by ? V q , ? T q , ? V k , ? T k , respectively. During training, both the query and key encoders are used to process the input from its own modality. The outputs of the key encoders are pushed into queues which are used to construct negative pairs for contrastive learning. During inference, only two query encoders are used for feature extraction. The retrieval is done by first computing the cosine similarity between the query feature and the offline extracted features of all candidates in the gallery, and then selecting the candidate that has the highest similarity score.</p><p>In the following sections, we will first introduce our proposed cross-modal momentum contrastive learning pipeline (CM-MoCo) in Section 3.1, and then explain how we effectively transfer the knowledge learned from large-scale generic image-text pairs in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning from limited TBPS data</head><p>Cross-modal momentum contrastive learning. One of the limitations to learn more discriminative features in previous work is caused by the limited negative pairs during the training stage. Note that MoCo <ref type="bibr" target="#b16">[16]</ref> provides a mechanism of building dynamic queues decoupled with batch size, which makes it possible to learn from more negative samples beyond a certain batch. Inspired by this, we propose cross-modal momentum contrastive learning to make the best use of current TBPS data.</p><p>Concretely, given a batch of person images V = {v 1 , ? ? ? , v B }, a batch of descriptions T = {t 1 , ? ? ? ,t B } and their identities ID = {id 1 , ? ? ? , id B }, we feed V and T into their corresponding query encoder and key encoder to obtain their normalized features:</p><formula xml:id="formula_0">V q = f V q (V ), V k = f V k (V ), T q = f T q (T ), T k = f T k (T ),<label>(1)</label></formula><p>where V q , V k , T q , and T k ? R B?D . V k , T k and ID will be pushed into three queues, i.e., visual queue (Q V ), textual queue (Q T ) and identity queue (Q ID ) for negative pair construction. To learn discriminative cross-modal features, for each image query v i , we define the cross-modal contrastive loss among its query feature V q i (as anchor), its corresponding textual key feature T k i (as positive key) and the keys stored in textual queue Q T (as negative keys), where Q T = {n|n ? Q T ? Q ID (n) / ? ID} indicating its identities are not in the current batch. In the meanwhile, we also apply the cross-modal contrastive learning in a symmetrical way when regarding each description t i as a query. The overall cross-modal contrastive loss L cmc is computed as Equation 2, where ? c denotes the tuneable temperature.</p><formula xml:id="formula_1">L cmc = ? B ? i=1 log ? ? ? ? ? e (V q i T k i /? c) e (V q i T k i /? c) + ? n? Q T e (V q i n/? c) ? ? ? ? ? ? B ? i=1 log ? ? ? ? ? e (T q i V k i /? c) e (T q i V k i /? c) + ? n? Q V e (T q i n/? c) ? ? ? ? ? .</formula><p>(2) After calculating the cross-modal contrastive loss, two query encoders are updated by the back propagation gradients. Following MoCo <ref type="bibr" target="#b16">[16]</ref>, the parameters of two key encoders, ? V k and ? T k , are updated by the rule given in <ref type="figure" target="#fig_2">Equation 3</ref>, where m is a momentum parameter.</p><formula xml:id="formula_2">? V k = m ? ? V k + (1 ? m) ? ? V q , ? T k = m ? ? T k + (1 ? m) ? ? T q .<label>(3)</label></formula><p>With our proposed L cmc , each query sample is compared with a large number of negative key samples. It thus allows the model to learn more discriminative features for TBPS. To better understand CM-MoCo, please refer to the pseudocode in the supplementary material. Joint training. Following previous works, we also incorporate widely used alignment loss L align <ref type="bibr" target="#b39">[39]</ref> and identity loss L id <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b44">44]</ref> into our end-to-end training pipeline (See details in the supplementary material). The overall loss L is the summation of the three losses:</p><formula xml:id="formula_3">L = L cmc + L align + L id .<label>(4)</label></formula><p>Post-processing. In the inference stage, we adopt the cross-modal k-reciprocal rerank algorithm <ref type="bibr" target="#b11">[12]</ref> to further improve the performance. The pair-wise rerank similarity is calculated by Jaccard Distance of k-nearest unimodal neighbors and k-nearest cross-modal neighbors, and then added to the original cosine similarity (See details in the supplementary material).  <ref type="figure">Figure 2</ref>: Illustrative diagram of our textual stream. We first tokenize a sentence into words, and then independently feed these words into a pre-trained frozen text backbone (e.g., CLIP text encoder) to get word-type embeddings. On top of that, a Bi-GRU with a max pooling layer is used to contextualize all word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transferring knowledge from generic image-text pairs</head><p>A conventional way for TBPS is to initialize our visual and textual encoders with backbones separately pre-trained on unimodal data, e.g., ResNet50 <ref type="bibr" target="#b15">[15]</ref> pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> and BERT <ref type="bibr" target="#b33">[33]</ref> pre-trained on large corpora. However, this initialization brings a significant heterogeneous gap which is difficult to be bridged with the current limited data of TBPS. To tackle this issue, a straightforward way is to initialize our encoders pre-trained on large-scale generic image-text pairs, e.g., MSCOCO captions <ref type="bibr" target="#b23">[23]</ref>, Flickr30k <ref type="bibr" target="#b29">[29]</ref> and WIT <ref type="bibr" target="#b30">[30]</ref>, and then fine-tune the whole model for TBPS. Unexpectedly, we empirically find this intuitive transfer strategy yields poor performance. This negative transfer is likely to be caused by the domain gap between the TBPS domain and that of the generic datasets. Such domain gap especially exists in the textual side, even under the unimodal pre-training scenario. Due to the highly fine-grained specialty of TBPS, the description sentences in TBPS are much longer than those in the generic data, and every word matters. However, text backbones pre-trained on generic data are likely to get more coarse-grained information and neglect some detailed words, thus negative transfer happens (See more discussions in the supplementary material). Nonetheless, we believe pre-training on generic data can still offer us more meaningful embeddings for each word because of largr-scale contrastive learning.</p><p>To address this problem, we propose a transfer learning strategy with three alterations for the text stream while leaving the visual stream unchanged. Concretely, as illustrated in <ref type="figure">Figure 2</ref>, we take CLIP Text Encoder (CLIP-TE) <ref type="bibr" target="#b30">[30]</ref> pre-trained on WIT as an example to demonstrate our alterations. Firstly, one of our alterations is feeding the whole sentence into CLIP-TE in a word-by-word manner to obtain word-type embeddings. For each word, its word-type embedding is represented by the [EOS] token from the last layer of CLIP-TE. Secondly, CLIP-TE is frozen in the whole training stage. In the actual implementation, an offline dictionary storing all word embeddings is computed in advance, and the frozen CLIP-TE is removed from the training process. Thirdly, to compensate the lack of sentence-level semantic information, we append a Bi-GRU <ref type="bibr" target="#b6">[7]</ref> followed by max pooling to contextualize all word-type embeddings in a sentence. Empirically, as shown in <ref type="table" target="#tab_2">Table 2</ref>, the model with our proposed transfer learning strategy yields a significant performance boost. This strategy allows the textual stream to effectively transfer the knowledge learned from large-scale generic image-text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Dataset. We conduct experiments on the CUHK-PEDES dataset <ref type="bibr" target="#b21">[21]</ref>, which is currently the only benchmark for TBPS. It contains 40,206 images of 13,003 different people, where each image has two descriptive sentences annotated by different people. As per standard, the dataset is split into 11,003 identities with 34,054 images in the training set, 1,000 identities with 3,074 images in the test set, and the remaining for the validation set. The average length of all sentences is 23 and the vocabulary size is 9408. Implementation details. During training, we use random horizontally flipping, random crop with padding, and random erasing <ref type="bibr" target="#b26">[26]</ref> as image data augmentation methods. Following previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43]</ref>, the stride of the last block in the ResNet are set to 1 to increase the resolution of the final feature map. The feature dimension D for both modalities is set to 256. All images are resized to 384 ? 128. Our model is trained with Adam optimizer <ref type="bibr" target="#b20">[20]</ref> for 80 epochs with an initial learning rate 1 ? 10 ?4 , which is decayed by a factor 0.1 at the 40 th epoch and 70 th epoch, respectively. At the beginning, we spend 5 warm-up epochs linearly increasing the learning rate from 1 ? 10 ?5 to 1 ? 10 ?4 . Each mini-batch has 128 image-text pairs with 4 images/sentences for each identity. ? c in L cmc is set to 0.07 and the momentum m is set to 0.999. Following NAFS <ref type="bibr" target="#b11">[12]</ref>, the number of nearest neighbors k used in rerank is set to 5. All experiments are conducted on one V100 GPU with Pytorch <ref type="bibr" target="#b28">[28]</ref>. Evaluation protocol. As per standard, we evaluate our model in a bi-directional manner, where the performance is measured by Rank-K (K=1, 5, 10). Specifically, given a text/image query, Rank-K reports the percentage of successful searches among all searches, where each successful search retrieves at least one corresponding person correctly among the top K results. In addition, for a comprehensive evaluation, we also adopt the mean Average Precision (mAP) of all queries <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43]</ref> as another retrieval criterion. Empirically, Rank-K can reflect models' accuracy on the first few retrieval results while mAP puts more emphasis on the order of the entire retrieval sequence predicted by the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>We compare our method with most published works on TBPS. We integrate the proposed CM-MoCo and transfer the knowledge from generic data to finalize our model. For a comprehensive comparison, we instantiate our method with both CLIP ResNet50 and CLIP ResNet101. Please notice that we cannot reach 100% fair comparisons with all other methods, because the details of the implementations vary. For example, in the pursuit of a more general and efficient structure, being designed in a single-scale architecture with only 256 feature dimensions leads our method into a natural inferior position.</p><p>As shown in <ref type="table">Table 1</ref>, our models' performances are comparable and even better with other complicated methods in terms of all metrics no matter whether rerank post-processing is used. Specifically, for the most important metric Rank-1, our model (ResNet101) gains approximately 9%/1% absolute improvement over the previous single-/multi-scale state-ofthe-art method. For the image to text retrieval, our method outperforms all others by a large margin, indicating our method's superiority in aligning two modalities. The highest mAP also demonstrates the entire retrieval sequence predicted by our models has a top-quality order. In addition to the satisfactory performance, our method also has the merits over the training efficiency, retrieval speed and offline features storage against previous state-of-theart methods (See detailed comparisons in the supplementary material).  <ref type="table">Table 1</ref>: Comparisons with previous methods on the CUHK-PEDES. Only global features are used during inference for our reproduced NAFS <ref type="bibr" target="#b11">[12]</ref>. "Arch."/"Dim." is the abbreviation for architecture/feature dimension. S/M stands for the methods designed in single-/multiscale architecture, and all single-scale methods are highlighted with gray background. ? stands for the results from HGAN <ref type="bibr" target="#b43">[43]</ref>. ? stands for the results reproduced with public codes/checkpoints released by their authors. Overall 1 st /2 nd best in red/blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>We evaluate the effectiveness of our framework by introducing our proposed CM-MoCo in Section 3.1 and transfer learning from cross-modal pre-training in Section 3.2. We adopt a baseline model trained without our proposed L cmc and transfer learning, where both visual and textual backbones are initialized with the models separately pre-trained on unimodal data, i.e., ResNet101 and BERT. To illustrate the power of cross-modal pre-training with generic image-text pairs in TBPS, in addition to adopting CLIP pre-trained on the unreleased huge-scale WIT <ref type="bibr" target="#b30">[30]</ref> (about 400 million pairs), we pre-train another model on a relative smaller dataset, MSCOCO captions <ref type="bibr" target="#b23">[23]</ref> (about 0.57 million pairs), to reach a comprehensive comparison (See more details for pre-training settings in the supplementary material). We can draw the following conclusions from the <ref type="table" target="#tab_2">Table 2</ref>: (1) Without our transfer strategy, no matter whether and how many generic image-text pairs are used for pre-training, directly fine-tuning the pre-trained model results in a negative transfer. This transfer strategy can effectively alleviate the domain gap coming from the textual side. More specifically, it leads to at least 6.66% performance improvement. (2) When the domain gap is well resolved, cross-modal pre-training yields better performance than unimodal pre-training. This is even more significant when the image-text pairs dataset for pre-training is scaled up, i.e., from MSCOCO to WIT. (3) The proposed CM-MoCo yields consistent improvement for all models. It further boosts the performance with 1.5% improvement in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Backbone Textual Backbone</head><p>Paired Data Transfer Strategy CM-MoCo Rank-1 Rank-5 Rank-10    <ref type="table">Table 4</ref>: Results for different queue sizes.</p><p>Transfer learning strategy. In this part, we validate the design of our transfer learning strategy proposed in Section 3.2. There are three proposed alterations, i.e., GRU, fixed textual encoder and non-contextualized word embedding. <ref type="table" target="#tab_3">Table 3</ref> demonstrates the effectiveness of each alteration. It is clear that appending a GRU to the pre-trained textual encoder directly allows the textual stream to learn fine-grained textual information on the basis of the previously learned coarse-grained one. Further, fixed textual encoder and non-contextualized word embedding boost the performance with about 4% and 1%, respectively. This confirms our conjecture that large-scale cross-modal pre-training can provide more meaningful word embeddings and image features, while the domain gap problem is needed to be addressed carefully. In summary, this strategy allows us to effectively transfer the knowledge from cross-modal pre-training. Queue sizes. We also evaluate the effect of the queue sizes that is used to store keys and construct negative pairs for CM-MoCo. From the results reported in <ref type="table">Table 4</ref>, it is obvious that a large queue size (1024 and 2048) in CM-MoCo improves the performance. However, further increasing this queue size yields worse performance. This phenomenon is probably due to the limited data in the TBPS dataset. A queue size which is too large for the entire dataset may store too many obsolete keys, which will mislead the learning direction. It is therefore detrimental for further cross-modal contrastive learning.  <ref type="figure" target="#fig_2">Figure 3</ref>(a) is a retrieval result with a detailed text query, our method successfully retrieves the target image with a high similarity score. <ref type="figure" target="#fig_2">Figure 3</ref>(b) and 3(c) show the results of two different text queries for the same target image. The second query describes the lady's coat and shoes as "black clothes" and "black shoes", while the third query is more fine-grained with "thigh-level black coat" and "tall black high heels" and thus gets a much better result. We can draw two conclusions from this <ref type="figure">figure:</ref> (1) Our method appears to be very effective in distinguishing fine-grained details in a given text query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative results</head><p>(2) When the text query has ambiguity, our method can still give reasonable results.</p><p>(3) The result sequence retrieved by our method conforms to human intuition. (c) The woman is wearing a thigh-level black coat and tall black high heels on her feet. <ref type="figure" target="#fig_2">Figure 3</ref>: Typical retrieval results. The image with red box is the correct matching. The number on top of each image represents the predicted similarity with given text query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel method for text-based person search (TBPS). Our model learns more discriminative features by using the proposed cross-modal momentum contrastive learning strategy, and effectively transfers the knowledge learned from generic imagetext pairs to compensate the data-scarce problem. This is demonstrated by the fact that our approach clearly outperforms prior art on CUHK-PEDES, often by a big margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Testing CLIP on person image classification</head><p>As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, CLIP cannot distinguish between fine-grained features when we test it on zero-shot fine-grained person image classification. In this toy experiment, each label consists of one color and one garment, e.g., "the color of her bag is orange". However, CLIP tends to predict almost all mentioned garments as orange, demonstrating that it cannot focus on fine-grained information well.  An intuitive explanation to this phenomenon is that CLIP is trained to distinguish different visual classes using text, which is limited for intra-class discrimination in TBPS. However, as it can distinguish different visual classes using a single word (representing the class label), it thus learns an informative cross-modal representation for each word. Therefore, we use the text encoder of CLIP to embed words in each sentence, and then append a Bi-GRU to contextualize them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training details B.1 Modified ResNet101 for CLIP image encoder</head><p>According to CLIP <ref type="bibr" target="#b30">[30]</ref>, this modified ResNet has three improvements over the vanilla version: (1) There are now 3 stem convolutions as opposed to 1 with an average pooling instead of max pooling. (2) It performs anti-aliasing strided convolutions, where an average pooling is prepended to convolutions with stride greater than 1. (3) The final pooling layer is a self-attention pooling instead of a global average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Pseudocode of CM-MoCo in Pytorch-style</head><p>To better demonstrate our proposed CM-MoCo, we provide a pseudocode in Pytorch-style as following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Alignment loss</head><p>We discard the widely used CMPM loss <ref type="bibr" target="#b42">[42]</ref> and utilize the logistic-based contrastive loss proposed in ViTAA <ref type="bibr" target="#b39">[39]</ref> as our cross-modal alignment loss. Particularly, for the visual side, given an image q-feature V q i and a batch of text q-features T q , the cross-modal cosine similarity S i is calculated by S i = V q i ? T T q , where S i ? R B and ? denotes matrix multiplication. For the textual side, the calculation is identical and implemented by multiplying the alignment loss by 2. The alignment loss is finally defined as following formula 5, where S + i /S ? i , ? p /? n and ?/? denotes the similarity, temperature and absolute margin for positive/negative pairs, respectively.</p><formula xml:id="formula_4">L align = 2 B B ? i=1 log 1 + e ?? p (S + i ??) + log 1 + e ? n( S ? i ?? ) .<label>(5)</label></formula><p>Our consideration on the alignment loss is two folds: (1) Unlike triplet loss only considers the relative distances or CMPM <ref type="bibr" target="#b42">[42]</ref> adopts KL divergence to associate the representations across different modalities in a batch, our alignment loss considers both relative and absolute distances between positive and negative pairs; (2) ? p and ? n can adjust the slope of the back propagation gradient according to 6, which will assign higher weights to more informative samples and then lower the risk of slow convergence or even model degeneration.</p><formula xml:id="formula_5">? L align ? S + i = ?? p 1 + e ? p( S + i ??) , ? L align ? S ? i = ? n 1 + e ? n( ? ?S ? i ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Identity loss</head><p>We also regard identity classification with N labels as an auxiliary task. Cross entropy loss 7 is adopted here to assist the learning of instance discriminative features. W ? R D?N denotes a shared projection matrix following visual and textual streams. Because person identities in the testing set do not appear in the training set, it is of importance to prevent the model from overfitting to the training identities. To this end, we replace the original one-hot label of each identity with a softer version by means of Label Smooth (LS) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b35">35]</ref> with the smooth factor ? = 0.1.</p><formula xml:id="formula_6">L id = 1 B B ? i=1 ? log ? ? e W id i V q i ? N j e W j V q i ? ? + 1 B B ? i=1 ? log ? ? e W id i T q i ? N j e W j T q i ? ? .<label>(7)</label></formula><p>B.5 Rerank post-processing</p><p>In the inference stage, only two q-encoders are used. We also incorporate the multimodal k-reciprocal rerank algorithm proposed in NAFS <ref type="bibr" target="#b11">[12]</ref> into our post-processing to further improve the performance. For the text-to-image task, the initial ranking list is obtained by sorting the cross-modal cosine similarity calculated by the text query t and each gallery image v. For each image v, the k-nearest neighboring images are obtained with the visual unimodal cosine similarity, denoted as N i2i (v, k). Similarly, the nearest image neighbors for the textual query N t2i (t, k) are obtained based on the cross-modal similarity. Finally, the pairwise rerank similarity D J (v,t) 8 is calculated by Jaccard Distance and added to the original cosine similarity with a weight of 0.05. For the image-to-text task, we extend this formula in a symmetrical manner to obtain D J (t, v).</p><formula xml:id="formula_7">D J (v,t) = 1 ? N i2i (v, k) N t2i (t, k) N i2i (v, k) N t2i (t, k) , D J (t, v) = 1 ? N t2t (t, k) N i2t (v, k) N t2t (t, k) N i2t (v, k) .<label>(8)</label></formula><p>C More evaluation results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 The model pre-trained on MSCOCO</head><p>There are many other available models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b36">36]</ref> pre-trained on large-scale generic image-text pairs <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b29">29]</ref>. However, we choose the experiment settings used in VSE++ <ref type="bibr" target="#b9">[10]</ref> to prepare our comparative experiments. Our consideration is two-fold: (1) VSE++ is designed in a two-stream manner, which guarantees a high inference speed for TBPS; <ref type="bibr" target="#b1">(2)</ref> No detection module, e.g., Faster-RCNN <ref type="bibr" target="#b31">[31]</ref>, is used in VSE++, leading to a more fair comparison. We change the triplet loss used in VES++ into our alignment loss and no hard example mining is used. The results of our model can be found in <ref type="table" target="#tab_7">Table 5</ref>.  C.2 Model size and retrieval efficiency <ref type="table">Table C</ref>.2 shows the comparisons of model size and retrieval efficiency between our method and the previous state of the art. In addition to the higher retrieval performance, our method also has three advantages: (1) Our architecture, no matter is built upon ResNet50 or 101, has much fewer parameters than those of other methods because of the single-scale architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>A smaller model size leads to less GPU memory usage and faster training speed. (2) Our method has the fastest retrieval time because only global features are used during retrieval. This advantage can guarantee real-time retrieval and thus is friendly to practical deployment.</p><p>(3) Our method has the least offline feature storage because we do not need to store local information and our features' embedding dimension (256) is quite smaller than that of NAFS (768) and TIPCB (2048). Small storage usage is crucial for practical cases with scaled-up data, otherwise it will increase the burden of the whole system and the cost of computing.  D More visualization results D.1 Visualization of self-attention pooling <ref type="figure" target="#fig_5">Figure 5</ref> visualizes the learned attention weight in the self-attention pooling layer of CLIP Image Encoder (ResNet101 version). We can conclude that the visual stream is capable of learning the salient parts related to the garments of a person rather than the background. This visualization further verifies that the model has the ability to learn reasonable features even without the help of multi-scale information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 More visualized retrieval results</head><p>We visualize several typical successful and failure cases of our retrieval results in <ref type="figure">Figure 6</ref> and 7, respectively. It is apparent that this failure cases are due to the ambiguity in the images or the pragmatic vagueness in the sentences. The predictions of our model are reasonable, and the more specific the search sentence is, the better our search results will be.</p><p>This person if facing the other way. He is wearing a black tee shirt and also black shorts.</p><p>The man is wearing a light blue polo shirt with two navy stripes, black slacks, and black shoes.</p><p>The man is seen from behind. He is wearing a white shirt and black pants.</p><p>The man has short, brown hair. He is wearing a dark colored jacket and dark pants.</p><p>A girl wearing a pink shirt, a pair of blue pants and a pair of pink and white shoes.</p><p>The man has short black hair, a black tshirt, camouflage knee length shorts and flip flops and is walking through the grass. <ref type="figure">Figure 7</ref>: Typical failure cases of retrieval results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrative diagram of our proposed architecture. The two Q-Encoders are gradient-updated while two K-Encoders are initialized from Q-Encoders and momentumupdated. The cross-modal contrastive (CMC) loss, alignment loss and identity loss are employed during training. The whole model is trained in an end-to-end manner and only two Q-Encoders are used for inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>visualizes some typical retrieval results (See more successful results and failure cases in the supplementary material). Concretely,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the probabilities predicted by CLIP<ref type="bibr" target="#b30">[30]</ref> for fine-grained zero-shot person image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#</head><label></label><figDesc>f_v_q, f_v_k: encoder networks for visual query and key # f_t_q, f_t_k: encoder networks for textual query and key # queue_t, queue_v, queue_id: queues to store K keys # m: momentum (0.999) # t: temperature (0.07)# ----------------------------------------------------------#bmm: batch matrix multiplication # mm: matrix multiplication # cat: concatenation # complement: get complement set f_v_k.params, f_t_k.params = f_v_q.params, f_t_q.params # initialize for v, t, pid in loader: # load a batch data with B samples v_q = f_v_q.forward(v) # visual queries: BxD t_q = f_t_q.forward(t) # textual queries: BxD v_k = f_v_k.forward(v) # visual keys: BxD t_k = f_t_k.forward(t) # textual keys: BxD # stop gradients for keys v_k, t_k = v_k.detach(), t_k.detach() # positive logits: Bx1 v_pos = bmm(v_q.view(B, 1, D), t_k.view(B, D, 1)) t_pos = bmm(t_q.view(B, 1, D), v_k.view(B, D, 1)) # get P indexes of the positive instances in the queue, # whose identity exist in the current batch pos_idx = queue_id.expand(B, K).eq(pid.unsqueeze(-1)).nonzero()[:, 1] neg_idx = arange(K).complement(pos_idx) # negative indexes: K-P # negative logits: Bx(K-P) v_neg = mm(v_q.view(B, D), queue_t.view(D, K))[:, neg_idx] t_neg = mm(t_q.view(B, D), queue_v.view(D, K))[:, neg_idx] # logits: Bx(1+K-P) logits_v = cat([v_pos, v_neg], dim=1) logits_t = cat([t_pos, t_neg], dim=1) # contrastive loss labels = zeros(B) # positives are the 0-th loss = CrossEntropyLoss(logits_v / t, labels) \ + CrossEntropyLoss(logits_t / t, labels) # gradient update loss.backward() # momentum update f_v_k.params = m * f_v_k.params + (1 -m) * f_v_k.params f_t_k.params = m * f_t_k.params + (1 -m) * f_t_k.params # update queues enqueue(queue_v, v_k) # enqueue the current batch enqueue(queue_t, t_k) enqueue(queue_id, pid) dequeue(queue_v) # dequeue the earliest batch dequeue(queue_t) dequeue(queue_id)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the last layer attention map calculated by [CLS] token and other patch tokens in the self-attention pooling layer of CLIP ResNet101. This figure contains attention maps, original images and images multiplied by resized attention map for four different identities randomly sampled from test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation experimental results for proposed components. Only the results without rerank for text-to-image task are reported. Paired data denotes the dataset used for pretraining. Transfer strategy denotes the proposed strategy mentioned in Section 3.2.</figDesc><table><row><cell cols="4">Embed. Type GRU Fixed Rank-1 Rank-5 Rank-10</cell></row><row><cell>Contextualized</cell><cell>0.15</cell><cell>0.76</cell><cell>1.23</cell></row><row><cell>Contextualized</cell><cell>57.23</cell><cell>75.39</cell><cell>82.39</cell></row><row><cell>Contextualized</cell><cell>61.39</cell><cell>79.73</cell><cell>86.84</cell></row><row><cell>Word-type</cell><cell>62.52</cell><cell>80.57</cell><cell>87.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results for different text encoders.</figDesc><table><row><cell cols="4">CM-MoCo Queue Size Rank-1 Rank-5 Rank-10</cell></row><row><cell>0</cell><cell>62.52</cell><cell>80.57</cell><cell>87.12</cell></row><row><cell>1024</cell><cell>63.52</cell><cell>81.77</cell><cell>88.52</cell></row><row><cell>2048</cell><cell>64.08</cell><cell>81.73</cell><cell>88.19</cell></row><row><cell>4096</cell><cell>63.06</cell><cell>81.50</cell><cell>87.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>He is wearing grey pants, a dark long sleeved sweater, and a light collared shirt underneath. He is carrying a black backpack on two shoulders. The woman with dark hair has all black clothes and shoes with a white handbag.</figDesc><table><row><cell>Ground Truth</cell><cell>0.6088</cell><cell>0.5909</cell><cell>0.5249</cell><cell>0.4679</cell><cell>0.4553</cell><cell>0.4412</cell><cell>0.4325</cell><cell>0.4230</cell><cell>0.4156</cell><cell>0.4113</cell></row><row><cell>(a) Ground Truth</cell><cell>0.6271</cell><cell>0.5404</cell><cell>0.5288</cell><cell>0.5264</cell><cell>0.4983</cell><cell>0.4979</cell><cell>0.4904</cell><cell>0.4857</cell><cell>0.4839</cell><cell>0.4798</cell></row><row><cell cols="2">(b) Ground Truth 0.5974</cell><cell>0.5601</cell><cell>0.5466</cell><cell>0.5428</cell><cell>0.4631</cell><cell>0.4262</cell><cell>0.4209</cell><cell>0.4199</cell><cell>0.4154</cell><cell>0.3768</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison between our pre-trained model and VSE++<ref type="bibr" target="#b9">[10]</ref> on MSCOCO<ref type="bibr" target="#b23">[23]</ref>. All results are calculated in MSCOCO 5k test split. FT, RC and rV denote fine-tune, random crop and rest validation set, respectively. Please refer to the paper of VSE++<ref type="bibr" target="#b9">[10]</ref> for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of model size and retrieval efficiency among ViTAA<ref type="bibr" target="#b39">[39]</ref>, NAFS<ref type="bibr" target="#b11">[12]</ref> and our method. Retrieval time is computed by retrieving all text queries (6156) through the whole image gallery (3074) of CUHK-PEDES test set<ref type="bibr" target="#b21">[21]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Xiao Han appreciates Freda Shi for her helpful discussion and Kecheng Zheng for sharing details of his implementation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A man wearing a gold t-shirt, a pair of black pants and a pair of black shoes.</p><p>A man with dark hair and light skin wearing a black tea shirt and black baggy pants just below the knee that have a light stripe and dark sandals.</p><p>The man has close cut hair, glasses, a grey shirt, a laptop bag, and dark pants with tan shoes. He is pulling a rolling suitcase behind him.</p><p>The woman is wearing a short sleeved, red shirt with a logo on the front, blue jeans, and black and white tennis shoes.</p><p>The pedestrian is carrying a black bag in their right hand wearing a multi colored scarf, black top, black pants, and white shoes.</p><p>The man is wearing a light blue shirt and grey pants. He is carrying a plastic bag in his hand. <ref type="figure">Figure 6</ref>: Typical successful cases of retrieval results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-based person search via attribute-aided matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Surbhi Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modal knowledge adaptation for language-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tipcb: A simple but effective part-based convolutional baseline for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruili</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Axmnet: Cross-modal context sharing attention network for person re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammarah</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Syed Safwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalid</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Contextual non-local alignment over full-scale representation for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Bernardo ?vila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<editor>Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R?mi Munos, and</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<title level="m">Bridging vision and language by large-scale multi-modal pre-training</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Poseguided multi-granularity attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pretraining for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person search challenges and solutions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving description-based person re-identification by multi-granularity image-text alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial representation learning for text-to-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lightningdot: Pre-training visual-semantic embeddings for real-time image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Text-based person search via multi-granularity embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vitaa: Visual-textual attributes alignment in person search by natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond intra-modality: A survey of heterogeneous person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE TPAMI</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical gumbel attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM TOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
