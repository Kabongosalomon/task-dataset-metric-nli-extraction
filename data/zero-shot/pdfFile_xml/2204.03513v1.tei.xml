<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Many-to-many Splatting for Efficient Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Many-to-many Splatting for Efficient Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion-based video frame interpolation commonly relies on optical flow to warp pixels from the inputs to the desired interpolation instant. Yet due to the inherent challenges of motion estimation (e.g. occlusions and discontinuities), most state-of-the-art interpolation approaches require subsequent refinement of the warped result to generate satisfying outputs, which drastically decreases the efficiency for multi-frame interpolation. In this work, we propose a fully differentiable Many-to-Many (M2M) splatting framework to interpolate frames efficiently. Specifically, given a frame pair, we estimate multiple bidirectional flows to directly forward warp the pixels to the desired time step, and then fuse any overlapping pixels. In doing so, each source pixel renders multiple target pixels and each target pixel can be synthesized from a larger area of visual context. This establishes a many-to-many splatting scheme with robustness to artifacts like holes. Moreover, for each input frame pair, M2M only performs motion estimation once and has a minuscule computational overhead when interpolating an arbitrary number of in-between frames, hence achieving fast multi-frame interpolation. We conducted extensive experiments to analyze M2M, and found that it significantly improves the efficiency while maintaining high effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation (VFI) aims to increase frame rates of videos by synthesizing intermediate frames in between the original ones <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40]</ref>. As a classic problem in video processing, VFI contributes to many practical applications, including slow-motion animation <ref type="bibr" target="#b11">[12]</ref>, video editing <ref type="bibr" target="#b21">[22]</ref>, video compression <ref type="bibr" target="#b44">[45]</ref>, etc. In recent years, a plethora of techniques for video frame interpolation have been proposed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>. However, frame interpolation remains an unsolved problem due to challenges like occlusions, large motion, and lighting changes.</p><p>The referenced research can roughly be categorized into motion-free and motion-based, depending on whether or not * Work primarily done while Ping was interning at Adobe. 300 3000 <ref type="figure">Figure 1</ref>. Performance for ?8 interpolation on a "2K" version of X-TEST <ref type="bibr" target="#b38">[39]</ref>. Runtimes for all methods were measured using a Titan X GPU. The size of each circle indicates the number of model parameters. Results for related methods include RIFE <ref type="bibr" target="#b10">[11]</ref>, SoftSplat <ref type="bibr" target="#b27">[28]</ref>, AdaCof <ref type="bibr" target="#b15">[16]</ref>, SepConv <ref type="bibr" target="#b29">[30]</ref>, XVFI <ref type="bibr" target="#b38">[39]</ref>, DAIN <ref type="bibr" target="#b1">[2]</ref>, ABME <ref type="bibr" target="#b32">[33]</ref>, and CAIN <ref type="bibr" target="#b31">[32]</ref>. We evaluate our proposed M2M splatting using two different off-the-shelf flow estimators, "PWC" denoting PWC-Net <ref type="bibr" target="#b41">[42]</ref> and "DIS" denoting DISFLow <ref type="bibr" target="#b14">[15]</ref>.</p><p>cues like optical flow are incorporated <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>. Motion-free models typically rely on kernel prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> or spatio-temporal decoding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>, which are effective but limited to interpolating frames at fixed time steps and their runtime increases linearly in the number of desired output frames. On the other end of the spectrum, motion-based approaches establish dense correspondences between frames and apply warping to render the intermediate pixels.</p><p>A common motion-based technique estimates bilateral flow for the desired time step and then synthesizes the intermediate frame via backward warping <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. The estimation of bilateral motion is challenging though and incorrect flows can easily degrade the interpolation quality. As a result, for each time step, these methods typically apply a synthesis network to refine the bilateral flows. Another motion-based solution is to forward warp pixels to the desired time step via optical flow <ref type="bibr" target="#b0">[1]</ref>. However, forward warping is subject to holes and ambiguities where multiple pixels map to the same location. Therefore, image refinement networks are commonly adopted to correct remaining artifacts <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46]</ref>. However, both of these approaches re-arXiv:2204.03513v1 [cs.CV] 7 Apr 2022  <ref type="figure">Figure 2</ref>. (a) Many-to-one splatting versus (b) many-to-many splatting for zooming motion in a scene containing blue and orange pixels. M2O splatting may results in holes, while M2M splatting allows for a more flexible image formation model. quire significant amounts of compute, and the refinement networks need to be executed for each of the desired interpolation instants. This decreases their efficiency in multiframe interpolation tasks since their runtime increases linearly in the number of desired output frames.</p><p>We address these challenges and strive for efficiency with a Many-to-Many (M2M) splatting framework. Specifically, our proposed M2M splatting estimates multiple bidirectional flow fields and then efficiently forward warps the input images to the desired time step before fusing any overlapping pixels. Since we directly operate on pixel colors, the quality and resolution of the underlying optical flow play a critical role. For this reason, we first apply an offthe-shelf optical flow estimator <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref> to extract the interframe motion between the two input frames at a coarse level. Based on this low-resolution optical flow estimate, a Motion Refinement Network (MRN) predicts multiple flow vectors for each pixel at the full-resolution which we then use for our image synthesis through many-splatting.</p><p>Conventional motion-based frame interpolation methods only estimate one inter-frame motion vector for each pixel <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>. However and as shown in <ref type="figure">Fig. 2</ref> (a), forward warping with such a motion field manifests as many-to-one splatting, leaving unnecessary holes in the warped result. To overcome this limitation, we model a many-to-many relationship among pixels by predicting multiple motion vectors for each of the input pixels, and then forward warping the pixels to multiple locations at the desired time step. As shown in <ref type="figure">Fig. 2 (b)</ref>, many-to-many splatting allows for more complex interactions among pixels, i.e. each source pixel is allowed to render multiple target pixels and each target pixel can be synthesized with a larger area of visual context. Unsurprisingly, many-to-many splatting leads to many more overlapping pixels. To merge these, we further introduce a learning-based fusion strategy which adaptively combines pixels that map to the same location.</p><p>Since the optical flow estimation step in our pipeline predicts time-invariant correspondence estimates, it only needs to be performed once for a given input frame pair. Once the many-to-many inter-frame motion has been established, generating new in-between frames only requires warping and fusing the input images. This is in stark con-trast to previous approaches that leverage refinement networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, allowing us to perform multi-frame interpolation an order of magnitude faster as shown in <ref type="figure">Fig. 1</ref>.</p><p>In summary, we propose 1) a Motion-Refinement Network that estimates a many-to-many relationship between the two input images, 2) a learning-based pixel fusion strategy which resolves ambiguities between overlapping pixels, and 3) a well-motivated Many-to-Many (M2M) splatting synthesis model for efficient and effective frame interpolation. Our experiments demonstrate that M2M achieves high effectiveness with fast speed, e.g. ?40 ms/f using a Titan X to perform ?8 interpolation of 2K videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion-based video frame interpolation approaches typically estimate optical flows <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref> from given frames, and then propagate pixels/features to the desired target time step <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. Forward warping is an efficient solution to achieve this goal <ref type="bibr" target="#b0">[1]</ref>. With bidirectional optical flow between given frames, Niklaus et al. <ref type="bibr" target="#b26">[27]</ref> directly forward warp the images as well as contextual features to the interpolation instant before utilizing a synthesis network to render the output frame. To make this splatting fully differentiable, they further introduce softmax splatting <ref type="bibr" target="#b27">[28]</ref> which allows them to train the feature extraction end-toend. Splatting has its downsides though, since it is not only necessary to address ambiguities of multiple pixels mapping to the same location but it is also necessary to handle the holes that are present in the sparse result.</p><p>To avoid having to handle these challenges, some methods are based on backward warping instead <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>. The necessary bilateral flow can, for example, be approximated from off-the-shelf flow estimates through a neural network <ref type="bibr" target="#b11">[12]</ref> or depth-based splatting <ref type="bibr" target="#b1">[2]</ref>. Park et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> extend these ideas and introduce a network to further improve the motion representations while Huang et al. <ref type="bibr" target="#b10">[11]</ref> learn to directly estimate bilateral flows. However, estimating bilateral flow is still challenging and the backward warped pixels may still suffer from artifacts. As a result, these methods also rely on image synthesis networks to improve the interpolation quality <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Though shown to be effective, the bilateral flow estimation and the image synthesis networks need to be fully executed for each desired output, leading to a linearly increasing runtime when interpolating more than one in-between frame.</p><p>In contrast to these methods, our M2M approach relies on many-to-many splatting to address the issues with forward warping without relying on an image synthesis network or bilateral flow approximation/estimation.</p><p>Another dominant research direction for VFI aims to avoid explicit motion estimation altogether. One popular approach is to resample input pixels with spatially adaptive filters <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>. Niklaus et al. kernels which in subsequent work are decomposed into separable kernels <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, which also formulate a manyto-many correlations between pixels. However, as local patches suffer from a limited spatial range, deformable convolutions are introduced to handle large motion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. To improve model efficiency, Ding et al. <ref type="bibr" target="#b8">[9]</ref> introduce model compression <ref type="bibr" target="#b15">[16]</ref>. Spatio-temporal decoding methods are also proposed to directly convert spatio-temporal features into target frames via channel attention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> or 3D convolutions <ref type="bibr" target="#b12">[13]</ref>. However, most of these methods generate outputs at a fixed time, typically halfway between the input images, which limits arbitrary-time interpolation and linearly increases the runtime for multi-frame interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Many-to-many Splatting Framework</head><p>In this section, we describe our Many-to-Many (M2M) splatting framework for video frame interpolation. Given an input frame pair, we first estimate the bidirectional motion with an off-the-shelf method <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>. A Motion Refinement Network ( <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>) then takes the off-the-shelf motion predictions as input and estimates multiple motion vectors as well as a reliability score for each individual pixel in the input frames. Lastly, all input pixels are forward warped to the desired target time step several times via each of the multiple motion vectors, and finally merged to generate the output via a pixel fusion that leverages the estimated reliability score. With full end-to-end supervision, our M2M framework is able to achieve not only efficiency but also effectiveness. In the following, we first present the Motion Refinement Network in Sec. 3.1, then introduce the multisplatting and fusion of pixels in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion Refinement Network</head><p>Optical flow is a common technique to model inter-frame motion in videos. Yet directly applying an off-the-shelf optical flow estimator and forward warping pixels based on this estimate may be challenging. Optical flow only models a single motion vector for each pixel, thus limiting the area that a pixel can splat to and thus potentially causing holes. Moreover, most optical flow estimators are supervised with training data at a relatively low resolution and forcing them to process high-resolution frames may yield poor results. In contrast, we present the Motion Refinement Network (MRN) to upsample and refine an off-the-shelf optical flow estimate while predicting multiple motion vectors per pixel. As shown in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>, the MRN pipeline is composed of three parts: Motion Feature Encoding, Low-rank Feature Modulation, and Output Decoding. Motion Feature Encoding aims to encode multi-stage motion features from the input frames {I 0 , I 1 } as well as the optical flow {F 0?1 , F 1?0 } estimated by an off-the-shelf estimator <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref> at a coarse resolution. As outlined in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>, the encoding process is designed in a hierarchical manner. At first, we extract two L-level image feature pyramids from I 0 and I 1 , with the zeroth-level being the images themselves. To generate the feature representations at each pyramid level, we utilize two convolutional layers with intermittent PReLU activations to downsample the features from the previous level by a factor of two. In our implementation, we use L = 4, and the numbers of feature channels from shallow to deep are 16, 32, 64, and 128 respectively.</p><p>Then, from the zeroth to the last level, we apply Joint Flow Encoding (JFE) modules as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref> (b) to progressively generate motion feature pyramids for the bidirectional flow fields F 0?1 and F 1?0 . In the l-th level's JFE module, the motion and image features from the previous level are warped towards each other. Specifically, the features from the pyramid corresponding to I 0 are warped towards I 1 and vice versa using the off-the-shelf optical flow estimates. Then, the original features and the warped features are combined and downsampled using a two-layer CNN to encode the l-th level's motion features. Low-rank Feature Modulation is designed to further enhance the motion feature representations with a low-rank constraint. The idea behind this module is that flow fields of natural dynamic scenes are highly structured due to the underlying physical constraints, which can be exploited by low-rank models to enhance the motion estimation quality <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>. To avoid formulating explicit optimization objectives like in previous methods, which may be inefficient in high-resolution applications, we draw inspirations from Canonical Polyadic (CP) decomposition <ref type="bibr" target="#b13">[14]</ref> and construct an efficient low-rank modulation module to enhance each flow's feature maps with low-rank characteristics.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (c), given an input feature map of size C ? H ? W , three groups of projectors are adopted to respectively shrink the feature maps into the channel, height, and width dimensions. Each projector is composed of a pooling layer, 1 ? 1 conv layers, and a sigmoid function. We apply M projectors for each of the three dimensions which results in three groups of 1-D features, whose sizes can be represented as M ? (C ? 1 ? 1) for the channel dimension, M ? (1 ? H ? 1) for the height dimension, and M ? (1 ? 1 ? W ) for the width dimension. Then, for each of the M vectors from the three dimensions, we apply the Kronecker Product to get a rank-1 tensor, whose shape is C ? H ? W . The M rank-1 tensors are later averaged point-wise. To ensure low-rank characteristic, M is set to be smaller than C, H, and W (we adopt M = 16 in this work). We combine the input features and the low-rank tensor via point-wise multiplication, where the latter serves as weights to modulate the former with low-rank characteristics.</p><p>Deep learning-based low-rank constraints have also been utilized for model compression <ref type="bibr" target="#b34">[35]</ref>, segmentation <ref type="bibr" target="#b4">[5]</ref> and image reconstruction <ref type="bibr" target="#b49">[50]</ref>. In this work we explore the application to motion modeling and demonstrate its effectiveness on the task of video frame interpolation. Output Decoding generates N motion vectors as well as the reliability scores for each input pixel based on the motion feature pyramids and the feature maps subject to the low-rank prior. We adopt deconv layers to enlarge the spa- </p><formula xml:id="formula_0">0 1 0?1 ? 0 0?1 1 0?1 2 0?1 4 0?1 3</formula><formula xml:id="formula_1">{F i 0?1 , F i 1?0 } N i=1</formula><p>as well as the corresponding reliability maps {S 0 , S 1 }, which are later utilized to fuse pixels that map to the same location when generating the new in-between frames. An example of these outputs is visualized in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pixel Warping and Fusion</head><p>The previously estimated multi-motion fields are first used to forward warp pixels to a given target time step. Later, we present a fusion strategy to combine the colors of overlapping pixels in the output. Since both the warping and fusion steps operate with pixels' colors without any subsequent post-processing steps, an intermediate frame can be interpolated with minuscule computational overhead. Pixel Warping. So far, we have generated N full-resolution bidirectional motion fields {F n 0?1 , F n 1?0 } N n=1 and pixelwise reliability scores {S 0 , S 1 } for the input video frame pair {I 0 , I 1 }. The next step is to synthesize an intermediate frame I t at the desired time step t ? (0, 1). Under the assumption of linear motion, we first scale each pixel's motion vectors by the desired interpolation time t as:</p><formula xml:id="formula_2">F n 0?t (i 0 ) = t ? F n 0?1 (i 0 ) F n 1?t (i 1 ) = (1 ? t) ? F n 1?0 (i 1 )<label>(1)</label></formula><p>where i 0 and i 1 denote the i-th source pixel in I 0 and I 1 respectively. Then, a source pixel i s is forward warped by its n-th motion vector to i n s?t = ? F (i s , F n s?t ) at the desired intermediate time t, with s ? {0, 1} representing the source frame, ? F is the forward warping operation, and F n s?t is the n-th sub-motion vector of i s as defined in Eq. 1.</p><p>We first consider utilizing a single motion vector for warping, which means each pixel is only warped to one location in the target frame. In dynamic scenes, the motion vectors may overlap with each other thus resulting in a many-to-one (M2O) propagation where the pixel set after fusion is smaller than the actual pixel set of frame. This results in holes as shown in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>. Though exploiting multiple source frames lessens this issue, M2O warping still restricts each source pixel to only render a small 4-pixel vicinity in the output frame. This limits the effectiveness in representing and thus interpolating regions with complex interactions among the pixels, as shown in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>. Fortunately, such limitations can be alleviated through many-to-many (M2M) pixel splatting by using multiple motion vectors to model the motion of each source pixel. We forward warp each pixel in the source s with N (N &gt; 1) sub-motion vectors to t, and get the set of warped pixels,</p><formula xml:id="formula_3">I s?t = N n=1? n s?t<label>(2)</label></formula><p>Many-to-many splatting relaxes the restriction that each source pixel can only contribute to a single location. Therefore it allows the underlying motion estimator to learn to reason about occlusions, and model complex color interactions across a larger area of pixels. Pixel Fusion. By applying M2M warping to all the input pixels in {I 0 , I 1 }, we get the complete warped pixel set where multiple target pixels may correspond to the same pixel locations:? t =? 0?t ? 1?t . To fuse warped pixels overlap with each other, we measure each of the pixels' importance from three aspects: the temporal relevance, brightness consistency, and the reliability score.</p><p>1) Temporal Relevance r i characterizes changes not based on motion (e.g. lighting changes) between a source frame and the target. For simplicity, we adopt linear interpolation by setting r i = 1 ? t if i comes from I 0 and r i = t otherwise, with t being the desired interpolation time.</p><p>2) Brightness Consistency b i indicates occlusions by comparing a frame to its target through backward warping:</p><formula xml:id="formula_4">b i = ?1 ? ||I 0 (i) ? I 1 (i + F 0?1 (i))|| 1 , if i ? I 0 , ?1 ? ||I 1 (i) ? I 0 (i + F 1?0 (i))|| 1 , if i ? I 1 ,<label>(3)</label></formula><p>The effectiveness of Eq. 3 is not decided only by the motion but also by the pixels' colors, which can be affected by various factors like noise, ambiguous appearance, and changes in shading <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. To enhance the robustness, we thus further adopt a learned per-pixel reliability score.</p><p>3) Reliability Score s i is jointly estimated together with the motion vectors through the Motion Refinement Network as introduced in Sec. 3.1 and learned from data.</p><p>With these three measurements, we fuse the overlapped pixels at a location j in the form of weighted summation,</p><formula xml:id="formula_5">I t (j) = i??t 1 i=j ? e (bi?si??) ? r i ? c i i??t 1 i=j ? e (bi?si??) ? r i<label>(4)</label></formula><p>where c i represents the i-th warped pixel's original color, ? is a learnable parameter adjusting the scale of weights,? t is the set of all the warped pixels at time t, and 1 i=j indicates if the warped pixel i is mapping to the pixel location j. We note that our final fusion function is similar to Soft-Splat <ref type="bibr" target="#b27">[28]</ref> in the form of softmax weighting, however our method differs in three aspects. First, we provide a solution to directly operate in the pixel color domain, while Soft-Splat splats features and utilizes an image synthesis network instead. Second, we propose a general framework for fusing pixels from multiple frame, while SoftSplat fuses each frame individually. Third, we introduce the learning based reliability score to fuse overlapping pixels in a data-driven manner while SoftSplat uses feature consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the section, we subsequently compare our proposed proposed to related state-of-the-art frame interpolation techniques and analyze it quantitatively as well as qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We supervise our proposed approach on the training split of Vimeo90K and test it on various datasets summarized as follows: 1) Vimeo90K <ref type="bibr" target="#b45">[46]</ref>, the test split containing 3,782 triplets at a resolution of 448?256 pixels. 2) UCF101 <ref type="bibr" target="#b40">[41]</ref>,  <ref type="table">Table 1</ref>. Quantitative results on the Vimeo90K, UCF101, ATD12K, and Xiph datasets. We compute models' GFLOPs and speed based on 640?480 inputs. The "share" denotes the part of compute independent from the desired frame rate, which is in contrast to "unshare". a dataset containing human action videos of size 256?256 pixels. A set of 379 triplets were selected by Liu et al. <ref type="bibr" target="#b18">[19]</ref> as a test set for frame interpolation. 3) Xiph <ref type="bibr" target="#b24">[25]</ref>, as proposed by Niklaus et al. <ref type="bibr" target="#b27">[28]</ref> where "Xiph-2K" is generated by downsampling 4K footage, and "Xiph-4k" is based on center-cropped 2K patches. 4) ATD12K <ref type="bibr" target="#b16">[17]</ref>, containing 2,000 triplets from various animation videos at a resolution of 960?480 pixels. 5) X-TEST <ref type="bibr" target="#b38">[39]</ref>, the test set from X4K1000FPS <ref type="bibr" target="#b38">[39]</ref>, containing 15 scenes extracted from 4K videos at 1000fps. We denote the original resolution as X-TEST(4K), and additionally adopt X-TEST(2K) by downsampling X-TEST(4K) by a factor of two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We train our proposed pipeline in an end-to-end manner. Given an output I t and the ground truth I gt t , we define the training loss as the sum of the Charbonnier loss <ref type="bibr" target="#b3">[4]</ref> and the census loss <ref type="bibr" target="#b20">[21]</ref>, L = L char + L cen . To train the model, we utilize the 51,312 triplets from the training split of Vimeo90K <ref type="bibr" target="#b45">[46]</ref>. We apply random data augmentations including spatial and temporal flipping, color jittering, and random cropping with 256?256 patches. We adopt Adam <ref type="bibr" target="#b19">[20]</ref> for optimization, with a weight decay of 1e-4. We train the model for 400k iterations with a batch size of 8, during which the learning rate is decayed from 1e-4 to 0 via cosine annealing. All experiments are implemented with PyTorch, and executed on a single Nvidia Titan X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art</head><p>We report two variants of our proposed approach based on different methods for estimating the off-the-shelf motion vectors. "M2M-PWC" is based on PWC-Net <ref type="bibr" target="#b41">[42]</ref>. In this setting, we jointly optimize PWC-Net during training and generate initial flows at 1/4 of the original resolution. The other variant is based on DISFlow <ref type="bibr" target="#b14">[15]</ref> and denoted as "M2M-DIS". In our experiments, we generate N =4 submotion vectors for each pixel. For comparisons, we re-port the performance of recent VFI approaches including: SepConv <ref type="bibr" target="#b29">[30]</ref>, DAIN [2] CAIN <ref type="bibr" target="#b6">[7]</ref>, AdaCoF <ref type="bibr" target="#b15">[16]</ref>, Soft-Splat <ref type="bibr" target="#b27">[28]</ref>, BMBC <ref type="bibr" target="#b31">[32]</ref>, RIFE <ref type="bibr" target="#b10">[11]</ref>, and ABME <ref type="bibr" target="#b32">[33]</ref>.</p><p>We first analyze the computational costs of these models in Tab. 1. We denote the required compute that is independent from the desired frame rate as "share", and "unshare" otherwise. Hence the total computational complexity for interpolating n frames can be calculated through "#share+n ? #unshare". Motion-free methods (including SepConv, CAIN, and AdaCof) and pure bilateral-motionbased methods (like RIFE and ABME) have no share compute (denoted as "N/A") and their computational complexity increases linearly in the number of desired frames. Approaches like SoftSplat, and BMBC can interpolate arbitrary frames, yet still suffer from both high compute and unshare compute. E.g. in the ?8 interpolation setting, they take 1.6 TFLOPs, and 3.1 TFLOPs respectively. In contrast, our M2M takes only 0.1 TFLOPs in total. <ref type="figure" target="#fig_5">Fig. 6</ref> (a) compares the average runtime for different methods subjct to varying interpolation factors. Our method is faster than all other methods in multi-frame settings. For ?16 interpolation our method takes about 5 ms to interpolate a frame, which is around 5?, 20?, and 100? faster than RIFE, SoftSplat, and ABME respectively.</p><p>Taking efficiency aside, our method achieves state-ofthe-art performance on multiple datasets. The metrics for ?2 interpolation are presented in Tab. 1. On Vimeo90K and UCF101, our M2M method is on par with the recently proposed real-time method RIFE and performs slightly worse than SoftSplat and ABME. On Xiph-2K, our M2M method achieves slightly lower PSNR than SoftSplat, yet achieves the highest SSIM among all the methods. Moreover, on the animation dataset ATD12K and the high-resolution dataset Xiph-"4K", our M2M method, especially M2M-PWC, outperforms previous methods in terms of both PSNR and SSIM. This demonstrates our methods' effectiveness when processing high-resolution videos and the ability to gener-   <ref type="bibr" target="#b32">[33]</ref>. All the run-times are measured on X-TEST(2K).</p><p>alize across domains such as animation videos. We report the results for ?8 interpolation on the X-TEST dataset, which contains diverse sequences with both high resolution and high frame rate, in Tab. 2. Our M2M method outperforms all previous methods on both the original 4K full resolution (4096?2160) and the downsampled 2K resolution (2048?1080) with substantial advantages in efficiency. For the models trained with Vimeo90K, ABME achieves the second-best PSNR in both 4K and 2K settings, but it takes 2,904ms to interpolate a 2K frame which is nearly 70? slower than M2M. To evaluate the temporal consistency, we compare the accuracy at each interpolation time step in <ref type="figure" target="#fig_5">Fig. 6 (b)</ref>. We found that previous methods tend to deteriorate when interpolating frames that are temporally centered between the inputs, while M2M achieves a flatter and smoother curve for intermediate frames. This shows that M2M interpolates frames with not only better quality, but also higher temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Method Analysis</head><p>Ablation of Modules. We first analyze the effectiveness of the different components of our method in Tab. <ref type="bibr" target="#b2">3</ref>  <ref type="table">Table 3</ref>. Ablative experiments (in PSNR) on Vimeo90K with different initial flow methods. "MRN" denotes the motion refinement network, "JFE" refers to the joint flow encoding module in MRN, "LFM" is the low-rank feature modulation, and "RS" denotes the reliability score in the fusion step that synthesizes the output.  <ref type="table">Table 4</ref>. Analyzing the impact of the number of sub-motion vectors for each pixel in our many-to-many splatting on Vimeo90K, with two different initial flow estimators.</p><formula xml:id="formula_6">N =1 N =2 N =4 N =8 PWC-</formula><p>start with a single motion vector for each pixel. The first row demonstrates that directly using the off-the-shelf flow for warping leads to sub-optimal accuracy. As shown in the second row, applying the refinement network without joint flow encoding (JFE) and low-rank feature modulation (LFM) can already significantly improve performance by 0.97 dB and 2.38 dB for PWC-Net and DISFlow respectively. Further applying either JFE or LFM leads to improvements of more than 0.15 dB for both off-the-shelf flow methods. And using both JFE and LFM helps to boost the performance to 35.15 dB and 34.78 dB, respectively. In the last two rows, we also show the impact of the reliability scores which are generated by the refinement network and utilized for the pixel fusion. Without this score, the performance degrades, thus highlighting the importance of this metric in comparison to only using photoconsistency. Effect of Number of Flows per Pixel. Tab. 4 compares the effect of using different numbers of the sub-motion vectors for the M2M splatting. When N =1, it reduces the warping to M2O mapping, and achieves the lowest accuracy. When increasing N to 4, M2M improves the accuracy by more than 0.1 dB, with a very slight increment in run-time (&lt;1ms). Also, and as shown in the last row, we noticed that further increasing the number of sub-motion vectors leads to marginal improvements. <ref type="figure">Fig. 7</ref> illustrates the visual results for M2O splatting and M2M splatting. Effect of Resolution for Initial Flow Estimation Our method relies on an off-the-shelf optical flow estimator to generate the initial flow. However, most optical flow estimation models are trained using a relatively low resolutions. Directly applying them to estimate the flow at 2K or 4K in-</p><formula xml:id="formula_7">(a) Overlapped inputs (b) Initial flow (c) N=1 (d) N=4</formula><p>(e) Ground-truth <ref type="figure">Figure 7</ref>. Comparison between many-to-one splatting and many-to-many splatting. Given the input frames (a), M2O splatting with initial flow (b) or single refined sub-motion vector (c) results in undesired visual artifacts for regions with complex motion. In comparison, our proposed M2M splatting with four sub-motion vectors (b) can interpolate with much higher quality.</p><p>R= Xiph-2K Xiph-"4k" X-TEST(2K) X-TEST(4K)  <ref type="table">Table 5</ref>. Impact of the resolution at which the initial optical flow estimator is applied on. "R" is the down-sampling factor. puts may result in sub-optimal results. We thus study the impact of the initial flow's resolution for interpolating highresolution frames in Tab. 5. Since PWC-Net is learningbased and pre-trained on small resolutions, it is less effective at processing high-resolution frames as demonstrated by the reduced interpolation quality on 4K data. By downsampling the input by a factor of 4 or 8, the accuracy improves significantly. In contrast, DISFlow is not supervised and hence less susceptible to similar domain gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions and Limitations.</head><p>Though our method achieves very high efficiency especially for high framerate interpolation, its accuracy on low-resolution datasets like Vimeo90K is behind several state-of-the-art methods. We believe that carefully tuning and enlarging the model capacity allows M2M to compete with these state-of-theart methods. The proposed method renders intermediate frames based on forward warping, which may be subject to holes in the output. In <ref type="figure">Fig. 8</ref>, we count the average number of remaining holes (in pixels) for different configurations on Vimeo90K. As we can see, our M2M splatting with N =4 is still subject to around 0.5-pixel holes in each  <ref type="figure">Figure 8</ref>. Analysis of number of remaining holes (in pixels) versus the number of sub-motion vectors in many-to-many splatting. frame on average. However, compared to the initial single sub-motion based M2O splatting, our method has significantly decreased the number of holes. Another limitation of our method is that the many-to-many splatting process may result in blurriness as shown in <ref type="figure">Fig. 7 (d)</ref>. This can be addressed by further improving the fusion strategy or applying a lightweight network to refine the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a many-to-many splatting technique to efficiently interpolate intermediate video frames. We first design a motion refinement network to generate multiple sub-motion vectors for each pixel. These submotion fields are then applied to forward warp the pixels to any desired time step, which are then fused to obtain the final output. By sharing the computation for the flow refinement and only requiring little compute to generate each frame, our method is especially well-suited for multi-frame interpolation. Experiments on multiple benchmark datasets demonstrate that the proposed method achieves effectiveness with superior efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc><ref type="bibr" target="#b28">[29]</ref> estimate spatially-varying Overview of the (a) Motion Refinement Network and its core modules: (b) Joint Flow Encoding and (c) Low-rank Feature Modulation. Given an image pair {I0, I1} and the initial bidrectional inter-frame flow {F 0?1 , F 1?0 }, the goal is to generate multiple refined bidirectional flows {F i 0?1 , F i 1?0 } N i=1 and the color reliability maps {S0, S1}. The "warp" in the JFE denotes backward warping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples of the MRN's output (N = 4). S0 shows low reliability in areas with complex motion as intuitively expected. {F N 0?1 } 4 n=1 refine the initial flow F 0?1 with better details, and decompose complex motion with shade changes (as indicated by the red circle) into multiple motion fields. tial size of the feature maps. That is, the decoder operates in L stages from coarse to fine while leveraging the features encoded by the JFE modules. At the last decoding stage, the full-resolution feature maps for the flow in each direction are converted into multiple fields</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of forward warping via many-to-one (M2O) splatting and many-to-many (M2M) splatting. (a) With one source frame, M2M splatting suffers less from banding artifacts and provides improved robustness to ambiguities near the boundaries of discontinuous motion. (b) Banding artifacts can be alleviated with multiple source frames, yet M2O splatting still suffers from stray effects at boundaries due to its image formation model that is less flexible than M2M splatting. Best viewed when zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Evaluating multi-frame interpolation. (a) Runtime in logarithmic scale for interpolating 640?480 video frames with different interpolation factors. (b) Per-frame accuracy for ?8 interpolation on X-TEXT(2K). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results for ?8 interpolation on the X-TEST dataset.</figDesc><table><row><cell></cell><cell>X-TEST(4K)</cell><cell>X-TEST(2K)</cell><cell>Runtime</cell></row><row><cell></cell><cell cols="2">PSNR SSIM PSNR SSIM</cell><cell>(ms/f)</cell></row><row><cell cols="2">SepConv [30] 23.94 .794</cell><cell>25.70 .800</cell><cell>693</cell></row><row><cell>DAIN [2]</cell><cell cols="2">26.78  *  .807  *  29.33 .910</cell><cell>3132</cell></row><row><cell>CAIN [7]</cell><cell>22.51 .775</cell><cell>23.62 .773</cell><cell>287</cell></row><row><cell>AdaCoF [16]</cell><cell>23.90 .727</cell><cell>26.03 .778</cell><cell>234</cell></row><row><cell cols="2">SoftSplat [28] 25.48 .725</cell><cell>29.73 .824</cell><cell>318</cell></row><row><cell>RIFE [11]</cell><cell>24.67 .797</cell><cell>27.49 .806</cell><cell>104</cell></row><row><cell>ABME [33]</cell><cell cols="2">30.16  *  .879  *  30.65 .912</cell><cell>2904</cell></row><row><cell>XVFI  ? [39]</cell><cell>30.12 .870</cell><cell>30.85 .913</cell><cell>203</cell></row><row><cell>M2M-PWC M2M-DIS</cell><cell>30.81 .912 30.18 .909</cell><cell>32.07 0.923 30.98 0.912</cell><cell>44 39</cell></row></table><note>? indicates model trained with X-TRAIN.* indicates the numbers are copied from</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tensor low-rank reconstruction for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<idno>ECCV, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion-aware dynamic architecture for efficient frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cdfi: Compression-driven network design for frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlocal sparse and low-rank regularization for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4527" to="4538" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rife: Real-time intermediate flow estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06294</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flavr: Flow-agnostic video representations for fast frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08512</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast optical flow using dense inverse search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Till</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Video frame interpolation via residue refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICASSP. IEEE. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep video color propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Cornill?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alexander Sorkine-Hornung, Markus Gross, and Christopher Schroers. Phasenet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xiph.org video test media (derf&apos;s collection)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Montgomery</surname></persName>
		</author>
		<ptr target="https://media.xiph.org/video/derf/" />
	</analytic>
	<monogr>
		<title level="m">Online</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Splattingbased synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10075</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting adaptive convolutions for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Asymmetric bilateral motion estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Im-net for high resolution video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Sabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omry</forename><surname>Sendik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stable lowrank tensor decomposition for compression of convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Huy</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sobolev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sozykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ermilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Gusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Tichavsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeriy</forename><surname>Glukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ECCV, 2020. 4</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised video interpolation using cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning general optical flow subspaces for egomotion estimation and detection of motion anomalies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xvfi: Extreme video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep animation video interpolation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lsm: Learning subspace minimization for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Time lens: Event-based video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video compression through image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training weakly supervised video frame interpolation with events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="14589" to="14598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zoom-in-to-check: Boosting video interpolation via instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A flexible recurrent residual pyramid network for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning tensor low-rank prior for hyperspectral image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video frame interpolation without temporal priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
