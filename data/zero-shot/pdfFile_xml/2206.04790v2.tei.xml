<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-24">24 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyank</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-24">24 Jul 2022</date>
						</imprint>
					</monogr>
					<note>2 Shreyank N Gowda et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale datasets have played a key role in the progress of research across AI problems. In computer vision, neural networks have existed for decades, but one of the enabling factors for the current revolution was the development of the large ImageNet <ref type="bibr" target="#b7">[8]</ref>. In the video domain, manually collecting and annotating data can be a prohibitively expensive process. In video action recognition, for example, collecting data requires an immense amount of manual labor, as it involves finding suitable videos, trimming them and classifying them.</p><p>Recent efforts in video focus on relieving the strong dependency of current methods to the size of labeled datasets. Some of these efforts <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43]</ref> involve increasing the number of data samples through data augmentation. This strategy aims to create new videos in the training set by performing transformations on the original annotated videos, where labels are known. This process adds diversity to the training data, while new videos are still realistic and plausible. In the simplest version of data augmentation in video, new data samples are generated by flipping the input video horizontally, or by cropping a subsection of the video. New methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b37">38]</ref> propose more sophisticated processes like combining two videos. VideoMix <ref type="bibr" target="#b37">[38]</ref> randomly crops regions of one video and pastes them onto another. ActorCut [43] goes one step further and uses the bounding box detections of humans on one video to paste them onto the background of another video. This increases the diversity of the new videos, and despite the lack of visual realism of the resulting videos, this strategy helps.</p><p>However, as datasets become larger, such data augmentation strategies become computationally expensive. The search space of possible video pairs and transformations is enormous and difficult to explore. The solution is often to sample the space randomly, or to manually design augmentation heuristics. Any exploration process is particularly burdening in the context of video data, where the augmentation process needs to be repeated in every frame, which may be orders of magnitude more expensive than for images.</p><p>In this paper we address the problem of sampling for data augmentation, and propose to learn to select pairs of videos. We show that this reduces the search space of augmented data points by orders of magnitude and improves the final accuracy of the classifier significantly. We leverage two observations. First, not all data points are as useful for classification. This idea has been exploited in the context of frame or clip selection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>. Second, we can learn to predict which data points will be useful without actually generating them. This is essential, as the space of transformations is huge, and if we needed to create each candidate augmented video, the process would be prohibitively expensive.</p><p>More concretely, we propose a data augmentation method which we call Learn2Augment. The proposed method contains a "Selector" network, which predicts a score of how useful a combination of two videos will be, without having to actually composite them. The Selector is trained using the accuracy of the classification as the cue. Since this metric depends on the classifier, it is not differentiable with respect to the Selector's parameters. Therefore we optimize the network using reinforcement learning. Once the Selector network is trained,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. We address the problem of data augmentation for video action recognition. Standard augmentation strategies in video are handdesigned and sample the space of possible augmented data points either at random, without knowing which augmented points will be better, or through heuristics. We propose to learn what makes a "good" video for action recognition and select only high-quality samples for augmentation. In particular, we choose video compositing of a foreground and a background video as the data augmentation process, which results in diverse and realistic new samples. We learn which pairs of videos to augment without having to actually composite them. This reduces the space of possible augmentations, which has two advantages: it saves computational cost and increases the accuracy of the final trained classifier, as the augmented pairs are of higher quality than average. We present experimental results on the entire spectrum of training settings: few-shot, semisupervised and fully supervised. We observe consistent improvements across all of them over prior work and baselines on Kinetics, UCF101, HMDB51, and achieve a new state-of-the-art on settings with limited data. We see improvements of up to 8.6% in the semi-supervised setting. Project Page: https://sites.google.com/view/learn2augment/home <ref type="figure">Fig. 1</ref>. Standard video augmentation techniques generate data using hand-designed heuristics (left). We propose to learn to select videos for augmentation, based on how effective they will be for learning to classify (middle). Our approach, Learn2Augment, improves classification across datasets and settings, including UCF101 (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>we use it to choose good pairs of videos, composite them, and train a classification network. In our experiments, for example in the case of the UCF101 dataset, using the Selector reduces the number of augmented videos by 92% while increasing the classification accuracy.</p><p>In the proposed method, each augmented video is created from a pair of videos using a composition of the segmented foreground of one video, including actor and objects, onto the background of the other video. This process yields diverse and realistic new data samples, which we demonstrate is important for learning. More concretely, results show an improvement of 4.4% over using a simpler transformation.</p><p>The Selector is indeed useful to reduce the number of videos for training the classifier. However, we also need to reduce the space of possible pairs for training the Selector network itself. For example, the number of possible pairs of videos in video datasets can be in the order of millions for small datasets or billions for large datasets. For this, we leverage the natural correlation between the occurrence of foreground activities and background scenes <ref type="bibr" target="#b4">[5]</ref>. This is, it is more likely to find someone playing football in a football field than at a restaurant. Instead of sampling at random the pairs of videos to train the Selector on, we sample pairs from classes that are semantically similar. In particular, we use the class names to obtain a semantic embedding, and match each class to their nearest neighbor in this space. Experiments show that this extremely simple design choice of Semantic Matching reduces the space of possible pairs of videos by several orders of magnitude (from quadratic to linear on the number of videos). This yields better results than choosing pairs at random, which may result in non-plausible scenarios, or choosing pairs from the same class, which may not add as much diversity.</p><p>In summary, the proposed Learn2Augment contains three core components: a Selector that learns to choose good videos to augment, a Semantic Matching method that improves optimization, and a Video Compositing that composites video pairs for augmentation. Experimental results show that all components contribute to the performance of the system in different ways, and the overall method obtains state-of-the-art in all datasets, and in all settings that involve limited training data. In addition, in the setting which considers the full training set, the proposed data augmentation technique improves upon the baseline on all datasets, including UCF101, HMDB51, and the large-scale Kinetics-400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Data Augmentation for Video Action Recognition. Standard data augmentation techniques in action recognition include horizontal flip and cropping, where new videos are created by selecting a box at each frame, and then resizing the resulting video to have the same size as the original one. While this strategy helps, generated videos do not add much diversity to the training set. Recent efforts such as ActorCut <ref type="bibr" target="#b42">[43]</ref> and VideoMix <ref type="bibr" target="#b37">[38]</ref> increase the diversity of new video samples by cutting and pasting the foreground of one video onto another. This general technique of combining two data samples has proven to be quite effective, even in the image domain <ref type="bibr" target="#b36">[37]</ref>. However, the resulting videos are not very realistic, and are used for training regardless of their quality. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> go one step further and synthesize new samples using GANs, and use "self-paced selection" to train, starting with easy samples and progressively choosing harder samples. Instead, we propose to create realistic data samples by segmenting, inpainting and blending the foreground of one video onto the background of another. Crucially, we learn to discard novel video samples that are not expected to be useful for classification, overall producing a more accurate data augmentation strategy.</p><p>Learning to Augment Data. The idea of learning to augment data has been used in other computer vision problems. In the image classification domain, this strategy has been done using the final classification loss as the training criterion <ref type="bibr" target="#b24">[25]</ref>, augmenting in feature space <ref type="bibr" target="#b8">[9]</ref>, and learning data augmentation policies <ref type="bibr" target="#b5">[6]</ref>. As in this paper, in the image domain it has been noted that the search space for data samples can be large and thus expensive <ref type="bibr" target="#b6">[7]</ref>.</p><p>Other computer vision domains like low level vision, also struggle with data dependency, as creating ground truth is particularly hard. In optical flow, Aut-oFlow <ref type="bibr" target="#b32">[33]</ref> recently introduced the strategy of learning to generate good training data for a target dataset.</p><p>Semi-supervised Video Action Recognition. Semi-supervised learning (SSL) also aims to reduce data dependence by learning from large sets of unlabeled samples and a small set of labeled ones. SSL in images has been widely explored. For example, some strategies include giving pseudo-labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> to samples where the classifier has high confidence, and adding these to the labeled training data. Other common approaches use consistency regularization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. Approaches that combine consistency regularization and entropy minimization <ref type="bibr" target="#b12">[13]</ref> have shown to be very effective in tackling the SSL task in images such as MixMatch <ref type="bibr" target="#b2">[3]</ref> and RemixMatch <ref type="bibr" target="#b1">[2]</ref>. SSL in videos however, has not been explored as much. One of the early works used extreme learning machines <ref type="bibr" target="#b16">[17]</ref> to perform SSL on videos. Recently, <ref type="figure">Fig. 3</ref>. Pipeline for compositing a single frame. The foreground is from the class "soccer juggling" and the background from the class "soccer penalty", which are semantic class neighbors. We can see objects such as 'person' and 'ball' are detected as objects of interest.</p><p>VideoSSL <ref type="bibr" target="#b17">[18]</ref> and Temporal Contrastive Learning (TCL) <ref type="bibr" target="#b29">[30]</ref> leverage SSL in videos. VideoSSL <ref type="bibr" target="#b17">[18]</ref> uses pseudo-labels and object cues from unlabeled samples to guide the learning process. TCL <ref type="bibr" target="#b29">[30]</ref> use a two-pathway contrastive learning model using unlabeled videos at two different speeds with the intuition that changing video speeds do not change the action being performed.</p><p>Data augmentation and SSL are two different families of techniques to relieve the dependence on labeled data, and in this paper we experiment with the combination of both, showing that they are actually complementary.</p><p>Sample Selection. Recent work <ref type="bibr" target="#b15">[16]</ref> has shown that not all data samples are as useful. Selecting a subset of high quality frames or clips at test time shows better results than using the entire video for action recognition. In this spirit, SMART <ref type="bibr" target="#b10">[11]</ref> uses an attention and relation network to learn scores for each frame in a video and then select only the high ranked ones for inference. Similarly, SCSampler <ref type="bibr" target="#b19">[20]</ref> uses a lightweight clip sampling model to select the salient clips in a video and use only those. Unlike the proposed method, these learn to choose single videos, which are already available, while we learn to choose pairs of videos to be composited, which are not already combined.</p><p>The most relevant work to ours is data valuation in the image domain, using RL <ref type="bibr" target="#b35">[36]</ref>, in the image domain where each sample is given a score of how effective the sample is, and at training time the sample is multiplied by this score. In our work, instead of learning the effectiveness of the training set, we leverage that knowledge for augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learn2Augment</head><p>In this section we describe in detail the architecture of the proposed Learn2Augment. In a nutshell, the goal is to learn to augment novel data points which are realistic and diverse, such that we can train a better classifier with them. For this, we train a Selector network, which predicts a score of how useful a given pair of videos is for augmentation. We pick pairs that have a high score to be augmented. The transformation we use for augmentation is Video Compositing.</p><p>Training the Selector using the entire dataset is infeasible, and sampling pairs of videos at random will yield unlikely pairs. Thus we sample pairs of videos using Semantic Matching. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of the proposed method and in Sec. 4 we describe how we train our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selector</head><p>Given two input videos V 1 and V 2 , the goal of the Selector is to predict a weight ?, rating the quality of the potential composited video. Note that the input to the Selector is two putative videos instead of the composited one. This means that at test time, we can predict how useful the composited video will be without having to actually create it.</p><p>The architecture of the Selector includes a standard video classification network to extract video features, which is ResNet3D-18 <ref type="bibr" target="#b14">[15]</ref> followed by a simple multi-layer perceptron (MLP) with 3 hidden layers of sizes 2048, 1024 and 512. Two videos are input to the Selector at a time, and their features and labels are concatenated and input to the MLP.</p><p>Since there is no ground truth of how "good" a video sample is for learning, we train the Selector using the change in validation loss of the classifier. This is, we argue that a "good" training sample is one which, if used for training, improves the validation loss of the classification network. In other words, if we take one optimization step training the classifier, after updating the weights, the validation loss will go down. Section 4.1 describes the training process in detail.</p><p>At test time, we use the Selector by sampling pairs of videos, choosing those pairs with high score ?, and input to the Video Compositing module, which we describe in Sec. 3.3. The resulting video is finally used to augment the training set for the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Matching (SM)</head><p>The number of pairs in the full dataset can be very large, as it grows with the square of the number of videos. For Kinetics <ref type="bibr" target="#b3">[4]</ref>, for example, we would encounter 360 billion pairs. Training the classifier using these is clearly infeasible, and thus we use the Selector. But training the Selector itself with all these samples is infeasible too. Sampling uniformly is a reasonable solution, but many video pairs may not be useful for learning. We leverage the observation that all combinations of actions and backgrounds are not equally likely <ref type="bibr" target="#b4">[5]</ref>. This natural correlation between actions and backgrounds helps to prune unlikely class combinations.</p><p>For this, we make the assumption that classes that are semantically similar are more likely to contain a foreground and a background that are plausible in the real world, and therefore more realistic for our data augmentation purposes. Thus, we use the class names to extract a language embedding using sen2vec <ref type="bibr" target="#b27">[28]</ref>, and use these embeddings to match each class to its nearest neighbor. We sample videos V 1 and V 2 from class c 1 and its closest neighbor c 2 respectively. This simple decision reduces the number of pairs to grow linearly with the size of the dataset, and furthermore increases the accuracy significantly with respect to sampling video pairs at random. More details on the numerical impact can be found in Sec. 5.3. Semantic class pairs and additional experiments using intraclass augmentation can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Video Compositing (VC)</head><p>The goal of the augmentation process is to composite two videos, to produce realistic, plausible and diverse new videos, that will improve the classification. <ref type="figure">Figure 3</ref> shows the overall pipeline for compositing a single frame.</p><p>Given two videos which will be used for foreground V f and background V b , we use a standard object segmentation network (MaskRCNN <ref type="bibr" target="#b13">[14]</ref>) to segment out people and objects in every frame of both videos. Objects categories in action datasets are not completely contained in the image dataset COCO <ref type="bibr" target="#b25">[26]</ref>, which is used for training MaskRCNN. However, we observe that object detections with high confidence tend to correspond to actual objects, even if the category is not correct (boxing bag is often classified as fire hydrant), and therefore are useful to our purpose. We could also have selected only the humans in the video, as action categories tend to be focused on humans. However, we find that the presence of specific objects is highly correlated with action categories (musical instruments in the classes "playing guitar" or "playing violin"). Therefore removing the original objects from the background and adding the ones from the foreground is essential for recognition. See numerical results of the impact of these decisions in the ablation study of Sec. 5.3.</p><p>We remove the segmented objects from the background video and fill in the holes using image inpainting <ref type="bibr" target="#b26">[27]</ref>, to obtain a clean background video V ? b . Finally, we combine the foreground objects and the background at each frame by simple composition, as in:</p><formula xml:id="formula_0">V t = V t f ? M t f + V ? t b ? (1 ? M t f ),<label>(1)</label></formula><p>where V t is the resulting composited frame at time t, V t f and V ? t b are frames of the foreground and background videos respectively, M t f is the binary mask with the union of all detected objects, and ? is the element-wise multiplication. <ref type="figure" target="#fig_1">Figure 4</ref> shows sample frames of the resulting videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimization of Learn2Augment</head><p>The optimization of the proposed Learn2Augment method has two stages. In the first stage, we train the Selector network using RL, as described in Sec. 4.1.</p><p>Once the Selector network is trained, in the second stage, we perform data augmentation to train the classifier. That is, we sample pairs of videos, pass them through the trained Selector, choose the pairs with high score, create new videos with these pairs through Video Compositing, and add them to the training set. We now describe the details of these two training stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training the Selector</head><p>As mentioned before, there is no ground truth to tell us how good an augmented data sample is. Instead, we use the validation loss of the classification network to train the Selector network. This function is not differentiable with respect to the parameters of the Selector. A common solution to dealing with this is to use RL <ref type="bibr" target="#b35">[36]</ref>.</p><p>Specifically, the state s t at time t is the batch of video pairs sampled using SM. The action a t is the subset of these video pairs selected for compositing and is represented as a vector of values between 0 and 1. The environment is the classification network and the validation process. This environment is used to compute a reward R(?) for choosing a particular action, where ? are the parameters of the Selector.</p><p>We calculate the reward in a single step, as the difference between the loss in the current batch and the moving average of losses in the previous S steps (where S = 5) denoted as ?, as in Eq. 2:</p><formula xml:id="formula_1">R(?) = ? ? 1 |D val | |D val | i=1 L cls (f ? (V i ), y i ) ? ? ? ?<label>(2)</label></formula><p>where L cls is the classification cross-entropy loss, f ? is the classifier network of parameters ?, V i and y i are an input video and its label respectively, D val is the validation set and |D val | is the number of samples in D val . The objective function that we want to maximize is the expected value of the reward:</p><formula xml:id="formula_2">J(?) = E(R(?)).<label>(3)</label></formula><p>To find the optimal policy, we would typically differentiate the objective function with respect to the parameters ?. However, the reward function is dependent on the validation loss, calculated with the classifier network, which does not involve ?. Instead, using REINFORCE <ref type="bibr" target="#b34">[35]</ref>, we approximate the objective function as:</p><formula xml:id="formula_3">? ? J(?) ? 1 M M i=1 R ? i (?) T ?1 t=0 ? ? log ? ? (a t i |s t i ) ,<label>(4)</label></formula><p>where, ? i is the i th state-action trajectory under the policy ? ? , M is the number of sample trajectories and T is the number of actions performed in a trajectory. Note that as we have single-step episodes, we can make several simplifications as M = 1, as T = 1, and as there is only one trajectory ? i , and thus R ? i (?) is just R(?). With these simplifications and substituting Eq. 3 in Eq. 4, we obtain:</p><formula xml:id="formula_4">? ? J(?) ? R(?)? ? log ? ? (D M |D B ),<label>(5)</label></formula><p>where D M corresponds to the subset of pairs of samples to composite and D B to all the pairs of samples in the batch. The Selector is updated by ?? ? J(?) where ? is the learning rate and ? is updated with the last calculated loss as seen in Eq 6.</p><formula xml:id="formula_5">? t = S ? 1 S ? t?1 + 1 |D val | |D val | i=1 (L cls (f ? (V i ), y i )) .<label>(6)</label></formula><p>Note that this training process does involve generating the composited videos for pairs in D M , to input to the classifier and compute the loss. However, crucially, during training this is a small portion (one order of magnitude smaller) of how many videos would need to be generated if we were to composite all pairs of videos.</p><p>Once the Selector is trained, we use it for actually filtering good pairs. At that point, given two videos and their labels, the Selector network predicts a policy ? of how likely it is to select the pair. The score ? is the value of ? for each pair. We use a threshold on that score to select the pairs of videos to augment.</p><p>In our experiments, we first determine a budget on the number of videos that we want to augment, and then pick the threshold to select the top-ranked video pairs. We use these selected pairs of videos as input to Video Compositing, add them to the training set, and use them to train the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training the Classifier</head><p>Similar to previous work which combines multiple samples for augmentation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref>, composited/mixed samples should include mixed labels. We adopt the strategy of Cutmix <ref type="bibr" target="#b36">[37]</ref>, where the foreground label y f and the background label y b are combined using a ratio ?, as:?</p><formula xml:id="formula_6">= ?y f + (1 ? ?)y b ,<label>(7)</label></formula><p>to obtain the mixed label?. A simple way to choose ? is to use the ratio of the foreground mask with respect to the overall video. Given the foreground video V f of dimensions T ? H ? W , and mask at each frame M f , the foreground ratio would be ? = M f /(T HW ). Instead of choosing ? to be directly proportional to the foreground ratio ?, we give slightly more weight to the foreground <ref type="bibr" target="#b42">[43]</ref>, as in Eq. 8, where ? = 4.</p><formula xml:id="formula_7">? = ?(? ? 1) ? + 1, ? ? [0, 1]<label>(8)</label></formula><p>We add composited videos V , and their mixed labels? to the training set, and train the classifier network using a standard cross-entropy loss, with stochastic gradient descent.</p><p>The choice of classifier is not tied to our method. In our experiments, we choose the widely used 3D ResNet-18 architecture, which allows us to compare directly to other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experiment extensively with Learn2Augment using three data settings, four datasets, and two splits. We also present ablation studies. In this section we first describe the details of the experiments and then discuss our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Details</head><p>Datasets. In order to provide comparison to prior work (e.g. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b29">30]</ref>), we use standard datasets for evaluation in action recognition, including HMDB51 <ref type="bibr" target="#b20">[21]</ref>, UCF101 <ref type="bibr" target="#b31">[32]</ref>, Kinetics-400 <ref type="bibr" target="#b3">[4]</ref>, and Kinetics-100, which includes the 100 classes with the largest amount of samples in Kinetics, as it is used in prior work <ref type="bibr" target="#b17">[18]</ref> and helps us compare directly. For experiments on the effect of pre-training the Selector, we use Kinetics-400. For the semi-supervised setting, we split the datasets following the protocol of VideoSSL <ref type="bibr" target="#b17">[18]</ref> and ActorCut <ref type="bibr" target="#b42">[43]</ref>. For few-shot we use the standard split <ref type="bibr" target="#b39">[40]</ref> and the Truze split <ref type="bibr" target="#b11">[12]</ref> which ensures no overlap of novel classes with Kinetics-400.</p><p>Problem Settings. We test the proposed method in three different settings. In the semi-supervised setting, a portion of the training set is artificially held out, and the rest of the training data is assumed to be available, but unlabeled. Tests are performed on different percentages of held out data. In the few-shot setting, some classes (novel classes) are assumed to have a very small number of training samples (one to five instances), while other classes have the full number of samples (seen classes). We effectively change the n-shot learning problem to a n + k-shot problem where k is the number of augmented samples. Finally, in the standard full set setting, all training data is available.</p><p>Training Settings. We use mini-batch stochastic gradient descent, with momentum of 0.9 and weight decay 0.001. For each video, we use an 8-frame clip, where the frames are uniformly sampled. We use batch size of 8. For UCF101 and Ki-netics100 in the SSL setting, we train the model for 400 epochs and for HMDB51, we train for 500 epochs. The initial learning rate is set to 0.1 and then decayed using cosine annealing policy. For the SSL setting, we use the data split proposed in VideoSSL <ref type="bibr" target="#b17">[18]</ref>. For the few-shot setting, we use the default hyperparameters of TRX <ref type="bibr" target="#b28">[29]</ref>, ARN <ref type="bibr" target="#b39">[40]</ref> and C3D-PN <ref type="bibr" target="#b30">[31]</ref>, respectively. In the fully supervised setting, we train R(2+1)D for 100 epochs on UCF101, HMDB51 and 50 epochs on Kinetics-400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Architectural Changes for Different Settings</head><p>We briefly explain the structural adaptations of our approach for each of the settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairs</head><p>Video Semantic Accuracy #Videos Selector Compositing Matching in % (S) <ref type="table" target="#tab_0">Table 1</ref>. Ablation study to explore the impact of each proposed component. All settings use the same number of samples for training, so that they can be compared fairly. The # Videos (S) corresponds to the search space in each scenario. As we can see, we obtain the best accuracy using just 12K instead of the standard scenario which would have had 10.4M i.e. a reduction of over 1000x.</p><formula xml:id="formula_8">? ? ? 58.9 12K ? ? ? 55.8 99K ? ? ? 54.5 12K ? ? ? 55.2 (1.2M) ? ? ? 52.9 (1.2M) ? ? ? 48.6 (10.4M) ? ? ? 50.8 99K ? ? ? 45.5 (10.4M)</formula><p>Semi-supervised Learning. Similar to VideoSSL <ref type="bibr" target="#b17">[18]</ref>, we first train the classifier on the available labeled data using the categorical cross-entropy loss. Once this network is trained, we do a forward pass of the unlabeled examples and assign pseudo-labels to those samples with high confidence. We use these pseudo-labels as additional data for augmentation. We also add a knowledge distillation loss inspired by VideoSSL <ref type="bibr" target="#b17">[18]</ref>. Details can be found in the supplementary material.</p><p>Few-shot Learning. We only augment the novel classes using Learn2Augment. We also do not perform label mixing and simply use the foreground label for the augmented sample. This incorporates our composited samples seamlessly into the meta-learning framework typically followed. We show results on the standard split, as on the recently proposed TruZe <ref type="bibr" target="#b11">[12]</ref>. TruZe ensures that the novel classes do not overlap with Kinetics-400.</p><p>Fully-supervised Learning. This is the simplest setting, where the Selector is trained on the full training set, and used for data augmentation to train the classifier. We explore two scenarios: training the classifier from scratch and using a model pre-trained on Sports1M <ref type="bibr" target="#b18">[19]</ref>.  <ref type="table">Table 2</ref>. Ablation study of compositing components. The version "w/o Inpaint" refers to pasting the foreground without first filling in the holes of removed objects in the background. The version "w/o Segmentation" refers to using bounding boxes instead of object segmentations. "w/o Objects" refers to copying and pasting only the humans in the scene, leaving the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>for each component, the combination of all three gives the best results. Further analysis can be found in the supplementary material.</p><p>The Video Compositing module also has multiple components. In <ref type="table">Table 2</ref>, we ablate these components and observe that removing objects is actually essential, and has the most significant impact, followed by using segmentation instead of a bounding box, and finally inpainting.</p><p>Although the compositing process is more computationally expensive than previous simpler mixing strategies, it is important to note that 1) the overall accuracy indeed improves, 2) the actual composition for training the classifier is done on a small subset of pairs of videos and 3) the Selector can be trained on a large dataset (e.g.: Kinetics) just once and can be reused for the smaller datasets without the need of fine-tuning (see <ref type="table" target="#tab_2">Table 3</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Augmenting in the Semi-supervised Setting</head><p>In this setting we artificially hold out a portion of the training set, with the goal of observing the behavior of different methods as the size of the training set changes.</p><p>In this setting, we use the remaining part of the dataset by producing pseudolabels, similar to VideoSSL <ref type="bibr" target="#b17">[18]</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows results in this semi-supervised setting. The L2A version of the method uses a Selector and a classifier trained only on the target dataset (in this case UCF101, HMDB51 or Kinetics-100). We observe that Learn2Augment improves on all settings over all previous methods.</p><p>The "L2A +Pre-training" row refers to Learn2Augment where the Selector has been pre-trained on Kinetics-400, without fine-tuning on the target dataset. We make two observations: First that pre-training on a large dataset helps, as the results from the pre-trained model are higher for all datasets and settings. Second that the Selector trained on Kinetics generalizes quite well to the smaller datasets without the need for fine-tuning. We do not test on Kinetics-100 with the pre-trained model, as this would mix training and testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Augmenting in the Few-shot Setting</head><p>We also explore the impact of the proposed method on the more extreme fewshot setting, where there are only a few examples per class. This is interesting because few-shot methods are already designed to address data scarcity.</p><p>We compare with the current state of the art in this setting, including CD3-PN <ref type="bibr" target="#b30">[31]</ref>, ARN <ref type="bibr" target="#b39">[40]</ref> and TRX <ref type="bibr" target="#b28">[29]</ref>, on the UCF101 and HMDB51 datasets. We observe that the proposed Learn2Augment method improves upon all existing approaches, suggesting data augmentation is complementary to few-shot methods. <ref type="table" target="#tab_3">Table 4</ref> shows the results of the experiments.  <ref type="table">Table 5</ref>. Augmenting standard datasets improves classification even with a model pre-trained on the largest existing dataset (Sports1M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Augmenting the Full Training Set</head><p>We finally explore the effect of augmenting the full dataset, both for smaller datasets, and the large-scale Kinetics. Results can be found on <ref type="table">Table 5</ref>. Again, Learn2Augment improves the performance on all datasets even for a pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Why Not Intra-class Augmentation?</head><p>One other possibility we explored is intra-class augmentation instead of using semantic classes. However, when we followed the same procedure on 20% labeled data of UCF101 we obtain an accuracy of 41.4% in comparison to 58.9% when using semantically similar classes. Similarly, in Kinetics100 we obtain an accuracy of 50.1% and 54.4% using 5% and 10% labeled data respectively. That is 9.4% and 8.9% lower than the results using semantic neighbors. We believe there to be two main concerns in intra-class augmentation. The first is that Cutmix <ref type="bibr" target="#b36">[37]</ref> has been shown to be an excellent regularization technique. This is aided by having samples that have soft labels (since they are a ratio of samples from different classes). However, using intra-class augmentation would force the labels to be the same as the ground truth class. The second reason is that samples of a particular class are clips that were part of the same video. This is the case in both HMDB51 and UCF101 and not so in Kinetics100. If we cut the background from one sample and paste the foreground onto this, it results in an identical sample to the original foreground sample. This is because the background is the same in both cases. All we end up doing then is training the model on multiple instances of the same data which leads to overfitting and hence a poor accuracy at test time. However, since the results are much worse for Kinetics100 as well, we believe that this could be a smaller contributing factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Distillation Loss for Semi-Supervised Learning (SSL)</head><p>Given frame a from video v, to distill appearance information of objects of interest, we use the softmax predictions of a ResNet <ref type="bibr" target="#b14">[15]</ref> image classifier. This network is pre-trained on Imagenet and not modified during training. Let the output of the ResNet be denoted as h(a) ? R M where M = 1000 which is the number of classes in Imagenet. We randomly select a frame from all videos (labeled, unlabeled and augmented) for training. The classifier model in our architecture, produces an embedding q(v) ? R M which is of the same dimensions and space of h(a). We train q(v) to match the output of h(a) by using a soft cross-entropy loss that treats the ResNet outputs as soft labels. This loss L d can be seen in Eq. 9. Our final loss function is a combination of L d and L s (categorical cross-entropy loss for video samples). This is done following the work in VideoSSL <ref type="bibr" target="#b17">[18]</ref>.</p><formula xml:id="formula_9">L d = ? v?(X?Z),a?v h(a) log (q(v))<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis of Number of Augmented Samples</head><p>We see a common pattern when adding augmented samples to the different SSL settings. This basically refers to increasing the number of augmented samples in the training set. We see that the accuracy increases initially, reaches a peak performance and then starts dropping slowly as can be seen in <ref type="figure" target="#fig_2">Figure 5</ref>. This makes sense as we don't expect every mixed example to be helpful for training. In fact, this helps us to define ? i for the selector. We can see <ref type="figure" target="#fig_2">Figure 5</ref> for the results from 0 augmentations to 5000 for 10% and 20% labeled data on UCF101. The sweet spot for the 10% labeled data is around 1200 augmentations and for the 20% labeled data is around 2000 augmentations. Both of which are obtained using ? i = 0.6. We decide the value of ? i based on these and results and use the same for HMDB51 and Kinetics100 for all settings. If we increase the value of ? i we obtain fewer samples and decreasing the value of ? i results in more number of samples for training. The value of ? i thus determines the number of augmented samples and also their quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Other Selector Choices</head><p>The design of the selector is a crucial aspect of our model. We want the selector to be able to learn what makes a good pair of videos for mixing without actually having to mix every single pair. However, for lower percentages of labeled data, we can generate all possible samples of semantic classes and convert a state-ofthe art frame selection model (SMART) <ref type="bibr" target="#b10">[11]</ref> to do sample importance instead of frame importance. We also consider a simple baseline of using a discriminator network to pick only realistic samples. We report the results in <ref type="table" target="#tab_5">Table 6</ref>. Another Results are for 10% and 20% of labeled data UCF101. We see that the performance increases initially, reaches a peak and slowly starts dropping.</p><p>approach was to randomly pick a certain amount of samples to train the classifier network.</p><p>We not only outperform all alternative approaches, we also do this by saving on both memory and computation cost. For example, in the 20 percent setting, SMART sees 99K videos and these 99k videos have to be precomputed and stored before training SMART. However, the proposed approach only needs 12K videos and outperforms SMART by up to 1.4%. This analysis is only to show a comparison to possible alternatives when storing data is feasible. The idea of trying these alternatives is only feasible in low percentage labeled data of small datasets like UCF101 and HMDB51. Even 50% labeled data in UCF101, results in having to mix over 400k videos while large scale datasets like Kinetics400 would lead to millions of mixes being needed making it practically unfeasible.  10 Why Re-train the Classifier Network?</p><p>Here, we are talking about the classifier network in our proposed architecture that the selector learns from (based on the validation loss). Training the Selector and the Classifier together is also possible. But we decide against this for 2 reasons. First, and the most important reason is that we want to save out on computational cost needed to generate an augmented sample. We showed that the selector network looks only at a fraction of samples before it understands what makes a good pair. Hence, we first train the selector by generating augmented samples taken from random samples of semantically similar classes. Once the selector is trained, we don't need to generate the mixed sample for all possible pairs and only generate the mixed samples for good pairs (the selector need not have seen these pairs before). We then augment the original dataset by samples that the selector believes will improve the classifiers performance We compare the performance of the joint training and re-training of the classifier network in <ref type="table" target="#tab_6">Table 7</ref>. We see that re-training the classifier network always yields the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Examples of Selected and Discarded Samples</head><p>To understand what made a good sample we visualize a few samples that were selected by the selector model and a few samples that were discarded. These can be seen in <ref type="figure" target="#fig_3">Figure 6</ref>. The samples are displayed as 4 frames for better visualization. Based on the small subset of examples seen, we believe that for good pairs to be selected some of the criteria could be coherent inpainting, similar camera movement, not too drastic a background change. We see some samples of discarded examples in <ref type="figure" target="#fig_4">Figure 7</ref>. Based on the small subset of examples seen we think possible bad pairs are due to bad video compositing (example 2 in <ref type="figure" target="#fig_4">Figure 7</ref>), varying camera movements (example 3 in <ref type="figure" target="#fig_4">Figure 7</ref>) or a drastic change in background (example 1 in <ref type="figure" target="#fig_4">Figure 7)</ref>. These are however based on the few examples we see.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Effect of Semantic Match in generalization ability.</head><p>We test the generalization ability of the semantic matching by comparing it with random matching which would correspond to row 4 of <ref type="table" target="#tab_0">Table 1</ref> in the main  paper. We observe that the performance does decrease. To strengthen this test, we tried the same experiment in the FSL setting, which is an extreme case for generalization. We augment data for two different methods, using the proposed L2A, using both semantic and random matching of classes. We observe that even in this setting, which is the most susceptible to overfitting, the semantic matching outperforms random matching. We will add this to the final version. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Limitations and Future Work</head><p>The main area of improvement is the time needed for training. Optimizing the Selector with RL is time-consuming, and so is compositing the initial samples for training it. Future work could address this by parameterizing the composition process and learn these parameters instead of compositing the pairs directly. It could also learn to select particular frames in a video, and avoid the computational cost of temporal redundancy. Finally, another possible direction is to learn what samples to discard from the initial dataset itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">Conclusion</head><p>While standard data augmentation strategies in action recognition are handcrafted, we propose to learn which pairs of videos are good to composite. In order to do this, our approach leverages three components. We train a Selector optimized with RL to choose which pairs of videos are good to composite. We reduce the search space by using samples from semantically similar classes. We perform a clean segmentation for mixing samples and remove actors as well as objects from foreground and background samples. With this, we obtain stateof-the-art results in semi-supervised and few-shot action recognition settings, and improve in the fully supervised setting. In particular, we see gains of up to 8.6% and 3.7% in the semi-supervised and few-shot settings. We also see an improvement of up to 17.4% when compared to standard augmentation in the fully supervised setting when training from scratch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the proposed Learn2Augment. Given a pair of videos and their labels, a Selector network gives a score ? of the quality of the potential composited video. At training time, the Selector is trained with the validation loss of the classification network. Once the Selector is trained, pairs of videos are sampled, and only the promising combinations with high score ? are composited and used for training the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Sample frames of rendered videos. While the segmentation contains errors, such as missing limbs or portions of the object, the action category remains clear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of performance with increasing number of augmented samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Visualizing selected examples. From top to bottom as (foreground, background) pairs: (flic-flac, cartwheel), (smile, laugh), (playing violin, playing cello), (front crawl, swimming backstroke). The first two are examples from HMDB51 and the last two from UCF101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Visualizing discarded examples. From top to bottom as (foreground, background) pairs: (somersault, diving), (climbing stairs, falling floor), (baby crawling, walking dog), (hammering, hammer throw).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>shows the ablation study of Learn2Augment, which illustrates the impact of each of the proposed elements in the design. The experiment is done on the UCF101 dataset, using 20% of the data i.e. in a semi-supervised setting. All three contributions (Selector, Semantic Matching and Video Compositing) improve accuracy. Crucially, Semantic Matching and the Selector also reduce greatly the number of possible video combinations, and the overall reduction is around three orders of magnitude. We see that Learn2Augment obtains a 13.4% improvement over the baseline. While there are improvements of up to 7.4%</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>L2A</cell><cell>58.9</cell></row><row><cell>L2A w/o Inpaint</cell><cell>57.6</cell></row><row><cell>L2A w/o Segmentation</cell><cell>56.8</cell></row><row><cell>L2A w/o Objects</cell><cell>55.7</cell></row><row><cell>L2A w/o All</cell><cell>54.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on the semi-supervised setting. Results for TCL and ActorCut are obtained by us running the author's code. All methods are run with a 3D ResNet-18 backbone for fair comparison. L2A +Pre-training refers to pre-training the selector and fixing it.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on UCF101 for the Few-Shot Learning setting, with different splits.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UCF101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HMDB51</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Split</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>C3D-PN [31]</cell><cell>S</cell><cell cols="10">57.1 66.4 71.7 75.5 78.2 38.1 47.5 50.3 55.6 57.4</cell></row><row><cell cols="2">C3D-PN + L2A S</cell><cell cols="10">60.8 68.9 73.3 76.6 79.1 39.8 48.9 51.5 57.3 58.2</cell></row><row><cell>ARN [40]</cell><cell>S</cell><cell cols="10">66.3 73.1 77.9 80.4 83.1 45.5 50.1 54.2 58.7 60.6</cell></row><row><cell>ARN + L2A</cell><cell>S</cell><cell cols="10">67.7 74.2 79.6 81.1 84.4 47.3 51.7 55.5 60.1 61.8</cell></row><row><cell>TRX [29]</cell><cell>S</cell><cell cols="10">77.5 88.8 92.8 94.7 96.1 50.5 62.7 66.9 73.5 75.6</cell></row><row><cell>TRX + L2A</cell><cell>S</cell><cell cols="10">79.2 89.2 93.2 95.0 96.3 51.9 63.8 68.2 74.4 77.0</cell></row><row><cell>C3D-PN [31]</cell><cell>T</cell><cell cols="10">50.9 61.9 67.5 72.9 75.4 28.8 38.5 43.4 46.7 49.1</cell></row><row><cell cols="2">C3D-PN + L2A T</cell><cell cols="10">52.5 63.8 70.1 75.2 78.2 29.9 40.1 44.5 47.7 50.8</cell></row><row><cell>ARN [40]</cell><cell>T</cell><cell cols="10">61.2 70.7 75.2 78.8 80.2 31.9 42.3 46.5 49.8 53.2</cell></row><row><cell>ARN + L2A</cell><cell>T</cell><cell cols="10">63.9 73.1 77.4 80.4 81.3 33.6 43.7 48.0 51.1 53.8</cell></row><row><cell>TRX [29]</cell><cell>T</cell><cell cols="10">75.2 88.1 91.5 93.1 93.5 33.5 46.7 49.8 57.9 61.5</cell></row><row><cell>TRX + L2A</cell><cell>T</cell><cell cols="10">76.8 88.9 92.7 93.8 94.1 35.0 48.1 51.1 59.2 62.1</cell></row><row><cell cols="12">Accuracies are reported for 5-way, 1, 2, 3, 4, 5-shot classification. S corresponds to the</cell></row><row><cell cols="12">split used in [40,29] and T is the TruZe split [12], which avoids overlapping classes with</cell></row><row><cell>Kinetics.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>39K 60.3 12K 56.1 5.2K 48.0 1.2K</figDesc><table><row><cell></cell><cell>50%</cell><cell>20%</cell><cell>10%</cell><cell>5%</cell></row><row><cell>Method</cell><cell>Acc SS</cell><cell>Acc SS</cell><cell>Acc SS</cell><cell>Acc SS</cell></row><row><cell>Random</cell><cell cols="4">61.9 430K 56.2 99K 51.8 44K 42.3 9.7K</cell></row><row><cell cols="5">Discriminator 62.8 430K 57.3 99K 52.2 44K 41.1 9.7K</cell></row><row><cell cols="5">SMART [11] 68.9 430K 58.9 99K 57.8 44K 46.5 9.7K</cell></row><row><cell>Proposed</cell><cell>72.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison of approaches for the use of Selector. All results are reported on UCF101. 'Acc' corresponds to accuracy and 'SS' corresponds to the number of mixed videos that the Selector looks at. All results are on different percentage of labeled data in UCF101.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of jointly training classifier and re-training it. We see that there is a consistent large improvement in re-training the classifier.</figDesc><table><row><cell>Method</cell><cell>50% 20% 10% 5%</cell></row><row><cell cols="2">Jointly trained 66.5 57.4 53.1 44.7</cell></row><row><cell>Retrained</cell><cell>72.1 60.3 56.1 48.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Results on FSL using the proposed Semantic Matching vs random matching using the TruZe<ref type="bibr" target="#b11">[12]</ref> split.</figDesc><table><row><cell cols="5">Method Class Matching 1-shot 3-shot 5-shot</cell></row><row><cell cols="2">C3D-PN Random</cell><cell>28.1</cell><cell>42.9</cell><cell>47.7</cell></row><row><cell cols="2">C3D-PN Semantic</cell><cell>29.9</cell><cell>44.5</cell><cell>50.8</cell></row><row><cell>TRX</cell><cell>Random</cell><cell>33.5</cell><cell>49.9</cell><cell>60.3</cell></row><row><cell>TRX</cell><cell>Semantic</cell><cell>35.0</cell><cell>51.1</cell><cell>62.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudolabeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Why can&apos;t i dance in the mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18613" to="18624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dataset augmentation in feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR (Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smart frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16235" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1451" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new split for evaluating true zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd DAGM German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAP</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="281" to="296" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What makes a video a video: Analyzing temporal information in video understanding models and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00769</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00769" />
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7366" to="7375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification of human actions based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1336" to="1341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Videossl: Semi-supervised learning for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1110" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Featmatch: Feature-based augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="479" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Smart augmentation learning an optimal data augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5858" to="5869" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018 -Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Temporalrelational crosstransformers for few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06184</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised action recognition with temporal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10389" to="10399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<title level="m">Prototypical networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Autoflow: Learning a better training set for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data valuation using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10842" to="10851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03457</idno>
		<title level="m">Videomix: Rethinking data augmentation for video classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Few-shot action recognition with permutation-invariant attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-paced video data augmentation by generative adversarial networks with insufficient samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3414003</idno>
		<ptr target="https://doi.org/10.1145/3394171.3414003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia. p. 1652-1660. MM &apos;20</title>
		<meeting>the 28th ACM International Conference on Multimedia. p. 1652-1660. MM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning representational invariances for data-efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2103.16565</idno>
		<ptr target="https://arxiv.org/abs/2103.16565" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
