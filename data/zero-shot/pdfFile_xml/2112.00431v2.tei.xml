<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Le?n Alc?zar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sacaba@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent and increasing interest in video-language research has driven the development of large-scale datasets that enable data-intensive machine learning techniques. In comparison, limited effort has been made at assessing the fitness of these datasets for the video-language grounding task. Recent works have begun to discover significant limitations in these datasets, suggesting that state-ofthe-art techniques commonly overfit to hidden dataset biases. In this work, we present MAD (Movie Audio Descriptions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annotations and focuses on crawling and aligning available audio descriptions of mainstream movies. MAD contains over 384, 000 natural language sentences grounded in over 1, 200 hours of videos and exhibits a significant reduction in the currently diagnosed biases for video-language grounding datasets. MAD's collection strategy enables a novel and more challenging version of video-language grounding, where short temporal moments (typically seconds long) must be accurately grounded in diverse long-form videos that can last up to three hours. We have released MAD's data and baselines code at https://github.com/ Soldelli/MAD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Imagine you want to find the moment in time, in a movie, when your favorite actress is eating a Gumbo dish in a New Orleans restaurant. You could do so by manually scrubbing the film to ground the moment. However, such a process is tedious and labor-intensive. This task is known as natural language grounding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>, and has gained significant momentum in the computer vision community. Beyond smart browsing of movies, the interest in this task stems from multiple real-world applications ranging from smart video search <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, video editing <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>, and helping pa- The circle size measures the language vocabulary diversity. The videos in MAD are orders of magnitude longer in duration than previous datasets (?110min), annotated with natural, highly descriptive, language grounding (&gt;60K unique words) with very low coverage in video (?4.1s). Coverage is defined as the average % duration of moments with respect to the total video duration.</p><p>tients with memory dysfunction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. The importance of solving this task has resulted in novel approaches and largescale deep-learning architectures that steadily push state-ofthe-art performance.</p><p>Despite those advances, recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> have diagnosed hidden biases in the most common video-language grounding datasets. Otani et al. <ref type="bibr" target="#b17">[18]</ref> have highlighted that several grounding methods only learn location priors, to the point of disregarding the visual information and only using language cues for predictions. While recent methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref> have tried to circumvent these limitations by either proposing new metrics <ref type="bibr" target="#b35">[36]</ref> or debiasing strategies <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>, it is still unclear if existing grounding datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref> provide the right setup to evalu- "Bearded dad is carrying his other sick son. Mum seems alone with her thoughts and the daughter is in complete silence." 00:38:36 00: <ref type="bibr">38:42</ref> "Mum hangs up the laundry outside the farmhouse."</p><p>Temporal Grounding ? ? <ref type="figure">Figure 2</ref>. Example from our MAD dataset. We select the movie "A quiet place" as representative for our dataset. As shown in the figure, the movie contains a large number of densely distributed temporally grounded sentences. The collected annotations can be very descriptive, mentioning people, actions, locations, and other additional information. Note that as per the movies plot, the characters are silent for the vast majority of the movie, rendering audio description essential for visually-impaired audience.</p><p>ate progress in this important task. This is partly because datasets used by video-language grounding models were not originally collected to solve this task. These datasets provide valuable video-language pairs for captioning or retrieval, but the grounding task requires high-quality (and dense) temporal localization of the language. <ref type="figure" target="#fig_0">Figure 1</ref> shows that the current datasets comprise relatively short videos, containing single structured scenes, and language descriptions that cover most of the video. Furthermore, the temporal anchors for the language are temporally biased, leading to methods not learning from any visual features and eventually overfitting to temporal priors for specific actions, thus limiting their generalization capabilities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. In this work, we address these limitations with a novel large-scale dataset called MAD (Movie Audio Descriptions). MAD builds atop (and includes part of) the LSMDC dataset <ref type="bibr" target="#b24">[25]</ref>, which is a pioneering work in leveraging audio descriptions to enable the investigation of a closely related task: text-to-video retrieval. Similar to LSMDC, we depart from the standard annotation pipelines that rely on crowd-sourced annotation platforms. Instead, we adopt a scalable data collection strategy that leverages professional, grounded audio descriptions of movies for visually impaired audiences. As shown in <ref type="figure">Figure 2</ref>, MAD grounds these audio descriptions on long-form videos, bringing novel challenges to the video-language grounding task.</p><p>Our data collection approach consists of transcribing the audio description track of a movie and automatically detecting and removing sentences associated with the actor's speech, yielding an authentic "untrimmed video" setup where the highly descriptive sentences are grounded in long-form videos. <ref type="figure" target="#fig_0">Figures 1 and 3</ref> illustrate the uniqueness of our dataset with respect to the current alternatives.</p><p>As showcased in <ref type="figure">Figure 2</ref>, MAD contains videos that, on average, span over 110 minutes, as well as grounded annotations covering short time segments, which are uniformly distributed in the video, and maintain the largest diversity in vocabulary. Video grounding in MAD requires a finer understanding of the video, since the average coverage of the sentences is much smaller than in current datasets.</p><p>The unique configuration of the MAD dataset introduces exciting challenges. First, the video grounding task is now mapped into the domain of long form video, preventing current methods to learn trivial temporal location priors, instead requiring a more nuanced understanding of the video and language modalities. Second, having longer videos means producing a larger number of segment proposals, which will make the localization problem far more challenging. Last, these longer sequences emphasize the necessity for efficient methods in inference and training, mandatory for real-world applications such as long live streaming videos or moment retrieval in large video collections <ref type="bibr" target="#b3">[4]</ref>.</p><p>Contributions. Our contributions are threefold. (1) We propose Movie Audio Description (MAD), a novel largescale dataset for video-language grounding, containing more than 384K natural language sentences anchored on more than 1.2K hours of video. (2) We design a scalable data collection pipeline that automatically extracts highly valuable video-language grounding annotations, leveraging speech-to-text translation on professionally generated audio descriptions. (3) We provide a comprehensive empirical study that highlights the benefits of our large-scale MAD dataset on video-language grounding as a benchmark, pointing out the difficulties faced by current video-language grounding baselines in long-form videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Video Grounding Benchmarks. Most of the current video grounding datasets were previously collected and tailored for other computer vision tasks (i.e., Temporal Activity Localization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>) and purposes (i.e., Human Action Recognition), then annotated for the video grounding task. This adaptation limits the diversity of the video corpus towards a specific set of actions and objects, and its corresponding natural language sentences to specific sets of verbs and nouns <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>. Currently, ActivityNet-Captions <ref type="bibr" target="#b7">[8]</ref> and Charades-STA <ref type="bibr" target="#b4">[5]</ref> are the most commonly used benchmarks for the task of video grounding. Both datasets have been collected atop pre-existing video datasets (ActivityNet <ref type="bibr" target="#b5">[6]</ref> and Charades <ref type="bibr" target="#b25">[26]</ref>) and have been diagnosed with severe biases by Otani et al. <ref type="bibr" target="#b17">[18]</ref>. These findings show that the annotations contain distinct biases where language tokens are often coupled with specific temporal locations. Moreover, strong priors also affect the temporal endpoints, with a large portion of the annotations spanning the entire video. As a consequence, current methods seem to mainly rely on such biases to make predictions, often disregarding the visual input altogether. In comparison, the unique setup of MAD prevents these drawbacks as keywords are not associated with particular temporal regions, and annotation timestamps are much shorter than the video's duration. Moreover, different from Charades-STA, MAD defines an official validation set for hyper-parameter tuning.</p><p>Unlike Charades-STA and ActivityNet-Captions, TACoS <ref type="bibr" target="#b21">[22]</ref> has not been diagnosed with annotation biases. However, its video corpus is small and limited to cooking actions recorded in a static-camera setting. Conversely, spanning over 22 genres across 90 years of cinema history, MAD covers a broad domain of actions, locations, and scenes. Moreover, MAD inherits a diverse set of visual and linguistic content from the broad movie genres, ranging from fiction to everyday life. Furthermore, DiDeMo <ref type="bibr" target="#b0">[1]</ref> was annotated atop Flickr videos with a discrete annotation scheme (i.e. in chunks of 5 seconds) for a maximum of 30 seconds, constraining the problem of video grounding to trimmed videos. Given these annotations, the grounding task can be simplified to choosing one out of 21 possible proposals for each video. Conversely, MAD provides a setup to explore solutions for grounding language in long-form videos, whose length can be up to 3 hours. In this scenario, naive sliding window techniques for proposal generation could produce hundreds of thousands of possible candidates. Therefore, developing efficient inference methods becomes a much more urgent requirement compared to previous benchmarks.</p><p>State-of-the-art video grounding methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> have relied on existing benchmarks to design novel modules (i.e., proposal generation, context modeling, and multi-modality fusion). However, most of these designs specifically target grounding in short videos and often rely on providing the entire video to the model when making a prediction. As the long-form setup introduced by MAD prohibits this, new methods will have the opportunity to investigate and bridge previous ideas to these new challenging and real-world constraints. Audio Descriptions. The pioneering works of Rohrbach et al. <ref type="bibr" target="#b23">[24]</ref> and Torabi et al. <ref type="bibr" target="#b30">[31]</ref> were the first to exploit audio descriptions to study the text-to-video retrieval task and its counterpart video-to-text. Rohrbach et al. <ref type="bibr" target="#b23">[24]</ref> introduced the MPII-MD dataset, while Torabi et al. <ref type="bibr" target="#b30">[31]</ref> presented M-VAD, both collected from audio descriptions in movies. Later, these datasets were fused to create the LSMDC dataset <ref type="bibr" target="#b24">[25]</ref>, which forms the core of the LSMDC annual challenge. Our annotation pipeline is similar in spirit to these works. However, MAD sizes the potential of movies' audio descriptions for a new scenario: language grounding in videos.</p><p>A concurrent work introduced a new benchmark based on audio descriptions in videos, called QuerYD <ref type="bibr" target="#b16">[17]</ref>. It is a dataset for retrieval and event localization in videos crawled from YouTube. This benchmark focuses on the short-form video setup with videos of less than 5 minutes average duration. QuerYD also leverages audio descriptions, which have been outsourced to volunteer narrators. Similar to our takeaways, the authors noticed that audio descriptions are generally more visually grounded and descriptive than previously collected annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Collecting the MAD Dataset</head><p>In this section, we outline MAD's data collection pipeline. We follow independent strategies for creating the training and testing set. For the former, we aim at automatically collecting a large set of annotations. For the latter, we re-purpose the manually refined annotations in LSMDC. We provide detailed statistics of MAD's annotations and compare them to existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MAD Training set</head><p>MAD relies on audio descriptions professionally created to make movies accessible to visually-impaired audiences. These descriptions embody a rich narrative describing the most relevant visual information. Thus, they adopt a highly descriptive and diverse language. Audio descriptions are often available as an alternative audio track that can replace the original one. Professional narrators curate them, and devote significant effort to describe a movie. The audio description process demands an average of 30 work hours to narrate a single hour of video <ref type="bibr" target="#b24">[25]</ref>. In comparison, previous datasets that have used the Amazon Mechanical Turk service for video-language grounding estimate the annotation effort to be around 3 hours for each video hour <ref type="bibr" target="#b7">[8]</ref>. Data Crawling. Not every commercially available movie is released with audio descriptions. However, we can obtain these audio descriptions from 3 rd party creators. In particular, we crawl our audio descriptions from a large opensource and online repository 1 . These audio files contain the original movie track mixed with the narrator's voice, carefully placed when actors are not speaking. One potential problem is that the audio descriptions can be misaligned with the original movie. Such misalignment comes either from a delay in the recording of the audio description (concerning the original movie) or from audio descriptions being created from different versions of the movie (with deleted or trimmed scenes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Videos</head><p>Alignment and Cleanup. Since the audio description track also contains the movie's original audio, we can resolve this misalignment by maximizing the cross-correlation between overlapping segments of the original audio track and the audio description track. We define the original audio signal (f ), the audio description signal (g), and the time delay (? delay ) between the two signals. The maximum of the cross-correlation function (denoted with the operator ) indicates the point in time where the signals exhibit the best alignment. As a result, the time delay ? delay between f and g is defined as follows:</p><formula xml:id="formula_0">? delay = arg max t ((f g)(t))<label>(1)</label></formula><p>To verify that our single time delay ? delay defines the best possible alignment between the audio descriptions and the original movies, we run our synchronization strategy over several (i.e., 20) uniformly distributed temporal windows. We verify that the delays estimated for all windows are consistent with each other, within a maximum range of ?0.1 seconds w.r.t the median value of the distribution of the 20 samples. We discard the movies that do not satisfy this criterion to ensure that the whole audio description track correctly aligns with the original movie's visual content.</p><p>Audio Transcriptions and Verification. After the two au-1 https://www.audiovault.net/ dio tracks are aligned, we transcribe the audio description file using Microsoft's Azure Speech-to-Text service 2 . Each recognized word is associated with a temporal timestamp. At this step in our pipeline, we have sentences temporally grounded to the original video stream. As the Speech-to-Text contains both the narrations and the actors' speech, we set out to remove the latter and only retain the audio description in textual form. To do so, we resort to the movie's original subtitles and use their timestamps as a surrogate for Voice Activity Detection (VAD). Particularly, we discard closed captions and retain subtitles associated with the actors' speech or subtitles for songs. Then, we remove from the Speech-to-Text output every sentence overlapping with the VAD temporal locations, obtaining our target audio description sentences. We post-process the text for automatic punctuation refinement using the free tool Punctuator <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From LSMDC to MAD Val/Test</head><p>As outlined in Section 3.1, the val/test sets of MAD also relies on audio descriptions for movies. Since the annotations in training are automatically generated, we decided to minimize the noise in the validation and test splits. Hence, we avoid the automatic collection of data for these sets and resort to the data made available by the LSMDC dataset <ref type="bibr" target="#b24">[25]</ref>. This dataset collected annotations from audio descriptions in movies targeting the video retrieval task. LSMDC manually refined the grammar and temporal boundaries of sentences. As a consequence, these annotations have clean language and precise temporal boundaries. We reformat a subset of the LSMDC data, adapt it for the video grounding task, and cast it as MAD's val and test sets.</p><p>LSMDC data for retrieval is made available only as video chunks, not full movies. To create data suitable for longform video language grounding, we collect 162 out of the 182 videos in LSMDC and their respective audio descriptions. Again, the full-length video data and the chunked video data provided by LSMDC might not be in sync. To  <ref type="figure">Figure 3</ref>. Histograms of moment start/end/duration in video-language grounding datasets. The plots represent the normalized (by video length) start/end histogram (a-b) and absolute duration distribution (c) for moments belonging to each of the five datasets. We notice severe biases in ActivityNet-Captions and Charades-STA, which show high peaks at the beginning and end of the videos. Conversely MAD does not show any particular preferred start/end temporal location. align a video chunk from LSMDC with our full-length movies, we follow a similar procedure as the one described in Section 3.1 for the audio alignment, but using visual information. We use CLIP <ref type="bibr" target="#b20">[21]</ref> to extract five frame-level features per second for both the video chunks from LSMDC and our full-length movie. Then, we use the maximum cross-correlation score to estimate the delay between the two. We run the alignment on 10 different window lengths and take the median value as the delay of the chunk.</p><p>Once the audiovisual data is aligned, we use the text annotations and re-define their original timestamps according to the calculated delays. This process creates full-length movies with curated grounding data for MAD's validation and test sets. In doing so, we obtain large and clean validation and test sets since LSMDC was curated by humans and has been used for years by the community. Our val/test sets evaluate video-grounding methods with more than 104K grounded phrases coming from more than 160 movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MAD Dataset Analysis</head><p>We now present MAD's statistics and compare them with legacy datasets for the task. <ref type="table" target="#tab_0">Table 1</ref> summarizes the most notable aspects of MAD regarding both the visual and language content of the dataset.</p><p>Scale and Scalability. MAD is the largest dataset in video hours and number of sentences. The training, validation, and test sets consist of 488, 50, and 112 movies, respectively. Although other datasets have a larger number of clips, MAD's videos are full movies which last 2 hours on average. In comparison, the average clip from other datasets spans just a few minutes. Overall, MAD splits contain a combined 50 days of continuous video. We highlight that our test set alone is already larger than any other video grounding dataset. We also emphasize that each movie in MAD is long and composed of several diverse scenes, making it a rich source for long-form video analysis. Also, as the cinema industry is ever-growing, we expect to periodically expand MAD with subsequent releases to fuel further innovation in this research direction.</p><p>Vocabulary Size. Besides having the largest video corpus, MAD also contains the largest and most diverse query set of any dataset. In <ref type="table" target="#tab_0">Table 1</ref>, we show that MAD contains the largest set of adjectives, nouns, and verbs among all available benchmarks. In almost every aspect, it is an order of magnitude larger. The number of sentences in MAD training, validation, and test is 280.5K, 32.1K, and 72.0K, respectively, one order of magnitude larger than the equivalent set in any other dataset. Overall, MAD contains 61.4K unique words, almost 4 times more than the 15.4K of ActivityNet-Captions <ref type="bibr" target="#b7">[8]</ref> (the highest among the other benchmarks). Finally, the average length per sentence is 12.7 words, which is similar to the other datasets.</p><p>Bias Analysis. <ref type="figure">Figure 3</ref> plots the histograms for start/end timestamps of moments in all grounding datasets. We notice clear biases in current datasets: Charades-STA <ref type="bibr" target="#b4">[5]</ref>, DiDeMo <ref type="bibr" target="#b0">[1]</ref>, and ActivityNet-Captions <ref type="bibr" target="#b7">[8]</ref>. Charades-STA and ActivityNet-Captions are characterized by tall peaks at the beginning ( <ref type="figure">Figure 3a</ref>) and end ( <ref type="figure">Figure 3b</ref>) of the video, meaning that most temporal annotations start at the video's start or finish at the video's end. This bias is learned quickly by modern machine learning algorithms, resulting in trivial groundings that span a full video. The smaller dataset TACoS also exhibits a similar bias, although it is less prominent. DiDeMo is limited by its annotation strategy, where chunks of 5 seconds are labeled up to a maximum of 30 seconds. This favors structured responses that roughly approximate the start and end points of a moment. In contrast, MAD has an almost uniform histogram. This means that moments of interest can start and end at any point in the video. We only observe a minor imbalance where the end of the movie has slightly more descriptions than the beginning. This is related to the standard structure of a film, where the main plot elements are resolved towards the end, thus creating more situations worth describing. <ref type="figure">Figure 3c</ref> plots the histograms for moment duration. MAD is characterized by shorter moments on average, having a long tail distribution  <ref type="table">Table 2</ref>. Benchmarking of grounding baselines on the MAD dataset. We report the performance of four baselines: Oracle, Random Chance, CLIP, VLG-Net, on the test split. The first two validate the choice of proposals by computing the upper bound to the performance and the random performance. CLIP and VLG-Net use visual and language features to score and rank proposals. For all experiments, we adopt the same proposal scheme as in VLG-Net <ref type="bibr" target="#b28">[29]</ref>, and use CLIP <ref type="bibr" target="#b20">[21]</ref> features for video (frames) and language embeddings.</p><p>with moments that last up to one minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We proceed with the experimental assessment of videolanguage grounding on the MAD dataset. We first describe the video grounding task in the MAD dataset along with its evaluation metrics and then report the performance of four selected baselines.</p><p>Task. Given an untrimmed video and a language query, the video-language grounding task aims to localize a temporal moment (? s , ? e ) in the video that matches the query <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Metric. Following the grounding literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>, we adopt Recall@K for IoU=? (R@K-IoU=?). Given a ranked set of video proposals, this metric measures if any of the top K ranked moments have an IoU larger than ? with the ground truth temporal endpoints. Results are averaged across all test samples. Given the long-form nature of our videos and the large amount of possible proposals, we investigate Recall@K for IoU=? with K?{1, 5, 10, 50, 100} and ??{0.1, 0.3, 0.5}. This allows us to evaluate for loose alignment (i.e. IoU=0.1) and approximate ranking (i.e., K=100), as well as tight predictions (i.e. IoU=0.5) and accurate retrieval (i.e. K=1).</p><p>Baselines. We benchmark MAD using four different grounding strategies, namely: Oracle, Random Chance, CLIP <ref type="bibr" target="#b20">[21]</ref>, and VLG-Net <ref type="bibr" target="#b28">[29]</ref>. The first two provide upper bounds and random performance for the recall metric given a predefined set of proposals. Oracle chooses the proposal with the highest IoU with the ground-truth annotation, while Random Chance chooses a random proposal with uniform probability. We also use CLIP, the pre-trained image-text architecture from <ref type="bibr" target="#b20">[21]</ref>, to extract frame-level and sentence-level features. The frame-level features for each proposal are combined using mean pooling, then we score each proposal using cosine similarity between the visual and the text features. Finally, we adopt VLG-Net <ref type="bibr" target="#b28">[29]</ref> as a representative, state-of-the-art method for the grounding task. VLG-Net leverages recent advances in Graph Convolution Networks (GCNs) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> to model individual modalities, while also enabling the aggregation of non-local and cross-modal context through graph convolutions. We use VLG-Net and adapt it to work with MAD's long-form videos. See the supplementary material for details.</p><p>Implementation Details. For the lower bound estimation using Random Chance, we average the performance over 100 independent runs. To favor a fair comparison against the CLIP baseline, we train VLG-Net using CLIP features for both modalities. We use the official VLG-Net's implementation with a clip size of 128 input frames spanning 25.6 seconds (frames are extracted at 5 fps). We train using the Adam optimizer <ref type="bibr" target="#b6">[7]</ref> with learning rate of 10 ?4 . For inference over an entire movie, we adopt a sliding window approach, where we stride the input window by 64 frames and discard highly redundant proposals through Non Maximum Suppression (NMS) with a threshold of 0.3. <ref type="table">Table 2</ref> summarizes the baseline performance on MAD. The Oracle evaluation achieves a perfect score across all metrics except for IoU=0.5. Only a negligible portion of the annotated moments cannot be correctly retrieved at a high IoU (0.5), this result showcases the suitability of the proposal scheme. The low performance of the Random Chance baseline reflects the difficulty of the task, given the vast pool of proposals extracted over a single video. For the least strict metric (R@100-IoU=0.1), this baseline only achieves 8.47%, while CLIP and VLG-Net baselines are close to 50%, a ?6? relative improvement. An even larger gap is present for the most strict metric, R@1-IoU=0.5, with a relative improvement of two orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Grounding Performance on MAD</head><p>The CLIP <ref type="bibr" target="#b20">[21]</ref> baseline is pre-trained for the task of text-to-image retrieval, and we do not fine-tune this model on the MAD dataset. Nevertheless, when evaluated in a zero-shot fashion, it results in a strong baseline for longform grounding, achieving the best R@K for the least strict IoU=0.1 at K={1, 5, 10}. Although IoU=0.1 corresponds to very loose grounding, this result is nonetheless valuable given a large number of negatives in the long-form setup, and the fact that MAD is characterized by containing short moments (4.1s on average). Although VLG-Net is trained for the task at hand, it achieves comparable or better  <ref type="table">Table 3</ref>. Short video setup. The table showcases the performance of the selected baselines in a short-video setup, where movies are chunked into three minutes (non-overlapping windows). VLG-Net, which falls behind CLIP in the long-form setup, achieves the best grounding performance in most metrics. We can conclude that a new generation of deep learning architectures will have to be investigated to tackle the specific properties of the MAD dataset.</p><p>performance with respect to CLIP only when a strict IoU (IoU=0.5) is considered. However, it lags behind CLIP for most other metrics. We believe the shortcomings of VLG-Net are due to two factors. (i) This architecture was developed to ground sentences in short videos, where the entire frame-set can be compared against a sentence in a single forward pass. Thus, it struggles in the long-form setup where we must compare the sentence against all segments of the movie and then aggregate the predictions in a postprocessing step. (ii) VLG-Net training procedure defines low IoU moments as negatives, thus favoring high performance only for higher IoUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Challenges of Long-form Video Grounding</head><p>This section presents an in-depth analysis of the performance of the selected baselines in the long-form setup. We first investigate how the performance changes when the evaluation is constrained over segments of the movie, whose length is comparable to current datasets. Then explore how methods behave as the size of the movie chunks changes. To this end, we split each video into nonoverlapping windows (short videos), and assign the annotations to the short-video with the highest temporal overlap.</p><p>Short-video Setup. In <ref type="table">Table 3</ref>, we set the short-video window length to three minutes. This duration is a candidate representative for short videos. The upper bound performance Oracle slightly decreases to 99.42% for IoU=0.5. This is a consequence of the division into short videos, which occasionally breaks down a few ground truth moments. The Random Chance baseline reports increased performance as the number of proposals generated is reduced. In particular for R@5-IoU=0.1, the performance increases from 0.44% <ref type="table">(Table 2)</ref> to 15.69% <ref type="table">(Table 3)</ref>, demonstrating that the short-video setup is less challenging compared to MAD's original long-form configuration. In a similar trend, performance substantially increases for both CLIP and VLG-Net baselines, with the latter now obtaining the best performances, in all cases. Recall Performance (%) VLG-Net R@5 IoU=0.5 CLIP R@5 IoU=0.5 VLG-Net R@1 IoU=0.5 CLIP R@1 IoU=0.5 <ref type="figure">Figure 4</ref>. Performance trend across different windows lengths.</p><p>We observe from the graph the decrease in performance for both CLIP and VLG-Net, as the evaluation window length increases. This demonstrates that current grounding methods cannot tackle the task in the long-form video setting.</p><p>From Short-to Long-form Results. <ref type="figure">Figure 4</ref> showcases the performance trend for the metrics R@{1, 5}-IoU=0.5, when the window length is changed from a small value (30 seconds) to the entire movie duration (average duration is 2hrs). The graph displays how the performance steadily drops as the window length increases, showing the challenging setup of long-form grounding enabled by MAD.</p><p>Takeaway. This set of experiments verifies that VLG-Net could successfully outperform the zero-shot CLIP baseline when evaluated in a short-video setup. We can conclude that current state-of-the-art grounding methods are not ready to tackle the long-form setting proposed by MAD. This opens the door to opportunities for the community to leverage previously developed techniques in a more challenging setting and potentially incorporate new constraints when designing deep learning architectures for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>This section focuses on the empirical assessment of the quality of MAD training data. To help the reader navigate the following experiments, <ref type="table" target="#tab_3">Table 4</ref> defines a naming convention associated with different splits of the data. LSMDC16 refers to the original data collected by <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> for the task of text-to-video retrieval. LSMDC-G is our adaptation of this dataset for the grounding task, as described in Section 3.2. We remind the reader that we could only retrieve 162 out of 182 movies included in LSMDC16. For these two datasets, we follow the original train/val/test partitions. Finally, we include MAD's split details.</p><p>Improving Grounding Performance with MAD Data. We are interested in evaluating the contribution our data can bring to the grounding task. We investigate the performance of VLG-Net in the long-form grounding setup when the  <ref type="table">(Table 5</ref> and <ref type="table">Table 6</ref>). LSMDC16 <ref type="bibr" target="#b24">[25]</ref> is the original data collected for retrieval. LSMDC-G is our adaptation to the grounding task. MAD is our proposed dataset.</p><p>training data change. All trained models are evaluated on the same test split: LSMDC-G test. The first row of <ref type="table">Table 5</ref> shows the performance when VLG-Net is exclusively trained on the LSMDC-G training split. This set only contains data that was manually curated in <ref type="bibr" target="#b24">[25]</ref>. The second row is trained with 32% of MADtraining data, which is equivalent to the size of LSMDC-G training split. We observe a drop in performance, which can be associated with the presence of noise introduced by the automatic annotation process in MAD. In the third row, we use the complete MAD training set. Here, MAD's scale allows us to overcome the performance issues associated with noisy data and improve performance to be comparable to only using the clean LSMDC-G for training. Using 100% of MAD data yields a relative improvement of 20% for R@5-IoU=0.5. Then, we investigate whether the performance of VLG-Net saturates given the amount of data available. To this end, we use 100% of LSMDC-G training and gradually augment it by adding MAD training samples. In these three experiments (rows 4-6), the performance steadily increases. These results suggest that current models for video grounding do benefit from larger-scale datasets, even though the automatically collected training data may be noisy. Moreover, designing scalable strategies for automatic dataset collection is crucial, as the drawbacks of noisy data can be off- set and even overcome with more training data.</p><p>Improving Retrieval Performance with MAD Data.</p><p>Adopting the same ablation procedure, we evaluate the possible contribution of our data in a related task, namely textto-video retrieval. Here, we format our MAD data similarly to LSMDC16, where short videos are trimmed around the annotated timestamps. For this experiment, we use CLIP4Clip <ref type="bibr" target="#b14">[15]</ref>, a state-of-the-art architecture for the retrieval task, as the baseline. <ref type="table">Table 6</ref> reports the performance when different amounts of data is used for training. Again, we see that training with the whole LSMDC16 or MAD leads to a very similar performance. Moreover, our previous argument also holds true in this task. Pouring more data into the task boosts the performance, motivating the benefit of having a scalable dataset like MAD.</p><p>Takeaway. The MAD dataset is able to boost performance in two closely related tasks, video grounding and text-tovideo retrieval, where we show that scale can compensate for potential noise present due to automatic annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The paper presents a new video grounding benchmark called MAD, which builds on high-quality audio descriptions in movies. MAD alleviates the shortcomings of previous grounding datasets. Our automatic annotation pipeline allowed us to collect the largest grounding dataset to date. The experimental section provides baselines for the task solution and highlights the challenging nature of the longform grounding task introduced by MAD. Our methodology comes with two main hypotheses and limitations: (i) Noise cannot be avoided but can be dealt with through scale. (ii) Due to copyright constraints, MAD's videos will not be publicly released. However, we will provide all necessary features for our experiments' reproducibility and promote future research in this direction.</p><p>This section provides additional statistics for the MAD dataset. First, we compare the automatically curated training set against the manually curated validation and test sets, highlighting similarities and differences. Second, we assess the presence of repetitive sentences which might be ambiguous for the language grounding in videos task. We follow by providing additional statistics about MAD's vocabulary and conclude the sections highlighting MAD's large visual and language diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Data splits comparison</head><p>As described in Section 3 of the main paper, the training set is automatically collected and annotated, whereas the val/test sets of MAD were adapted from the LSMDC dataset <ref type="bibr" target="#b24">[25]</ref>. Considering this difference, we analyze the key statistics and discrepancies between the training and the val/test sets in detail. We summarize these results in <ref type="table">Table 7</ref> and <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>As shown in <ref type="table">Table 7</ref>, the training set contains about 3/4 of the total video hours and query sentences in MAD, val/test sets contain 1/4. The average video duration in the two splits is similar, with training videos being, on average, only 6.2% shorter than those in val/test. Moreover, the average temporal span of the moments is very similar in  <ref type="table">Table 7</ref>. Comparison between MAD training and MAD val/test splits. We verify that the two splits follow similar distributions. We assess that the average video duration, moment length, and sentence length have similar values. Moreover, we highlight how 2/3 of the video content is reserved for the training split. The size of the training split is also reflected in the total number of queries, with the training set being 2.7? larger than the val/test set.  the two splits, with a difference of only 0.1 seconds, on average. Regarding the language queries, the training set has slightly longer sentences than the val/test sets, with on average 2.9 extra words per sentence. We attribute this fact to the automatic annotations procedure of the training set. We observe that sometimes consecutive sentences that are annotated in a short temporal span can be joined together by our annotation pipeline. This does not happen for the val/test set, as sentences were manually refined. <ref type="table">Table 7</ref> also highlights a significant difference between the two splits regarding the vocabulary size. The training vocabulary (57.6K tokens) is almost three times larger than the one of val/test (21.9K tokens). Note that the vocabulary size correlates with the diversity in the language queries. Thus, a more extensive vocabulary is a desirable feature in training, considering that real-world application scenarios might use a variety of words to express similar semantics. Finally, the overlap between val/test and training vocabularies is 83%, accounting for 18.1K unique words. There are 3.8K val/test tokens that do not overlap the training set vocabulary. However, these tokens only account for 0.69% of the total tokens in the val/test splits (1.1M ). Moreover, there are 39.5K unique tokens in the training set that are not present in the val/test. Such unique tokens account for 6.6% of the total training tokens (3.8M ). These features of the dataset will be valuable to evaluate the generalization capabilities of models developed in MAD.  <ref type="figure">Figure 6</ref>. Sentences length distribution. Queries length is measured in number of tokens. <ref type="figure" target="#fig_5">Figure 5</ref> shows the distribution of the relative start time of a moment <ref type="figure" target="#fig_5">(Fig. 5a</ref>) and the relative end time of a moment <ref type="figure" target="#fig_5">(Fig. 5b</ref>). <ref type="figure" target="#fig_5">Fig. 5c</ref> shows the distribution of segments by duration. We show MAD's training split in blue and val/test in red. We observe that the two splits have similar distributions in all three sub-figures. However, we notice that the training set has slightly more moments at the very beginning and at the very end of the videos <ref type="figure" target="#fig_5">(Fig. 5a and 5b</ref>). We attribute this discrepancy to the fact that we did not remove the audio descriptions from the movie's opening and credits, as there is not an automatic and reliable way to drop them; LSMDC manually removed them. We opt for including such annotations in our dataset. Overall, this design decision has little impact on the data distribution but saves manual effort and keeps our data collection method scalable. For the moment's duration <ref type="figure" target="#fig_5">(Fig. 5c</ref>), both splits exhibit a bias towards short instances and have a long tail distribution, with moments lasting up to 50 seconds for training and 30 seconds for val/test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Sentences uniqueness</head><p>Repeating sentences within a movie can be a source of ambiguity for the video-language grounding task. Our automated annotation pipeline, does not enforce individual sentences to be semantically or grammatically different. To quantify this phenomenon, we compute the METEOR similarity score between each pair of sentences within a movie. The METEOR metric is a widely used metric in NLP <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> which correlates well with human judgment on sentence similarity. We use the implementation provided by the NLTK library <ref type="bibr" target="#b1">[2]</ref> and empirically observe that the scores are bounded between [0, 1]. Given these boundaries, we consider a sentence to be unique if its METEOR score with every other sentence in the movie is below th=0.99. Following this threshold, 99.7% of sentences can be considered unique. If we lower the threshold to th=0.9, the uniqueness decreases slightly to 99.2%. This suggests that only a few sentences repeat in each movie. We emphasize that this estimation cannot directly assess the semantic similarity between sentences, which is a much harder matching problem   and requires further research, but remains a good approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Additional language statistics</head><p>The MAD dataset contains about 384K query sentences. The average sentence length is 12.7 tokens (see Tab 1 in the main paper) with a standard deviation of 8.1 tokens. We show in <ref type="figure">Figure 6</ref> the distribution of the number of tokens per sentence which showcases the variability in query length in the entire dataset. It is known in the field of computational linguistics that natural language usually follows a long-tailed distribution. We find that it is also the case in the textual annotations of MAD. We compute the frequency distribution of the vocabulary words and find that only 471 unique tokens out of 61.4K repeat more than 1000 times. In comparison, that number increases to 6.3K if we relax the frequency threshold to only 50 repetitions. This means that 90% of the tokens in the vocabulary (55.1K) appear less than 50 times in the entire queries corpus. <ref type="figure" target="#fig_8">Figure 7</ref> shows the distributions of genres and years of MAD movies. We can see that MAD has a wide range in the years the movies are produced (from the 1940s to the last decade) and a large variety of genres. A movie's production year is closely related to its picture quality, filming, edition techniques <ref type="bibr" target="#b19">[20]</ref>, character's attire, apparel, action types, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Diversity</head><p>The movie genre characterizes how people behave and talk, storytelling techniques, the overall scenes setup, and how fast-paced is the information displayed. These diversities are contained in MAD's videos and descriptions, thus endowing our dataset with a large diversity in video content and related query sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. VLG-Net Long-Form Adaptation</head><p>In the paper, we select VLG-Net <ref type="bibr" target="#b28">[29]</ref> as a representative model of the state-of-the-art architectures for the natural language grounding in videos task. The challenging long-form nature of the MAD dataset requires some technical changes in the architecture. We detail below the three main upgrades made to this baseline to enable the training and inference over very long videos.</p><p>(i) Input. VLG-Net's default inputs are either frames or snippet-level features that span an entire video. As videos are of different durations, VLG-Net interpolates or extrapolates the features to a predefined length before feeding them to the remaining of the architecture. We change such modeling strategy with the following one: we consider a window of consecutive frames features (i.e., 128) and input each window independently to the model instead of an entire video. Frames are sampled at a constant frame rate (i.e., 5 frames per second).</p><p>During training, for a given sentence and corresponding grounding timestamps, we randomly select a window that contains the annotation's temporal extent. Let us draw an example to understand this approach better. Given a clip's frameset V = {v i } nv i=1 and an associated sentence S. We can map the grounding timestamps from the time domain to the frame-index one which we regard as (t s , t e ) such that t s ? 1 and t e ? n v . At training time, we sample a starting index (t * s ) in the interval [t e ? W, t s ] and construct our training window as the sequence of frames {v i } t * s +W i=t * s with W = 128. Note that t e ? W ? ts.</p><p>This process can be seen as a temporal jittering process. Thanks to this jittering process, the window enclosing the ground-truth segment changes at every epoch (as t * s changes at every epoch) for a given sentence. This strategy can be interpreted as a regularization technique that prevents the model from leveraging intra-window biases in the input representation. Moreover, it promotes the model to understand the multi-modal input better and predict the best temporal extent for each language query.</p><p>At inference time, we adopt a sliding window technique, which strides a fixed window over the entire movie. The window size is kept fixed to 128 frames, and we use a stride of 64 frames. For each window, VLG-Net produces a set of proposals with an associated confidence score. The window-level predictions are then collected and sorted according to the confidence score. The recall metric is measured at the video-level.</p><p>(ii) Negative samples. The original VLG-Net implementation does not make use of negative samples during training. This means that only positive video-language pairs are used. Following the change in the input modeling, the model now only has access to a local portion of the video when making a prediction. Therefore, it is deemed necessary to train the VLG-Net architecture using negatives/unpaired video-language pairs. This teaches the model to predict low confidence scores for windows that do not contain visual information relevant to the query being grounded (which are the majority during inference).</p><p>Negative samples are defined as a video window (128 frames) with IoU is equal to 0 with the ground truth temporal span of a given sentence. With respect to the previous example, a negative video sample is considered as a sequence of consecutive frames of size W which starting index (t * s ) is sampled outside of the interval [t s ? W, t e ].</p><p>At training time, for each sentence, we randomly select a negative sample within the same movie with a probability p or a positive sample (i.e., window containing the ground truth) with probability 1 ? p. Our experiments show that selecting a negative 70% of the times yields the best performance. We do not consider cross-movie negative samples.</p><p>(iii) Modules. In Section 4, we described how, to promote a fair comparison against the CLIP baseline <ref type="bibr" target="#b20">[21]</ref>, we adopted CLIP's visual and language features as inputs for the VLG-Net baseline. Notably, the language feature extraction strategy poses a technical challenge. The original sentence tokenizer used by VLG-Net has the capability of extracting syntactic dependencies that are represented as edges in the SyntacGCN module. Because CLIP uses a different tokenizer, we could not retrieve such syntactic dependencies; hence we remove the SyntacGCN module and only retaining the LSTM layers for the language branch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of video-language grounding datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Histograms of moment start/end/duration in MAD splits. The plots represent the normalized (by video length) start/end distributions (a-b), and absolute duration distribution (c) for moments belonging to the training and val/test splits of MAD. The figure showcases that both training and val/test splits follow the same distributions with minor differences between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Movies release year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Diversity. The Figure depicts the wide diversity contained in the dataset. Spanning 22 different genres and 90 years of cinema history, MAD presents a highly diverse dataset for the video grounding task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of video-language grounding datasets. We report relevant statistics to compare our MAD dataset against other video grounding benchmarks. MAD provides the largest dataset with 1207hrs of video and 384.6K language queries, the longest form of video (avg. 110.77min), the most diverse language vocabulary with 61.4K unique words, and the shortest moment for grounding (avg. 4.1s).</figDesc><table><row><cell>Language Queries</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CLIP [21] 6.57 15.05 20.26 37.92 47.73 3.13 9.85 14.13 28.71 36.98 1.39 5.44 8.38 18.80 24.99 VLG-Net [29] 3.64 11.66 17.89 39.78 51.24 2.76 9.31 14.65 34.27 44.87 1.65 5.99 9.77 24.93 33.95</figDesc><table><row><cell></cell><cell></cell><cell>IoU=0.1</cell><cell></cell><cell>IoU=0.3</cell><cell></cell><cell>IoU=0.5</cell></row><row><cell>Model</cell><cell cols="7">R@1 R@5 R@10 R@50 R@100 R@1 R@5 R@10 R@50 R@100 R@1 R@5 R@10 R@50 R@100</cell></row><row><cell cols="2">100.00 Random Chance 0.09 Oracle</cell><cell>? 0.44 0.88 4.33 ? ?</cell><cell>? 8.47</cell><cell>100.00 ? 0.04 0.19 0.39 1.92 ? ?</cell><cell>? 3.80</cell><cell>99.99 ? 0.01 0.07 0.14 0.71 ? ?</cell><cell>? 1.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Retrieval 155 / 12 / 17 101.1 K / 7.4 K / 10.1 K LSMDC-G Grounding 138 / 11 / 13 89.7 K / 6.7 K / 7.6 K MAD Grounding 488 / 50 / 112 280.5 K / 32.1 K / 72.0 K Data split cheat-sheet. This table clarifies the data splits used in the following experiments</figDesc><table><row><cell>Dataset Name</cell><cell>Task</cell><cell>Videos Train / Val / Test</cell><cell>Annotations Train / Val / Test</cell></row><row><cell>LSMDC16 [25]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https : / / azure . microsoft . com / en -us / services / cognitive-services/speech-to-text/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. MAD Detailed Statistics</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing Moments in Video With Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">12</biblScope>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memory dysfunction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Budson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="692" to="699" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Temporal localization of moments in video collections with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12763</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TALL: Temporal Activity Localization via Language Query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Jiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhenheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV<address><addrLine>Nevatia, Ram</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dense-Captioning Events in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A survey on temporal sentence grounding in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08039</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><forename type="middle">Delgadillo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Kassem</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-aware biaffine localizing network for temporal sentence grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulai</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="11235" to="11244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly cross-and self-modal graph attention network for query-based moment localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4070" to="4078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local-Global Video-Text Interactions for Temporal Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Queryd: A video dataset with high-quality text and audio narrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea-Maria</forename><surname>Oncescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2265" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uncovering hidden challenges in query-based video moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to cut by watching movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Le?n</forename><surname>Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6858" to="6868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Le?n</forename><surname>Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05569</idno>
		<title level="m">Moviecuts: A new dataset and benchmark for cut type recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grounding Action Descriptions in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominikus</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annemarie</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Niket Tandon, and Bernt Schiele. A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1470" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The mediamill trecvid 2009 semantic video search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kvd</forename><surname>Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bouke</forename><surname>Od Rooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huurnink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Liempt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugalhoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trancosoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tahir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID workshop</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Surrey</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vlg-net: Video-language graph matching network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sisi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Tegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bidirectional recurrent neural network with attention mechanism for punctuation restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ottokar</forename><surname>Tilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Alum?e</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using descriptive video services to create a large data source for video annotation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards episodic memory support for dementia patients by recognizing objects, faces and text in eye gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint German/Austrian Conference on Artificial Intelligence (K?nstliche Intelligenz)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transcript to video: Efficient clip sequencing from texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11851</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A closer look at temporal sentence grounding in videos: Datasets and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/2101.09028, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A closer look at temporal sentence grounding in videos: Datasets and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09028</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense Regression Network for Video Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards debiasing temporal sentence grounding in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04321</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Songyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Houwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Jianlong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video selfstitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13658" to="13667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cascaded prediction network via segment tree for temporal video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="4197" to="4206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embracing uncertainty: Decoupling and de-bias for robust temporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8445" to="8454" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
