<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RCT: RANDOM CONSISTENCY TRAINING FOR SEMI-SUPERVISED SOUND EVENT DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Shao</surname></persName>
							<email>shaonian@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Westlake University &amp; Westlake Institute for Advanced Study</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Loweimi</surname></persName>
							<email>e.loweimi@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre for Speech Technology Research (CSTR)</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
							<email>lixiaofei@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Westlake University &amp; Westlake Institute for Advanced Study</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RCT: RANDOM CONSISTENCY TRAINING FOR SEMI-SUPERVISED SOUND EVENT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: semi-supervised learning</term>
					<term>sound event detection</term>
					<term>data augmentation</term>
					<term>consistency regularization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sound event detection (SED), as a core module of acoustic environmental analysis, suffers from the problem of data deficiency. The integration of semi-supervised learning (SSL) largely mitigates such problem. This paper researches on several core modules of SSL, and introduces a random consistency training (RCT) strategy. First, a hard mixup data augmentation is proposed to account for the additive property of sounds. Second, a random augmentation scheme is applied to stochastically combine different types of data augmentation methods with high flexibility. Third, a self-consistency loss is proposed to be fused with the teacher-student model, aiming at stabilizing the training. Performance-wise, the proposed modules outperform their respective competitors, and as a whole the proposed SED strategies achieve 44.0% and 67.1% in terms of the PSDS1 and PSDS2 metrics proposed by the DCASE challenge, which notably outperforms other widely-used alternatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sound conveys a substantial amount of information about the environment. The skill of recognizing the surrounding environment is taken for granted by humans while it is a challenging task for machines <ref type="bibr" target="#b0">[1]</ref>. Sound event detection (SED) aims to detect sound events within an audio stream by labeling the events as well as their corresponding occurrence timestamps. Taking advantage of deep neural networks, promising results have been obtained for SED <ref type="bibr" target="#b1">[2]</ref>. However, the high annotation cost poses obstacles on its further development.</p><p>Two widely applied solutions for such data deficiency problem are data augmentation (DA) and semi-supervised learning (SSL). DA artificially enlarges the training dataset size in various forms including data warping, oversampling, etc. <ref type="bibr" target="#b2">[3]</ref>, while SSL leverages abundant unlabelled data to improve the model generalization capacity. For example, in computer vision (CV), different DA methods were proposed to transform the training images including rotation, cropping, etc. <ref type="bibr" target="#b2">[3]</ref>. To combine multiple augmentation methods, RandAugment <ref type="bibr" target="#b3">[4]</ref> presents a random strategy which arbitrarily selects one transformation in each training step. On the other hand, mixup <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> conducts a linear interpolation of two classes of data points to oversample the dataset, by which the mixed samples could push the decision boundaries into low-density regions. Essentially, DA regularizes the model by constraining the predicted labels to be invariant to any noise applied on the inputs. Such idea is known as consistency regularization (CR) in SSL. When using CR for training, the model predictions are constrained to be invariant to * Corresponding author. any noise not only on the inputs <ref type="bibr" target="#b6">[7]</ref> but also on the hidden states <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. However, there exists a potential risk known as confirmation bias when the consistency loss is too heavily weighted in training <ref type="bibr" target="#b9">[10]</ref>. To alleviate such risk, MeanTeacher [10] applies a consistency constraint in the model parameter space, which holds an exponential moving average (EMA) of the training student model to generate pseudo labels for unlabelled data. Other techniques such as interpolation consistency training (ICT) <ref type="bibr" target="#b6">[7]</ref>, unsupervised data augmentation (UDA) <ref type="bibr" target="#b10">[11]</ref> further combine mixup or RandAugmet with MeanTeacher and CR, obtaining state-of-the-art SSL models, respectively.</p><p>The efforts on semi-supervised SED achieved promising results <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, thanks to the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge task 4 1 , which establishes a systematically organized semi-supervised dataset <ref type="bibr" target="#b1">[2]</ref> containing a reasonable amount of weakly-labelled (cliplevel annotated) and unlabelled audio clips. The recent top-rank systems not only apply audio-specified DA methods including SpecAugment <ref type="bibr" target="#b16">[17]</ref>, time shift <ref type="bibr" target="#b15">[16]</ref>, pitch shift <ref type="bibr" target="#b17">[18]</ref>, etc., but also draw lessons from the SSL methods such as CR, MeanTeacher, etc. The top-rank system in 2018 DCASE challenge <ref type="bibr" target="#b11">[12]</ref> introduces MeanTeacher into the semi-supervised SED, which becomes a role system for many variant systems <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Similar to ICT <ref type="bibr" target="#b6">[7]</ref>, shift consistency training (SCT) <ref type="bibr" target="#b15">[16]</ref> proposes a way of unifying mixup, time shift and CR, obtaining a compatibly better semi-supervised SED model. Utilizing SCT and ICT, Zheng et al. <ref type="bibr" target="#b12">[13]</ref> obtained the top rank in DCASE 2021 challenge task 4. Although multiple audio-specified DA methods <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> are leveraged in these models, the proper adaptation and effective combination of these SSL techniques are not scrutinised to be optimally applied in audio processing.</p><p>In this work, concerned with the proper usages of SSL techniques in audio processing, we propose a SSL strategy for SED, which consists of three novel modules: i) Hard mixup. Vanilla mixup <ref type="bibr" target="#b4">[5]</ref> is trivially applied in many audio processing studies <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, while its adaptability for audio processing is not fully investigated. In vanilla mixup, the mixed label is a soft convex combination of the labels from original samples. Instead, Hard mixup preserves the hard label of the original samples, as the mixup of sound events yields concurrent sound events due to the additive property of audio signal. E.g. The mixed label [0.2, 0, 0.8] in vanilla mixup would be presented as [1, 0, 1] in hard mixup. Hard mixup is the first algorithm which investigates the proper usage of the mixup method in the audio processing tasks.</p><p>ii) RandomWarping. Direct combination of various DA methods is not guaranteed to result in a performance gain, because of the complexity of finding an optimal set in a large hyperparameter searching space <ref type="bibr" target="#b2">[3]</ref>. Although many efforts have been made toward unifying various data augmentation schemes in CV <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>, they could not be trivially adapted for audio processing, since methods such as time shift <ref type="bibr" target="#b15">[16]</ref> or pitch shift <ref type="bibr" target="#b17">[18]</ref> are specifically designed for audio. RandomWarping is among the first attempts toward unifying data augmentation methods for audio processing. We perturb each training sample with a randomly selected transformation, which allows taking advantage of different types of augmentation techniques in an unified way. It is a simple yet effective policy, while finding the optimal magnitude for each transformation can be challenging. We empirically find the optimal magnitude values for three data augmentation methods and achieve a consistent performance gain.</p><p>iii) Self-consistency loss. One of the challenges in SSL is designing the unsupervised loss for unlabelled data. In ICT <ref type="bibr" target="#b6">[7]</ref> for mixup and SCT <ref type="bibr" target="#b15">[16]</ref> for time shift, the MeanTeacher model and student model respectively process the original and augmented samples. In this paper, we propose an additional selfconsistency loss to the MeanTeacher loss, which constraints the student model to give consistent predictions for original and augmented samples. Such self-consistency constraint always holds regardless of the correctness of the predictions, and thus would stabilize the training procedure.</p><p>The combination of these three modules is referred to as random consistency training (RCT), and the flowchart is shown in <ref type="figure">Fig. 1</ref>. With better adaptability for corresponding audio signals, each proposed module experimentally outperform its competing methods in the literature. As a whole, the proposed SSL strategy notably outperforms its counterparts, and achieves top performance on the DCASE 2021 challenge dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Proposed Method</head><p>SED is defined as a multi-class detection problem where the onset and offset timestamps of multiple sound events should be recognized from the input audio clips. We denote the timefrequency domain audio clips as X (l) i ? R T ?K , where T is the number of frames (same for all clips in experiment) and K is the dimension of LogMel filterbank features. Three types of data annotations are used for training data, i.e. weakly labelled, strongly labelled and unlabelled, which are indicated by superscript l ? {w,s,u}, respectively. i denotes the index of data sample among a total of N (l) data points of one annotation type. Let C and Y (l) i be the number of sound event classes and data labels, respectively. The weakly labelled and strongly labelled data have the clip-level and frame-level labels denoted by Y</p><formula xml:id="formula_0">(w) i ? R C and Y (s) i</formula><p>? R T ?C , respectively. Since the required time resolution for SED is much lower than the one of sound frames, pooling layers are applied in the CNN, resulting in a coarser time resolution T rather than T for the predictions.</p><p>The baseline model is a CRNN <ref type="bibr" target="#b11">[12]</ref>, consisting of a 7-layer CNN with Context Gating layers <ref type="bibr" target="#b22">[23]</ref>, cascaded by a 2-layer bidirectional GRU. An attention module is added at the end to produce different levels of predictions <ref type="bibr" target="#b11">[12]</ref> and MeanTeacher <ref type="bibr" target="#b9">[10]</ref> is employed for SSL. As shown in <ref type="figure">Fig. 1</ref>, a teacher model is obtained by an EMA of the student CRNN model to provide pseudo labels for unlabelled samples. The training loss is L = LSupervised + LMeanTeacher, where LSupervised is the cross-entropy loss for the labelled data, and LMeanTeacher is the MeanTeacher mean square error (MSE) loss for the unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Random data augmentation for audio</head><p>RandAugment <ref type="bibr" target="#b3">[4]</ref> is proposed as an efficient way of combining different types of image transformations. In RCT, we take such </p><formula xml:id="formula_1">? , , ? , ? , ? ? , , ? , ? . ? , ? , ? ? ? , ? ? ? , ? , ? , ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-consistency loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MeanTeacher loss</head><p>Total unsupervised loss EMA random selection loss object <ref type="figure">Figure 1</ref>: Flowchart of RCT: both hard mixup and audio warping are first applied for data augmentation; MeanTeacher <ref type="bibr" target="#b9">[10]</ref> and self-consistency are used for SSL training. Subscripts R and M stand for RandomWarping and hard mixup, respectively.</p><p>idea to construct general audio warping methods for consistency regularization. Random data augmentation is accomplished by combining it with the proposed hard mixup, which means each training sample is augmented by hard mixup and random warping, respectively. As shown in <ref type="figure">Fig. 1</ref>, the batch size is thus tripled in each training step.</p><p>Hard mixup: The vanilla mixup <ref type="bibr" target="#b4">[5]</ref> conducts an interpolation of two data points belonging to different classes, aiming to smooth the decision boundary. Such operation is feasible for images, whereas the interpolation of two audio clips produces a new clip due to the additive property of the sounds. As a result, combinations of multiple audio clips could be regarded as a realistic sample containing concurrent sound events, and should be recognized as an audio clip involving all sound events. Thus, the audio mixup is proposed to directly add multiple samples together, and the mixture is labelled with all the classes in all original samples. Since each sound clip reflect the realistic energies of all sound sources, the energy of the mixed sample is remained unchanged. Moreover, we noticed that combining more than two audio clips brings extra benefits. That is, it further condenses the distribution of sound events and can help the model toward better discriminating the sound events. Therefore, we randomly add two or three samples together in hard mixup.</p><p>RandomWarping: We use three audio warping methods in this work, and their warping magnitudes are set to d ? {1, 2, . . . , 9}. The only hyperparameter requires to be tuned is one unique dmax for all audio warping methods. For each mini batch, one warping method is randomly chosen with a random magnitude d uniformly distributed in <ref type="bibr">[1, dmax]</ref>. This magnitude controls the audio warping intensities of the following methods:</p><p>i) Time shift <ref type="bibr" target="#b15">[16]</ref> circularly shifts each audio clip along time axis with a duration of 1 ? d seconds.</p><p>ii) Time mask <ref type="bibr" target="#b16">[17]</ref> randomly selects 5d intervals from the audio clip to be masked to 0. Since the shortest sound event lasts for 0.5 s, the length of each mask interval is set to 0.1 s.</p><p>iii) Pitch shift <ref type="bibr" target="#b17">[18]</ref> randomly raises or lowers the pitch of the audio clip by 1/2 ? d semitones, where both pitches and formants are stretched. The magnitude unit of each method (1 s in time shift, 0.1 s in time mask, 1/2 semitone in pitch shift) is empirically selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-consistency training</head><p>The MeanTeacher loss <ref type="bibr" target="#b9">[10]</ref> used in SED <ref type="bibr" target="#b11">[12]</ref> has already shown a notable capacity in mining information from unlabelled data. To further leverage the unlabelled data, we propose to apply self-consistency regularization in addition to the MeanTeacher loss. Let? </p><formula xml:id="formula_2">LSC = r(step) 1 N (w) C N (w) i D (w) aug (? (w) i ) ?? (w) i 2 2 + r(step) 1 N (s) CT N (s) i D (s) aug (? (s) i ) ?? (s) i 2 2 ,<label>(1)</label></formula><p>where ? 2 denotes the Euclidean norm for a vector/matrix, r(step) is a ramp-up function varying along the training step. D aug denote transformations applied on the predictions of original samples, as the labels should be correspondingly changed for augmented samples. For pitch shift and time mask, there is no need to change the labels. Time shift should accordingly shift the strong labels along time axis. As for hard mixup, the mixed audio clip includes all the sound classes presented in the original audio clips. However, the labels for the combined sound classes cannot be trivially obtained by adding the predictions of original samples, since the summation of two soft-predictions/labels is meaningless. Instead, we define the label transformation for hard mixup as</p><formula xml:id="formula_3">D (l) mixup (? (l) i ) = ?i?M harden(? (l) i ),<label>(2)</label></formula><p>where ? denotes element-wise OR operation, M is an arbitrary set consists of two or three data samples used in hard mixup, and harden(?) is an element-wise binary hardening function which ceils (or floors) the matrix elements to 1 (or 0) if the elements are larger than 0.95 (or smaller than 0.05). This transformation first hardens the predictions of original samples, from which the active/inactive sound classes are combined. The prediction of original and augmented samples (?i and Yi) are both used for gradient updating, which means they are considered as the pseudo labels for each other in training. This is also one reason of choosing symmetric MSE function as an extra component in the loss function. The total loss L used for training the CRNN model will be</p><formula xml:id="formula_4">L = LSupervised + LMeanTeacher + LSC,<label>(3)</label></formula><p>where LMeanTeacher is the average of MeanTeacher MSE losses for the original sample and two augmented samples (hard mixup and random warping), as shown in <ref type="figure">Fig. 1</ref>. The unsupervised loss is composed of the MeanTeacher loss and self-consistency regularization. Such self-consistency constraint between the original and augmented samples always holds regardless of the correctness of the predictions. This is different from ICT <ref type="bibr" target="#b6">[7]</ref> that merges the MeanTeacher loss and the consistency loss as one single loss. In ICT, the MeanTeacher loss is set as the MSE loss between the prediction of augmented and original samples, where the predictions are obtained by the teacher model and student model, respectively. Such pseudo labels highly rely on the correctness of the MeanTeacher predictions, while incorrect pseudo labels may mislead the student model and consequently reduce the training efficiency.  <ref type="figure">Figure 2</ref>: The relative performance gain as a function of maximum transformation magnitude (dmax). The transformation magnitude d ? U <ref type="bibr">[1, dmax]</ref>. Relative performance gain is computed using the baseline performance (PSDS1 = 34.74%, PSDS2 = 53.66%). The markers and vertical lines represent the mean and standard deviation computed using three trials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results and Discussion</head><p>Our codes for this work have been released on our website 2 . We use the baseline model <ref type="bibr" target="#b11">[12]</ref> on DCASE 2021 Task 4 dataset 3 to test the performance of the proposed method. The dataset consists of 1578 weakly-labelled, 10000 synthesized stronglylabelled and 14412 unlabelled audio clips. Each 10-second audio clip is resampled to 16 kHz and frame blocked with a length of 128 ms (2048 samples) and a hop length of 16 ms (256 samples). After 2048-point fast Fourier transform, 128-dimensional LogMel features are extracted for each frame, converting the 10-second audio clip into a 626 ? 128 spectrogram. All samples are normalized to [?1, 1] before training. The batch size is set to 48, consisting of 12 weakly-labelled, 12 strongly-labelled and 24 unlabelled data points. The learning rate ramps up to 10 ?3 until epoch 50 and is scheduled by Adam optimizer <ref type="bibr" target="#b23">[24]</ref> until the end of training, i.e. 200 epochs. The weight for MeanTeacher and self-consistency losses, r(step), linearly ramps up from 0 to 2 at epoch 50 and is then kept unchanged. The system performance is evaluated through polyphonic sound detection scores (PSDSs) <ref type="bibr" target="#b24">[25]</ref> according to the DCASE 2021 challenge guidelines. The metrics takes both response speed (PSDS1) and cross-trigger performance (PSDS2) into account; the larger the better for both metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ablation study on SED</head><p>In the random warping policy, the only hyperparameter that needs to be grid-searched is the maximum transformation magnitude dmax. As shown in <ref type="figure">Fig. 2</ref>, the performance improves as the maximum transformation magnitude increases until about 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE Loss</head><p>Baseline (MeanTeacher) ICT <ref type="bibr" target="#b6">[7]</ref> Self-consistency (proposed) <ref type="figure">Figure 3</ref>: Cross-entropy loss of strongly-supervised validation data when training with or without self-consistency loss, comparing with the ICT scheme <ref type="bibr" target="#b6">[7]</ref>. or 6; accordingly we set dmax to 5. <ref type="table" target="#tab_1">Table 1</ref> shows the result of ablation study, in which the proposed modules are individually added step by step. As seen, the proposed schemes including hard mixup, RandomWarping and self-consistency, lead to noticeable positive contributions. To further investigate the efficacy of the proposed methods, we also conduct experiments to replace hard mixup and selfconsistency by vanilla mixup <ref type="bibr" target="#b4">[5]</ref> and ICT-like consistency <ref type="bibr" target="#b6">[7]</ref>, respectively. While vanilla mixup is slightly better in crosstrigger (PSDS2), hard mixup gives more significant gain in response time (PSDS1). The proposed self-consistency outperforms the ICT-like consistency in both metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with other semi-supervised strategies</head><p>To evaluate the proposed SSL strategy collectively, we conduct a thorough comparison with other widely used SSL strategies, including ICT <ref type="bibr" target="#b6">[7]</ref>, SCT <ref type="bibr" target="#b15">[16]</ref> and their combination, employing the same baseline network used for the proposed strategy. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison results, and <ref type="figure">Fig. 3</ref> gives the validation loss curves of the models with baseline MeanTeacher, ICT and self-consistency. Compared with the baseline MeanTeacher model, ICT largely improves the performance by its proposed teacher-student consistency loss, while the always-hold selfconsistency loss further improves the performance, which can be reflected by a lower validation loss curve shown in <ref type="figure">Fig. 3</ref>. SCT performance is not as good as ICT, which indicates that the time shift is not as efficient as interpolation (mixup). Combining ICT and SCT does not outperform ICT alone, which implies that the naive addition of ICT and SCT <ref type="bibr" target="#b15">[16]</ref> is not an effective way to combine multiple different augmentations schemes. In contrast, as shown in <ref type="table" target="#tab_1">Table 1</ref>, the proposed strategy is able to efficiently combine multiple different augmentations. Overall, the proposed method remarkably outperforms ICT and SCT, due to the strength of each proposed module and the efficient stochastic combination of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with DCASE2021 submissions</head><p>To further assess the efficacy of the proposed method and conduct fair comparisons with heavily processed DCASE 2021 submitted models, we also took advantage of some existing post-processing and ensembling techniques in our model. A temperature factor of 2.1 is used for inference temperature tuning as in <ref type="bibr" target="#b12">[13]</ref>. Median filter is applied to smooth the frame-level predictions. Following <ref type="bibr" target="#b25">[26]</ref>, the length of each median filter is calculated by length class = 0.55 ? avg duration class /durationframe and manually search for the best value with a range of ?2. The length of class-wise median filters are set to {3, 28, 7, 4, 7, 22, 48, 19, 10, 50}, for the event classes of {alarm bell ringing, blender, cat, dishes, dog, elec-  tric shaver toothbrush, frying, running water, speech, vacuum cleaner}, respectively. Moreover, model ensembling was applied to fuse the predictions of multiple differently trained models. We trained eleven models with different variants of RCT: substituting time masking with frequency masking <ref type="bibr" target="#b16">[17]</ref>; adding FilterAug <ref type="bibr" target="#b13">[14]</ref> into audio warping choices; randomly selecting one or two methods in audio warping; and, reducing the weight of the MeanTeacher loss. We found that all different variants achieve reasonable performances. This demonstrates the flexibility of RCT and its capacity in incorporating new audio transformations into the framework with a low tuning cost overhead.</p><p>The proposed system is compared with DCASE2021 topranked submissions in <ref type="table" target="#tab_3">Table 3</ref>. The scores of DCASE2021 submissions are directly quoted from the challenge results. The proposed system noticeably outperforms all other systems employing the baseline CRNN network, which verifies the superiority of the proposed RCT strategy. Furthermore, as seen the performance of the proposed system is very close to the two first-ranked submissions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. They both use more powerful networks, i.e. SKUnit and RCRNN, which were able to largely improve the performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. The proposed framework is independent of the network and can be easily applied along with more advanced architectures to achieve higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we developed a novel semi-supervised learning (SSL) strategy, named random consistency training (RCT), for sound event detection (SED) task. The proposed method improves several core modules of SSL, including unsupervised training loss and data augmentation schemes. It leads to achieving competitive performance on the DCASE 2021 challenge dataset. RCT provides a generic framework which can be effectively employed along with more advanced augmentation schemes and architectures. Besides, since RCT is not taskspecific, it can potentially be applied in various audio and image processing tasks which is another broad avenue for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>R C denote the weak (cliplevel) predictions of original and augmented samples of the student model, respectively. Similarly let? (s) i ? R T ?C and Y (s) i ? R T ?C for the strong (frame-level) predictions. Selfconsistency regulates the model by an extra MSE term added to the loss function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study for RCT. Different modules are added step by step and each score is obtained by averaging three trials.</figDesc><table><row><cell>Model</cell><cell cols="2">PSDS1 (%) PSDS2 (%)</cell></row><row><cell>Baseline</cell><cell>34.7</cell><cell>53.7</cell></row><row><cell>+ Vanilla mixup [5]</cell><cell>34.9</cell><cell>57.9</cell></row><row><cell>+ Hard mixup</cell><cell>36.4</cell><cell>57.4</cell></row><row><cell>+ RandomWarping</cell><cell>38.1</cell><cell>58.5</cell></row><row><cell>+ ICT consistency [7]</cell><cell>38.0</cell><cell>59.2</cell></row><row><cell>+ Self-consistency</cell><cell>40.1</cell><cell>61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparing the proposed SSL strategy with other alternatives. Each score is obtained by averaging three trials.</figDesc><table><row><cell>Model</cell><cell cols="2">PSDS1 (%) PSDS2 (%)</cell></row><row><cell>Baseline [12]</cell><cell>34.7</cell><cell>53.7</cell></row><row><cell>SCT [16]</cell><cell>36.0</cell><cell>55.6</cell></row><row><cell>ICT [7]</cell><cell>37.7</cell><cell>57.7</cell></row><row><cell>ICT+SCT [16]</cell><cell>37.0</cell><cell>58.7</cell></row><row><cell>RCT (proposed)</cell><cell>40.1</cell><cell>61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparing the proposed system with DCASE2021 top-ranked submissions. All models are named in the form of network architecture plus the SSL strategy.</figDesc><table><row><cell>Model</cell><cell cols="2">PSDS1 (%) PSDS2 (%)</cell></row><row><cell>CRNN (baseline) [12]</cell><cell>34.7</cell><cell>53.7</cell></row><row><cell>FBCRNN+MLFL [20]</cell><cell>40.1</cell><cell>59.7</cell></row><row><cell>CRNN+IPL [15]</cell><cell>40.7</cell><cell>65.3</cell></row><row><cell>CRNN+DA [21]</cell><cell>41.9</cell><cell>63.8</cell></row><row><cell>CRNN+HeavyAug. [14]</cell><cell>43.4</cell><cell>63.9</cell></row><row><cell>RCRNN+NS [19]</cell><cell>45.1</cell><cell>67.9</cell></row><row><cell>SKUnit+ICT/SCT [5]</cell><cell>45.4</cell><cell>67.1</cell></row><row><cell>CRNN+RCT (proposed)</cell><cell>44.0</cell><cell>67.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://dcase.community/challenge2022 arXiv:2110.11144v3 [eess.AS] 27 Mar 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/Audio-WestlakeU/RCT 3 http://dcase.community/challenge2021/task-sound-eventdetection-and-separation-in-domestic-environments</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational Analysis of Sound Scenes and Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">253</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3008" to="3017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from betweenclass examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3635" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning with pseudoensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Detection and Classification of Acoustic Scenes and Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiakai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Mean teacher convolution system for dcase 2018 task 4</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zheng ustc team&apos;s submission for dcase2021 task4 -semi-supervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Heavily augmented sound event detection utilizing weak predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved pseudo-labeling method for semi-supervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sound event detection by consistency training and pseudo-labeling with featurepyramid convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A software framework for musical data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-training with noisy student model and semi-supervised loss function for dcase 2021 challenge task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sound event detection using metric learning and focal loss for dcase 2021 task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ebbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integrating advantages of recurrent and transformer structures for sound event detection in multiple scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhiyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Josef</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.06905" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metrics for polyphonic sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised sound event detection based on mean teacher with power pooling and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2020 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
