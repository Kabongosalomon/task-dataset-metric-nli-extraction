<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Tail Learning via Logit Adjustment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-13">July 13, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Krishna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menon</forename><surname>Sadeep</surname></persName>
							<email>sadeep@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayasumana</forename><surname>Ankit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singh</forename><surname>Rawat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
							<email>aveit@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
							<email>sanjivk@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">New</forename><surname>York</surname></persName>
						</author>
						<title level="a" type="main">Long-Tail Learning via Logit Adjustment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-13">July 13, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes na?ve learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance. A reference implementation of our methods is available at:</p><p>https://github.com/google-research/google-research/tree/master/logit_adjustment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 1</ref><p>: Comparison of approaches to long-tail learning. Weight normalisation re-scales the classification weights; by contrast, we add per-label offsets to the logits. Margin approaches uniformly increase the margin between a rare positive and all negatives <ref type="bibr" target="#b5">[Cao et al., 2019]</ref>, or decrease the margin between all positives and a rare negative <ref type="bibr" target="#b52">[Tan et al., 2020]</ref> to prevent suppression of rare labels' gradients. By contrast, we increase the margin between a rare positive and a dominant negative.</p><p>This has a firm statistical grounding: unlike recent techniques, it is consistent for minimising the balanced error (cf. (2)), a common metric in long-tail settings which averages the per-class errors. This grounding translates into strong empirical performance on real-world datasets.</p><p>In summary, our contributions are: (i) we present two realisations of logit adjustment for long-tail learning, applied either post-hoc ( ?4.2) or during training ( ?5.2) (ii) we establish that logit adjustment overcomes limitations in recent proposals (see <ref type="table">Table 1</ref>), and in particular is Fisher consistent for minimising the balanced error (cf. (2)); (iii) we confirm the efficacy of the proposed techniques on real-world datasets ( ?6). In the course of our analysis, we also present a general version of the softmax cross-entropy with a pairwise label margin (11), which offers flexibility in controlling the relative contribution of labels to the overall loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem setup and related work</head><p>Consider a multiclass classification problem with instances X and labels Y = [L] . = {1, 2, . . . , L}. Given a sample S = {(x n , y n )} N n=1 ? P N , for unknown distribution P over X ? Y, our goal is to learn a scorer f : X ? R L that minimises the misclassification error P x,y y / ? argmax y ?Y f y (x) .</p><p>Typically, one minimises a surrogate loss : Y ? R L ? R, such as the softmax cross-entropy, </p><p>For p y (x) ? e fy(x) , we may view p(x) . = [p 1 (x), . . . , p L (x)] ? ? |Y| as an estimate of P(y | x).</p><p>The setting of learning under class imbalance or long-tail learning is where the distribution P(y) is highly skewed, so that many (rare or "tail") labels have a very low probability of occurrence. Here, the misclassification error is not a suitable measure of performance: a trivial predictor which classifies every instance to the majority label will attain a low misclassification error. To cope with this, a natural alternative is the balanced error <ref type="bibr" target="#b7">[Chan and Stolfo, 1998</ref><ref type="bibr" target="#b2">, Brodersen et al., 2010</ref><ref type="bibr" target="#b41">, Menon et al., 2013</ref>, which averages each of the per-class error rates:</p><formula xml:id="formula_1">BER(f ) . = 1 L y?[L] P x|y y / ? argmax y ?Y f y (x) .<label>(2)</label></formula><p>This can be seen as implicitly using a balanced class-probability function P bal (y | x) ? 1 L ? P(x | y), as opposed to the native P(y | x) ? P(y) ? P(x | y) that is employed in the misclassification error.</p><p>Broadly, extant approaches to coping with class imbalance (see also Table 2) modify:</p><p>(i) the inputs to a model, for example by over-or under-sampling <ref type="bibr" target="#b31">[Kubat and Matwin, 1997</ref><ref type="bibr" target="#b8">, Chawla et al., 2002</ref><ref type="bibr" target="#b56">, Wallace et al., 2011</ref><ref type="bibr" target="#b42">, Mikolov et al., 2013</ref><ref type="bibr" target="#b38">, Mahajan et al., 2018</ref><ref type="bibr">, Yin et al., 2018</ref> (ii) the outputs of a model, for example by post-hoc correction of the decision threshold <ref type="bibr" target="#b14">[Fawcett and</ref><ref type="bibr">Provost, 1996, Collell et al., 2016]</ref> or weights <ref type="bibr" target="#b28">[Kim and</ref><ref type="bibr">Kim, 2019, Kang et al., 2020]</ref> (iii) the internals of a model, for example by modifying the loss function <ref type="bibr" target="#b59">[Xie and Manski, 1989</ref><ref type="bibr" target="#b43">, Morik et al., 1999</ref><ref type="bibr" target="#b10">, Cui et al., 2019</ref><ref type="bibr">, Zhang et al., 2017</ref><ref type="bibr" target="#b5">, Cao et al., 2019</ref><ref type="bibr" target="#b52">, Tan et al., 2020</ref> Family Method Reference Post-hoc correction Modify threshold <ref type="bibr" target="#b14">[Fawcett and Provost, 1996</ref><ref type="bibr" target="#b47">, Provost, 2000</ref><ref type="bibr" target="#b39">, Maloof, 2003</ref><ref type="bibr" target="#b29">, King and Zeng, 2001</ref><ref type="bibr" target="#b9">, Collell et al., 2016</ref> Normalise weights <ref type="bibr">[Zhang et al., 2019</ref><ref type="bibr" target="#b28">, Kim and Kim, 2019</ref><ref type="bibr">, Kang et al., 2020</ref> Data modification Under-sampling <ref type="bibr" target="#b31">[Kubat and</ref><ref type="bibr">Matwin, 1997, Wallace et al., 2011</ref>] Over-sampling <ref type="bibr" target="#b8">[Chawla et al., 2002]</ref> Feature transfer <ref type="bibr">[Yin et al., 2018]</ref> Loss weighting Loss balancing <ref type="bibr" target="#b59">[Xie and Manski, 1989</ref><ref type="bibr" target="#b43">, Morik et al., 1999</ref><ref type="bibr" target="#b41">, Menon et al., 2013</ref>] Volume weighting <ref type="bibr" target="#b10">[Cui et al., 2019</ref>] Average top-k loss <ref type="bibr" target="#b13">[Fan et al., 2017</ref>] Domain adaptation <ref type="bibr" target="#b25">[Jamal et al., 2020]</ref> Margin modification Cost-sensitive SVM <ref type="bibr">Vasconcelos, 2010, Iranmehr et al., 2019]</ref> Range loss <ref type="bibr">[Zhang et al., 2017]</ref> Label-aware margin <ref type="bibr" target="#b5">[Cao et al., 2019]</ref> Equalised negatives <ref type="bibr" target="#b52">[Tan et al., 2020]</ref>  One may easily combine approaches from the first stream with those from the latter two. Consequently, we focus on the latter two in this work, and describe some representative recent examples from each.</p><p>Post-hoc weight normalisation. Suppose f y (x) = w y ?(x) for classification weights w y ? R D and representations ? : X ? R D , as learned by a neural network. (We may add per-label bias terms to f y by adding a constant feature to ?.) A fruitful avenue of exploration involves decoupling of representation and classifier learning <ref type="bibr">[Zhang et al., 2019]</ref>. Concretely, we first learn {w y , ?} via standard training on the long-tailed training sample S, and then predict for x ? X</p><formula xml:id="formula_2">argmax y?[L] w y ?(x)/? ? y = argmax y?[L] f y (x)/? ? y ,<label>(3)</label></formula><p>for ? &gt; 0, where ? y = P(y) in <ref type="bibr" target="#b28">Kim and Kim [2019]</ref>, <ref type="bibr" target="#b60">Ye et al. [2020]</ref> and ? y = w y 2 in <ref type="bibr">Kang et al. [2020]</ref>. Further to the above, one may also enforce w y 2 = 1 during training <ref type="bibr" target="#b28">[Kim and Kim, 2019]</ref>. Intuitively, either choice of ? y upweights the contribution of rare labels through weight normalisation. The choice ? y = w y 2 is motivated by the observations that w y 2 tends to correlate with P(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss modification.</head><p>A classic means of coping with class imbalance is to balance the loss, wherein (y, f (x)) is weighted by P(y) ?1 <ref type="bibr">Manski, 1989, Morik et al., 1999]</ref>: for example,</p><formula xml:id="formula_3">(y, f (x)) = 1 P(y) ? log 1 + y =y e f y (x)?fy(x) .<label>(4)</label></formula><p>While intuitive, balancing has minimal effect in separable settings: solutions that achieve zero training loss will necessarily remain optimal even under weighting <ref type="bibr" target="#b4">[Byrd and Lipton, 2019]</ref>. Intuitively, one would like instead to shift the separator closer to a dominant class. <ref type="bibr" target="#b33">Li et al. [2002]</ref>, <ref type="bibr" target="#b58">Wu et al. [2008]</ref>, <ref type="bibr" target="#b40">Masnadi-Shirazi and Vasconcelos [2010]</ref>, <ref type="bibr" target="#b24">Iranmehr et al. [2019]</ref>, <ref type="bibr" target="#b17">Gottlieb et al. [2020]</ref> thus proposed Figure 1: Mean and standard deviation over 5 runs of per-class weight norms for a ResNet-32 under momentum and Adam optimisers. We use long-tailed ("LT") versions of CIFAR-10 and CIFAR-100, and sort classes in descending order of frequency; the first class is 100 times more likely to appear than the last class. Both optimisers yield solutions with comparable balanced error. However, the weight norms have incompatible trends: under momentum, the norms are strongly correlated with class frequency, while with Adam, the norms are anti-correlated or independent of the class frequency. Consequently, weight normalisation under Adam is ineffective for combatting class imbalance.</p><p>to add per-class margins into the hinge loss. <ref type="bibr" target="#b5">[Cao et al., 2019]</ref> proposed to add a per-class margin into the softmax cross-entropy:</p><p>(y, f (x)) = log 1 + y =y</p><formula xml:id="formula_4">e ?y ? e f y (x)?fy(x) ,<label>(5)</label></formula><p>where ? y ? P(y) ?1/4 . This upweights rare "positive" labels y, which enforces a larger margin between a rare positive y and any "negative" y = y. Separately, <ref type="bibr" target="#b52">Tan et al. [2020]</ref> proposed</p><formula xml:id="formula_5">(y, f (x)) = log 1 + y =y e ? y ? e f y (x)?fy(x) ,<label>(6)</label></formula><p>where ? y ? 0 is an non-decreasing transform of P(y ). The motivation is that, in the original softmax cross-entropy without {? y }, a rare label often receives a strong inhibitory gradient signal as it disproportionately appear as a negative for dominant labels. See also <ref type="bibr" target="#b35">Liu et al. [2016</ref><ref type="bibr" target="#b36">Liu et al. [ , 2017</ref>, <ref type="bibr" target="#b57">Wang et al. [2018]</ref>, <ref type="bibr" target="#b27">Khan et al. [2019]</ref> for similar weighting of negatives in the softmax.</p><p>Limitations of existing approaches. Each of the above methods are intuitive, and have shown strong empirical performance. However, a closer analysis identifies some subtle limitations.</p><p>Limitations of weight normalisation. Post-hoc weight normalisation with ? y = w y 2 per <ref type="bibr">Kang et al. [2020]</ref> is motivated by the observation that the weight norm w y 2 tends to correlate with P(y). However, we now show this assumption is highly dependent on the choice of optimiser.</p><p>We consider long-tailed versions of CIFAR-10 and CIFAR-100, wherein the first class is 100 times more likely to appear than the last class. (See ?6.2 for more details on these datasets.) We optimise a ResNet-32 using both SGD with momentum and Adam optimisers. <ref type="figure">Figure 1</ref> confirms that under SGD, w y 2 and the class priors P(y) are correlated. However, with Adam, the norms are either anti -correlated or independent of the class priors. This marked difference may be understood in light of recent study of the implicit bias of optimisers <ref type="bibr" target="#b50">[Soudry et al., 2018]</ref>; cf. Appendix F. One may hope to side-step this by simply using ? y = P(y); unfortunately, even this choice has limitations (see ?4.2).</p><p>Limitations of loss modification. Enforcing a per-label margin per (5) and (6) is intuitive, as it allows for shifting the decision boundary away from rare classes. However, when doing so, it is important to ensure Fisher consistency <ref type="bibr" target="#b34">[Lin, 2004]</ref> (or classification calibration <ref type="bibr" target="#b0">[Bartlett et al., 2006]</ref>) of the resulting loss for the balanced error. That is, the minimiser of the expected loss (equally, the empirical risk in the infinite sample limit) should result in a minimal balanced error. Unfortunately, both <ref type="formula" target="#formula_4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref> are not consistent in this sense, even for binary problems; see ?5.2, ?6.1 for details.</p><p>3 Logit adjustment for long-tail learning: a statistical view</p><p>The above suggests there is scope for improving performance on long-tail problems, both in terms of post-hoc correction and loss modification. We now show how a statistical perspective on the problem suggests simple procedures of each type, both of which overcome the limitations discussed above.</p><p>Recall that our goal is to minimise the balanced error (2). A natural question is: what is the best possible or Bayes-optimal scorer for this problem, i.e., f * ? argmin f : X?R L BER(f ). Evidently, such an f * must depend on the (unknown) underlying distribution P(x, y). Indeed, we have <ref type="bibr" target="#b41">[Menon et al., 2013]</ref>, <ref type="bibr">[Collell et al., 2016, Theorem 1]</ref> </p><formula xml:id="formula_6">argmax y?[L] f * y (x) = argmax y?[L] P bal (y | x) = argmax y?[L] P(x | y),<label>(7)</label></formula><p>where P bal is the balanced class-probability as per ?2. In words, the Bayes-optimal prediction is the label under which the given instance x ? X is most likely. Consequently, for fixed class-conditionals P(x | y), varying the class priors P(y) arbitrarily will not affect the optimal scorers. This is intuitively desirable: the balanced error is agnostic to the level of imbalance in the label distribution.</p><p>To further probe <ref type="formula" target="#formula_6">(7)</ref>, suppose the underlying class-probabilities P(y | x) ? exp(s * y (x)), for (unknown) scorer s * : X ? R L . Since by definition P bal (y | x) ? P(y | x)/P(y), (7) becomes</p><formula xml:id="formula_7">argmax y?[L] P bal (y | x) = argmax y?[L] exp(s * y (x))/P(y) = argmax y?[L] s * y (x) ? ln P(y),<label>(8)</label></formula><p>i.e., we translate the (unknown) distributional scores or logits based on the class priors. This simple fact immediately suggests two means of optimising for the balanced error:</p><p>(i) train a model to estimate the standard P(y | x) (e.g., by minimising the standard softmax-cross entropy on the long-tailed data), and then explicitly modify its logits post-hoc as per <ref type="formula" target="#formula_7">(8)</ref> (ii) train a model to estimate the balanced P bal (y | x), whose logits are implicitly modified as per <ref type="formula" target="#formula_7">(8)</ref> Such logit adjustment techniques -which have been a classic approach to class-imbalance <ref type="bibr" target="#b47">[Provost, 2000]</ref> -neatly align with the post-hoc and loss modification streams discussed in ?2. However, unlike most previous techniques from these streams, logit adjustment is endowed with a clear statistical grounding: by construction, the optimal solution under such adjustment coincides with the Bayesoptimal solution (7) for the balanced error, i.e., it is Fisher consistent for minimising the balanced error. We shall demonstrate this translates into superior empirical performance ( ?6). Note also that logit adjustment may be easily extended to cover performance measures beyond the balanced error, e.g., with distinct costs for errors on dominant and rare classes; we leave a detailed study and contrast to existing cost-sensitive approaches <ref type="bibr" target="#b24">[Iranmehr et al., 2019</ref><ref type="bibr" target="#b17">, Gottlieb et al., 2020</ref> to future work.</p><p>We now study each of the techniques (i) and (ii) in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Post-hoc logit adjustment</head><p>We now detail to perform post-hoc logit adjustment on a classifier trained on long-tailed data. We further show this bears similarity to recent weight normalisation schemes, but has a subtle advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The post-hoc logit adjustment procedure</head><p>Given a sample S ? P N of long-tailed data, suppose we learn a neural network with logits f y (x) = w y ?(x). Given these, one typically predicts the label argmax y?[L] f y (x). When trained with the softmax cross-entropy, one may view p y (x) ? exp(f y (x)) as an approximation of the underlying P(y | x), and so this equivalently predicts the label with highest estimated class-probability.</p><p>In post-hoc logit adjustment, we propose to instead predict, for suitable ? &gt; 0:</p><formula xml:id="formula_8">argmax y?[L] exp(w y ?(x))/? ? y = argmax y?[L] f y (x) ? ? ? log ? y ,<label>(9)</label></formula><p>where ? ? ? Y are estimates of the class priors P(y), e.g., the empirical class frequencies on the training sample S. Effectively, (9) adds a label-dependent offset to each of the logits. When ? = 1, this can be seen as applying (8) with a plugin estimate of P(y | x), i.e., p y (x) ? exp(w y ?(x)). When ? = 1, this can be seen as applying <ref type="formula" target="#formula_7">(8)</ref> to temperature scaled estimatesp y (x) ? exp(? ?1 ? w y ?(x)). To unpack this, recall that (8) justifies post-hoc logit thresholding given access to the true probabilities P(y | x). In principle, the outputs of a sufficiently high-capacity neural network aim to mimic these probabilities. In practice, these estimates are often uncalibrated <ref type="bibr" target="#b19">[Guo et al., 2017]</ref>. One may thus need to first calibrate the probabilities before applying logit adjustment. Temperature scaling is one means of doing so, and is often used in the context of distillation <ref type="bibr" target="#b23">[Hinton et al., 2015]</ref>.</p><p>One may treat ? as a tuning parameter to be chosen based on some measure of holdout calibration, e.g., the expected calibration error <ref type="bibr" target="#b45">[Murphy and</ref><ref type="bibr">Winkler, 1987, Guo et al., 2017]</ref>, probabilistic sharpness <ref type="bibr" target="#b32">, Kuleshov et al., 2018</ref>, or a proper scoring rule such as the log-loss or squared error . One may alternately fix ? = 1 and aim to learn inherently calibrated probabilities, e.g., via label smoothing <ref type="bibr" target="#b51">[Szegedy et al., 2016</ref><ref type="bibr" target="#b44">, M?ller et al., 2019</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to existing post-hoc techniques</head><p>Post-hoc logit adjustment with ? = 1 is not a new idea in the class imbalance literature. Indeed, this is a standard technique when creating stratified samples <ref type="bibr" target="#b29">[King and Zeng, 2001]</ref>, and when training binary classifiers <ref type="bibr" target="#b14">[Fawcett and Provost, 1996</ref><ref type="bibr" target="#b47">, Provost, 2000</ref><ref type="bibr" target="#b39">, Maloof, 2003</ref>. In multiclass settings, this has been explored in Zhou and <ref type="bibr">Liu [2006]</ref>, <ref type="bibr" target="#b9">Collell et al. [2016]</ref>. However, ? = 1 is important in practical usage of neural networks, owing to their lack of calibration. Further, we now explicate that post-hoc logit adjustment has an important advantage over recent post-hoc weight normalisation techniques.</p><p>Recall that weight normalisation involves learning a scorer f y (x) = w y ?(x), and then post-hoc normalising the weights via w y /? ? y for ? &gt; 0. We demonstrated in ?2 that using ? y = w y 2 may be ineffective when using adaptive optimisers. However, even with ? y = ? y , there is a subtle contrast to post-hoc logit adjustment: while the former performs a multiplicative update to the logits, the latter performs an additive update. The two techniques may thus yield different orderings over labels, since</p><formula xml:id="formula_9">w T 1 ?(x) ? 1 &lt; w T 2 ?(x) ? 2 &lt; ? ? ? &lt; w T L ?(x) ? L =? ?= e w T 1 ?(x) ? 1 &lt; e w T 2 ?(x) ? 2 &lt; ? ? ? &lt; e w T L ?(x) ? L .</formula><p>Weight normalisation is thus not consistent for minimising the balanced error, unlike logit adjustment. Indeed, if a rare label y has negative score w y ?(x) &lt; 0, and there is another label with positive score, then it is impossible for the weight normalisation to give y the highest score. By contrast, under logit adjustment, w T y ?(x) ? ln ? y will be lower for dominant classes, regardless of the original sign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The logit adjusted softmax cross-entropy</head><p>We now show how to directly bake logit adjustment into the softmax cross-entropy. We show that this approach has an intuitive relation to existing loss modification techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The logit adjusted loss</head><p>From ?3, the second approach to optimising for the balanced error is to directly model P bal (y | x) ? P(y | x)/P(y). To do so, consider the following logit adjusted softmax cross-entropy loss for ? &gt; 0:</p><formula xml:id="formula_10">(y, f (x)) = ? log e f y (x)+? ?log ? y y ?[L] e f y (x)+? ?log ? y = log 1 + y =y ? y ? y ? ? e (f y (x)?f y (x)) . (10)</formula><p>Given a scorer that minimises the above, we now predict argmax y?[L] f y (x) as usual.</p><p>Compared to the standard softmax cross-entropy (1), the above applies a label-dependent offset to each logit. Compared to (9), we directly enforce the class prior offset while learning the logits, rather than doing this post-hoc. The two approaches have a deeper connection: observe that (10) is equivalent to using a scorer of the form g y (x) = f y (x) + ? ? log ? y . We thus have</p><formula xml:id="formula_11">argmax y?[L] f y (x) = argmax y?[L] g y (x) ? ? ? log ? y .</formula><p>Consequently, one can equivalently view learning with this loss as learning a standard scorer g(x), and post-hoc adjusting its logits to make a prediction. For convex objectives, we thus do not expect any difference between the solutions of the two approaches. For non-convex objectives, as encountered in neural networks, the bias endowed by adding ? ? log ? y to the logits is however likely to result in a different local minima.</p><p>For more insight into the loss, consider the following pairwise margin loss</p><formula xml:id="formula_12">(y, f (x)) = ? y ? log 1 + y =y e ? yy ? e (f y (x)?fy(x)) ,<label>(11)</label></formula><p>for label weights ? y &gt; 0, and pairwise label margins ? yy representing the desired gap between scores for y and y . For ? = 1, our logit adjusted loss (10) corresponds to (11) with ? y = 1 and ? yy = log ? y ?y . This demands a larger margin between rare positive (? y ? 0) and dominant negative (? y ? 1) labels, so that scores for dominant classes do not overwhelm those for rare ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to existing loss modification techniques</head><p>A cursory inspection of (5), (6) reveals a striking similarity to our logit adjusted softmax crossentropy (10). The balanced loss (4) also bears similarity, except that the weighting is performed outside the logarithm. Each of these losses are special cases of the pairwise margin loss (11) enforcing uniform margins that only consider the positive or negative label, unlike our approach.</p><p>For example, ? y = 1 ?y and ? yy = 0 yields the balanced loss (4). This does not explicitly enforce a margin between the labels, which is undesirable for separable problems <ref type="bibr" target="#b4">[Byrd and Lipton, 2019]</ref>. When ? y = 1, the choice ? yy = ? ?1/4 y yields (5). Finally, ? yy = log F (? y ) yields (6), where F : [0, 1] ? (0, 1] is some non-decreasing function, e.g., F (z) = z ? for ? &gt; 0. These losses thus either consider the frequency of the positive y or negative y , but not both simultaneously.</p><p>The above choices of ? and ? are all intuitively plausible. However, ?3 indicates that our loss in (10) has a firm statistical grounding: it ensures Fisher consistency for the balanced error.</p><p>Theorem 1. For any ? ? R L + , the pairwise loss in <ref type="formula" target="#formula_0">(11)</ref> is Fisher consistent with weights and margins</p><formula xml:id="formula_13">? y = ? y /? y ? yy = log ? y /? y .</formula><p>Observe that when ? y = ? y , we immediately deduce that the logit-adjusted loss of (10) is consistent. Similarly, ? y = 1 recovers the classic result that the balanced loss is consistent. While the above is only a sufficient condition, it turns out that in the binary case, one may neatly encapsulate a necessary and sufficient condition for consistency that rules out other choices; see Appendix B.1. This suggests that existing proposals may thus underperform with respect to the balanced error in certain settings, as verified empirically in ?6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion and extensions</head><p>One may be tempted to combine the logit adjusted loss in <ref type="formula" target="#formula_0">(10)</ref> with the post-hoc adjustment of (9). However, following ?3, such an approach would not be statistically coherent. Indeed, minimising a logit adjusted loss encourages the model to estimate the balanced class-probabilities P bal (y | x).</p><p>Applying post-hoc adjustment will distort these probabilities, and is thus expected to be harmful.</p><p>More broadly, however, there is value in combining logit adjustment with other techniques. For example, Theorem 1 implies that it is sensible to combine logit adjustment with loss weighting; e.g., one may pick ? yy = ? ? log (? y /? y ), and ? y = ? ? ?1 y . This is similar to <ref type="bibr" target="#b5">Cao et al. [2019]</ref>, who found benefits in combining weighting with their loss. One may also generalise the formulation in Theorem 1, and employ ? yy = ? 1 ? log ? y ? ? 2 ? log ? y , where ? 1 , ? 2 are constants. This interpolates between the logit adjusted loss (? 1 = ? 2 ) and a version of the equalised margin loss (? 1 = 0). <ref type="bibr" target="#b5">Cao et al. [2019,</ref> Theorem 2] provides a rigorous generalisation bound for the adaptive margin loss under the assumption of separable training data with binary labels. The inconsistency of the loss with respect to the balanced error concerns the more general scenario of non-separable multiclass data data, which may occur, e.g., owing to label noise or limitation in model capacity. We shall subsequently demonstrate that encouraging consistency can lead to gains in practical settings. We shall further see that combining the ? implicit in this loss with our proposed ? can lead to further gains, indicating a potentially complementary nature of the losses.</p><p>Interestingly, for ? = ?1, a similar loss to (10) has been considered in the context of negative sampling for scalability : here, one samples a small subset of negatives based on the class priors ?, and applies logit correction to obtain an unbiased estimate of the unsampled loss function based on all the negatives <ref type="bibr" target="#b1">[Bengio and Senecal, 2008]</ref>. Losses of the general form (11) have also been explored for structured prediction <ref type="bibr">[Zhang, 2004</ref><ref type="bibr" target="#b46">, Pletscher et al., 2010</ref><ref type="bibr" target="#b20">, Hazan and Urtasun, 2010</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>We now present experiments confirming our main claims: (i) on simple binary problems, existing weight normalisation and loss modification techniques may not converge to the optimal solution ( ?6.1); (ii) on real-world datasets, our post-hoc logit adjustment outperforms weight normalisation, and one can obtain further gains via our logit adjusted softmax cross-entropy ( ?6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results on synthetic dataset</head><p>We consider a binary classification task, wherein samples from class y ? {?1} are drawn from a 2D Gaussian with isotropic covariance and means ? y = y ? (+1, +1). We introduce class imbalance by setting P(y = +1) = 5%. The Bayes-optimal classifier for the balanced error is (see Appendix G)</p><formula xml:id="formula_14">f * (x) = +1 ?? P(x | y = +1) &gt; P(x | y = ?1) ?? (? 1 ? ? ?1 ) x &gt; 0,<label>(12)</label></formula><p>i.e., it is a linear separator passing through the origin. We compare this separator against those found by several margin losses based on (11): standard ERM (? yy = 0), the adaptive loss <ref type="bibr" target="#b5">[Cao et al., 2019]</ref> (? yy = ? ?1/4 y ), an instantiation of the equalised loss <ref type="bibr" target="#b52">[Tan et al., 2020]</ref> (? yy = log ? y ), and our logit adjusted loss (? yy = log ? y ?y ). For each loss, we train an affine classifier on a sample of 10, 000 instances, and evaluate the balanced error on a test set of 10, 000 samples over 100 independent trials. <ref type="figure" target="#fig_2">Figure 2</ref> confirms that the logit adjusted margin loss attains a balanced error close to that of the Bayes-optimal, which is visually reflected by its learned separator closely matching that in (12). This is in line with our claim of the logit adjusted margin loss being consistent for the balanced error, unlike other approaches. <ref type="figure" target="#fig_2">Figure 2</ref> also compares post-hoc weight normalisation and logit adjustment for varying scaling parameter ? (c.f. (3), (9)). Logit adjustment is seen to approach the performance of the Bayes predictor; any weight normalisation is however seen to hamper performance. This verifies the consistency of logit adjustment, and inconsistency of weight normalisation ( ?4.2).    <ref type="table" target="#tab_2">Table 3</ref>, 7]. Here, ? = ? * refers to using the best possible value of tuning parameter ? . See <ref type="figure" target="#fig_4">Figure 3</ref> for plots as a function of ? , and the "Discussion" subsection for further extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results on real-world datasets</head><p>We present results on the CIFAR-10, CIFAR-100, ImageNet and iNaturalist 2018 datasets. Following prior work, we create "long-tailed versions" of the CIFAR datasets by suitably downsampling examples per label following the Exp profile of <ref type="bibr" target="#b10">Cui et al. [2019]</ref>, <ref type="bibr" target="#b5">Cao et al. [2019]</ref> with imbalance ratio ? = max y P(y)/min y P(y) = 100. Similarly, we use the long-tailed version of ImageNet produced by <ref type="bibr" target="#b37">Liu et al. [2019]</ref>. We employ a ResNet-32 for CIFAR, and a ResNet-50 for ImageNet and iNaturalist. All models are trained using SGD with momentum; see Appendix D for more details. See also Appendix E.1 for results on CIFAR under the Step profile considered in the literature.</p><p>Baselines. We consider: (i) empirical risk minimisation (ERM) on the long-tailed data, (ii) post-hoc weight normalisation <ref type="bibr">[Kang et al., 2020]</ref> per (3) (using ? y = w y 2 and ? = 1) applied to ERM, (iii) the adaptive margin loss <ref type="bibr" target="#b5">[Cao et al., 2019]</ref> per <ref type="formula" target="#formula_4">(5)</ref>, and (iv) the equalised loss <ref type="bibr" target="#b52">[Tan et al., 2020]</ref> per <ref type="formula" target="#formula_5">(6)</ref>, with ? y = F (? y ) for the threshold-based F of <ref type="bibr" target="#b52">Tan et al. [2020]</ref>. <ref type="bibr" target="#b5">Cao et al. [2019]</ref> demonstrated superior performance of their adaptive margin loss against several other baselines, such as the balanced loss of (4), and that of <ref type="bibr" target="#b10">Cui et al. [2019]</ref>. Where possible, we report numbers for the baselines (which use the same setup as above) from the respective papers. See also our concluding discussion about extensions to such methods that improve performance.</p><p>We compare the above methods against our proposed post-hoc logit adjustment (9), and logit adjusted loss (10). For post-hoc logit adjustment, we fix the scalar ? = 1; we analyse the effect of tuning this in <ref type="figure" target="#fig_4">Figure 3</ref>. We do not perform any further tuning of our logit adjustment techniques.</p><p>Results and analysis.    offers gains over ERM, these are improved significantly by post-hoc logit adjustment (e.g., 8% relative reduction on CIFAR-10). Similarly loss correction techniques are generally outperformed by our logit adjusted softmax cross-entropy (e.g., 6% relative reduction on iNaturalist). <ref type="figure" target="#fig_4">Figure 3</ref> studies the effect of tuning the scaling parameter ? &gt; 0 afforded by post-hoc weight normalisation (using ? y = w y 2 ) and post-hoc logit adjustment. Even without any scaling, post-hoc logit adjustment generally offers superior performance to the best result from weight normalisation (cf <ref type="table" target="#tab_2">. Table 3)</ref>; with scaling, this is further improved. See Appendix E.4 for a plot on ImageNet-LT. <ref type="figure" target="#fig_5">Figure 4</ref> breaks down the per-class accuracies on CIFAR-10, CIFAR-100, and iNaturalist. On the latter two datasets, for ease of visualisation, we aggregate the classes into ten groups based on their frequency-sorted order (so that, e.g., group 0 comprises the top L 10 most frequent classes). As expected, dominant classes generally see a lower error rate with all methods. However, the logit adjusted loss is seen to systematically improve performance over ERM, particularly on rare classes.</p><p>While our logit adjustment techniques perform similarly, there is a slight advantage to the loss function version. Nonetheless, the strong performance of post-hoc logit adjustment corroborates the ability to decouple representation and classifier learning in long-tail settings <ref type="bibr">[Zhang et al., 2019]</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the advantage of logit adjustment over recent post-hoc and loss modification proposals, under standard setups from the literature. We believe further improvements are possible by fusing complementary ideas, and remark on four such options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and extensions</head><p>First, one may use a more complex base architecture; our choices are standard in the literature, but, e.g., <ref type="bibr">Kang et al. [2020]</ref> found gains on ImageNet-LT by employing a ResNet-152, with further gains from training it for 200 as opposed to the customary 90 epochs. <ref type="table" target="#tab_5">Table 4</ref> confirms that logit adjustment similarly benefits from this choice. For example, on iNaturalist, we obtain an improved balanced error of 31.15% for the logit adjusted loss. When training for more (200) epochs per the suggestion of <ref type="bibr">Kang et al. [2020]</ref>, this further improves to 30.12%.</p><p>Second, one may combine together the ?'s for various special cases of the pairwise margin loss.  Third, <ref type="bibr" target="#b5">Cao et al. [2019]</ref> observed that their loss benefits from a deferred reweighting scheme (DRW), wherein the model begins training as normal, and then applies class-weighting after a fixed number of epochs. On CIFAR-10-LT and CIFAR-100-LT, this achieves 22.97% and 57.96% error respectively; both are outperformed by our vanilla logit adjusted loss. On iNaturalist with a ResNet-50, this achieves an error of 32.0%, outperforming our 33.6%. (Note that our simple combination of the relative and adaptive margins outperforms these reported numbers of DRW.) However, given the strong improvement of our loss over that in <ref type="bibr" target="#b5">Cao et al. [2019]</ref> when both methods use SGD, we expect that employing DRW (which applies to any loss) may be similarly beneficial for our method.</p><p>Fourth, per ?2, one may perform data augmentation; e.g., see <ref type="bibr">Tan et al. [2020, Section 6]</ref>. While further exploring such variants are of empirical interest, we hope to have illustrated the conceptual and empirical value of logit adjustment, and leave this for future work.  Consequently, under constant weights ? y = 1, the Bayes-optimal score will satisfy f * y (x) + log ? y = log ? y (x), or f * y (x) = log</p><p>?y <ref type="formula">(x)</ref> ?y . Now suppose we use generic weights ? ? R L + . The risk under this loss is</p><formula xml:id="formula_15">E x,y [ ? (y, f (x))] = y?[L] ? y ? E x|y=y [ ? (y, f (x))] = y?[L] ? y ? E x|y=y [ ? (y, f (x))] = y?[L] ? y ? ? y ? E x|y=y [ (y, f (x))] ? y?[L]? y ? E x|y=y [ (y, f (x))] ,</formula><p>where? y ? ? y ? ? y . Consequently, learning with the weighted loss is equivalent to learning with the original loss, on a distribution with modified base-rates?. Under such a distribution, we have class-conditional distribution</p><formula xml:id="formula_16">? y (x) =P(y | x) = P(x | y) ?? ? P(x) = ? y (x) ?? y ? y ? P(x) P(x) ? ? y (x) ? ? y .</formula><p>Consequently, suppose ? y = ?y(x) ?y , which is the Bayes-optimal prediction for the balanced error.</p><p>In sum, a consistent family can be obtained by choosing any set of constants ? y &gt; 0 and setting</p><formula xml:id="formula_17">? y = ? y ? y ? yy = log ? y ? y .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B On the consistency of binary margin-based losses</head><p>It is instructive to study the pairwise margin loss (11) in the binary case. Endowing the loss with a temperature parameter ? &gt; 0, we get 1</p><formula xml:id="formula_18">(+1, f ) = ? +1 ? ? log(1 + e ???+1 ? e ???f ) (?1, f ) = ? ?1 ? ? log(1 + e ????1 ? e ??f )<label>(13)</label></formula><p>for constants ? ?1 , ? &gt; 0 and ? ?1 ? R. Here, we have used ? +1 = ? +1,?1 and ? ?1 = ? ?1,+1 for simplicity. The choice ? ?1 = 1, ? ?1 = 0 recovers the temperature scaled binary logistic loss. Evidently, as ? ? +?, these converge to weighted hinge losses with variable margins, i.e.,</p><formula xml:id="formula_19">(+1, f ) = ? +1 ? [? +1 ? f ] + (?1, f ) = ? ?1 ? [? ?1 + f ] + .</formula><p>We study two properties of this family losses. First, under what conditions are the losses Fisher consistent for the balanced error? We shall show that in fact there is a simple condition characterising this. Second, do the losses preserve properness of the original binary logistic loss? We shall show that this is always the case, but that the losses involve fundamentally different approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Consistency of the binary pairwise margin loss</head><p>Given a loss , its Bayes optimal solution is f * ? argmin f : X?R E [ (y, f (x))]. For consistency with respect to the balanced error in the binary case, we require this optimal solution f * to satisfy</p><formula xml:id="formula_20">f * (x) &gt; 0 ?? ?(x) &gt; ?, where ?(x) .</formula><p>= P(y = 1 | x) and ? . = P(y = 1) <ref type="bibr" target="#b41">[Menon et al., 2013]</ref>. This is equivalent to a simple condition on the weights ? and margins ? of the pairwise margin loss.</p><p>Lemma 2. The losses in (13) are consistent for the balanced error iff</p><formula xml:id="formula_21">? +1 ? ?1 ? ?(? ? ? +1 ) ?(? ? ? ?1 ) = 1 ? ? ? ,</formula><p>where ?(z) = (1 + exp(z)) ?1 .</p><p>Proof of Lemma 2. Denote ?(x) . = P(y = +1 | x), and ? . = P(y = +1). From Lemma 3 below, the pairwise margin loss is proper composite with invertible link function ? : [0, 1] ? R ? {??}. Consequently, since by definition the Bayes-optimal score for a proper composite loss is f * (x) = ?(?(x)) [Reid and Williamson, 2010], to have consistency for the balanced error, from (14), (15), we require</p><formula xml:id="formula_22">? ?1 (0) = ? ?? 1 1 ? (+1,0) (?1,0) = ? ?? 1 ? (+1, 0) (?1, 0) = 1 ? ?? ? (+1, 0) (?1, 0) = 1 ? ? ? ?? ? +1 ? ?1 ? ?(? ? ? +1 ) ?(? ? ? ?1 ) = 1 ? ? ? .</formula><p>From the above, some admissible parameter choices include:</p><p>? ? +1 = 1 ? , ? ?1 = 1 1?? , ? ?1 = 1; i.e., the standard weighted loss with a constant margin</p><p>? ? ?1 = 1, ? +1 = 1 ? ? log 1?? ? , ? ?1 = 1 ? ? log ? 1?? ; i.e., the unweighted loss with a margin biased towards the rare class, as per our logit adjustment procedure The second example above is unusual in that it requires scaling the margin with the temperature; consequently, the margin disappears as ? ? +?. Other combinations are of course possible, but note that one cannot arbitrarily choose parameters and hope for consistency in general. Indeed, some inadmissible choices are na?ve applications of the margin modification or weighting, e.g.,</p><p>? ? +1 = 1 ? , ? ?1 = 1 1?? , ? +1 = 1 ? ? log 1?? ? , ? ?1 = 1 ? ? log ? 1?? ; i.e., combining both weighting and margin modification</p><p>? ? ?1 = 1, ? +1 = 1 ? ? (1 ? ?), ? ?1 = 1 ? ? ?; i.e., specific margin modification Note further that the choices of <ref type="bibr" target="#b5">Cao et al. [2019]</ref>, <ref type="bibr" target="#b52">Tan et al. [2020]</ref> do not meet the requirements of Lemma 2.</p><p>We make two final remarks. First, the above only considers consistency of the result of loss minimisation. For any choice of weights and margins, we may apply suitable post-hoc correction to the predictions to account for any bias in the optimal scores. Second, as ? ? +?, any constant margins ? ?1 &gt; 0 will have no effect on the consistency condition, since ?(? ? ? ?1 ) ? 1. The condition will be wholly determined by the weights ? ?1 . For example, we may choose ? +1 = 1 ? , ? ?1 = 1 1?? , ? +1 = 1, and ? ?1 = ? 1?? ; the resulting loss will not be consistent for finite ?, but will become so in the limit ? ? +?. For more discussion on this particular loss, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Properness of the pairwise margin loss</head><p>In the above, we appealed to the pairwise margin loss being proper composite, in the sense of Reid and Williamson <ref type="bibr">[2010]</ref>. Intuitively, this specifies that the loss has Bayes-optimal score of the form f * (x) = ?(?(x)), where ? is some invertible function, and ?(x) = P(y = 1 | x). We have the following general result about properness of any member of the pairwise margin family.</p><p>Lemma 3. The losses in (13) are proper composite, with link function</p><formula xml:id="formula_23">?(p) = 1 ? ? log ? ? a ? b q ? c ? a ? b q ? c 2 + 4 ? a q ? ? ? log 2,</formula><p>where a = ?+1 ??1 ? e ??? +1 e ??? ?1 , b = e ????1 , c = e ???+1 , and q = 1?p p .</p><p>Proof of Lemma 3. The above family of losses is proper composite iff the function</p><formula xml:id="formula_24">? ?1 (f ) = 1 1 ? (+1,f ) (?1,f )<label>(14)</label></formula><p>is invertible <ref type="bibr">[Reid and Williamson, 2010, Corollary 12]</ref>. We have</p><p>(+1, f ) = ?? +1 ? e ???+1 ? e ???f 1 + e ???+1 ? e ???f (?1, f ) = +? ?1 ? e ????1 ? e ??f 1 + e ????1 ? e ??f .</p><p>The invertibility of ? ?1 is immediate. To compute the link function ?, note that p = 1</p><formula xml:id="formula_26">1 ? (+1,f ) (?1,f ) ?? 1 p = 1 ? (+1, f ) (?1, f ) ?? ? (+1, f ) (?1, f ) = 1 ? p p ?? ? +1 ? ?1 ? e ???+1 ? e ???f 1 + e ???+1 ? e ???f ? 1 + e ????1 ? e ??f e ????1 ? e ??f = 1 ? p p ?? ? +1 ? ?1 ? e ???+1 e ????1 ? 1 e ??f + e ???+1 ? 1 + e ????1 ? e ??f e ??f = 1 ? p p ?? a ? 1 + b ? g g 2 + c ? g = q,</formula><p>where a = ?+1 ??1 ? e ??? +1 e ??? ?1 , b = e ????1 , c = e ???+1 , g = e ??f , and q = 1?p p . Thus,</p><formula xml:id="formula_27">a ? 1 + b ? g g 2 + c ? g = q ?? g 2 + c ? g 1 + b ? g = a q ?? g 2 + c ? a ? b q ? g ? a q = 0 ?? g = a?b q ? c ? a?b q ? c 2 + 4 ? a q 2 .</formula><p>As a sanity check, suppose a = b = c = ? = 1. This corresponds to the standard logistic loss. Then,</p><formula xml:id="formula_28">?(p) = log 1 q ? 1 ? 1 q ? 1 2 + 4 ? 1 q 2 = log p 1 ? p ,</formula><p>which is the standard logit function. <ref type="figure">Figure 5</ref> and 6 compares the link functions for a few different settings:</p><p>? the balanced loss, where ? +1 = 1 ? , ? ?1 = 1 1?? , and ? ?1 = 1 ? an unequal margin loss, where ? ?1 = 1, ? +1 = 1 ? ? log 1?? ? , and ? ?1 = 1 ? ? log ? 1?? ? a balanced + margin loss, where ? +1 = 1 ? , ? ?1 = 1 1?? , ? +1 = 1, and ? ?1 = ? 1?? . The property ? ?1 (0) = ? for ? = P(y = 1) holds for the first two choices with any ? &gt; 0, and the third choice as ? ? +?. This indicates the Fisher consistency of these losses for the balanced error. However, the precise way this is achieved is strikingly different in each case. In particular, each loss implicitly involves a fundamentally different link function.</p><p>To better understand the effect of parameter choices, <ref type="figure" target="#fig_9">Figure 7</ref> illustrates the conditional Bayes risk curves, i.e., L(p) = p ? (+1, ?(p)) + (1 ? p) ? (+1, ?(p)).</p><p>We remark here that for the balanced error, this function takes the form L(p) = p? p &lt; ? +(1?p)? p &gt; ? , i.e., it is a "tent shaped" concave function with a maximum at p = ?.</p><p>For ease of comparison, we normalise this curves to have a maximum of 1. <ref type="figure" target="#fig_9">Figure 7</ref> shows that simply applying unequal margins does not affect the underlying conditional Bayes risk compared to the standard log-loss; thus, the change here is purely in terms of the link function. By contrast, either balancing the loss or applying a combination of weighting and margin modification results in a closer approximation to the conditional Bayes risk curve for the cost-sensitive loss with cost ?.  ?y . The first balanced + margin loss uses ? ?1 = ?, ? +1 = 1, ? +1 = 1 ? . The second balanced + margin loss uses ? ?1 = ? 1?? , ? +1 = 1, ? +1 = 1 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Relation to cost-sensitive SVMs</head><p>We recapitulate the analysis of <ref type="bibr" target="#b40">Masnadi-Shirazi and Vasconcelos [2010]</ref> in our notation. Consider a binary cost-sensitive learning problem with cost parameter c ? (0, 1). The Bayes-optimal classifier for this task corresponds to f * (x) = ?(x) &gt; c . The case c = 0.5 is the standard classification problem.</p><p>Suppose we wish to design a weighted, variable margin SVM for this task, i.e.,</p><formula xml:id="formula_29">(+1, f ) = ? +1 ? [? +1 ? f ] + (?1, f ) = ? ?1 ? [? ?1 + f ] +</formula><p>where ? ?1 , ? ?1 ? 0. The conditional risk for this loss is</p><formula xml:id="formula_30">L(?, f ) = ? ? (+1, f ) + (1 ? ?) ? (?1, f ) = ? ? ? ? ? (1 ? ?) ? ? ?1 ? (? ?1 + f ) if f &gt; ? +1 ? ? ? +1 ? (? +1 ? f ) + (1 ? ?) ? ? ?1 ? (? ?1 + f ) if f ? [?? ?1 , ? +1 ] ? ? ? +1 ? (? +1 ? f ) if f &lt; ?? ?1 .</formula><p>As this is a piecewise linear function, which is decreasing for f &lt; ?? ?1 and increasing for f &gt; ? +1 , the only possible minimum is at {? +1 , ?? ?1 }. To ensure consistency, we seek the minimum to be ? +1 iff ? &gt; c. Observe that</p><formula xml:id="formula_31">L(?, ? +1 ) &lt; L(?, ?? ?1 ) ?? (1 ? ?) ? ? ?1 &lt; ? ? ? +1 ?? ? 1 ? ? &gt; ? ?1 ? +1 ?? ? &gt; ? ?1 ? ?1 + ? +1 .</formula><p>Consequently, we must have</p><formula xml:id="formula_32">? ?1 ? ?1 + ? +1 = c ?? ? +1 ? ?1 = 1 ? c c .</formula><p>Observe here that the margin terms ? ?1 do not appear in the consistency condition: thus, as long as the weights are suitably chosen, any choice of margin terms will result in a consistent loss.</p><p>However, the margins do influence the form conditional Bayes risk: this is</p><formula xml:id="formula_33">L(?) = (1 ? ?) ? ? ?1 ? (? ?1 + ? +1 ) if ? &gt; c ? ? ? ?1 ? (? ?1 + ? +1 ) if ? &lt; c.</formula><p>For the purposes of normalisation, it is natural to require this function to attain a maximum at 1. This corresponds to choosing</p><formula xml:id="formula_34">? ?1 + ? +1 = 1 c ? 1 ? +1 .</formula><p>In the class-imbalance setting, c = ?, and so we require</p><formula xml:id="formula_35">? +1 ? ?1 = 1 ? ? ? ? ?1 + ? +1 = 1 ? ? 1 ? +1</formula><p>for consistency and normalisation respectively. This gives two degrees of freedom: the choice of ? +1 (which determines ? ?1 ), and then the choice of ? +1 (which determines ? ?1 ). For example, we could pick ? +1 = 1 ? , ? ?1 = 1 1?? , ? +1 = 1, ? ?1 = ? 1?? .</p><p>To relate this to <ref type="bibr" target="#b40">Masnadi-Shirazi and Vasconcelos [2010]</ref>, the latter considered separate costs C ?1 , C +1 for a false positive and false negative respectively. With this, they suggested to use <ref type="bibr">Masnadi-Shirazi and Vasconcelos [2010, Equation 34</ref>]</p><formula xml:id="formula_36">(+1, f ) = d ? e d ? f + (?1, f ) = a ? b a + f + with ? +1 = e d = 1, d = ? +1 = C +1 , a = ? ?1 = 2C ?1 ? 1, and ? ?1 = b a = 1 a . The constraints C 1 ? 2C ?1 ? 1 and C ?1 ? 1 are also enforced.</formula><p>Under this setup, the cost ratio is C?1 C?1+C+1 . In the class-imbalance setting, we have C?1 C?1+C+1 = ?, and so C +1 = 1?? ? ?C ?1 . By the consistency condition, we have C +1 = ? +1 = 1?? ? ?? ?1 = 1?? ? ?(2C ?1 ?1). Thus, we must set C ?1 = 1, and so C +1 = 1?? ? . Thus, we obtain the parameters ? +1 = 1?? ? , ? ?1 = 1, ? +1 = 1, ? ?1 = ? 1?? . By rescaling the weights, we obtain ? +1 = 1 ? , ? ?1 = 1 1?? , ? +1 = 1, ? ?1 = ? 1?? . Observe that this is exactly one of the losses considered in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental setup</head><p>Intending a fair comparison, we use the same setup for all the methods for each dataset. All networks are trained with SGD with a momentum value of 0.9. Unless otherwise specified, linear learning rate warm-up is used in the first 5 epochs to reach the base learning rate, and a weight decay of 10 ?4 is used. Other dataset specific details are given below.</p><p>CIFAR-10 and CIFAR-100: We use a CIFAR ResNet-32 model trained for 200 epochs. The base learning rate is set to 0.1, which is decayed by 0.1 at the 160th epoch and again at the 180th epoch. Mini-batches of 128 images are used.</p><p>We also use the standard CIFAR data augmentation procedure used in previous works such as <ref type="bibr" target="#b5">Cao et al. [2019]</ref>, <ref type="bibr" target="#b22">He et al. [2016]</ref>, where 4 pixels are padded on each size and a random 32 ? 32 crop is taken. Images are horizontally flipped with a probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet:</head><p>We use a ResNet-50 model trained for 90 epochs. The base learning rate is 0.4, with cosine learning rate decay. We use a batch size of 512 and the standard data augmentation comprising of random cropping and flipping as described in <ref type="bibr" target="#b18">Goyal et al. [2017]</ref>. Following <ref type="bibr">Kang et al. [2020]</ref>, we use a weight decay of 5 ? 10 ?4 on this dataset. iNaturalist: We again use a ResNet-50 and train it for 90 epochs with a base learning rate of 0.4 and cosine learning rate decay. The data augmentation procedure is the same as the one used in ImageNet experiment above. We use a batch size of 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional experiments</head><p>We present here additional experiments:</p><p>(i) we present results for CIFAR-10 and CIFAR-100 on the Step profile <ref type="bibr" target="#b5">[Cao et al., 2019]</ref> with ? = 100</p><p>(ii) we further verfiy that weight norms may not correlate with class priors under Adam (iii) we include the results of post-hoc correction, and a breakdown of per-class errors, on ImageNet-LT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Results on CIFAR-LT with</head><p>Step-100 profile <ref type="table" target="#tab_8">Table 5</ref> summarises results on the Step-100 profile. Here, with ? = 1, weight normalisation slightly outperforms logit adjustment. However, with ? &gt; 1, logit adjustment is again found to be superior (54.80); see <ref type="figure" target="#fig_10">Figure 8</ref>.    <ref type="figure">Figure 9</ref> breaks down the per-class accuracies on ImageNet-LT. As before, the logit adjustment procedure shows significant gains on rarer classes. <ref type="figure">Figure 10</ref> compares post-hoc correction techniques as the scaling parameter ? is varied on ImageNet-LT. As before, logit adjustment with suitable tuning is seen to be competitive with weight normalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Per-class errors on ImageNet-LT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Post-hoc correction on ImageNet-LT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Per-group errors</head><p>Following <ref type="bibr" target="#b37">Liu et al. [2019]</ref>, <ref type="bibr">Kang et al. [2020]</ref>, we additionally report errors on a per-group basis, where we construct three groups of classes: "Many", comprising those with at least 100 training examples; "Medium", comprising those with at least 20 and at most 100 training examples; and "Few", comprising those with at most 20 training examples. This is a coarser level of granularity than the grouping employed in the previous section, and the body. <ref type="figure" target="#fig_13">Figure 11</ref> shows that the logit adjustment procedure shows consistent gains over all three groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Does weight normalisation increase margins?</head><p>Suppose that one uses SGD with a momentum, and finds solutions where w y 2 tracks the class priors. One intuition behind normalisation of weights is that, drawing inspiration from the binary case, this ought to increase the classification margins for tail classes.</p><p>Unfortunately, this intuition is not necessarily borne out. Consider a scorer f y (x) = w T y ?(x), where w y ? R d and ? : X ? R d . The functional margin for an example (x, y) is <ref type="bibr" target="#b30">[Koltchinskii et al., 2001]</ref> ? f (x, y)</p><formula xml:id="formula_37">. = w T y ?(x) ? max y =y w T y ?(x).<label>(16)</label></formula><p>This generalises the classical binary margin, wherein by convention Y = {?1}, w ?1 = ?w 1 , and ? f (x, y)</p><formula xml:id="formula_38">. = y ? w T 1 ?(x) = 1 2 ? w T y ?(x) ? w T ?y ?(x) ,<label>(17)</label></formula><p>which agrees with (16) upto scaling. One may also define the geometric margin in the binary case to  be the distance of (x, y) from its classifier:</p><formula xml:id="formula_39">? g,b (x) . = |w 1 ? ?(x)| w 1 2 .<label>(18)</label></formula><p>Clearly, ? g,b (x) = |? f (x,y)| w1 2 , and so for fixed functional margin, one may increase the geometric margin by minimising w 1 2 . However, the same is not necessarily true in the multiclass setting, since here the functional and geometric margins do not generally align <ref type="bibr" target="#b54">[Tatsumi et al., 2011, Tatsumi and</ref><ref type="bibr" target="#b53">Tanino, 2014]</ref>. In particular, controlling each w y 2 does not necessarily control the geometric margin.</p><p>G Bayes-optimal classifier under Gaussian class-conditionals</p><formula xml:id="formula_40">Suppose P(x | y) = 1 ? 2?? ? exp ? x ? ? y 2 2? 2</formula><p>for suitable ? y and ?. Then,</p><formula xml:id="formula_41">P(x | y = +1) &gt; P(x | y = ?1) ?? exp ? x ? ? +1 2 2? 2 &gt; exp ? x ? ? ?1 2 2? 2 ?? x ? ? +1 2 2? 2 &lt; x ? ? ?1 2 2? 2 ?? x ? ? +1 2 &lt; x ? ? ?1 2 ?? 2 ? (? +1 ? ? ?1 ) T x &gt; ? +1 2 ? ? ?1 2 .</formula><p>Now use the fact that in our setting, ? +1 2 = ? ?1 2 .</p><p>We remark also that the class-probability function is P(y = +1 | x) = P(x | y = +1) ? P(y = +1) P(x) = P(x | y = +1) ? P(y = +1) y P(x | y ) ? P(y ) = 1 1 + P(x|y=?1)?P(y=?1) P(x|y=+1)?P(y=+1)</p><p>. Now,</p><formula xml:id="formula_42">P(x | y = ?1) P(x | y = +1) = exp x ? ? +1 2 ? x ? ? ?1 2 2? 2 = exp ? +1 2 ? ? ?1 2 ? 2 ? (? +1 ? ? ?1 ) T x 2? 2 = exp ?(? +1 ? ? ?1 ) T x ? 2 .</formula><p>Thus,</p><formula xml:id="formula_43">P(y = +1 | x) = 1 1 + exp(?w T * x + b * )</formula><p>, where w * = 1 ? 2 ?(? +1 ?? ?1 ), and b * = log P(y=?1) P(y=+1) . This implies that a sigmoid model for P(y = +1 | x), as employed by logistic regression, is well-specified for the problem. Further, the bias term b * is seen to take the form of the log-odds of the class-priors per (8), as expected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(y, f (x)) = logy ?[L] e f y (x) ? f y (x) = log 1 + y =y e f y (x)?fy(x) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Results on synthetic binary classification problem. Our logit adjusted loss tracks the Bayes-optimal solution and separator (left &amp; middle panel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of balanced error for post-hoc correction techniques when varying scaling parameter ? (c.f. (3),(9)). Post-hoc logit adjustment consistently outperforms weight normalisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Per-class error rates of loss modification techniques. For (b) and (c), we aggregate the classes into 10 groups. ERM displays a strong bias towards dominant classes (lower indices). Our proposed logit adjusted softmax loss achieves significant gains on rare classes (higher indices).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(y, f (x)) = ? log ? y ? e fy(x)y ?[L] ? y ? e f y (x) = ? log e fy(x)+log ?y y ?[L] e f y (x)+log ? y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>?y ?y . Then, f * y (x) = log? y (x) ?y = log ?y(x) ?y + C(x), where C(x) does not depend on y. Consequently, argmax y?[L] f * y (x) = argmax y?[L]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Comparison of link functions for various losses assuming ? = 0.2, with ? = 1 (left) and ? = 8 (right). The balanced loss uses ? y = 1 ?y . The unequal margin loss uses ? y = 1 ? ? log 1?? ? . The balanced + margin loss uses ? ?1 = ? 1?? , ? +1 = 1, ? +1 = 1 ? . Comparison of link functions for various losses assuming ? = 0.2, with ? = 1 (left) and ? = 8 (right). The balanced loss uses ? y = 1 ?y . The unequal margin loss uses ? y = 1 ? ? log 1??y ?y . The balanced + margin loss uses ? ?1 = ? 1?? , ? +1 = 1, ? +1 = 1 ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of conditional Bayes risk functions for various losses assuming ? = 0.2, with ? = 1 (left) and ? = 8 (right). The balanced loss uses ? y = 1 ?y . The unequal margin loss uses ? y = 1 ? ? log 1??y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Post-hoc adjustment onStep-100 profile, CIFAR-10 and CIFAR-100. Logit adjustment outperforms weight normalisation with suitable tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Comparison of per-class balanced error on ImageNet-LT. Classes are sorted in order of frequency, and bucketed into 10 groups. Post-hoc correction on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of per-group errors. We construct three groups of classes: "Many", comprising those with at least 100 training examples; "Medium", comprising those with at least 20 and at most 100 training examples; and "Few", comprising those with at most 20 training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Summary of different approaches to learning under class imbalance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Post-hoc logit adjustment matches the Bayes performance with suitable scaling (right panel); however, any weight normalisation fails.</figDesc><table><row><cell>Method</cell><cell cols="4">CIFAR-10-LT CIFAR-100-LT ImageNet-LT iNaturalist</cell></row><row><cell>ERM</cell><cell>27.16</cell><cell>61.64</cell><cell>53.11</cell><cell>38.66</cell></row><row><cell>Weight normalisation (? = 1) [Kang et al., 2020]</cell><cell>24.02</cell><cell>58.89</cell><cell>52.00</cell><cell>48.05</cell></row><row><cell cols="2">Weight normalisation (? = ?  *  ) [Kang et al., 2020] 21.50</cell><cell>58.76</cell><cell>49.37</cell><cell>34.10</cell></row><row><cell>Adaptive [Cao et al., 2019]</cell><cell>26.65  ?</cell><cell>60.40  ?</cell><cell>52.15</cell><cell>35.42  ?</cell></row><row><cell>Equalised [Tan et al., 2020]</cell><cell>26.02</cell><cell>57.26</cell><cell>54.02</cell><cell>38.37</cell></row><row><cell>Logit adjustment post-hoc (? = 1)</cell><cell>22.60</cell><cell>58.24</cell><cell>49.66</cell><cell>33.98</cell></row><row><cell>Logit adjustment post-hoc (? = ?  *  )</cell><cell>19.08</cell><cell>57.90</cell><cell>49.56</cell><cell>33.80</cell></row><row><cell>Logit adjustment loss (? = 1)</cell><cell>22.33</cell><cell>56.11</cell><cell>48.89</cell><cell>33.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Test set balanced error (averaged over 5 trials) on real-world datasets. We use a ResNet-32 for the CIFAR datasets, and ResNet-50 for the ImageNet and iNaturalist datasets. Here, ? , are numbers for "LDAM + SGD" from Cao et al. [2019, Table 2, 3] and "? -normalised" from Kang et al. [2020,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note>summarises our results, which demonstrate our proposed logit adjustment techniques consistently outperform existing methods. Indeed, while weight normalisation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Test set balanced error (averaged over 5 trials) on real-world datasets with more complex base</cell></row><row><cell cols="2">architectures. Employing a ResNet-152 is seen to systematically improve all methods' performance,</cell></row><row><cell cols="2">with logit adjustment remaining superior to existing approaches. The final row reports the results of</cell></row><row><cell cols="2">combining logit adjustment with the adaptive margin loss of Cao et al. [2019], which yields further</cell></row><row><cell>gains on iNaturalist.</cell><cell></cell></row><row><cell>? y ?y + 1 y ? 1/4</cell><cell>-results in a top-1 accuracy of</cell></row></table><note>Indeed, we find that combining our relative margin with the adaptive margin of Cao et al. [2019] -i.e., using the pairwise margin loss with ? yy = log31.56% on iNaturalist. When using a ResNet-152, this further improves to 29.22% when trained for 90 epochs, and 28.02% when trained for 200 epochs. While such a combination is nominally heuristic, we believe there is scope to formally study such schemes, e.g., in terms of induced generalisation performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learning for deep face recognition with long-tail data. CoRR, abs/1803.09014, 2018. Bianca Zadrozny and Charles Elkan. Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '01, page 204-213, New York, NY, USA, 2001. Association for Computing Machinery. ISBN 158113391X. Denote ? y (x) = P(y | x). Suppose we employ a margin ? yy = log</figDesc><table><row><cell>Supplementary material for "Long tail learning via logit</cell></row><row><cell>adjustment"</cell></row><row><cell>A Proofs of results in body</cell></row><row><cell>Proof of Theorem 1. ? y ?y . Then, Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions, 2019. the loss is</cell></row><row><cell>Tong Zhang. Class-size independent generalization analsysis of some discriminative multi-category</cell></row><row><cell>classification methods. In Proceedings of the 17th International Conference on Neural Information</cell></row><row><cell>Processing Systems, NIPS'04, page 1625-1632, Cambridge, MA, USA, 2004. MIT Press.</cell></row><row><cell>X. Zhang, Z. Fang, Y. Wen, Z. Li, and Y. Qiao. Range loss for deep face recognition with long-</cell></row><row><cell>tailed training data. In 2017 IEEE International Conference on Computer Vision (ICCV), pages</cell></row><row><cell>5419-5428, 2017.</cell></row><row><cell>Zhi-Hua Zhou and Xu-Ying Liu. Training cost-sensitive neural networks with methods addressing</cell></row><row><cell>the class imbalance problem. IEEE Transactions on Knowledge and Data Engineering (TKDE),</cell></row><row><cell>18(1), 2006.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Test set balanced error (averaged over 5 trials) on CIFAR-10-LT and CIFAR-100-LT under the Step-100 profile; lower is better. On CIFAR-100-LT, weight normalisation edges out logit adjustment. SeeFigure 8for a demonstrated that tuned versions of the same outperfom weight normalisation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Compared to the multiclass case, we assume here a scalar score f ? R. This is equivalent to constraining that y?[L] fy = 0 for the multiclass case.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convexity, classification, and risk bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">473</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Senecal</surname></persName>
		</author>
		<idno>1045-9227</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The balanced accuracy and its posterior distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brodersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng Soon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaas</forename><forename type="middle">E</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="3121" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05381</idno>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving minority class prediction using case-specific feature weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning with non-uniform class and cost distributions: Effects and a distributed multi-classifier approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><forename type="middle">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD-98 Workshop on Distributed Data Mining</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reviving threshold-moving: a simple plug-in bagging ensemble for binary and multiclass imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Collell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drazen</forename><surname>Prelec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustubh</forename><forename type="middle">R</forename><surname>Patil</surname></persName>
		</author>
		<idno>abs/1606.08698</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum likelihood in cost-sensitive learning: Model specification, approximations, and upper bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Dmochowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">C</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3313" to="3332" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning with average top-k loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="497" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining data mining and machine learning for effective user profiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strictly proper scoring rules, prediction, and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilmann</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">477</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic forecasts, calibration and sharpness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilmann</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadoua</forename><surname>Balabdaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="268" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Apportioned margin approach for cost sensitive large margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee-Ad</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryeh</forename><surname>Kontorovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Approximated structured prediction for learning large scale graphical models. CoRR, abs/1006.2899</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1006.2899" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cost-sensitive support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arya</forename><surname>Iranmehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adjusting decision boundary for class imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Logistic regression in rare events data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Langche</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="163" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Some new bounds on the generalization error of combined classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Koltchinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<editor>T. K. Leen, T. G. Dietterich, and V. Tresp</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="245" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Addressing the curse of imbalanced training sets: One-sided selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate uncertainties for deep learning using calibrated regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The perceptron algorithm with uneven margins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaz</forename><forename type="middle">S</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning, ICML &apos;02</title>
		<meeting>the Nineteenth International Conference on Machine Learning, ICML &apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1558608737</biblScope>
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A note on margin-based loss functions in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno>0167-7152</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno>978-3-030-01216-8</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning when data sets are imbalanced and when costs are unequal and unknown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Maloof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2003 Workshop on Learning from Imbalanced Datasets</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Risk minimization, probability elicitation, and costsensitive SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Masnadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ICML&apos;10<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">9781605589077</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the statistical consistency of algorithms for binary classification under class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harikrishna</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivani</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Combining statistical learning with a knowledge-based approach -a case study in intensive care monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Morik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brockhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno>1-55860-612-2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning (ICML)</title>
		<meeting>the Sixteenth International Conference on Machine Learning (ICML)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="268" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A general framework for forecast verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">H</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1330" to="1338" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Entropy and margin maximization for structured output learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pletscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jos? Luis Balc?zar, Francesco Bonchi, Aristides Gionis, and Mich?le Sebag</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
	<note>Machine Learning and Knowledge Discovery in Databases</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Machine learning from imbalanced data sets 101</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-2000 Workshop on Imbalanced Data Sets</title>
		<meeting>the AAAI-2000 Workshop on Imbalanced Data Sets</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive weighted learning for unbalanced multicategory classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingye</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Composite binary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2387" to="2422" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Support vector machines maximizing geometric margins for multi-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Tatsumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuzo</forename><surname>Tanino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="815" to="840" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Performance evaluation of multiobjective multiclass support vector machines maximizing geometric margins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Tatsumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Akao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Kawachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuzo</forename><surname>Tanino</surname></persName>
		</author>
		<idno>2155-3289</idno>
	</analytic>
	<monogr>
		<title level="j">Numerical Algebra, Control &amp; Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">151</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Class imbalance, redux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Trikalinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Asymmetric support vector machines: Low false-positive learning under the user tolerance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Pei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Syan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;08</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="749" to="757" />
		</imprint>
	</monogr>
	<note>ISBN 9781605581934</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The logit model and response-based samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Manski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="302" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Identifying and compensating for feature deviation in imbalanced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-You</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sampling-bias-corrected neural modeling for large corpus item recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems, RecSys &apos;19</title>
		<meeting>the 13th ACM Conference on Recommender Systems, RecSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
	<note>ISBN 9781450362436</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
