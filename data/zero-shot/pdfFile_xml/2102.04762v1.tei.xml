<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Referring Segmentation in Images and Videos with Cross-Modal Self-Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Referring Segmentation in Images and Videos with Cross-Modal Self-Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Referring segmentation</term>
					<term>actor and action segmentation</term>
					<term>cross-modal</term>
					<term>self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of referring segmentation in images and videos with natural language. Given an input image (or video) and a referring expression, the goal is to segment the entity referred by the expression in the image or video. In this paper, we propose a cross-modal self-attention (CMSA) module to utilize fine details of individual words and the input image or video, which effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the visual input. We further propose a gated multi-level fusion (GMLF) module to selectively integrate self-attentive cross-modal features corresponding to different levels of visual features. This module controls the feature fusion of information flow of features at different levels with high-level and low-level semantic information related to different attentive words. Besides, we introduce cross-frame self-attention (CFSA) module to effectively integrate temporal information in consecutive frames which extends our method in the case of referring segmentation in videos. Experiments on benchmark datasets of four referring image datasets and two actor and action video segmentation datasets consistently demonstrate that our proposed approach outperforms existing state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L ANGUAGE is a natural media of communication between human and computers. In the computer vision community, there has been a lot of work at the intersection of vision and language. Many novel and interesting tasks of scene understanding are explored to combine linguistic and visual data simultaneously, such as image captioning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, visual question answering <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and visual grounding <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In this paper, we investigate the referring segmentation problem in images and videos. Given an input image (or video) and a referring expression in natural language, the goal is to produce a segmentation mask of the referred entity in the image (or frames of the video). A reliable solution of this problem will provide a natural way for human-machine interactions. Referring segmentation is a challenging problem at the intersection of computer vision and natural language processing. Learning correlations between visual and linguistic modalities in the spatial regions is necessary to accurately identify the referred entity and produce the corresponding segmentation mask.</p><p>Referring segmentation is different from other well-explored segmentation tasks that assume a pre-defined category set, including semantic segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, instance segmentation <ref type="bibr" target="#b10">[11]</ref>, or biology-inspired models based on the human visual cognition system for saliency detection <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The object-of-interest in referring segmentation has to be implicitly inferred according to arXiv:2102.04762v1 [cs.CV] 9 Feb 2021 the query expression in the natural language form. (Best viewed in color) Illustration of our cross-modal self-attention mechanism. It is composed of three joint operations: self-attention over language (shown in red), self-attention over image representation (shown in green), and cross-modal attention between language and image (shown in blue). The visualizations of linguistic and spatial feature representations (in bottom row) show that the proposed model can focus on specific key words in the language and spatial regions in the image that are necessary to produce precise referring segmentation masks. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates some examples for this task. Given an image or a clip of a video with an expression query, the goal is to produce a pixel-level segmentation mask specified by the query shown on the top of the corresponding image/video frames. We can observe that the referring expression is not limited to specifying object categories (e.g. "person", "dog"). It can take any free form of language descriptions which may contain appearance attributes (e.g. "greeny"), actions (e.g. "holding", "walking") and relative relationships (e.g. "near", "right of"), etc. It is also noteworthy that the referred entity can be more flexibly represented than traditional segmentation tasks. For instance, in the second example of <ref type="figure" target="#fig_0">Fig. 1</ref>, the referred regions can not be represented by a simple word. Instead, the referred entity can be a certain part of the image with relative relationships to other explicit objects (e.g., "right leg").</p><p>Several existing works have been proposed to tackle this problem. A popular approach (e.g. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>) in this area is to use convolutional neural network (CNN) to extract visual feature, and separably adopt recurrent neural network (RNN) or another CNN to represent the referring expression. The resultant visual and linguistic representations are then combined by concatenation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, visual context <ref type="bibr" target="#b15">[16]</ref> or dynamic filter <ref type="bibr" target="#b16">[17]</ref> to produce the final pixel-wise segmentation result. However, the limitation of this approach is that the language encoding module directly encodes the referring expression as a whole and may ignore fine details of some individual words in the referring expression, which are important to identify the referred entities and produce an accurate segmentation mask.</p><p>Some previous works (e.g. <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>) focus on learning multimodal interaction in a sequential manner to process each word in the referring expression. Specifically, the visual feature is sequentially merged with every encoding word from the output of LSTM-based <ref type="bibr" target="#b19">[20]</ref> language model at each step to infer a multimodal feature representation. But the multimodal interaction in these works only considers the linguistic and visual information individually within their local contexts. It may not sufficiently capture global interaction information essential for semantic understanding and segmentation.</p><p>To address the limitations of aforementioned methods, we propose a cross-modal self-attention (CMSA) module to effectively learn long-range dependencies from multimodal features that represent both visual and linguistic information. Our model can adaptively focus on important regions in the image and informative keywords in the language description. <ref type="figure">Fig. 2</ref> shows an example that illustrates the cross-modal self-attention module, where the correlations among words in the language and regions in the image are presented. We further develop a gated multi-level fusion (GMLF) module to refine the segmentation mask of the referred entity. The gated fusion module is designed to selectively leverage multi-level self-attentive features which contain diverse visual semantic information related to different attentive words. In addition, we extend self-attention mechanism into the temporal domain to model dependencies across frames in a video clip. The proposed cross-frame self-attention (CFSA) module can exploit temporary correlation information from a clip of the video to generate a robust representation for each frame.</p><p>In summary, our main contributions include: (1) A cross-modal self-attention (CMSA) module for referring segmentation. Our module effectively captures the long-range dependencies between linguistic and visual contexts to produce a robust multimodal feature representation for the task. (2) A gated multi-level fusion (GMLF) module to selectively integrate multi-level self-attentive features and effectively capture fine details for producing precise segmentation masks. (3) A cross-frame self-attention (CFSA) module for extracting effective feature information from a clip of the video for the current frame. (4) An extensive empirical study on four referring image segmentation benchmark datasets and two actor and action video segmentation datasets. The experimental results demonstrate that our proposed method achieves superior performance compared with state-of-the-art methods.</p><p>A preliminary version of this work has appeared as <ref type="bibr" target="#b20">[21]</ref>. Compared with <ref type="bibr" target="#b20">[21]</ref>, we improve the experimental results of our model by introducing a word attention-aware pooling module that extracts a more robust multimodal feature of individual word for an expression sentence. (2) The work in <ref type="bibr" target="#b20">[21]</ref> only considers referring segmentation in images. In this paper, we also consider referring segmentation in videos. In particular, we propose a crossframe self-attention (CFSA) module that captures the correlation between frames in a video clip. This module can be used to build more robust visual feature of each frame and improve the performance of referring segmentation in videos. (3) We perform more extensive experimental evaluations and more comprehensive review of related works.</p><p>The rest of this paper is organized as follows. We first introduce the related work in Sec. 2. Then we describe the proposed approach with detailed descriptions for multimodal feature generation, cross-modal self-attention, gated multi-level fusion and cross-frame self-attention in Sec. 3. Sec. 4 presents experimental setup such as implementation details and datasets, and extensive experimental results for performance evaluation and comparison. Finally, conclusions are drawn in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review several lines of research related to our work in the fields of attention, semantic segmentation, referring image localization and segmentation, actor and action video segmentation and referring video localization and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention</head><p>Attention mechanism has been shown to be a powerful technique in deep learning models and has been widely used in various tasks in natural language processing <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> to capture keywords for context. In the multimodal tasks, word attention has been used to re-weight the importance of image regions for image caption generation <ref type="bibr" target="#b0">[1]</ref>, image question answering <ref type="bibr" target="#b4">[5]</ref> and referring image segmentation <ref type="bibr" target="#b15">[16]</ref>. In addition, attention is also used for modeling subject, relationship and object <ref type="bibr" target="#b23">[24]</ref> and for referring relationship comprehension <ref type="bibr" target="#b6">[7]</ref>. The diverse attentions of query, image and objects are calculated separately and then accumulated circularly for visual grounding in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Self-attention <ref type="bibr" target="#b22">[23]</ref> is proposed to attend a word to all other words for learning relations in the input sequence. It significantly improves the performance for machine translation. This technique is also introduced in videos to capture long-term dependencies across temporal frames <ref type="bibr" target="#b24">[25]</ref>. Inspired by bidirectional encoder representations from transformers (BERT) <ref type="bibr" target="#b25">[26]</ref>, a VisualBERT model is proposed by <ref type="bibr" target="#b26">[27]</ref> to simultaneously integrate BERT and self-attention mechanism with bounding boxes for several visionand-language tasks. Another BERT-like model ViLBERT <ref type="bibr" target="#b27">[28]</ref> addresses vision and language inputs in two separate streams and then use co-attentional transformer layers to learn a joint taskagnostic representation. Different from these works, we propose a cross-modal self-attention module to bridge attentions across language and vision. Our network can achieve fine-grained interaction between each region in the visual features and each word in the linguistic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic segmentation</head><p>Semantic segmentation has achieved great advances in recent years. Fully convolutional networks (FCN) <ref type="bibr" target="#b28">[29]</ref> take advantage of fully convolutional layers to train a segmentation model in an end-to-end way by replacing fully connected layers in CNN with convolutional layers. In order to alleviate the down-sampling issue, deconvolution network <ref type="bibr" target="#b8">[9]</ref> that consists of deconvolution and unpooling layers are appended to the CNN for detailed structures and scales of the objects. DeepLab <ref type="bibr" target="#b9">[10]</ref> further adopts dilated convolution to enlarge the receptive field and uses atrous spatial pyramid pooling for multi-scale segmentation. An improved pyramid pooling module <ref type="bibr" target="#b29">[30]</ref> further enhances the use of multi-scale structure. These methods enlarges the semantic context to produce precise segmentation masks. Lower level features are explored to bring more detailed information to complement high-level features for generating more accurate segmentation masks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Referring image localization and segmentation</head><p>Referring image localization aims to localize specific objects in an image according to the description of a referring expression. It has been explored in natural language object retrieval <ref type="bibr" target="#b5">[6]</ref> and modeling relationship <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In order to obtain a more precise result, referring image segmentation is proposed to produce a segmentation mask instead of a bounding box. This problem was first introduced in <ref type="bibr" target="#b13">[14]</ref>, where CNN and LSTM are used to extract visual and linguistic features, respectively. They are then concatenated together for spatial mask prediction. To better achieve word-to-image interaction, <ref type="bibr" target="#b17">[18]</ref> directly combines visual features with each word feature from a language LSTM to recurrently refine segmentation results. Dynamic filter <ref type="bibr" target="#b18">[19]</ref> for each word further enhances this interaction. In <ref type="bibr" target="#b15">[16]</ref>, word attention is incorporated in the image regions to model key-wordaware context. Low-level visual features are also exploited for this task in <ref type="bibr" target="#b14">[15]</ref>, where Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b31">[32]</ref> progressively refines segmentation masks from high-level to lowlevel features sequentially. In this paper, we propose to adaptively integrate multi-level self-attentive features by the gated fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Actor and action video segmentation</head><p>The task of actor and action video segmentation pairs actor and its action to infer reasonable activities with the consideration of the main object. In <ref type="bibr" target="#b32">[33]</ref>, a multi-layer conditional random field model is introduced to use supervoxel segmentation and features from a video for multiple-label cases in the Actor-Action Dataset. A multi-task detection method is proposed in <ref type="bibr" target="#b33">[34]</ref> to explore objectaction relationships of objects and their performing actions in a video from joint learning. Their work can be further extended to produce pixel-wise segmentation masks from bounding boxes. However, these video segmentation methods have a limited actoraction class space due to the demand of a pre-defined set of actor-action pairs. The recent advance of word embedding shows potential to enlarge a fixed class vocabulary to rich and flexible language descriptions. Several works have supplemented existing video datasets with linguistic sentences. A person search model utilizes a spatio-temporal human detection model for candidate tubes and multimodal feature embedding methods to retrieval person with person descriptions <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Referring video localization and segmentation</head><p>Conventionally, a target object needs to be specified in the first frame of a video by a pixel-level mask or scribbles for guiding video object segmentation in the remaining frames. Referring video localization and segmentation is a relatively new task. It uses referring expressions to identify the object of interest in videos. A language grounding model with temporal consistency and a static segmentation model are introduced by <ref type="bibr" target="#b35">[36]</ref> to solve referring video segmentation problem. In <ref type="bibr" target="#b36">[37]</ref>, object localization and tracking is specified by a natural language query rather than specifying the target in the first frame of a video by a bounding box. A closely related work to this paper is <ref type="bibr" target="#b16">[17]</ref>, which provides sentences for actor-action pairs and proposes an encoder-decoder architecture with dynamic filters. Recently, an asymmetric crossguided attention network <ref type="bibr" target="#b37">[38]</ref> leverages cross-modality attention mechanism to implement feature interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED MODEL</head><p>The overall architecture of our model is shown in <ref type="figure">Fig. 3</ref>. The input consists of an image or a clip of a video, with a referring expression as the query. We first use a CNN to extract visual feature maps at different levels from the input image (red box at top left in <ref type="figure">Fig. 3</ref>) or a clip of a video enhanced by the crossframe self-attention module (gold box at bottom left in <ref type="figure">Fig. 3</ref>).  <ref type="figure">Fig. 3</ref>. An overview of our approach. The proposed model consists of three components including multimodal feature generation, cross-modal self-attention (CMSA) and a gated multi-level fusion. Multimodal feature generation are constructed from the visual feature, the spatial coordinate feature and the language feature for each word. For referring segmentation in video, a clip of adjacent frames is used to extract temporal correlation feature by cross-frame self-attention (CFSA) for producing visual feature of the current frame as shown in the gold box. Then the multimodal feature at each level is fed to a cross-modal self-attention module to build long-range dependencies across individual words and spatial regions. Finally, the gated multi-level fusion module combines the features from different levels to produce the final segmentation mask. The red arrows and gold arrows show the input and output for the image and video, respectively.</p><p>Each word in the referring expression is represented as a vector of word embedding. Every word vector is then appended to the visual feature map to produce a multimodal feature map. Thus, there is a multimodal feature map for each word in the referring expression. We then introduce self-attention <ref type="bibr" target="#b22">[23]</ref> mechanism to combine the feature maps of different words into a cross-modal self-attentive feature map. The self-attentive feature map captures rich information and long-range dependencies of both linguistic and visual information of the inputs. In the end, the self-attentive features from multiple levels are combined via a gating mechanism to produce the final features used for generating the segmentation output. For referring segmentation in videos, the input consists of a video and a referring expression (bottom left in <ref type="figure">Fig. 3</ref>). In this case, we first use a cross-frame self-attention module to capture the temporal correlations between frames within a video clip. We then use the output of this module to build better visual features for frames in a video. Then we apply the remaining pipeline in the architecture to produce the segmentation mask in each frame of the video.</p><p>Our model is motivated by several observations. First of all, in order to solve referring segmentation, we typically require detailed information of certain individual words (e.g. words like "left", "right"). Previous works (e.g. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>) take word vectors as inputs and use LSTM to produce a vector representation of the entire referring expression. The vector representation of the entire referring expression is then combined with the visual features for referring image segmentation. The potential limitation of this technique is that the vector representation produced by LSTM captures the meaning of the entire referring expression while missing sufficiently detailed information of some individual words needed for the referring image segmentation task. Our model addresses this issue by maintaining word vector for each word in the entire referring expression instead of using LSTM. Therefore, it can better capture more detailed word-level information. Secondly, some previous works (e.g. <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>) process each word in the referring expression and concatenate it with visual features to infer the referred object in a sequential order using a recurrent network. The limitation of these methods is that these methods only look at each spatial region of feature maps locally and lack the interaction over long-range spatial regions in global context which is essential for semantic understanding and segmentation. In contrast, our model uses a cross-modal self-attention module that can effectively model long-range dependencies between linguistic and visual modalities. Thirdly, different from <ref type="bibr" target="#b14">[15]</ref> which adopts ConvLSTM to refine segmentation with multi-scale visual features sequentially, the proposed method employs a novel gated fusion module for combining multi-level self-attentive features. Lastly, we introduce the cross-frame self-attention module to build longrange dependencies across frames in the video. It learns temporal correlations from adjacent frames for the current frame and extends our model to video domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimodal Feature Generation</head><p>We first introduce the generic modules of the proposed model by taking an image as input for simplification, then we will present details of the cross-frame self-attention module and illustrate how to extend our model to video domain. Let I denote an input image and N represent the length of a referring expression. Then each individual word in the referring expression can be denoted as w n , n ? 1, 2, ..., N . We use a backbone CNN network to extract visual features from the input image I. The feature map is extracted from a specific CNN layer and represented as V ? R H?W ?Cv , where H, W and C v are the dimensions of height, width and feature channel, respectively. For ease of presentation, we only use features extracted from one particular CNN layer for now. Later in Sec. 3.3, we present an extension of our method that uses multi-level features from multiple CNN layers for gated fusion.</p><p>For the language description with N words, each word w n is encoded as a one-hot vector to project it into a compact word embedding represented as e n ? R C l by a lookup table. Different from previous methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> that simply apply LSTM to process the word vectors sequentially and take the output hidden layer to encode the entire description sentence with a sentence vector, we propose to keep the individual word vector and introduce a cross-modal self-attention module to capture longrange correlations between these words and spatial regions in the image. More details will be presented in Sec. 3.2.</p><p>In addition to visual features and word vectors, spatial coordinate features have also been shown to be useful for referring image segmentation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Following prior works, we define an 8-D spatial coordinate feature map at each spatial position using the implementation in <ref type="bibr" target="#b17">[18]</ref>. Specifically, the first 3-dimensions of the spatial coordinate feature map encode the normalized horizontal positions. The next 3-dimensions encode normalized vertical positions. The last 2-dimensions encode the normalized width and height information of the image.</p><p>After obtaining the visual, linguistic and spatial features, we finally construct a joint multimodal feature representation at each spatial position for each word by concatenating the visual features, word vectors, and spatial coordinate features. Let p be a spatial position in the feature map V , i.e. p ? {1, 2, ..., H ? W }. We use v p ? R Cv to denote the "slice" of the visual feature vector at a specific spatial location p. The spatial coordinate feature of the position p is denoted as s p ? R 8 . Thus the multimodal feature f pn can be defined corresponding to the position p and the n-th word as follows:</p><formula xml:id="formula_0">f pn = Concat v p ||v p || 2 , e n ||e n || 2 , s p<label>(1)</label></formula><p>where || ? || 2 denotes the L 2 norm of a vector and Concat(?) denotes the concatenation of several input vectors. The multimodal feature vector f pn encodes information about the combination of a specific position p in the image and the n-th word w n in the referring expression with a total dimension of (C v + C l + 8). We use F = {f pn : ?p, ?n} to represent the collection of features f pn for different spatial positions and words. The final dimension</p><formula xml:id="formula_1">of F is N ? H ? W ? (C v + C l + 8).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-Modal Self-Attention</head><p>The generated multimodal feature F maintains rich and fine features for each spatial position and word. However, it is quite huge to further process directly and may contain a lot of redundant information. Additionally, the size of F is variable depending on the number of words in the referring expression. It is difficult to directly exploit F to produce the segmentation output. In recent years, the attention mechanism [1], <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> has been shown to be a powerful technique that can capture important information from raw features in either linguistic or visual representation. Different from above works, we propose a cross-modal self-attention module to jointly exploit attentions over detailed ultimodal features. In particular, inspired by the success of self-attention <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, our designed cross-modal selfattention module can capture long-range dependencies in three aspects including self-attention over language, self-attention over image representation and cross-modal attention between the words in a referring expression and different spatial positions in the input image. The proposed module takes F as the input and produces a feature map that summarizes F after learning the correlation between the language expression and the visual context. Given a multimodal feature vector f pn , the cross-modal selfattention module first produces a set of query, key and value pair by linear transformations as q pn = W q f pn , k pn = W k f pn and v pn = W v f pn at each spatial position p and the n-th word, where {W q , W k , W v } are part of the model parameters to be learned. Each query, key and value is reduced from the high dimension of multimodal features to the dimension of 512 in our implementation, i.e. W q , W k , W v ? R 512?(Cv+C l +8) , for computation efficiency.</p><p>We then compute the cross-modal self-attentive feature v pn as follows:</p><formula xml:id="formula_2">v pn = p n a p,n,p ,n v p n , where (2) a p,n,p ,n = Softmax(q T p n k pn )<label>(3)</label></formula><p>where a p,n,p ,n is the attention score that takes into account of the correlation between (p, n) and any other combinations of spatial position and word (p , n ).</p><p>Then v pn is transformed back to the same dimension as f pn via another linear layer and is added element-wise with f pn to form a residual connection as f pn = W v v pn + f pn . This allows the insertion of this module into to the backbone network without breaking its behavior <ref type="bibr" target="#b38">[39]</ref>. Word attention-aware pooling: In order to collect the attention distribution of each word and produce a single output feature map for segmentation, we propose the word attention-aware pooling to summarize attention scores by pooling separate multimodal feature of individual word to a feature representation for the whole expression sentence.</p><p>From the collection of the attention scores {a p,n,p ,n : ?p, ?n, ?p , ?n } in Eq. 3, we sum up all combinations for different pairs of (p, n) and (p , n ) for each word n. As a result, we can get a vector of length N as the accumulation of attention scores for all words. The word attention score over all words in the expression can be calculated by softmax operation as: a n = Softmax( p,p ,n a p,n,p ,n )</p><p>Using Eq. 4, we can obtain word attentions from the intermediate result of the self-attention matrix and use this information to adaptively pool individual word feature vector together for the expression sentence. To be more specific, we multiply each multimodal feature vector f pn by its corresponding word attention a n and accumulate f pn over all words to result in the final feature representation: f p = n a n f pn</p><p>The final feature map F = { f p : ?p} denotes the collection of f p at all spatial positions, i.e. F ? R H?W ?(Cv+C l +8) . Unlike previous methods that keep the entire input feature size <ref type="bibr" target="#b24">[25]</ref> or use average-pooling to generate spatial feature map <ref type="bibr" target="#b20">[21]</ref>, the proposed word attention-aware pooling effectively reduces the size of input multimodal feature and fits different numbers of input words present in the sentence. It also obtains informative features by taking advantage of the intermediate result of the self-attention operation. The summarization of the attention score for each word help generate effective multimodal representation of the referring expression from word-level features. <ref type="figure">Fig. 4</ref> illustrates the process of generating cross-modal self-attentive features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gated Multi-Level Fusion</head><p>The feature representation F obtained from Eq. 5 is specific to a particular layer in CNN. Previous work <ref type="bibr" target="#b14">[15]</ref> has shown that  <ref type="figure">Fig. 4</ref>. An illustration of the process of generating the cross-modal selfattentive (CMSA) feature from an image and a language expression ("man in yellow shirt"). We use ? and ? to denote matrix multiplication and element-wise summation, respectively. The softmax operation is performed over each row which indicates the attentions across each visual and language cell in the multimodal feature. The word attentionaware pooling summarizes word-level features for multimodal representation of the referring expression. We also visualize the internal linguistic and spatial representations. Please refer to Sec. 4.3 and Sec. 4.4 for more details.</p><p>fusing features at multiple scales can improve the performance of referring image segmentation. In this section, we introduce a novel gated fusion technique to integrate multi-level features.</p><p>Let F (i) be the cross-modal self-attentive feature map at the i-th level. Following <ref type="bibr" target="#b14">[15]</ref>, we use ResNet based DeepLab-101 as the backbone CNN and consider feature maps at three levels (i = 1, 2, 3) corresponding to ResNet blocks Res3, Res4 and Res5. Let C vi be the channel dimension of the visual feature map at the i-th level of the network. We use F (i) = { f (i) p : ?p} to indicate the collection of cross-modal self-attentive features f (i) p ? R Cv i +C l +8 for different spatial locations corresponding to the ith level. Our goal is to fuse the feature maps F (i) (i = 1, 2, 3) to produce a fused feature map for producing the final segmentation output. Note that the feature maps F (i) have different channel dimensions at different level i. At each level, we apply a 1 ? 1 convolutional layer to make the channel dimensions of different levels consistent and result in an output X (i) .</p><p>For the i-th level, we use linear transformations for X (i) to generate a memory gate m i and a reset gate r i (r i , m i ? R Hi?Wi ), respectively. These gates play similar roles to the gates in LSTM. Different from stage-wise memory updates <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b39">[40]</ref>, the computation of gates at each level is decoupled from other levels. The gates at each level control how much the feature at each level contributes to the final fused feature. Each level also has its own contextual controller G i which modulates the information flow from other levels to the i-th level. This process can be summarized as:</p><formula xml:id="formula_5">G i = (1 ? m i ) X i + j?{1,2,3}\{i} ? j m j X j F i o = r i tanh(G i ) + (1 ? r i ) X i , ?i ? {1, 2, 3}<label>(6)</label></formula><p>where denotes Hadamard product. ? j is a learnable parameter to adjust the relative ratio of the memory gate which controls information flow of features from different levels j combined to the current level i.</p><p>In order to obtain the segmentation mask, we aggregate the feature maps F i o from the three levels and apply a 3 ? 3 convolutional layer followed by the sigmoid function. This sequence of operations outputs a probability map (P ) indicating the likelihood of each pixel being the foreground in the segmentation mask, i.e.:</p><formula xml:id="formula_6">P = ? C 3?3 3 i=1 F i o (7)</formula><p>where ?(?) and C 3?3 denote the sigmoid and 3 ? 3 convolution operation, respectively. A binary cross-entropy loss function is defined on the predicted output and the ground-truth segmentation mask Y as follows:</p><formula xml:id="formula_7">L = ? 1 ? ? m=1 (Y (m) log P (m) + (1 ? Y (m)) log(1 ? P (m))) (8)</formula><p>where ? is the whole set of pixels in the image and m is m-th pixel in it. We use the Adam algorithm <ref type="bibr" target="#b40">[41]</ref> to optimize the loss in Eq. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cross-Frame Self-Attention</head><p>The task of actor and action video segmentation from a sentence extends a fixed number of actor and action pairs to an open set of vocabulary with rich language descriptions. It is similar to the referring image segmentation, but requires producing a segmentation mask in frames of a video according to the referring query. We use this task to demonstrate the referring segmentation in videos. We propose a cross-frame self-attention module to learn temporal correlations from adjacent frames for the current frame. For an input frame T , we extract an adjacent clip of the current frame with the interval of ? . The CNN backbone is then used to extract features for every frame in the clip and result in a set of features as {V t } T +? T ?? . Similar to the implementation in Eq. 2 and Eq. 3, let v t,p ? R Cv be a feature vector in a frame t and at a specific spatial position p. The cross-frame self-attention feature v tp can be obtained by:</p><formula xml:id="formula_8">v tp = p t a t,p,t ,p v t p , where<label>(9)</label></formula><p>a t,p,t ,p = Softmax(q T t p k tp )</p><p>where a t,p,t ,p is the attention score that represents the correlation between (t, n) and any other combinations of frame and spatial position (t , p ). Attention-aware pooling of frames is applied over frames to generate the temporal correlation feature v p as follows:</p><formula xml:id="formula_10">v p = t v tp (Softmax( t ,p,p a t,p,t ,p ))<label>(11)</label></formula><p>The generated feature map V = { v p : ?p} collects v p at all spatial positions. Then the temporal correlation feature V is used to update the visual feature V t at the frame t as: V t := V t + V . The final visual feature V t incorporates correlations learned at different positions across all frames in the clip to better capture temporal information for the referring segmentation in videos. Finally, V t can be seen as the visual feature V in Sec. 3.1 to combine with word vectors and spatial features for multimodal feature, and is followed by cross-modal self-attention module in Sec. 3.2 and gated multi-level fusion in Sec. 3.3 for producing segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first introduce the experimental setup including implementation details, experimental datasets and evaluation metrics in Sec. 4.1. Then we perform detailed ablation experiments to demonstrate the advantage and the relative contribution of each component of the proposed approach in Sec. 4.2. Comprehensive quantitative and qualitative results of our method are evaluated and compared with other state-of-the-art methods in Sec. 4.3. We also provide visualization and failure cases to help gain insights of our model in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation details</head><p>Following previous contemporary work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we adopt DeepLab-101 <ref type="bibr" target="#b9">[10]</ref> as our backbone network to extract visual features. The initial weights of the network are pretrained from Pascal VOC 2012 dataset <ref type="bibr" target="#b41">[42]</ref> for effective feature representation. The multi-level visual features are extracted from DeepLab-101 ResNet blocks Res3, Res4, Res5 as the inputs for multimodal features. We resize and zero-pad the spatial resolution of all input images as 320 ? 320 and obtain the same width and height of feature maps at different levels due to the dilated convolution. The dimension used in X (i) for gated fusion is fixed to 500. In the language model, the maximum length of query expression are set to 20 and we embed each word to a vector of C l = 1000 dimensions. For the segmentation in videos, we set ? = 5 to truncate totally 10 adjacent frames besides a target frame for CF SA. The network is trained with Adam optimizer <ref type="bibr" target="#b40">[41]</ref> with an initial learning rate of 2.5e ?4 and weight decay of 5e ?4 . The learning rate is gradually decreased using the polynomial policy <ref type="bibr" target="#b9">[10]</ref> with the power of 0.9. For fair comparisons, the final segmentation masks are refined by DenseCRF <ref type="bibr" target="#b42">[43]</ref> which is a common post-processing operation for precise segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Referring image segmentation datasets</head><p>We perform extensive experiments on four referring image segmentation datasets to evaluation performance of our approach including UNC <ref type="bibr" target="#b43">[44]</ref>, UNC+ <ref type="bibr" target="#b43">[44]</ref>, G-Ref <ref type="bibr" target="#b44">[45]</ref> and ReferIt <ref type="bibr" target="#b45">[46]</ref>. The train, validation and test sets follow the setting at <ref type="bibr" target="#b17">[18]</ref>. The UNC dataset contains 19,994 images with 142,209 referring expressions for 50,000 objects. All images and expressions are collected from the MS COCO <ref type="bibr" target="#b46">[47]</ref> dataset interactively with a two-player game <ref type="bibr" target="#b45">[46]</ref> where one player gives an expression description and anther player labels the corresponding referred region in the image. Two or more objects of the same object category can appear in each image.</p><p>The UNC+ dataset is similar to the UNC dataset. but with a restriction that no location words are allowed in the referring expression. In this case, expressions regarding to referred objects totally depend on the appearance and scene context descriptions. It consists of 141,564 expressions for 49,856 objects in 19,992 images.</p><p>The G-Ref dataset is also collected based on MS COCO. It contains 104,560 expressions referring to 54,822 objects from 26,711 images. Annotations of this dataset come from Amazon Mechanical Turk instead of a two-player game. The average length of expressions is 8.4 words which is longer than that of other datasets (less than 4 words). It is more challenging to understand referring expressions for identifying the object of interest.</p><p>The ReferIt dataset is built upon the IAPR TC-12 <ref type="bibr" target="#b47">[48]</ref> dataset. It has 130,525 expressions referring to 96,654 distinct object masks in 19,894 natural images. In addition to objects, it also contains mask annotations for stuff classes such as water, sky and ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Actor and action video segmentation datasets</head><p>Referring segmentation in videos is a relatively new problem. It also requires a referring expression to describe the expected object in the sequence of a video. Recently, Gavrilyuk et al. <ref type="bibr" target="#b16">[17]</ref> complement segmentation masks in videos with referring expression descriptions in Actor-Action Dataset (A2D) dataset <ref type="bibr" target="#b32">[33]</ref> and Jointannotated HMDB (JHMDB) dataset <ref type="bibr" target="#b48">[49]</ref> which are originally developed for video actor and action segmentation.</p><p>The A2D Sentences dataset is the largest general actor and action video segmentation dataset with pixel-level mask labels collected from Youtube database. It consists of 3,782 videos in total in which 3,036 and 746 videos are served as train and test sets, respectively. There are seven classes of actors such as adult, baby and dog, and eight actions such as climbing, jumping, running and walking in the dataset. Referring expressions are augmented by <ref type="bibr" target="#b16">[17]</ref> with 6,656 sentences including 811 different nouns, 225 verbs and 189 adjectives. These linguistic annotations are labeled to discriminate each actor instance from multiple objects appeared in a video. Furthermore, it provides more diverse and finer granularities of descriptions where the actor class adult in the referring sentences can be expressed as woman, man, person, player and so on. The average length of sentences contains 7.3 words.</p><p>The JHMDB Sentences dataset is a subset of HMDB51 database <ref type="bibr" target="#b49">[50]</ref>, containing 928 videos with 21 different actions involving brush hair, clap, sit, walk, etc. It also provides a 2D articulated human puppet for scale, pose, segmentation and a coarse viewpoint. The same strategy as for the A2D Sentences dataset is applied for referring expression annotations and ends up with 928 sentences including 158 different nouns, 53 verbs and 23 adjectives. It should be noted that all 928 videos in the JHMDB Sentences dataset are used for testing and evaluating the generalization ability of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Evaluation metrics</head><p>In order to fairly compare with other methods, we follow previous work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> to use overall intersection-over-union (Overall IoU ) and Precision@X (prec@X) as the evaluation metrics. The Overall IoU metric is a ratio between intersection and union of the predicted segmentation mask and the ground truth averaged over all test data. The prec@X metric measures the percentage of test images with an IoU score higher than the threshold X, where X ? {0.5, 0.6, 0.7, 0.8, 0.9} in the experiments. For actor and action video segmentation, we additionally calculate M ean IoU for balance between various sizes of target mask regions in different videos <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We first perform ablation experiments on different variants of cross-modal self-attention module and then investigate the design choice of different attention methods and the relative contributions of multimodal feature representation and multi-level feature fusion of our proposed model. Ablation study of variants for cross-modal self-attention module. The proposed word attention-aware pooling method is compared with different pooling strategies (max pooling and average pooling) to collect the attention distribution of each word. All methods use the same base model (DeepLab-101) without multi-level feature fusion module or DenseCRF for postprocessing. <ref type="table">ReferIt  val  testA  testB  val  testA  testB  val  test  max</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNC UNC+ G-Ref</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Variants of cross-modal self-attention module</head><p>In order to evaluate the effectiveness of the proposed word attention-aware pooling method, we report the results of alternative methods that replace the word attention-aware pooling in our method with max pooling or average pooling. These methods use the same base model without multi-level feature fusion module or DenseCRF for postprocessing. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the word attention-aware pooling method consistently outperforms other two pooling methods. It demonstrates that the proposed word attention-aware pooling method can better collect the attention distribution of each word for the multimodal feature representation. The improvement is particularly significant on the G-Ref dataset. This also proves the power of the proposed word attention-aware pooling method for longer referring expressions since the G-Ref dataset has the longest average length of expressions among the four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Attention methods</head><p>We perform experiments to evaluate different attention methods in the multimodal feature representation. We alternatively use no attention, word attention, pixel attention and word-pixel pair attention by zeroing out the respective components in Eq. 2 to evaluate the influence of different attention combinations. As shown in the top of <ref type="table">Table 2</ref>, attention methods achieve better segmentation performance than the no attention method. The proposed cross-modal self-attention outperforms all other attention methods significantly. This demonstrates that the attention method plays an important role in object segmentation and the language-to-vision correlation can be better learned together within our cross-modal self-attention method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Multimodal feature representation</head><p>This experiment evaluates the effectiveness of the multimodal feature representation. Similar to the baselines, i.e. multimodal LSTM interaction in <ref type="bibr" target="#b17">[18]</ref> and convolution integration in <ref type="bibr" target="#b14">[15]</ref>, we directly take the output of the Res5 of the network to test the performance of multimodal feature representation without the multi-level feature fusion. We use CMSA-W to denote the proposed method in Sec. 3.2. In addition, a variant method CMSA-S which also uses the same cross-modal self-attentive feature, instead encodes the whole sentence to one single language vector by LSTM.</p><p>As shown in the middle four rows of <ref type="table">Table 2</ref>, the proposed cross-modal self-attentive feature based approaches achieve significantly better performance than other baselines, which manifests the advantage of our cross-modal self-attention module. Moreover, the word based method CMSA-W outperforms sentence based method CMSA-S. It reveals that encoding individual word in the referring expression can help capture detailed and useful information for multimodal feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Multi-level feature fusion</head><p>This experiment verifies the relative contribution of the proposed gated multi-level fusion module. Here we use our cross-modal self-attentive features as inputs and compare with several wellknown network structures used for obtaining fine details of the segmentation mask, such as Deconvolution (Deconv) <ref type="bibr" target="#b8">[9]</ref> and Pyramid Pooling Module (PPM) <ref type="bibr" target="#b29">[30]</ref> in semantic segmentation, and ConvLSTM <ref type="bibr" target="#b14">[15]</ref> in referring image segmentation.</p><p>In order to clearly understand the benefit of our multi-level feature fusion method, we also develop another self-gated method denoted as CM SA + Gated that uses the same gate generation method in Sec. 3.3 to generate memory gates and directly multiply by its own features without interactions with features from other levels. As presented in the bottom five rows in <ref type="table">Table 2</ref>, the proposed gated multi-level fusion outperforms these other multiscale feature methods which demonstrates the gated multi-level feature fusion module also benefits segmentation performance in terms of multi-level feature fusion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Quantitative results of referring image segmentation</head><p>We compare the referring image segmentation results of our method with other existing state-of-the-art approaches including recurrent multimodal interaction (RMI) <ref type="bibr" target="#b17">[18]</ref>, dynamic multimodal network (DMN) <ref type="bibr" target="#b18">[19]</ref>, key-word-aware network (KWA) <ref type="bibr" target="#b15">[16]</ref>, recurrent refinement networks (RRN) <ref type="bibr" target="#b14">[15]</ref> and our preliminary work CMSA <ref type="bibr" target="#b20">[21]</ref>. We report both prec@X and OverallIoU at all four datasets in <ref type="table" target="#tab_2">Table 3</ref>, <ref type="table" target="#tab_3">Table 4</ref>, <ref type="table" target="#tab_4">Table 5</ref> and <ref type="table">Table 6</ref>, respectively. Our proposed method consistently outperforms all other methods on all four datasets in terms of each threshold of prec@X and overall IoU . The improvement is particularly significant on the more challenging datasets UNC+ and G-Ref. It leads to 4.12%, 5.64% and 2.22% absolute improvement in overall IoU on the val, testA and testB set of UNC+ and nearly doubles the performance of prec@0.9 which requires a precise segmentation mask compared with the groundtruth. This demonstrates the superior performance of our model for identifying the referred objects with appearance and scene context word only since there is no location word available on the UNC+ dataset. Furthermore, the substantial gain (6.39% in overall IoU) on the G-Ref dataset proves the advantage of our model in capturing long-range dependencies for cross-modal features and capturing the referred objects based on referring expressions where this dataset contains longer and richer query expressions. The consistent improvement compared with the preliminary work CMSA <ref type="bibr" target="#b20">[21]</ref> also proves the word attention-aware pooling in the cross-modal self-attention module can summarizes more effective features from word attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Quantitative results of actor and action video segmentation</head><p>We further compare the proposed model for the task of actor and action video segmentation with three baseline methods i.e., referring segmentation model <ref type="bibr" target="#b13">[14]</ref>, lingual specification model <ref type="bibr" target="#b36">[37]</ref> and referring video segmentation model <ref type="bibr" target="#b16">[17]</ref>. <ref type="table" target="#tab_5">Table 7</ref> and <ref type="table" target="#tab_6">Table 8</ref> present the segmentation results on the A2D Sentences dataset and JHMDB Sentences dataset, respectively. The proposed referring image segmentation model obtains comparable performance on the A2D Sentences dataset and better performance on the JHMDB Sentences dataset compared with the prior best model in <ref type="bibr" target="#b16">[17]</ref>. After adding CFSA to bring temporary information of videos, our model achieves superior segmentation results over almost all evaluation metrics except a slightly inferior result at prec@0.5 on the A2D Sentences dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Qualitative results</head><p>Some qualitative examples generated by our network are shown in <ref type="figure" target="#fig_4">Figure 5</ref> for referring image segmentation and <ref type="figure" target="#fig_5">Figure 6</ref> for actor and action video segmentation. To better understand the benefit of multi-level self-attentive features, we visualize the linguistic representation to show attention distributions at different levels by Eq. 4 in Sec. 3.2. In this way we can get a vector of length N and repeat this operation for all three levels to finally obtain a matrix of 3?N . This matrix is shown in the 2nd column of <ref type="figure" target="#fig_4">Fig. 5</ref> and the 1st column of <ref type="figure" target="#fig_5">Figure 6</ref>. We can see that the attention distribution over words corresponding to a particular feature level is different.</p><p>Features at higher levels (e.g. l 3 ) tend to focus on words that refer to objects (e.g. "bowl", "elephant", "vase"). Features at lower levels (e.g. l 1 , l 2 ) tend to focus on words that refer to attributes (e.g. "white","walking") or relationships (e.g. "left of", "middle right","second") which helps complement detailed visual cues to localize the object of interest. As shown in the <ref type="figure" target="#fig_4">Fig. 5</ref>, our model can handle the scenarios that the referred object is at relative position to other objects with the same category, such as the bowl is far left compared with the others in the first example and even a specific order of the vase as "second from right" in the fourth example. Our model can also identify the particular object which is performing opposite move status to others shown in the second example. In another two interesting examples, the referred object can be successfully identified with discriminated appearance and location words only in the third example and exclude unexpected regions by indicating a negation word "not". Query: "big black car is falling down the hill" Query: "an orange ball is jumping up and down"</p><p>Query: "a black cat is sitting on the back" Query: "a gray white cat on the left is approaching"</p><p>Query: "white fuzzy dog is running in the corridor" Query: "person walking the dog" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization and Failure Cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Visualization</head><p>We visualize spatial feature representations with various query expressions for a given image. This helps to gain further insights on the learned model. We adopt the same technique in <ref type="bibr" target="#b14">[15]</ref> to generate visualization "chair" "couch" "red couch" "the red couch closest to you" "crowd" "people" "umbrella" "a purple umbrella" "soccer player" "soccer player "soccer player "soccer player on the grass" in red uniform" is sliding" Query: "boy on right"</p><p>Query: "giraffe standing next to a giant giraffe"</p><p>Query: "green car running on the street" <ref type="figure">Fig. 8</ref>. Some failure examples of our model. In the first two rows, we show examples of failure cases in images. Here we show the original image (1st column), the output of our method (2nd column) and the ground-truth (3rd column). The failures of images are due to factors such as language ambiguity (1st row), and similar object appearance and occlusion (2nd row). We also show a failure case of video in the last two rows. Row 3 shows frames in a video and row 4 shows the corresponding outputs of our method. Our model fails to precisely segment the object in the last frame because of the disappearing of the fast moving object.</p><p>heatmaps over spatial locations. It is created by normalizing the strongest activated channel of the last feature map, which is then upsampled to match with the size of the original input image. Note that here we attempt various referring expressions that are different from those in the test set to show the generalization ability of our model. <ref type="figure" target="#fig_6">Fig. 7</ref> presents these visualized heatmaps. It can be observed that our model is able to correctly respond to different query expressions with various categories, locations and relationships. For instance, it can identify the object of the same category with different noun words like "chair" and "couch", and "crowd" and "people" in the first two rows. When the query is "people" and "umbrella", our model can highlight every person and umbrella in the image as shown in the second row. Similarly, in the last row, when the query is "soccer player", it captures all players present in the scenario. For more specific phrases such as "soccer player in red uniform" and "soccer player is sliding", the model accurately identifies the correspond players being referred to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Failure cases</head><p>We also present some interesting failure examples in <ref type="figure">Fig. 8</ref>. These failure cases are caused by the ambiguity of the language (e.g. two boys on right in the first example), similar object appearance and occlusion (e.g. two giraffes are standing front and back with quite similar appearance). The bottom two rows show three frames of a video and our segmentation masks. Our methods fails to produce the precise segmentation mask because of the disappearing of the fast moving object (e.g., the green car is quickly passing through the camera with varying deformations). Some of these failure cases may potentially be fixed by applying object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have proposed cross-modal self-attention and gated multi-level fusion modules to address two crucial challenges in the referring segmentation task, and cross-frame self-attention module to extend our model to video domain. Our cross-modal self-attention module captures long-range dependencies between visual and linguistic modalities, which results in a better feature representation to focus on important information for referred entities. The proposed gated multi-level fusion module adaptively integrates features from different levels via learnable gates for each individual level. In addition, the cross-frame self-attention module effectively extract temporal correlation feature from adjacent frames for the current frame. The proposed network achieves state-of-the-art results on all four referring image benchmark datasets and two actor and action video segmentation datasets. 61771301, 61922064, U2033210], in part by the Zhejiang Provincial Natural Science Foundation [Grant No. LR17F030001] and in part by the FoS research chair program and the GETS program at the University of Manitoba.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Referring segmentation in images and videos. Given an image (top two examples) or a clip of a video (bottom example) with a referring expression, the segmentation mask is produced according to the corresponding expression query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. (Best viewed in color) Illustration of our cross-modal self-attention mechanism. It is composed of three joint operations: self-attention over language (shown in red), self-attention over image representation (shown in green), and cross-modal attention between language and image (shown in blue). The visualizations of linguistic and spatial feature representations (in bottom row) show that the proposed model can focus on specific key words in the language and spatial regions in the image that are necessary to produce precise referring segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Small white dog walking on the right Query for image: Query for video:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Some actor and action video segmentation examples are presented in the Figure 6. In the first row, the falling car and the jumping ball have huge variations in terms of shape, location, size, etc. Our model successfully captures these variations. The examples in the middle and bottom rows show the scenarios where there Query: "the bowl of food to the left of all the other bowls" Query: "an elephant walking opposite direction of the others" Query: "white face middle right" Query: "second vase from right" Query: "sky, not the clouds" Qualitative examples of referring image segmentation: (a) original image; (b) visualization of the linguistic representation (attentions to word at each of the three feature levels); (c) segmentation mask and; (d) ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative examples of actor and action video segmentation. From left to right: the first three and last three examples represent segmentation masks for different frames in a video according to the query expressions. are multiple objects existing in the videos. The proposed approach works well to distinguish different objects of the same (middle examples) or different (bottom examples) categories according to the corresponding descriptions. Surprisingly at the bottom video, when the referred object "person" disappears in the last frame, our model can still make an ideal prediction that it does not output any segmentation mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>(Best viewed in color) Visualization of spatial feature representation. These spatial heatmaps show the responses of the network to different query expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Comparison of segmentation performance with the state-of-the-art methods on the UNC dataset in terms of prec@X and IoU.</figDesc><table><row><cell>Method</cell><cell>Set</cell><cell cols="5">prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</cell><cell>Overall IoU</cell></row><row><cell>RMI [18]</cell><cell>val</cell><cell>42.99</cell><cell>33.24</cell><cell>22.75</cell><cell>12.11</cell><cell>2.23</cell><cell>45.18</cell></row><row><cell>DMN [19]</cell><cell>val</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.78</cell></row><row><cell>RRN [15]</cell><cell>val</cell><cell>61.66</cell><cell>52.50</cell><cell>42.40</cell><cell>28.13</cell><cell>8.51</cell><cell>55.33</cell></row><row><cell>CMSA [21]</cell><cell>val</cell><cell>66.44</cell><cell>59.70</cell><cell>50.77</cell><cell>35.52</cell><cell>10.96</cell><cell>58.32</cell></row><row><cell>Ours</cell><cell>val</cell><cell>69.24</cell><cell>62.46</cell><cell>53.34</cell><cell>37.84</cell><cell>11.99</cell><cell>58.92</cell></row><row><cell>RMI [18]</cell><cell>testA</cell><cell>42.99</cell><cell>33.59</cell><cell>23.69</cell><cell>12.94</cell><cell>2.44</cell><cell>45.69</cell></row><row><cell>DMN [19]</cell><cell>testA</cell><cell>65.83</cell><cell>57.82</cell><cell>46.80</cell><cell>27.64</cell><cell>5.12</cell><cell>54.83</cell></row><row><cell>RRN [15]</cell><cell>testA</cell><cell>64.13</cell><cell>54.66</cell><cell>44.37</cell><cell>29.15</cell><cell>8.08</cell><cell>57.26</cell></row><row><cell>CMSA [21]</cell><cell>testA</cell><cell>70.02</cell><cell>63.71</cell><cell>54.64</cell><cell>38.04</cell><cell>11.65</cell><cell>60.61</cell></row><row><cell>Ours</cell><cell>testA</cell><cell>73.87</cell><cell>67.76</cell><cell>58.53</cell><cell>40.52</cell><cell>12.04</cell><cell>62.01</cell></row><row><cell>RMI [18]</cell><cell>testB</cell><cell>44.99</cell><cell>34.21</cell><cell>22.69</cell><cell>11.84</cell><cell>2.65</cell><cell>45.57</cell></row><row><cell>DMN [19]</cell><cell>testB</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.13</cell></row><row><cell>RRN [15]</cell><cell>testB</cell><cell>59.35</cell><cell>50.32</cell><cell>39.82</cell><cell>27.30</cell><cell>10.05</cell><cell>53.95</cell></row><row><cell>CMSA [21]</cell><cell>testB</cell><cell>61.32</cell><cell>53.84</cell><cell>44.69</cell><cell>32.78</cell><cell>12.58</cell><cell>55.09</cell></row><row><cell>Ours</cell><cell>testB</cell><cell>64.55</cell><cell>56.62</cell><cell>47.11</cell><cell>34.41</cell><cell>12.78</cell><cell>56.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Comparison of segmentation performance with the state-of-the-art methods on the UNC+ dataset in terms of prec@X and IoU.</figDesc><table><row><cell>Method</cell><cell>Set</cell><cell cols="5">prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</cell><cell>Overall IoU</cell></row><row><cell>RMI [18]</cell><cell>val</cell><cell>20.52</cell><cell>14.02</cell><cell>8.46</cell><cell>3.77</cell><cell>0.62</cell><cell>29.86</cell></row><row><cell>DMN [19]</cell><cell>val</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.88</cell></row><row><cell>RRN [15]</cell><cell>val</cell><cell>37.32</cell><cell>28.96</cell><cell>20.31</cell><cell>11.33</cell><cell>2.66</cell><cell>39.75</cell></row><row><cell>CMSA [21]</cell><cell>val</cell><cell>44.63</cell><cell>37.86</cell><cell>29.86</cell><cell>20.10</cell><cell>5.33</cell><cell>43.76</cell></row><row><cell>Ours</cell><cell>val</cell><cell>45.48</cell><cell>38.44</cell><cell>30.08</cell><cell>19.77</cell><cell>5.63</cell><cell>43.87</cell></row><row><cell>RMI [18]</cell><cell>testA</cell><cell>21.22</cell><cell>14.43</cell><cell>8.99</cell><cell>3.91</cell><cell>0.49</cell><cell>30.48</cell></row><row><cell>DMN [19]</cell><cell>testA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.22</cell></row><row><cell>RRN [15]</cell><cell>testA</cell><cell>40.80</cell><cell>31.66</cell><cell>22.74</cell><cell>12.78</cell><cell>2.78</cell><cell>42.15</cell></row><row><cell>CMSA [21]</cell><cell>testA</cell><cell>51.19</cell><cell>43.42</cell><cell>34.25</cell><cell>22.41</cell><cell>5.82</cell><cell>47.60</cell></row><row><cell>Ours</cell><cell>testA</cell><cell>51.41</cell><cell>43.19</cell><cell>34.40</cell><cell>23.65</cell><cell>6.08</cell><cell>47.79</cell></row><row><cell>RMI [18]</cell><cell>testB</cell><cell>20.78</cell><cell>14.56</cell><cell>8.80</cell><cell>4.58</cell><cell>0.80</cell><cell>29.50</cell></row><row><cell>DMN [19]</cell><cell>testB</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.29</cell></row><row><cell>RRN [15]</cell><cell>testB</cell><cell>32.42</cell><cell>24.69</cell><cell>17.10</cell><cell>9.92</cell><cell>2.78</cell><cell>36.11</cell></row><row><cell>CMSA [21]</cell><cell>testB</cell><cell>35.02</cell><cell>28.68</cell><cell>21.74</cell><cell>13.89</cell><cell>4.54</cell><cell>37.89</cell></row><row><cell>Ours</cell><cell>testB</cell><cell>37.57</cell><cell>30.89</cell><cell>23.95</cell><cell>15.32</cell><cell>5.26</cell><cell>38.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Comparison of segmentation performance with the state-of-the-art methods on the G-Ref dataset in terms of prec@X and IoU. Comparison of segmentation performance with the state-of-the-art methods on ReferIt dataset in terms of prec@X and IoU.</figDesc><table><row><cell>Method</cell><cell>Set</cell><cell cols="5">prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</cell><cell>Overall IoU</cell></row><row><cell>RMI [18]</cell><cell>val</cell><cell>27.77</cell><cell>21.06</cell><cell>13.92</cell><cell>6.83</cell><cell>1.43</cell><cell>34.52</cell></row><row><cell>DMN [19]</cell><cell>val</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.76</cell></row><row><cell>KWA [16]</cell><cell>val</cell><cell>27.85</cell><cell>21.01</cell><cell>13.42</cell><cell>6.60</cell><cell>1.97</cell><cell>36.92</cell></row><row><cell>RRN [15]</cell><cell>val</cell><cell>36.00</cell><cell>29.77</cell><cell>22.78</cell><cell>14.06</cell><cell>3.74</cell><cell>36.45</cell></row><row><cell>CMSA [21]</cell><cell>val</cell><cell>40.38</cell><cell>32.56</cell><cell>24.77</cell><cell>16.10</cell><cell>4.59</cell><cell>39.98</cell></row><row><cell>Ours</cell><cell>val</cell><cell>44.71</cell><cell>37.26</cell><cell>29.10</cell><cell>19.02</cell><cell>5.59</cell><cell>42.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Set</cell><cell cols="5">prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</cell><cell>Overall IoU</cell></row><row><cell>RMI [18]</cell><cell>test</cell><cell>46.08</cell><cell>38.90</cell><cell>30.77</cell><cell>20.62</cell><cell>8.54</cell><cell>58.73</cell></row><row><cell>DMN [19]</cell><cell>test</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.81</cell></row><row><cell>KWA [16]</cell><cell>test</cell><cell>45.87</cell><cell>39.80</cell><cell>32.82</cell><cell>23.81</cell><cell>11.79</cell><cell>59.09</cell></row><row><cell>RRN [15]</cell><cell>test</cell><cell>56.71</cell><cell>49.22</cell><cell>40.36</cell><cell>28.39</cell><cell>12.68</cell><cell>63.63</cell></row><row><cell>CMSA [21]</cell><cell>test</cell><cell>57.96</cell><cell>50.34</cell><cell>41.04</cell><cell>28.82</cell><cell>13.05</cell><cell>63.80</cell></row><row><cell>Ours</cell><cell>test</cell><cell>58.25</cell><cell>51.04</cell><cell>42.12</cell><cell>29.96</cell><cell>13.25</cell><cell>63.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7</head><label>7</label><figDesc>Comparison of segmentation performance with the state-of-the-art methods on A2D Sentences dataset in terms of prec@X, meanIoU and Overall IoU.</figDesc><table><row><cell>Method</cell><cell cols="5">prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</cell><cell cols="2">Overall IoU Mean IoU</cell></row><row><cell>Hu et al. [14]</cell><cell>34.8</cell><cell>23.6</cell><cell>13.3</cell><cell>3.3</cell><cell>0.1</cell><cell>47.4</cell><cell>35.0</cell></row><row><cell>Li et al. [37]</cell><cell>38.7</cell><cell>29.0</cell><cell>17.5</cell><cell>6.6</cell><cell>0.1</cell><cell>51.5</cell><cell>35.4</cell></row><row><cell>Gavrilyuk et al. [17]</cell><cell>50.0</cell><cell>37.6</cell><cell>23.1</cell><cell>9.4</cell><cell>0.4</cell><cell>55.1</cell><cell>42.6</cell></row><row><cell>Ours</cell><cell>46.7</cell><cell>38.5</cell><cell>27.9</cell><cell>13.6</cell><cell>1.7</cell><cell>59.2</cell><cell>40.5</cell></row><row><cell>Ours+CFSA</cell><cell>48.7</cell><cell>43.1</cell><cell>35.8</cell><cell>23.1</cell><cell>5.2</cell><cell>61.8</cell><cell>43.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8</head><label>8</label><figDesc>Comparison of segmentation performance with the state-of-the-art methods on JHMDB Sentences dataset in terms of prec@X, Overall IoU and Mean IoU.</figDesc><table><row><cell>Method</cell><cell cols="5">prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</cell><cell cols="2">Overall IoU Mean IoU</cell></row><row><cell>Hu et al. [14]</cell><cell>63.3</cell><cell>35.0</cell><cell>8.5</cell><cell>0.2</cell><cell>0.0</cell><cell>54.6</cell><cell>52.8</cell></row><row><cell>Li et al. [37]</cell><cell>57.8</cell><cell>33.5</cell><cell>10.3</cell><cell>0.6</cell><cell>0.0</cell><cell>52.9</cell><cell>49.1</cell></row><row><cell>Gavrilyuk et al. [17]</cell><cell>69.9</cell><cell>46.0</cell><cell>17.3</cell><cell>1.4</cell><cell>0.0</cell><cell>54.1</cell><cell>54.2</cell></row><row><cell>Ours</cell><cell>76.0</cell><cell>60.4</cell><cell>36.4</cell><cell>6.4</cell><cell>0.0</cell><cell>60.6</cell><cell>57.0</cell></row><row><cell>Ours+CFSA</cell><cell>76.4</cell><cell>62.5</cell><cell>38.9</cell><cell>9.0</cell><cell>0.1</cell><cell>62.8</cell><cell>58.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported in part by the NSERC, in part by the National Natural Science Foundation of China [Grant Nos.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive linear transformation for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5514" to="5524" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7746" to="7755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object segmentation via effective integration of saliency and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1742" to="1756" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="38" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5958" to="5966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint learning of object and action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4163" to="4172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatio-temporal person retrieval via natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1453" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video object segmentation with language referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="123" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymmetric cross-guided attention network for actor and action video segmentation from natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3939" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The segmented and annotated iapr tc-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?pez-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Villase?or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
