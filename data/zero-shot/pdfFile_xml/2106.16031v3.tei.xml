<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ResViT: Residual vision transformers for multi-modal medical image synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onat</forename><surname>Dalmaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmut</forename><surname>Yurt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><forename type="middle">?</forename><surname>Ukur</surname></persName>
						</author>
						<title level="a" type="main">ResViT: Residual vision transformers for multi-modal medical image synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-medical image synthesis</term>
					<term>transformer</term>
					<term>residual</term>
					<term>vision</term>
					<term>adversarial</term>
					<term>generative</term>
					<term>unified</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial models with convolutional neural network (CNN) backbones have recently been established as state-of-the-art in numerous medical image synthesis tasks. However, CNNs are designed to perform local processing with compact filters, and this inductive bias compromises learning of contextual features. Here, we propose a novel generative adversarial approach for medical image synthesis, ResViT, that leverages the contextual sensitivity of vision transformers along with the precision of convolution operators and realism of adversarial learning. ResViT's generator employs a central bottleneck comprising novel aggregated residual transformer (ART) blocks that synergistically combine residual convolutional and transformer modules. Residual connections in ART blocks promote diversity in captured representations, while a channel compression module distills task-relevant information. A weight sharing strategy is introduced among ART blocks to mitigate computational burden. A unified implementation is introduced to avoid the need to rebuild separate synthesis models for varying source-target modality configurations. Comprehensive demonstrations are performed for synthesizing missing sequences in multicontrast MRI, and CT images from MRI. Our results indicate superiority of ResViT against competing CNN-and transformer-based methods in terms of qualitative observations and quantitative metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Medical imaging plays a pivotal role in modern healthcare by enabling in vivo examination of pathology in the human body. In many clinical scenarios, multi-modal protocols are desirable that involve a diverse collection of images from multiple scanners (e.g., CT, MRI) <ref type="bibr" target="#b0">[1]</ref>, or multiple acquisitions from a single scanner (multi-contrast MRI) <ref type="bibr" target="#b1">[2]</ref>. Complementary information about tissue morphology, in turn, empower physicians to diagnose with higher accuracy and confidence. Unfortunately, numerous factors including uncooperative patients and excessive scan times prohibit ubiquitous multimodal imaging <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. As a result, there has been evergrowing interest in synthesizing unacquired images in multimodal protocols from the subset of available images, bypassing costs associated with additional scans <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Medical image synthesis aims to predict target-modality images for a subject given source-modality images acquired under a limited scan budget <ref type="bibr" target="#b6">[7]</ref>. This is an ill-posed inverse problem since medical images are high dimensional, targetmodality data are absent during inference, and there exist</p><p>This study was supported in part by a TUBITAK BIDEB scholarship awarded to O. Dalmaz, and by TUBA GEBIP 2015 and BAGEP 2017 fellowships awarded to T. ? ukur (Corresponding author: Tolga ? ukur).</p><p>O. Dalmaz, M. Yurt, and T. ? ukur are with the Department of Electrical and Electronics Engineering, and the National Magnetic Resonance Research Center (UMRAM), Bilkent University, Ankara, Turkey (e-mails: {onat, mahmut, cukur}@ee.bilkent.edu.tr). T. ? ukur is also with the Neuroscience Program, Sabuncu Brain Research Center, Bilkent University, TR-06800 Ankara, Turkey.</p><p>nonlinear differences in tissue contrast across modalities <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Unsurprisingly, recent adoption of deep learning methods for solving this difficult problem has enabled major performance leaps <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b20">[21]</ref>. In learning-based synthesis, network models effectively capture a prior on the joint distribution of source-target images <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Earlier studies using CNNs for this purpose reported significant improvements over traditional approaches <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b27">[28]</ref>. Generative adversarial networks (GANs) were later introduced that leverage an adversarial loss to increase capture of detailed tissue structure <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b34">[35]</ref>. Further improvements were attained by leveraging enhanced architectural designs <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref>, and learning strategies <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref>. Despite their prowess, prior learningbased synthesis models are fundamentally based on convolutional architectures that use compact filters to extract local image features <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Exploiting correlations among small neighborhoods of image pixels, this inductive bias reduces the number of model parameters to facilitate learning. However, it also limits expressiveness for contextual features that reflect long-range spatial dependencies <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>Medical images contain contextual relationships across both healthy and pathological tissues. For instance, bone in the skull or CSF in the ventricles broadly distribute over spatially contiguous or segregated brain regions, resulting in dependencies among distant voxels. While pathological tissues have less regular anatomical priors, their spatial distribution (e.g., location, quantity, shape) can still show disease-specific patterns <ref type="bibr" target="#b46">[47]</ref>. For instance, multiple diffuse brain lesions are present in multiple sclerosis (MS) and Alzheimer's (AD); commonly located near periventricular and juxtacortical regions in MS, and near hippocampus, entorhinal cortex and isocortex in AD <ref type="bibr" target="#b47">[48]</ref>. Meanwhile, few lesions manifest as spatiallycontiguous clumps in cancer; with lesions typically located near the cerebrum and cerebellum in gliomas, and near the skull in meningiomas <ref type="bibr" target="#b47">[48]</ref>. Thus, the distribution of pathology also involves context regarding the position and structure of lesions with respect to healthy tissue. In principle, synthesis performance can be enhanced by priors that capture these relationships. Vision transformers are highly promising for this goal since attention operators that learn contextual features can improve sensitivity for long-range interactions <ref type="bibr" target="#b48">[49]</ref>, and focus on critical image regions for improved generalization to atypical anatomy such as lesions <ref type="bibr" target="#b49">[50]</ref>. However, adopting vanilla transformers in tasks with pixel-level outputs is challenging due to computational burden and limited localization <ref type="bibr" target="#b50">[51]</ref>. Recent studies instead consider hybrid architectures or computation-efficient attention operators to adopt transformers in medical imaging tasks <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b56">[57]</ref>.</p><p>Here, we propose a novel deep learning model for medical image synthesis, ResViT, that translates between multi-modal imaging data. ResViT combines the sensitivity of vision transformers to global context, the localization power of CNNs, and the realism of adversarial learning. ResViT's generator follows an encoder-decoder architecture with a central bottleneck to distill task-critical information. The encoder and decoder contain CNN blocks to leverage local precision of convolution operators <ref type="bibr" target="#b57">[58]</ref>. The bottleneck comprises novel aggregated residual transformer (ART) blocks to synergistically preserve local and global context, with a weight-sharing strategy to minimize model complexity. To improve practical utility, a unified ResViT implementation is introduced that consolidates models for numerous source-target configurations. Demonstrations are performed for synthesizing missing sequences in multi-contrast MRI, and CT from MRI. Comprehensive experiments on imaging datasets from healthy subjects and patients clearly indicate the superiority of the proposed method against competing methods. Code to implement the ResViT model is publicly available at https://github.com/icon-lab/ResViT. <ref type="bibr">?</ref> We introduce the first adversarial model for medical image synthesis with a transformer-based generator to translate between multi-modal imaging data. <ref type="bibr">?</ref> We introduce novel aggregated residual transformer (ART) blocks to synergistically preserve localization and context. <ref type="bibr">?</ref> We introduce a weight sharing strategy among ART blocks to lower model complexity and mitigate computational burden. ? We introduce a unified synthesis model that generalizes across multiple configurations of source-target modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The immense success of deep learning in inverse problems has motivated its rapid adoption in medical imaging <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Medical image synthesis is a particularly illposed problem since target images are predicted without any target-modality data <ref type="bibr" target="#b31">[32]</ref>. Earlier studies in this domain have proposed local networks based on patch-level processing <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. While local networks offer benefits over traditional approaches, they can show limited sensitivity to broader context across images <ref type="bibr" target="#b21">[22]</ref>. Later studies adopted deep CNNs for image-level processing with increasing availability of large imaging databases. CNN-based synthesis has been successfully demonstrated in various applications including synthesis across MR scanners <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b62">[63]</ref>- <ref type="bibr" target="#b64">[65]</ref>, multi-contrast MR synthesis <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b27">[28]</ref>, and CT synthesis <ref type="bibr" target="#b65">[66]</ref>- <ref type="bibr" target="#b68">[69]</ref>. Despite significant improvements they enable, CNNs trained with pixel-wise loss terms tend to suffer from undesirable loss of detailed structure <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>To improve capture of structural details, GANs <ref type="bibr" target="#b28">[29]</ref> were proposed to learn the distribution of target modalities conditioned on source modalities <ref type="bibr" target="#b69">[70]</ref>. Adversarial losses empower GANs to capture an improved prior for recovery of highspatial-resolution information <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In recent years, GAN-based methods were demonstrated to offer state-of-theart performance in numerous synthesis tasks, including data augmentation as well as multi-modal synthesis <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Important applications of GAN models include CT to PET <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, MR to CT <ref type="bibr" target="#b74">[75]</ref>- <ref type="bibr" target="#b76">[77]</ref>, unpaired crossmodality <ref type="bibr" target="#b77">[78]</ref>- <ref type="bibr" target="#b80">[81]</ref>, 3T-to-7T <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>, and multi-contrast MRI synthesis <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b41">[42]</ref>.</p><p>While GAN models have arguably emerged as a gold standard in recent years, they are not without limitation. In particular, GANs are based on purely convolutional operators known to suffer from poor across-subject generalization to atypical anatomy and sub-optimal learning of long-range spatial dependencies <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Recent studies have incorporated spatial or channel attention mechanisms to modulate CNNderived feature maps <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b83">[84]</ref>- <ref type="bibr" target="#b87">[88]</ref>. Such modulation motivates the network to give greater focus to regions that may suffer from greater errors <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b84">[85]</ref>. While attention maps might be distributed across image regions, multiplicative gating of local CNN features offers limited expressiveness in modeling of global context <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>.</p><p>To incorporate contextual representations, transformerbased methods have received recent interest in imaging tasks such as segmentation <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b90">[91]</ref>, reconstruction <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b53">[54]</ref>, and synthesis <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>. Among relevant methods are Transformer GAN that suppresses noise in low-dose PET images <ref type="bibr" target="#b51">[52]</ref>, TransCT that suppresses noise in low-dose CT images <ref type="bibr" target="#b52">[53]</ref>, and SLATER that recovers MR images from undersampled k-space acquisitions <ref type="bibr" target="#b53">[54]</ref>. While these methods reconstruct images for single-modality data, ResViT translates imaging data across separate modalities. Furthermore, Transformer GAN is an adversarial model with convolutional encoder-decoder and a bottleneck that contains a transformer without external residual connections. TransCT is a nonadversarial model where CNN blocks first learn textural components of low-frequency (LF) and high-frequency (HF) image parts; and a transformer without external residual connections then combines encoded HF and textural LF maps. In comparison, ResViT is an adversarial model that employs a hybrid architecture in its bottleneck comprising a cascade of residual transformer and residual CNN modules. Unlike SLATER based on an unconditional model that maps latent variables to images via cross-attention transformers, ResViT is a conditional model based on self-attention transformers.</p><p>Few recent studies have independently introduced transformer-based methods for medical image synthesis. VTGAN generates retinal angiograms from fundus photographs <ref type="bibr" target="#b54">[55]</ref> and GANBERT performs MR-to-PET synthesis <ref type="bibr" target="#b55">[56]</ref>, whereas ResViT performs multi-contrast MRI and MR-to-CT synthesis. Both VTGAN and GANBERT use entirely convolutional generators and only include transformers in their discriminators. In contrast, ResViT incorporates transformers in its generator to explicitly leverage long-range context. The closest study to our work is PTNet that performs one-to-one translation between T 1and T 2 -weighted images in infant MRI <ref type="bibr" target="#b56">[57]</ref>. However, PTNet is a non-adversarial model without a discriminator, and it follows a convolution-free architecture. In contrast, ResViT is an adversarial model with a hybrid CNN-transformer architecture to achieve high localization and contextual sensitivity along with a high degree of realism in synthesized images. Furthermore, a broader set of tasks are considered for ResViT including one-to-one and many-to-one translation.</p><p>A unique component of ResViT is the novel ART blocks in its generator that contain a cascade of transformer and CNN modules equipped with skip connections. These residual paths enable effective aggregation of contextual and convolutional representations. Based on this powerful component, we provide the first demonstrations of a transformer architecture for many-to-one synthesis tasks and a unified synthesis model for advancing practicality over task-specific methods. The generator in ResViT follows an encoder-decoder architecture bridged with a central information bottleneck to distill task-specific information. The encoder and decoder comprise convolutional layers to maintain local precision and inductive bias in learned structural representations. Meanwhile, the information bottleneck comprises a stack of novel aggregated residual transformer (ART) blocks. ART blocks learn contextual representations via vision transformers, and synergistically fuse CNNbased local and transformer-based global representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THEORY AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Residual Vision Transformers</head><p>Here we propose a novel adversarial method for medical image synthesis named residual vision transformers, ResViT, that can unify various source-target modality configurations into a single model for improved practicality. ResViT leverages a hybrid architecture of deep convolutional operators and transformer blocks to simultaneously learn high-resolution structural and global contextual features ( <ref type="figure" target="#fig_0">Fig. 1</ref>). The generator subnetwork follows an encoder -information bottleneck -decoder pathway, and the discriminator subnetwork is composed of convolutional operators. The generator's bottleneck contains a stack of novel aggregated residual transformer (ART) blocks. Each ART block is organized as the cascade of a transformer module that extracts hidden contextual features, and a CNN module that extracts hidden local features of input feature maps. Importantly, external skip connections are inserted around both modules to create multiple paths of information flow through the block. These paths propagate multiple sets of features to the output: The central segment of ResViT containing ART blocks acts as an information bottleneck for spatial and feature dimensions of medical image representations. On the one hand, the central segment processes feature maps that have been spatially downsampled by the encoder. This increases the relative emphasis on mid-to high-level spatial information over lower-level information <ref type="bibr" target="#b57">[58]</ref>. On the other hand, ART blocks contain channel-compression (CC) modules that process concatenated feature maps from the previous ART block and the transformer module. CC modules downsample the concatenated maps in the feature dimension to distill a task-relevant set of convolutional and contextual features.</p><p>Given the computational efficiency of convolutional layers, CNNs pervasively process feature maps at high spatial resolution to improve sensitivity for local features <ref type="bibr" target="#b57">[58]</ref>. In contrast, vision transformers include computationally exhaustive selfattention layers, so they typically process feature maps at relatively lower resolution <ref type="bibr" target="#b48">[49]</ref>. To ensure that both the residual CNNs and transformers in ART blocks receive input feature maps at their expected resolutions, we incorporated down and upsampling blocks respectively at the input and output of transformer modules. This design ensures compatibility between the resolutions of feature maps extracted from CNN and transformer modules. In the remainder of this section, we explain the detailed composition of each architectural component, and we describe the loss functions to train ResViT. 1) Encoder: The first component of ResViT is a deep encoder network that contains a series of convolutional layers to capture a hierarchy of localized features of source images. Note that ResViT can serve as a unified synthesis model, so its encoder receives as input the full set of modalities within the imaging protocol, both source and target modalities ( <ref type="figure" target="#fig_2">Fig. 2</ref>). Source modalities are input via an identity mapping, whereas unavailable target modalities are masked out:</p><formula xml:id="formula_0">X G i = a i ? m i<label>(1)</label></formula><p>where i denotes the channel index of the encoder input i ? {1, 2, . . . , I}, m i is the image for the ith modality. In Eq. (1), a i denotes the availability of the ith modality:</p><formula xml:id="formula_1">a i = 1 if m i is a source modality 0 if m i is a target modality<label>(2)</label></formula><p>During training, various different configurations of sourcetarget modalities are considered within the multi-modal protocol (e.g., T 1 , T 2 ? PD; T 2 , PD ? T 1 ; T 1 , PD ? T 2 for a threecontrast MRI protocol). During inference, the specific sourcetarget configuration is determined via the availability conditions in individual test subjects. Given the availability-masked multi-channel input, the encoder uses convolutional operators to learn latent structural representations shared across the consolidated synthesis tasks. The encoder maps the multichannel input X G onto the embedded latent feature map f ne ? R N C ,H,W via convolutional filters, where N C is the number of channels, H is the height and W is the width of the feature map. These representations are then fed to the information bottleneck. 2) Information Bottleneck: Next, ResViT employs a residual bottleneck to distill task-relevant information in the encoded features. Note that convolution operators have greater power in capturing localized features, whereas attention operators are more sensitive to context-driven features. To simultaneously maintain localization power and contextual sensitivity, we introduce ART blocks that aggregate the information from residual convolutional and transformer branches ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Receiving as input the jth layer feature maps f j ? R N C ,H,W , an ART block first processes the feature maps via a vision transformer. Due to computational constraints, the transformer expects feature maps at smaller resolutions compared to convolutional layers. Thus, the spatial dimensions (H, W ) of f j ? R N C ,H,W are lowered by a downsampling block (DS):</p><formula xml:id="formula_2">f j ? R N C ,H ,W = DS(f j )<label>(3)</label></formula><p>where DS is implemented as a stack of strided convolutional layers, f j ? R N C ,H ,W are downsampled feature maps with W = W/M , H = H/M , M denoting the downsampling factor. A transformer branch then processes f j to extract contextual information. Accordingly, f j is first split into N P = W H /P 2 non-overlapping patches of size (P, P ), and the patches are then flattened to N C P 2 -dimensional vectors. The transformer embeds patches onto an N D -dimensional space via trainable linear projections, supplemented with learnable positional encoding:</p><formula xml:id="formula_3">z 0 = [f 1 j P E ; f 2 j P E ; . . . ; f N P j P E ] + P pos E (4) where z 0 ? R N P ,N D are the input patch embeddings, f p j ? R N C P 2</formula><p>is the pth patch, P E is the embedding projection, and P pos E is the learnable positional encoding. Next, the transformer encoder processes patch embeddings via a cascade of L layers of multi-head self-attention (MSA) <ref type="bibr" target="#b91">[92]</ref> and multi-layer perceptrons (MLP) <ref type="bibr" target="#b92">[93]</ref>. The output of the lth layer in the transformer encoder is given as:</p><formula xml:id="formula_4">z l = MSA(LN(z l?1 )) + z l?1 (5) z l = MLP(LN(z l )) + z l<label>(6)</label></formula><p>MSA layers in Eq. 5 employ S separate self-attention heads:</p><formula xml:id="formula_5">MSA(z) = [SA 1 (z); SA 2 (z); . . . ; SA S (z)]U msa<label>(7)</label></formula><p>where SA s stands for the sth attention head with s ? {1, 2, . . . , S} and U msa denotes the learnable tensor projecting attention head outputs. SA layers compute a weighted combination of all elements of the input sequence z:</p><formula xml:id="formula_6">SA(z) = Av</formula><p>where v is value, and attention weights A a,b are taken as pairwise similarity between the query q and key k:</p><formula xml:id="formula_7">A a,b = sof tmax(q a k T b /N D 0.5 )<label>(8)</label></formula><p>Note that q, k, v are respectively obtained as learnable</p><formula xml:id="formula_8">projections T q , T k , T v of z.</formula><p>The output of the transformer encoder z L is then deflattened to form g j ? R N D ,H ,W . Resolution of g j is increased to match the size of input feature maps via an upsampling block US based on transposed convolutions:</p><formula xml:id="formula_9">g j ? R N C ,H,W = US(g j )<label>(9)</label></formula><p>where g j ? R N C ,H,W are upsampled feature maps output by the transformer module. Channel-wise concatenation is performed to fuse global context learned via the transformer with localized features captured via convolutional operators.</p><p>To distill learned structural and contextual representations, the channels of the concatenated feature maps are then compressed via a channel compression (CC) module:</p><formula xml:id="formula_10">h j ? R N C ,H,W = CC(concat(f j , g j ))<label>(10)</label></formula><p>where h j are compressed feature maps. CC uses two parallel convolutional branches of varying kernel size. Finally, the feature maps are processed via a residual CNN (ResCNN) <ref type="bibr" target="#b57">[58]</ref>:</p><formula xml:id="formula_11">f j+1 ? R N C ,H,W = ResCNN(h j )<label>(11)</label></formula><p>where f j+1 denotes the output of the ART block at the jth network layer.</p><p>3) Decoder: The last component of the generator is a deep decoder based on transposed convolutional layers. Because ResViT can serve as a unified model, its decoder can synthesize all contrasts within the multi-modal protocol regardless of the specific source-target configuration <ref type="figure" target="#fig_2">(Fig. 2)</ref>. The decoder receives as input the feature maps f A distilled by the bottleneck and produces multi-modality images? G i ?? G in separate channels, where A is the total number of ART blocks, and? G i denotes the ith synthesized modality. 4) Parameter Sharing Transformers: Multiple ART blocks are used in the information bottleneck to increase the capacity of ResViT in learning contextual representations. That said, multiple independent transformer blocks would inevitably elevate memory demand and risk of overfitting due to an excessive number of parameters. To prevent these risks, a weight-sharing strategy is adopted where the model weights for the transformer encoder are tied across separate ART blocks. The tied parameters include the projection matrices T q , T k , T v for query, key, value along with projection tensors for attention heads U msa in M SA layers, and weight matrices in M LP layers. Remaining parameters in transformer modules including down/upsampling blocks, patch embeddings and positional encodings are kept independent. During backpropagation, updates for tied weights are computed based on the summed error gradient across ART blocks. 5) Discriminator: The discriminator in ResViT is based on a conditional PatchGAN architecture <ref type="bibr" target="#b42">[43]</ref>. The discriminator performs patch-level differentiation between acquired and synthetic images. This implementation increases sensitivity to localized details related to high-spatial-frequency information. As ResViT can serve as a unified model by generating all modalities in the multi-modal protocol including sources, an availability-guided selective discriminator is employed:</p><formula xml:id="formula_12">X D i (source) = X G i = a i ? m i<label>(12)</label></formula><formula xml:id="formula_13">X D i (syn target) = (1 ? a i ) ? Y G i<label>(13)</label></formula><formula xml:id="formula_14">X D i (acq target) = (1 ? a i ) ? m i<label>(14)</label></formula><p>where X D i (source) are source images, X D i (syn target) are synthesized target images, and X D i (acq target) are acquired target images. The conditional discriminator receives as input the concatenation of source and target images:</p><formula xml:id="formula_15">X D (synthetic) = concat(X D i (source), X D i (syn target)) (15) X D (acquired) = concat(X D i (source), X D i (acq target))<label>(16)</label></formula><p>where X D (synthetic) is the concatenation of source and synthetic target images, and X D (acquired) is the concatenation of the source and acquired target images. 6) Loss Function: The first term in the loss function is a pixel-wise L 1 loss defined between the acquired and synthesized target modalities:</p><formula xml:id="formula_16">L pix = I i=1 (1 ? a i )E[||G(X G ) i ? m i || 1 ]<label>(17)</label></formula><p>where E denotes expectation, and G denotes the generator subnetwork in ResViT. ResViT takes as input source modalities to reconstruct them at the output. Thus, the second term is a pixel-wise consistency loss between acquired and reconstructed source modalities based on an L 1 distance:</p><formula xml:id="formula_17">L rec = I i=1 a i E[||G(X G ) i ? m i || 1 ]<label>(18)</label></formula><p>The last term is an adversarial loss defined via the conditional discriminator (D):</p><formula xml:id="formula_18">L adv = ? E[D(X D (acquired) 2 ] ? E[(D(X D (synthetic)) ? 1) 2 ]<label>(19)</label></formula><p>The three terms are linearly combined to form the overall objective:</p><formula xml:id="formula_19">L ResV iT = ? pix L pix + ? rec L rec + ? adv L adv<label>(20)</label></formula><p>where ? pix , ? rec , and ? adv are the weightings of the pixelwise, reconstruction, and adversarial losses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>We demonstrated the proposed ResViT model on two multi-contrast brain MRI datasets (IXI: https://braindevelopment.org/ixi-dataset/, BRATS <ref type="bibr" target="#b93">[94]</ref>- <ref type="bibr" target="#b95">[96]</ref>) and a multi-modal pelvic MRI-CT dataset <ref type="bibr" target="#b96">[97]</ref>.</p><p>1) IXI Dataset: T 1 -weighted, T 2 -weighted, and PD-weighted brain MR images from 53 healthy subjects were analyzed. 25 subjects were reserved for training, 10 were reserved for validation, and 18 were reserved for testing. From each subject, 100 axial cross-sections containing brain tissues were selected. Acquisition parameters were as follows. T 1 -weighted images: TE = 4.603ms, TR = 9.813ms, spatial resolution = 0.94?0.94?1.2mm 3 . T 2 -weighted images: TE = 100ms, TR = 8178.34ms, spatial resolution = 0.94?0.94?1.2mm 3 . PD-weighted images: TE = 8ms, TR = 8178.34ms, spatial resolution = 0.94?0.94?1.2mm 3 . The multi-contrast images in this dataset were unregistered. Hence, T 2 -and PD-weighted images were spatially registered onto T 1 -weighted images prior to modelling. Registration was performed via an affine transformation in FSL <ref type="bibr" target="#b97">[98]</ref> based on mutual information.</p><p>2) BRATS Dataset: T 1 -weighted, T 2 -weighted, post-contrast T 2 -weighted, and T 2 Fluid Attenuation Inversion Recovery (FLAIR) brain MR images from 55 subjects were analyzed. 25 subjects were reserved for training, 10 were reserved for validation, and 20 were reserved for testing. From each subject, 100 axial cross-sections containing brain tissues were selected. Please note that the BRATS dataset contains images collected under various clinical protocols and scanners at multiple institutions. As publicly shared, multi-contrast images are co-registered to the same anatomical template, interpolated to 1?1?1mm 3 resolution and skull-stripped.</p><p>3) MRI-CT Dataset: T 2 -weighted MR and CT images of the male pelvis from 15 subjects were used. 9 subjects were reserved for training, 2 were reserved for validation, and 4 were reserved for testing. From each subject, 90 axial crosssections were analysed. Acquisition parameters were as follows. T 2 -weighted images: Group 1, TE = 97ms, TR = 6000-6600ms, spatial resolution = 0.875?0.875?2.5mm 3 . Group 2, TE = 91-102ms, TR = 12000-16000ms, spatial resolution = 0.875-1.1?0.875-1.1?2.5mm 3 . CT images: Group 1, spatial resolution = 0.98?0.98?3mm 3 , Kernel = B30f. Group 2: spatial resolution = 0.1?0.1?2mm 3 , Kernel = FC17. This dataset contains images collected under various protocols and scanners for each modality. As publicly shared, multi-modal images are co-registered onto T 2 -weighted MR scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Competing Methods</head><p>We demonstrated the proposed ResViT model against several state-of-the-art image synthesis methods. The baseline methods included convolutional models (task-specific models: pGAN <ref type="bibr" target="#b23">[24]</ref>, pix2pix <ref type="bibr" target="#b42">[43]</ref>, medSynth <ref type="bibr" target="#b31">[32]</ref>; unified models: MM-GAN <ref type="bibr" target="#b40">[41]</ref>, pGAN uni ), attention-augmented convolutional models (A-UNet <ref type="bibr" target="#b49">[50]</ref>, SAGAN <ref type="bibr" target="#b84">[85]</ref>), and transformer models (task-specific: TransUNet <ref type="bibr" target="#b50">[51]</ref>, PTNet <ref type="bibr" target="#b56">[57]</ref>; unified: TransUNet uni ). Hyperparameters of each competing method were optimized via identical cross-validation procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Convolutional models:</head><p>pGAN A convolutional GAN model with ResNet backbone was considered <ref type="bibr" target="#b23">[24]</ref>. pGAN comprises CNN-based generator and discriminator networks. Its generator follows an encoderbottleneck-decoder pathway, where the encoder and decoder are identical to those in ResViT. The bottleneck contains a cascade of residual CNN blocks.</p><p>pix2pix A convolutional GAN model with U-Net backbone was considered <ref type="bibr" target="#b42">[43]</ref>. pix2pix has a CNN-based generator with an encoder-decoder structure tied with skip connections.</p><p>medSynth A convolutional GAN model with residual U-Net backbone was considered as provided at https://github.com/ginobilinie/medSynthesisV1 <ref type="bibr" target="#b31">[32]</ref>. The generator of medSynth contains a long-skip connection from the first to the last layer.</p><p>MM-GAN A unified synthesis model based on a convolutional GAN was considered <ref type="bibr" target="#b40">[41]</ref>. MM-GAN comprises CNN-based generator and discriminator networks, where the generator is based on U-Net. MM-GAN trains a single network under various source-target modality configurations. The original MM-GAN architecture was directly adopted, except for curriculum learning to ensure standard sample selection for all competing methods. The unification strategy in MM-GAN matches the unification strategy in ResViT.</p><p>pGAN uni A unified version of the pGAN model was trained to consolidate multiple synthesis tasks. The unification procedure was identical to that of ResViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Attention-augmented convolutional models:</head><p>Attention U-Net (A-UNet) A CNN-based U-Net architecture with additive attention gates was considered <ref type="bibr" target="#b49">[50]</ref>. Here we adopted the original A-UNet model as the generator of a conditional GAN model, where the discriminator was identical to that in ResViT.</p><p>Self-Attention GAN (SAGAN) A CNN-based GAN model with self-attention modules incorporated into the generator was considered <ref type="bibr" target="#b84">[85]</ref>. Here we adapted the original SAGAN model designed for unconditional mapping by inserting the self-attention modules into the pGAN model as described in <ref type="bibr" target="#b98">[99]</ref>. For fair comparison, the number and position of attention modules in SAGAN were matched to those of transformer modules in ResViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Transformer models:</head><p>TransUNet A recent hybrid CNN-transformer architecture was considered <ref type="bibr" target="#b50">[51]</ref>. Here, we adopted the original TransUNet model as the generator of a conditional GAN architecture with an identical discriminator to ResViT. We further replaced the segmentation head with a convolutional layer for synthesis.</p><p>PTNet A recent convolution-free transformer architecture was considered <ref type="bibr" target="#b56">[57]</ref>. Here we adopted the original PTNet model as the generator of a conditional GAN architecture with an identical discriminator to ResViT.</p><p>TransUNet uni The TransUNet model was unified to consolidate multiple synthesis tasks. The unification procedure was identical to that of ResViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architectural Details</head><p>The encoder in the ResViT model contained three convolutional layers of kernel size 7, 3, 3 respectively. The feature map in the encoder output was of size R 256,64,64 , and this dimensionality was retained across the information bottleneck. The decoder contained three convolutional layers of kernel size 3, 3, 7 respectively. The information bottleneck contained nine ART blocks. The downsampling blocks preceding transformers contained two convolutional layers with stride 2 and kernel size 3. The upsampling blocks succeeding transformers contained two transposed convolutional layers with stride 2 and kernel size 3. Down and upsampling factors were set to M = 4. Channel compression lowered the number of channels from 512 to 256. The transformer encoder was adopted by extracting the transformer component of the ImageNetpretrained model R50+ViT-B/16 (https://github.com/googleresearch/vision transformer). The transformer encoder expected an input map of 16?16 spatial resolution. Patch flattening was performed with size P = 1 yielding a sequence length of 256 <ref type="bibr" target="#b48">[49]</ref>. Note that transformer modules contain substantially higher number of parameters compared to convolutional modules. Thus, retaining a transformer in each ART block results in significant model complexity, inducing computational burden and suboptimal learning. To alleviate these issues, transformer modules in ART blocks utilized tied weights, and they were only retained in a subset of ART blocks while remaining blocks reduced to residual CNNs.</p><p>The configuration of transformer modules, i.e. their total number and position, was selected via cross-validation experiments. Due to the extensive number of potential configura-   tions, a pre-selection process was implemented. Accordingly, performance for a transformer module inserted in a single ART block (A 1 , A 2 , ..., A 9 ) was measured, and the top half of positions was pre-selected. Composite configurations with multiple transformer modules were then formed based on the pre-selected blocks (A 1 ? A 5 , A 1 ? A 6 ? A 9 etc.). We observed that retaining more than 2 modules elevated complexity without any performance benefits. Validation performance for the best performing configurations ( <ref type="table" target="#tab_1">Table I</ref> for three representative tasks (T 1 , T 2 ? PD in IXI, T 1 , T 2 ? FLAIR in BRATS, and MRI ? CT in MRI-CT). Consistently across tasks, the (A 1 ? A 6 ) configuration yielded near-optimal performance and so it was selected for all experiments thereafter. We also tuned the intrinsic complexity of transformer modules. To do this, two variant modules were examined: "base" and "large". The "base" module contained 12 layers with latent dimensionality N d = 768, 12 attention heads, and 3073 hidden units in each layer of the M LP . Meanwhile, the "large" module contained 24 layers with latent dimensionality N d = 1024, 16 attention heads, and 4096 hidden units in each layer of the MLP. Validation performances based on the two variant modules are listed in <ref type="table" target="#tab_1">Table II</ref>. The "base" module that offers higher performance for lower computational complexity was selected for consequent experiments.</p><formula xml:id="formula_20">A 1 ? A 5 , A 1 ? A 6 , A 1 ? A 9 , A 5 ? A 9 , A 4 ? A 9 , A 5 ? A 9 , A 1 ? A 6 ? A 9 ) are listed in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Modeling Procedures</head><p>For fair comparisons among competing methods, all models were implemented adversarially using the same PatchGAN discriminator and the loss function in Eq. 20. Task-specific models used adversarial and pixel-wise losses, whereas unified models used adversarial, pixel-wise, and reconstruction losses. Learning rate, number of epochs, and loss-term weighting were selected via cross-validation. Validation performance was measured as Peak Signal to Noise Ratio (PSNR) on three representative tasks (T 1 , T 2 ? PD in IXI, T 1 , T 2 ? FLAIR in BRATS, and MRI ? CT in MRI-CT). We considered different learning rates in the set {10 ?5 , 10 ?4 , 2x10 ?4 , 5x10 ?4 , 10 ?3 } and number of epochs in the set {5, 10, ..., 200}. Eq. 20 contains only two degrees of freedom regarding the loss-  term weights, and prior studies have reported models with higher weighting for pixel-wise over adversarial loss <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Thus, we considered ? pix in {20, 50, 100, 150} and ? adv = 1. Note that ? rec = 0 by definition in task-specific models with fixed configuration of source and target modalities, while ? rec = ? pix was used in unified models as both loss terms measure the L 1 -norm difference between reference and generated images for individual modalities. To minimize potential biases among competing methods, a common set of parameters that consistently yielded near-optimal performance were prescribed for all methods: 2 ? 10 ?4 learning rate, 100 training epochs, ? adv = 1, ? pix = 100 for task-specific models, and ? adv = 1, ? rec = 100, ? pix = 100 for unified models. All competing methods were trained via the Adam optimizer <ref type="bibr" target="#b99">[100]</ref> with ? 1 = 0.5, ? 2 = 0.999. The learning rate was constant for the first 50 epochs and linearly decayed to 0 in the remaining epochs. Transformer modules in TransUNet and ResViT were initiated with ImageNet pre-trained versions for object classification <ref type="bibr" target="#b100">[101]</ref>. ART blocks were initiated without transformer modules and then fine-tuned for 50 epochs following insertion of transformers at a higher learning rate of 10 ?3 as in <ref type="bibr" target="#b48">[49]</ref>. Elevated learning rate during the second half of the training procedure was not adopted for other methods as it diminished performance. Modelling was performed via the PyTorch framework on Nvidia RTX A4000 GPUs. Inference times are listed in <ref type="table" target="#tab_1">Table III</ref>. Synthesis quality was assessed via PSNR and Structural Similarity Index (SSIM) <ref type="bibr" target="#b101">[102]</ref>. Metrics were calculated between ground truth and synthesized target images. Mean and standard deviations of metrics were reported across an independent test set, non-overlapping with training-validation sets. Significance of performance differences were evaluated with signed-rank tests (p&lt;0.05). Tests were conducted on subjectaverage metrics, except MRI ? CT where cross-sectional metrics were tested in each subject due to limited number of test subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiments</head><p>1) Multi-Contrast MRI Synthesis: Experiments were conducted on the IXI and BRATS datasets to demonstrate synthesis performance in multi-modal MRI. In the IXI dataset, oneto-one tasks of T 2 ? PD; PD ? T 2 and many-to-one tasks of T 1 , T 2 ? PD; T 1 , PD ? T 2 ; T 2 , PD ? T 1 were considered. In the BRATS dataset, one-to-one tasks of T 2 ?FLAIR; FLAIR?T 2 , many-to-one tasks of T 1 , T 2 ? FLAIR; T 1 , FLAIR ? T 2 ; T 2 , FLAIR ? T 1 were considered. In both datasets, task-specific ResViT models were compared against pGAN, pix2pix, medSynth, A-UNet, SAGAN, TransUNet, and PTNet. Meanwhile, unified ResViT models were demonstrated against pGAN uni , MM-GAN, and TransUNet uni .</p><p>2) MRI to CT Synthesis: Experiments were performed on the MRI-CT dataset to demonstrate across-modality synthesis performance. A one-to-one synthesis task of deriving target CT images from source MR images was considered. The taskspecific ResViT model was compared against pGAN, pix2pix, medSynth, A-UNet, SAGAN, TransUNet, and PTNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3:</head><p>ResViT was demonstrated on the IXI dataset for two representative many-to-one synthesis tasks: a) T 1 , T 2 ? PD, b) T 2 , PD ? T 1 . Synthesized images from all competing methods are shown along with the source images and the reference target image. ResViT improves synthesis performance in regions that are depicted sub-optimally in competing methods. Overall, ResViT generates images with lower artifact and noise levels and sharper tissue depiction.  IV: Performance of task-specific synthesis models in many-to-one (T 1 , T 2 ? PD, T 1 , PD ? T 2 , and T 2 , PD ? T 1 ) and one-to-one (T 2 ? PD and PD ? T 2 ) tasks in the IXI dataset. PSNR (dB) and SSIM are listed as mean?std across test subjects. Boldface indicates the top-performing model for each task.</p><formula xml:id="formula_21">T 1 , T 2 ? PD T 1 , PD ? T 2 T 2 , PD ? T 1 T 2 ? PD PD ? T 2 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM</formula><p>3) Ablation Studies: Several lines of ablation experiments were conducted to demonstrate the value of the individual components of the ResViT model, including both architectural design elements and training strategies. Experiments were performed on three representative tasks: namely T 1 , T 2 ? PD in IXI, T 1 , T 2 ? FLAIR in BRATS, and MRI ? CT. First, we assessed the performance contribution of the three main components in ResViT: transformer modules, convolutional modules and adversarial learning. Variant models were trained when transformer modules were ablated from ART blocks, when residual CNNs were ablated from transformer-retaining ART blocks, and when the adversarial loss term and the discriminator were ablated. In addition to PSNR and SSIM, we measured the Fr?chet inception distance (FID) <ref type="bibr" target="#b102">[103]</ref> between the synthesized and ground truth images to evaluate the importance of adversarial learning.</p><p>Second, we probed the design and training procedures of ART blocks. We assessed the utility of tied weights across transformer modules, and multiple transformer-retaining ART blocks. Variant models were trained separately using untied weights in transformers, and based on a single transformerretraining module at either first or sixth ART blocks. We also examined the importance of model initiation with ImageNet pre-trained transformer modules, and delayed insertion of transformer modules during training. Variant models were built by using randomly initialized transformer modules, by inserting pre-trained transformer modules into ART blocks at the beginning of training, and by inserting randomly initialized transformer modules at the beginning of training.</p><p>Third, we investigated the design of skip connections and down/upsampling modules. We considered benefits of external skip connections in ART blocks for residual learning. Variant models were trained by removing skip connections around either the transformer or convolution modules in ART. We also assessed alternative designs for down/upsampling modules in ART to mitigate added model complexity. In a first variant, original down/upsampling modules were replaced with unlearned maxpooling modules for downsampling and bilinear interpolation modules for upsampling. In a second variant, additional downsampling layers in the encoder and upsampling layers in the decoder were included in order to remove down/upsampling modules in ART blocks.</p><p>Next, we inspected the relative strength of contextual features in the distilled task-relevant representations in ART blocks. For a quantitative assessment, we compared the L 2norm of the contextual feature map derived by the transformer module against that of the input feature map to the ART block relayed through the transformer's skip connection. Note that <ref type="figure">Fig. 4</ref>: ResViT was demonstrated on the BRATS dataset for two representative many-to-one synthesis tasks: a) T 1 , T 2 ? FLAIR, b) T 2 , FLAIR ? T 1 . Synthesized images from all competing methods are shown along with the source images and the reference image. ResViT improves synthesis performance, especially in pathological regions (e.g., tumors, lesions) in comparison to competing methods. Overall, ResViT images have better-delineated tissue boundaries and lower artifact/noise levels.  V: Performance of task-specific synthesis models in many-to-one tasks (T 1 , T 2 ? FLAIR, T 1 , FLAIR ? T 2 , and T 2 , FLAIR ? T 1 ) and one-to-one tasks (T 2 ?FLAIR and FLAIR?T 2 ) across test subjects in the BRATS dataset. Boldface indicates the top-performing model for each task. these two maps are distilled via the channel compression (CC) module following concatenation. Thus, we also compared the L 2 -norm of the combination weights in the CC module for the contextual versus input features.</p><formula xml:id="formula_22">T 1 , T 2 ? FLAIR T 1 , FLAIR ? T 2 T 2 , FLAIR ? T 1 T 2 ?FLAIR FLAIR?T 2 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR</formula><p>To interpret the information that self-attention mechanisms focus on during synthesis tasks, we computed and visualized the attention maps as captured by the transformer modules in ResViT. Attention maps were calculated based on the Attention Rollout technique, and a single average map was extracted for a given transformer module <ref type="bibr" target="#b103">[104]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-Contrast MRI Synthesis 1) Task-Specific Synthesis Models:</head><p>We demonstrated the performance of ResViT in learning task-specific synthesis models for multi-contrast MRI. ResViT was compared against convolutional models (pGAN, pix2pix, medSynth), attentionaugmented CNNs (A-UNet, SAGAN), and recent transformer architectures (TransUNet, PTNet). First, brain images of healthy subjects in the IXI dataset were considered. PSNR and SSIM metrics are listed in <ref type="table" target="#tab_1">Table IV</ref> for many-to-one and one-to-one tasks. ResViT achieves the highest performance in both many-to-one (p&lt;0.05) and one-to-one tasks (p&lt;0.05). On average, ResViT outperforms convolutional models by 1.71dB PSNR and 1.08% SSIM, attention-augmented models by 1.40dB PSNR and 1.45% SSIM, and transformer models by 2.33dB PSNR and 1.79% SSIM (p&lt;0.05). Representative images for T 1 , T 2 ? PD and T 2 , PD ? T 1 are displayed in <ref type="figure">Fig. 3a,b</ref>. Compared to baselines, ResViT synthesizes target images with lower artifact levels and sharper tissue depiction.</p><p>We then demonstrated task-specific ResViT models on the BRATS dataset containing images of glioma patients. PSNR and SSIM metrics are listed in <ref type="table">Table V</ref> for manyto-one and one-to-one tasks. ResViT again achieves the highest performance in many-to-one (p&lt;0.05) and one-toone tasks (p&lt;0.05), except T 2 ?FLAIR where A-UNet has slightly higher SSIM. On average, ResViT outperforms convolutional models by 1.01dB PSNR and 1.41% SSIM, attentionaugmented models by 0.84dB PSNR and 1.24% SSIM, and transformer models by 1.56dB PSNR and 1.63% SSIM (p&lt;0.05). Note that the BRATS dataset contains pathology with large across-subject variability. As expected, attentionaugmented models show relative benefits against pure convolutional models, yet ResViT that explicitly models contextual relationships still outperforms all baselines. Representative target images for T 1 , T 2 ? FLAIR and T 2 , FLAIR ? T 1 are displayed in <ref type="figure">Fig. 4a,b</ref>, respectively. Compared to baselines, ResViT synthesizes target images with lower artifact levels and sharper tissue depiction. Importantly, ResViT reliably captures brain lesions in patients in contrast to competing methods with inaccurate depictions including TransUNet.</p><p>Superior depiction of pathology in ResViT signals the importance of ART blocks in simultaneously maintaining local precision and contextual consistency in medical image synthesis. In comparison, transformer-based TransUNet and PTNet yield relatively limited synthesis quality that might be attributed to several fundamental differences between the models. TransUNet uses only a transformer in its bottleneck ResViT uni was demonstrated against other unified models on brain MRI datasets for two representative tasks: a) T 1 , PD ? T 2 in IXI, b) T 1 , FLAIR ? T 2 in BRATS. Synthesized images from all competing methods are shown along with the source images and the reference target image. ResViT uni improves synthesis performance especially in pathological regions (tumors, lesions) in comparison to competing methods. Overall, ResViT uni generates images with lower artifact and noise levels and more accurate tissue depiction for tasks in both datasets.  while propagating shallow convolutional features via encoderdecoder skip connections, and its decoder increases spatial resolution via bilinear upsampling that might be ineffective in suppressing high-frequency artifacts <ref type="bibr" target="#b104">[105]</ref>. In contrast, ResViT continues encoding and propagating convolutional features across the information bottleneck to create a deeper feature representation, and it employs transposed convolutions within upsampling modules to mitigate potential artifacts. PTNet is a convolution-free architecture that relies solely on self-attention operators that have limited localization ability <ref type="bibr" target="#b48">[49]</ref>. Instead, ResViT is devised as a hybrid CNN-transformer architecture to improve sensitivity for both local and contextual features.</p><formula xml:id="formula_23">T 1 , T 2 ? PD T 1 , PD ? T 2 T 2 , PD ? T 1 PSNR SSIM PSNR SSIM PSNR SSIM</formula><p>2) Unified Synthesis Models: Task-specific models are trained and tested to perform a single synthesis task to improve performance, but a separate model has to be built for each task. Next, we demonstrated ResViT in learning unified synthesis models for multi-contrast MRI. A unified ResViT (ResViT uni ) was compared against unified convolutional (pGAN uni , MM-GAN) and transformer models (TransUNet uni ). Performance of unified models were evaluated at test time on manyto-one tasks in IXI <ref type="table" target="#tab_1">(Table VI)</ref> and BRATS <ref type="table" target="#tab_1">(Table VII)</ref>. ResViT uni maintains the highest performance in many-to-one tasks in both IXI (p&lt;0.05) and BRATS (p&lt;0.05). In IXI, ResViT uni outperforms pGAN uni by 1.12dB PSNR and 0.70% SSIM, MM-GAN by 2.37dB PSNR and 1.80% SSIM, and TransUNet uni by 2.69dB PSNR and 1.67% SSIM (p&lt;0.05). In BRATS, ResViT outperforms pGAN uni by 0.74dB PSNR and 0.93% SSIM, MM-GAN by 0.77dB PSNR and 0.90% SSIM, and TransUNet uni by 1.08dB PSNR and 1.43% SSIM  (p&lt;0.05). Representative target images are displayed in <ref type="figure" target="#fig_3">Fig.  5</ref>. ResViT synthesizes target images with lower artifacts and sharper depiction than baselines. These results suggest that a unified ResViT model can successfully consolidate models for varying source-target configurations.</p><formula xml:id="formula_24">T 1 , T 2 ? FLAIR T 1 , FLAIR ? T 2 T 2 , FLAIR ? T 1 PSNR SSIM PSNR SSIM PSNR SSIM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Across-Modality Synthesis</head><p>We also demonstrated ResViT in across-modality synthesis. T 2 -weighted MRI and CT images in the pelvic dataset were considered. ResViT was compared against pGAN, pix2pix, medSynth, A-UNet, SAGAN, TransUNet, and PTNet. PSNR and SSIM metrics are listed in <ref type="table" target="#tab_1">Table VIII</ref>. ResViT yields the highest performance in each subject (p&lt;0.05). On average, ResViT outperforms convolutional models by 1.89dB PSNR and 3.20% SSIM, attention-augmented models by 0.75dB PSNR and 1.95% SSIM, and transformer models by 1.52dB PSNR and 2.40% SSIM (p&lt;0.05). Representative target images are displayed in <ref type="figure" target="#fig_4">Fig. 6</ref>   MRI, attention-augmented models and TransUNet offer more noticeable performance benefits over convolutional models. That said, ResViT still maintains further elevated performance, particularly near bone structures in CT images. This finding suggests that the relative importance of contextual representations is higher in MRI-CT synthesis. With the help of its residual transformer blocks, ResViT offers reliable performance with accurate tissue depiction in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>We performed a systematic set of experiments to demonstrate the added value of the main components and training strategies used in ResViT. First, we compared ResViT against ablated variants where the convolutional modules in ART blocks, transformer modules in ART blocks, or the adversarial term in training loss were separately removed. <ref type="table" target="#tab_1">Table IX</ref> lists performance metrics in the test set for three representative synthesis tasks. Consistently across tasks, ResViT yields optimal or near-optimal performance. ResViT achieves higher PSNR and SSIM in representative tasks compared to variants without transformer or convolutional modules (p&lt;0.05). It also yields lower FID than these variants, except in MRI ? CT where ablation of the convolutional module slightly decreases FID. Importantly, ResViT maintains notably lower FID compared to the variant without adversarial loss (albeit slightly lower SSIM in T 1 , T 2 ? FLAIR and PSNR, SSIM in MRI ? CT). This is expected since FID is generally considered as a more suited metric to examine the perceptual benefits of adversarial learning than PSNR or SSIM that reflect heavier influence from relatively lower frequencies <ref type="bibr" target="#b102">[103]</ref>. Representative synthesized images are also displayed in <ref type="figure" target="#fig_5">Fig. 7a</ref>. ResViT images more closely mimic the reference images, and show greater spatial acuity compared against the variant without adversarial loss. Taken together, these results indicate that adversarial learning enables ResViT to more closely capture the distributional properties of target-modality images.</p><p>Second, we compared ResViT against ablated variants where the weight tying procedure across transformer modules was neglected, or transformer modules in one of the two retaining ART blocks were removed. Table X lists performance metrics in the test set. ResViT yields higher performance than variants across representative tasks (p&lt;0.05), except for the variant only retraining A 6 that yields similar SSIM in T 1 , T 2 ? PD. These results demonstrate the added value of the weight tying procedure and the transformer configuration in ResViT. We also compared ResViT against ablated variants where the pre-training of transformer modules or their delayed insertion during training were selectively neglected, as listed in       Next, we inspected the relative strength of transformerderived contextual features in distilled representations within ART blocks. To do this, we computed the L 2 -norms of contextual feature maps output by the transformer module, and input feature maps from the previous ART block relayed through the skip connection of the transformer module. We also computed the relative weighting of the two feature maps as the L 2 -norms of respective combination weights in the channel compression (CC) module. Measurements for ResViT models trained in representative tasks are listed in <ref type="table" target="#tab_1">Table XIII</ref>. We find that contextual and input feature maps, and their respective combination weights in CC blocks have comparable strength, demonstrating that contextual features are a substantial component of image representations in ART blocks.  Lastly, we wanted to visually interpret the benefits of the self-attention mechanisms in ResViT towards synthesis performance. <ref type="figure" target="#fig_5">Fig. 7b</ref> displays representative attention maps in ResViT. Synthetic images and error maps are also shown for ResViT as well as pGAN, which generally offered the closest performance to ResViT in our experiments. We find that the attention maps exhibit higher intensity in critical regions such as brain lesions in multi-contrast MRI and pelvic bone structure in MR-to-CT synthesis. Importantly, these regions of higher attentional focus are also the primary regions where the synthesis errors are substantially diminished with ResViT compared to pGAN. Taken together, these results suggest that the transformer-based ResViT model captures contextual relationships related to both healthy and pathological tissues to improve synthesis performance.</p><formula xml:id="formula_25">T 1 , T 2 ? PD T 1 , T 2 ? FLAIR MRI ? CT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this study, we proposed a novel adversarial model for image translation between separate modalities. Traditional GANs employ convolutional operators that have limited ability to capture long-range relationships among distant regions <ref type="bibr" target="#b45">[46]</ref>. The proposed model aggregates convolutional and transformer branches within a residual bottleneck to preserve both local precision and contextual sensitivity. To our knowledge, this is the first adversarial model for medical image synthesis with a transformer-based generator. We further introduced a weightsharing strategy among transformer modules to lower model complexity. Finally, a unification strategy was implemented to learn an aggregate model that copes with numerous sourcetarget configurations without training separate models.</p><p>We demonstrated ResViT for missing modality synthesis in multi-contrast MRI and MRI-CT imaging. ResViT outperformed several state-of-the-art convolutional and transformer models in one-to-one and many-to-one tasks. We trained all models with an identical loss function to focus on architectural influences to synthesis performance. In unreported experiments, we also trained competing methods that were proposed with different loss functions using their original losses, including PTNet with mean-squared loss <ref type="bibr" target="#b56">[57]</ref> and medSynth with mean-squared, adversarial and gradient-difference losses <ref type="bibr" target="#b31">[32]</ref>. We observed that ResViT still maintains similar performance benefits over competing methods in these experiments. Yet, it remains important future work to conduct an in-depth assessment of optimal loss terms for ResViT, including gradientdifference and difficulty-aware losses for the generator <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b106">[107]</ref>, and edge-preservation and binary cross-entropy losses for the discriminator <ref type="bibr" target="#b106">[107]</ref>, <ref type="bibr" target="#b107">[108]</ref>.</p><p>Trained with image-average loss terms, CNNs have difficulty in coping with atypical anatomy that substantially varies across subjects <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b42">[43]</ref>. To improve generalization, recent studies have proposed self-attention mechanisms in GAN models over spatial or channel dimensions <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b84">[85]</ref>. Specifically, attention maps are used for multiplicative modulation of CNNderived feature maps. This modulation encourages the network to focus on critical image regions with relatively limited task performance. While attention maps can be distributed across image regions, they mainly capture implicit contextual information via modification of local CNN features. Since feature representations are primarily extracted via convolutional filtering, the resulting model can still manifest limited expressiveness for global context. In contrast, the proposed architecture uses dedicated transformer blocks to explicitly model long-range spatial interactions in medical images.</p><p>Few recent studies have independently proposed transformer-based models for medical image synthesis <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>. In <ref type="bibr" target="#b55">[56]</ref>, a transformer is included in the discriminator of a traditional GAN for MR-to-PET synthesis. In <ref type="bibr" target="#b56">[57]</ref>, a UNet-inspired transformer architecture is proposed for infant MRI synthesis <ref type="bibr" target="#b56">[57]</ref>. Differing from these efforts, our work makes the following contributions. (1) Compared to <ref type="bibr" target="#b55">[56]</ref> that uses transformers to learn a prior for target PET images, we employ transformers in ResViT's generator to learn latent contextual representations of source images. (2) Unlike <ref type="bibr" target="#b56">[57]</ref> that uses mean-squared error loss amenable to over-smoothing of target images <ref type="bibr" target="#b23">[24]</ref>, we leverage an adversarial loss to preserve realism. (3) While <ref type="bibr" target="#b56">[57]</ref> uses a convolution-free transformer architecture, we instead propose a hybrid architecture that combines localization capabilities of CNNs with contextual sensitivity of transformers. (4) While <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b56">[57]</ref> consider only task-specific, one-to-one synthesis models, here we uniquely introduce many-to-one synthesis models and a unified model that generalizes across multiple source-target configurations.</p><p>UNet-style models follow an encoder-decoder architecture with an hourglass structure <ref type="bibr" target="#b42">[43]</ref>. Because spatial resolution is substantially lower in the midpoint of the hourglass (e.g. 16x16 maps), these models typically introduce skip connections between the encoder and decoder layers to facilitate preservation of low-level features. In contrast, ResViT is a ResNet-style model where encoded representations pass through a bottleneck of residual blocks before reaching the decoder <ref type="bibr" target="#b57">[58]</ref>, and encoder-decoder skip connections are omitted due to several reasons. First, ResViT maintains relatively high resolution at the output of its encoder (e.g. 64x64 maps), so its bottleneck represents relatively lower-level information. Second, each ART block is organized as a transformer-CNN cascade with skip connections around both modules, creating a residual path between the input and output of each block. This eventually bridges the encoder output to the decoder input, creating a native residual path in ResViT. Lastly, we observed during early stages of the study that a variant model that included encoder-decoder skip connections caused a minor performance drop, suggesting that these extra connections might reduce the effectiveness of the central information bottleneck.</p><p>Here, ResViT models were initialized with transformers pretrained on 16x16 input feature maps. In turn, 256x256 images were 16-fold downsampled cumulatively across the encoder and transformer modules, and the transformer used a patch size of P=1 and sequence length of 256. Several strategies can be adopted to use ResViT at different image resolutions. In a first scenario, the downsampling rate and patch size can be preserved, while the sequence length is adjusted. For instance, a 512x512 image would be downsampled to a 32x32 feature map, resulting in a sequence of 1024 patches. While a transformer pre-trained on 32x32 maps would be ideal, vision transformers can reliably handle variable sequence lengths without retraining so the original transformer can still be used <ref type="bibr" target="#b48">[49]</ref>. Note that longer sequences would incur a quadratic increase in processing and memory load in both cases <ref type="bibr" target="#b48">[49]</ref>. In a second scenario, the original transformer with sequence length 256 can be maintained, while either the patch size or the downsampling rate is adjusted. For a 512x512 image, P=2 (2x2 patches) on a 32x32 map (16-fold downsampled) or P=1 on a 16x16 map (32-fold downsampled) could be used. Both options would achieve on par computational complexity to the original architecture, albeit the transformer would process feature maps at a relatively lower resolution compared to the resolution of the input image. It is unlikely that this would significantly affect ResViT's sensitivity to local features since the primary component of ART that captures local features is the residual CNN module whose resolution can be preserved. If the input image does not have a power-of-two size, the abovementioned strategies can be adopted after zero-padding to round up the resolution to the nearest power of two, or by implementing the encoder with non-integer downsampling rates <ref type="bibr" target="#b108">[109]</ref>. Note that computer vision studies routinely finetune transformers at different image resolutions than encountered during pre-training without performance loss <ref type="bibr" target="#b48">[49]</ref>, so ResViT might also demonstrate similar behavior. It remains important future work to investigate the comparative utility of the discussed resolution-adaptation strategies in medical image synthesis.</p><p>Several lines of development can help further improve ResViT's performance. Here, we considered synthesis tasks in which source and target modalities were registered prior to training, and they were paired across subjects. When registration accuracy is limited, a spatial registration block can be incorporated into the network. Furthermore, a cycleconsistency loss <ref type="bibr" target="#b43">[44]</ref> can be incorporated in the optimization objective to allow the use of unregistered images. This latter strategy would also permit training of ResViT models on unpaired datasets <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>. Data requirements for model training can be further alleviated by adopting semi-supervised strategies that allow mixing of paired and unpaired training data <ref type="bibr" target="#b74">[75]</ref>, or that would enable training of synthesis models directly from undersampled acquisitions <ref type="bibr" target="#b109">[110]</ref>. Finally, ResViT might benefit from incorporation of multi-scale modules in the decoder to improve preservation of fine image details <ref type="bibr" target="#b107">[108]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Here we introduced a novel synthesis approach for multimodal imaging based on a conditional deep adversarial network. In an information bottleneck, ResViT aggregates convolutional operators and vision transformers, thereby improving capture of contextual relations while maintaining localization power. A unified implementation was introduced that prevents the need to rebuild models for varying source-target configurations. ResViT achieves superior synthesis quality to stateof-the-art approaches in multi-contrast brain MRI and multimodal pelvic MRI-CT datasets. Therefore, it holds promise as a powerful candidate for medical image synthesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The generator in ResViT follows an encoder-decoder architecture bridged with a central information bottleneck to distill task-specific information. The encoder and decoder comprise convolutional layers to maintain local precision and inductive bias in learned structural representations. Meanwhile, the information bottleneck comprises a stack of novel aggregated residual transformer (ART) blocks. ART blocks learn contextual representations via vision transformers, and synergistically fuse CNNbased local and transformer-based global representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Input features from the previous network layer passing through skip connections of transformer and CNN modules; (b) Contextual features computed by the transformer module passing through the skip connection of the CNN module; (c) Local features computed by the CNN module based on input features reaching through the skip connection of the transformer module; (d) Hybrid local-contextual features computed by the transformer-CNN cascade. Therefore, the main motivation for use of residual transformer and residual CNN modules in ART blocks is to learn an aggregated representation that synergistically combines lower-level input features along with their contextual, local, and hybrid local-contextual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>ResViT is a conditional image synthesis model that can unify various source-target modality configurations into a single model for improved practicality. a) During training, ResViT takes as input the entire set of images within the multimodal protocol, including both source and target modalities. For model consolidation across multiple synthesis tasks, various configurations of source-target modalities are expressed in terms of availability conditions in ResViT. b) During inference, the specific source-target configuration is determined via the availability conditions in each given test subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: ResViT uni was demonstrated against other unified models on brain MRI datasets for two representative tasks: a) T 1 , PD ? T 2 in IXI, b) T 1 , FLAIR ? T 2 in BRATS. Synthesized images from all competing methods are shown along with the source images and the reference target image. ResViT uni improves synthesis performance especially in pathological regions (tumors, lesions) in comparison to competing methods. Overall, ResViT uni generates images with lower artifact and noise levels and more accurate tissue depiction for tasks in both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>ResViT was demonstrated on the pelvic MRI-CT dataset for the T 2 -weighted MRI ? CT task. Synthesized images from all competing methods are shown along with the source and reference images. ResViT enhances synthesis of relevant morphology in the CT domain as evidenced by the elevated accuracy near bone structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>a) ResViT was compared against a variant where the adversarial term was removed from the loss function. Representative results are shown for T 1 , T 2 ? PD in IXI, T 1 , T 2 ? FLAIR in BRATS, and MRI ? CT in the pelvic dataset. Adversarial loss improves the acuity of synthesized images. b) Representative results from ResViT and pGAN are shown along with the reference images for T 2 , FLAIR ? T 1 , T 1 , FLAIR ? T 2 , and T 1 , T 2 ? FLAIR in BRATS; and MRI ? CT in the pelvic dataset. Error maps between the synthetic and reference images for each method are displayed, along with the attention map for the first transformer module of ResViT. Here, the attention maps were overlaid onto the reference image for improved visualization. Attention maps focus on image regions where ResViT substantially reduces synthesis errors compared to pGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Validation performance of candidate ResViT configurations in representative synthesis tasks. Performance is taken as PSNR (dB) between synthesized and reference target images. A i denotes the presence of a transformer module in the ith ART block.</figDesc><table><row><cell>Transformer size</cell><cell cols="3">T 1 , T 2 ? PD T 1 , T 2 ? FLAIR MRI ? CT PSNR PSNR PSNR</cell></row><row><cell>Base</cell><cell>33.34</cell><cell>24.88</cell><cell>26.56</cell></row><row><cell>Large</cell><cell>33.14</cell><cell>24.60</cell><cell>26.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Validation performance of ResViT models with varying sizes of transformer modules in representative synthesis tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Average inference times (msec) of synthesis models per single cross-section.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>ResViT 33.92 0.977 35.71 0.977 29.58 0.952 32.90 0.972 34.24 0.972 ?1.44 ?0.004 ?1.20 ?0.005 ?1.37 ?0.011 ?1.20 ?0.005 ?1.09 ?0.005 pGAN 32.91 0.966 33.95 0.965 28.71 0.941 32.20 0.963 33.05 0.963 ?0.94 ?0.005 ?1.06 ?0.006 ?1.08 ?0.013 ?1.00 ?0.005 ?0.95 ?0.007 pix2pix 32.25 0.974 33.62 0.973 28.35 0.949 30.72 0.956 30.74 0.950 ?1.24 ?0.006 ?1.31 ?0.009 ?1.24 ?0.016 ?1.28 ?0.007 ?1.63 ?0.012 medSynth 33.23 0.967 32.66 0.963 28.43 0.938 32.20 0.964 30.41 0.956 ?1.09 ?0.005 ?1.30 ?0.007 ?1.01 ?0.013 ?1.10 ?0.006 ?3.98 ?0.025 A-UNet 32.24 0.963 32.43 0.959 28.95 0.916 32.05 0.960 33.32 0.961 ?0.92 ?0.014 ?1.36 ?0.007 ?1.21 ?0.013 ?1.04 ?0.009 ?1.08 ?0.007 SAGAN 32.50 0.964 33.71 0.965 28.62 0.942 32.07 0.963 32.96 0.962 ?0.93 ?0.005 ?1.00 ?0.006 ?1.10 ?0.013 ?0.98 ?0.006 ?1.01 ?0.007 TransUNet 32.53 0.968 32.49 0.960 28.21 0.941 30.90 0.960 31.73 0.958 ?0.97 ?0.005 ?1.18 ?0.008 ?1.30 ?0.013 ?1.35 ?0.006 ?1.44 ?0.008 PTNet 30.92 0.952 32.62 0.954 27.59 0.923 31.58 0.958 30.84 0.947 ? 0.99 ?0.006 ?1.96 ?0.019 ?1.36 ?0.021 ?1.30 ?0.007 ?2.54 ?0.033</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>?1.13 ?0.014 ?1.20 ?0.011 ?1.31 ?0.009 ?1.07 ?0.014 ?0.92 ?0.015 ?1.10 ?0.015 ?1.13 ?0.012 ?1.54 ?0.011 ?1.15 ?0.014 ?1.52 ?0.015 ?1.21 ?0.015 ?1.53 ?0.012 ?1.72 ?0.011 ?1.93 ?0.016 ?0.88 ?0.014 ?1.45 ?0.016 ?0.76 ?0.011 ?1.62 ?0.012 ?1.88 ?0.017 ?0.82 ?0.014 .873 24.56 0.891 ?1.24 ?0.017 ?1.21 ?0.012 ?1.35 ?0.010 ?1.57 ?0.015 ?0.94 ?0.014 ?1.17 ?0.014 ?1.22 ?0.012 ?1.42 ?0.011 ?1.35 ?0.015 ?0.88 ?0.014 ?1.26 ?0.014 ?0.92 ?0.010 ?1.69 ?0.011 ?1.75 ?0.015 ?0.81 ?0.015 ?1.24 ?0.031 ?1.23 ?0.016 ?1.88 ?0.014 ?0.85 ?0.014 ?0.88 ?0.015</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell></row><row><cell>ResViT</cell><cell>25.84</cell><cell>0.886</cell><cell>26.90</cell><cell>0.938</cell><cell>26.20</cell><cell>0.924</cell><cell>24.97 0.870 25.78 0.908</cell></row><row><cell>pGAN</cell><cell>24.89</cell><cell>0.867</cell><cell>26.51</cell><cell>0.922</cell><cell>25.72</cell><cell>0.918</cell><cell>24.01 0.864 25.09 0.894</cell></row><row><cell>pix2pix</cell><cell>24.31</cell><cell>0.862</cell><cell>26.12</cell><cell>0.920</cell><cell>25.80</cell><cell>0.918</cell><cell>23.15 0.869 24.52 0.883</cell></row><row><cell>medSynth</cell><cell>23.93</cell><cell>0.863</cell><cell>26.44</cell><cell>0.921</cell><cell>25.72</cell><cell>0.914</cell><cell>23.36 0.864 24.41 0.888</cell></row><row><cell cols="8">A-UNet 23.69 0SAGAN 24.36 0.857 26.48 0.924 25.67 0.918 24.62 0.869 26.41 0.919 25.91 0.918 24.02 0.860 25.10 0.893</cell></row><row><cell>TransUNet</cell><cell>24.34</cell><cell>0.872</cell><cell>26.51</cell><cell>0.920</cell><cell>25.76</cell><cell>0.921</cell><cell>23.70 0.864 24.62 0.891</cell></row><row><cell>PTNet</cell><cell>23.78</cell><cell>0.851</cell><cell>25.09</cell><cell>0.905</cell><cell>22.19</cell><cell>0.920</cell><cell>23.01 0.851 24.78 0.894</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc></figDesc><table /><note>Performance of unified synthesis models in many- to-one tasks T 1 , T 2 ? PD, T 1 , PD ? T 2 , and T 2 , PD ? T 1 ) across test subjects in the IXI dataset. Boldface indicates the top-performing model for each task.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII :</head><label>VII</label><figDesc>Performance of unified synthesis models in manyto-one tasks (T 1 , T 2 ? FLAIR, T 1 , FLAIR ? T 2 , and T</figDesc><table /><note>2 , FLAIR ? T 1 ) across test subjects in the BRATS dataset. Boldface indicates the top-performing model for each task.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VIII :</head><label>VIII</label><figDesc></figDesc><table /><note>Performance for the across-modality synthesis task (T 2 -weighted MRI ? CT) across test subjects in the pelvic MRI-CT dataset. Boldface indicates the top-performing model for each task.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table XI .</head><label>XI</label><figDesc>Our results indicate that ResViT outperforms all ablated variants (p&lt;0.05), except for the variant without delayed insertion that yields similar SSIM in T 1 , T 2 ? PD.</figDesc><table><row><cell></cell><cell cols="2">T 1 , T 2 ? PD</cell><cell cols="3">T 1 , T 2 ? FLAIR</cell><cell cols="2">MRI ? CT</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell cols="2">FID PSNR SSIM</cell><cell>FID</cell><cell>PSNR</cell><cell>SSIM</cell><cell>FID</cell></row><row><cell>ResViT</cell><cell cols="5">33.92 ?1.44 ?0.004 0.977 14.47 25.84 ?1.13 ?0.014 0.886 18.58</cell><cell cols="2">28.45 ?1.35 ?0.009 0.931 60.28</cell></row><row><cell cols="2">w/o trans. 32.91</cell><cell cols="2">0.966 14.56 24.96</cell><cell cols="2">0.868 19.21</cell><cell>26.73</cell><cell>0.899 95.38</cell></row><row><cell>modules</cell><cell cols="2">?0.96 ?0.005</cell><cell cols="2">?1.10 ?0.005</cell><cell></cell><cell cols="2">?0.91 ?0.008</cell></row><row><cell cols="2">w/o conv. 33.49</cell><cell cols="2">0.971 14.84 25.11</cell><cell cols="2">0.874 20.30</cell><cell>28.19</cell><cell>0.922 60.16</cell></row><row><cell>modules</cell><cell cols="2">?1.34 ?0.005</cell><cell cols="2">?1.02 ?0.014</cell><cell></cell><cell cols="2">?1.15 ?0.009</cell></row><row><cell>w/o adv.</cell><cell>33.75</cell><cell cols="2">0.977 15.80 22.95</cell><cell cols="2">0.891 40.68</cell><cell>28.58</cell><cell>0.932 65.49</cell></row><row><cell>loss</cell><cell cols="2">?1.45 ?0.005</cell><cell cols="2">?1.93 ?0.015</cell><cell></cell><cell cols="2">?1.13 ?0.007</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE IX :</head><label>IX</label><figDesc>Test performance of ResViT and variants ablated of transformer modules, convolutional modules or adversarial loss. FID is a summary metric across the entire test set. Boldface indicates the top-performing model for each task.</figDesc><table><row><cell></cell><cell cols="2">T 1 , T 2 ? PD</cell><cell cols="2">T 1 , T 2 ? FLAIR</cell><cell cols="2">MRI ? CT</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>A1 ? A6</cell><cell>33.92 ?1.44</cell><cell>0.977 ?0.004</cell><cell>25.84 ?1.13</cell><cell>0.886 ?0.014</cell><cell>28.45 ?1.35</cell><cell>0.931 ?0.009</cell></row><row><cell>A1 ? A6</cell><cell>33.72</cell><cell>0.973</cell><cell>25.19</cell><cell>0.879</cell><cell>28.16</cell><cell>0.923</cell></row><row><cell>(untied weights)</cell><cell>?1.23</cell><cell>?0.005</cell><cell cols="2">?1.18 ? 0.014</cell><cell>?1.11</cell><cell>?0.007</cell></row><row><cell>A1</cell><cell>33.51 ?1.15</cell><cell>0.971 ?0.005</cell><cell>24.98 ?1.60</cell><cell>0.883 ?0.015</cell><cell>28.06 ?1.31</cell><cell>0.921 ?0.008</cell></row><row><cell>A6</cell><cell>33.78 ?1.34</cell><cell>0.977 ?0.004</cell><cell>25.25 ?1.20</cell><cell>0.880 ?0.014</cell><cell>27.95 ?1.22</cell><cell>0.921 ?0.008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE X :</head><label>X</label><figDesc>Test performance of ResViT (A 1 ? A 6 ) and variants ablated of weight tying and individual transformer modules. Boldface indicates the top-performing model for each task.</figDesc><table><row><cell></cell><cell cols="2">T 1 , T 2 ? PD</cell><cell cols="2">T 1 , T 2 ? FLAIR</cell><cell cols="2">MRI ? CT</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>ResViT</cell><cell>33.92 ?1.44</cell><cell>0.977 ?0.004</cell><cell>25.84 ?1.13</cell><cell>0.886 ?0.014</cell><cell>28.45 ?1.35</cell><cell>0.931 ?0.009</cell></row><row><cell>w/o pre-training</cell><cell>33.55 ?1.25</cell><cell>0.971 ?0.005</cell><cell>24.86 ?1.28</cell><cell>0.881 ? 0.016</cell><cell>27.94 ?1.25</cell><cell>0.912 ?0.009</cell></row><row><cell>w/o del.</cell><cell>33.35</cell><cell>0.977</cell><cell>24.89</cell><cell>0.873</cell><cell>28.01</cell><cell>0.924</cell></row><row><cell>insertion</cell><cell>?1.13</cell><cell>?0.004</cell><cell>?1.18</cell><cell>?0.015</cell><cell>?1.27</cell><cell>?0.008</cell></row><row><cell>w/o pre-training</cell><cell>33.58</cell><cell>0.971</cell><cell>24.74</cell><cell>0.869</cell><cell>27.66</cell><cell>0.913</cell></row><row><cell>or del. insertion</cell><cell>?1.16</cell><cell>?0.005</cell><cell>?1.30</cell><cell>?0.016</cell><cell>?0.78</cell><cell>?0.006</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE XI :</head><label>XI</label><figDesc>Test performance of ResViT and variants ablated of pre-training and delayed insertion procedures for transformers. Boldface indicates the top-performing model for each task.Third, we examined the utility of the skip connections and down/upsampling blocks in the proposed architecture. We compared ResViT against variants built by removing the skip connection around the transformer module or around the CNN module in ART blocks.Table XIIlists performance metrics in the test set. ResViT yields higher performance than all variants (p&lt;0.05). Our results indicate that ResViT benefits substantially from residual learning in ART blocks. We also compared ResViT against variants built by replacing down/upsampling modules in ART blocks with unlearned maxpooling/bilinear interpolation modules, and by increasing encoder downsampling and decoder upsampling rates to remove down/upsampling modules in ART blocks entirely. ResViT outperforms all variants as listed in Table XII (p&lt;0.05), except for MRI?CT where the variant</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE XII :</head><label>XII</label><figDesc>Test performance of ResViT and variants built by: removing skip connections in convolutional modules, removing skip connections in transformer modules, using unlearned down/upsampling blocks in ART, removing down/upsampling blocks in ART via a higher degree of down/upsampling in the encoder/decoder. Boldface indicates the top-performing model for each task.with unlearned down/upsampling and ResViT yield similar SSIM. These results demonstrate the benefits of the proposed down/upsampling scheme in ResViT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE XIII :</head><label>XIII</label><figDesc>Feature maps and corresponding combination weights for the channel compression (CC) module were inspected in ResViT. Averaged across the test set and ART blocks, L 2 -norm of feature maps from the transformer module (g) and feature maps input by the previous ART block (f ) are listed along with combination weights for g and for f .</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Pichler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Judenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pfannenberg</surname></persName>
		</author>
		<title level="m">Multimodal Imaging Approaches: PET/CT and PET/MRI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="109" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-contrast, isotropic, singleslab 3d MR imaging in multiple sclerosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moraal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roosendaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pouwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vrenken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schijndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guttmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2311" to="2320" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Problems and preferences in pediatric imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian J. Radiol. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Artifacts in magnetic resonance imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krupa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bekiesi?ska-Figatowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pol. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is synthesizing MRI contrast useful for inter-modality analysis?&quot; in MICCAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="631" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial synthesis learning enables segmentation without target modality ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Assad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ISBI</title>
		<meeting>IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1217" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advances and challenges in super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modality propagation: Coherent synthesis of subject-specific scans with datadriven regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="606" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward implementing an MRIbased PET attenuation-correction method for neurologic studies on the MR-PET brain prototype</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Catana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Der Kouwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Benner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fenchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1431" to="1438" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-atlas-based CT synthesis from conventional MRI with patch-based refinement for MRIbased radiotherapy planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10133</biblScope>
			<biblScope unit="page">101331</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Atlas based intensity transformation of brain MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Brain Image Analysis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and cross-modality synthesis of 3d medical images using weakly-supervised joint convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5787" to="5796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-modality image synthesis via weakly coupled and geometry co-regularized joint dictionary learning</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="815" to="827" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Whole brain segmentation and labeling from CT using synthetic MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random forest regression for magnetic resonance image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="475" to="488" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-domain synthesis of medical images using efficient location-sensitive deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="677" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised crossmodal synthesis of subject-specific scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="630" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prediction of CT substitutes from MR images based on local diffeomorphic mapping for brain PET attenuation correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1635" to="1641" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image quality transfer via random forest regression: Applications in diffusion MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating CT image from MRI data using structured random forest and auto-context model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative patch-based super-resolution for diffusion-weighted images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coupe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Manj?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="245" to="261" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Whole image synthesis using a deep encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sevetlidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal MR synthesis via modality-invariant latent representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="803" to="814" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image synthesis in multi-contrast MRI with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2375" to="2388" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pseudo-healthy image synthesis for white matter lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hammers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dickie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extended modality propagation: Image synthesis of pathological cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cordier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2598" to="2608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust multi-modal MR image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fluid-attenuated inversion recovery MRI synthesis from multisequence MRI using three-dimensional fully convolutional networks for multiple sclerosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bodini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durrleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Colliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stankoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14005</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of NIPS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">High-resolution medical image synthesis using progressively grown generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ostmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d cGAN based cross-modality MR image synthesis for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fripp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ISBI</title>
		<meeting>IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Medical image synthesis with deep convolutional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Trullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2720" to="2730" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MedGAN: Medical image translation using GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>K?stner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nikolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Grap</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">101684</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CollaGAN: Collaborative GAN for missing image data imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Diamondgan: Unified multi-modal generative adversarial networks for MRI sequences synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="795" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hi-net: Hybrid-fusion network for multi-modal MR image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2772" to="2781" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sc-GAN: 3d self-attention conditional GAN with spectral normalization for multi-modal neuroimaging synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sepehrband</surname></persName>
		</author>
		<idno>bioRxiv:2020.06.09.143297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">mustGAN: multi-stream generative adversarial networks for MR image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101944</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Synthesizing multi-contrast MR images via novel 3d conditional variational auto-encoding GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mob. Netw. Appl</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ea-GANs: Edge-aware generative adversarial networks for cross-modality MR image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fripp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1750" to="1762" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Missing MRI pulse sequence synthesis using multi-modal generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Synthesize high-quality multi-contrast magnetic resonance imaging from multi-echo acquisition using multi-task deep generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3089" to="3099" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<title level="m">On convergence and stability of GANs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Grainger &amp; Allison&apos;s Diagnostic Radiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schaefer-Prokop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grainger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Allison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Elsevier Health Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chimelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vinters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brandner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropathology: A Reference Text of CNS Pathology</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Elsevier Health Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mcdonagh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention U-Net: Learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d transformer-gan for high-quality pet reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transct: Dual-path transformer for low dose computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI, 2021</title>
		<imprint>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised mri reconstruction via zero-shot learned adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Korkmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?zbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">VtGAN: Semi-supervised retinal image synthesis and disease prediction using vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06757</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ihsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Sreenivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04393</idno>
		<title level="m">GANbert: Generative adversarial networks with bidirectional encoder representations from transformers for MRI to PET synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Ptnet: A high-resolution infant MRI synthesizer based on transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ettehadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Semanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13993</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generative adversarial network in medical imaging: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Babyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101552</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning based imaging data completion for improved brain disease diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast patch-based pseudo-CT synthesis from t1-weighted MR images for PET/MR attenuation correction in brain studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torrado-Carvajal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Herraiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alcain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garcia-Ca?amaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hernandez-Tamames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rozenholc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Malpica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="136" to="143" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Convolutional neural network for reconstruction of 7T-like images from 3T MRI using appearance and anatomical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rekik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Reconstruction of 7T-like images from 3T MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2085" to="2097" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dual-domain convolutional neural networks for improving structural information in 3t MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imag</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">MR-based synthetic CT generation using a deep convolutional neural network method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1408" to="1419" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Estimating CT image from MRI data using 3D fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="170" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Novel deep learning-based CT synthesis algorithm for MRI-guided PET attenuation correction in brain PET/MR imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE NSS/MIC</title>
		<meeting>IEEE NSS/MIC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Improved MR to CT synthesis for PET/MR attenuation correction using imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Markiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thielemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Data augmentation using generative adversarial networks (cycleGAN) to improve generalizability in CT segmentation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pickhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">GAN-based synthetic medical image augmentation for increased cnn performance in liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cross-modality synthesis from CT to PET using FCN and GAN networks for improved automated lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unpaired PET/CT image synthesis of liver region using CycleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fourcade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lacombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Medical Information Processing and Analysis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11583</biblScope>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep CT to MR synthesis using paired and unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2361</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unpaired MR to CT synthesis with explicit structural constrained adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ISBI</title>
		<meeting>IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1096" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep MR to CT synthesis using unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dinkla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savenije</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seevinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Isgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Synthetic CT generation from non-attenuation corrected PET images for whole-body PET imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">215016</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Unpaired brain MR-to-CT synthesis using a structure-constrained cy-cleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prince</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04536</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Cross-modality image synthesis from unpaired data using cyclegan: Effects of gradient consistency loss and training data size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hiasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Otake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sugano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adversarial image synthesis for unpaired multi-modal cardiac data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dharmakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">7t MRI super-resolution with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bourdon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guillevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IST Electronic Imaging Symposium</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unpaired deep crossmodality synthesis with fast training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Attention-aware discrimination for MRto-CT image translation using cycle-consistent generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kearney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Ziemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Yom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">190027</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Tripartite-GAN: Synthesizing liver contrast-enhanced MRI to improve tumor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kassam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101667</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Sara-GAN: Self-attention and relative average discriminator based generative adversarial networks for fast compressed sensing MRI reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Menpes-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Sacnn: Self-attention convolutional neural network for low-dose CT denoising with selfsupervised perceptual loss network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2289" to="2301" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03024</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Transmed: Transformers advance multi-modal medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05940</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Convolution-free medical image segmentation using transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasylechko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13645</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Neural networks: a comprehensive foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Prentice Hall PTR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">MR and CT data with multiobserver delineations of organs in the pelvic area-part of the gold atlas project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sohlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kjell?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>S?derstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1295" to="1300" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A global optimisation methof for robust affine registration of brain images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Three-dimensional self-attention conditional GAN with spectral normalization for multimodal neuroimaging synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Lan ; A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Toga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepehrband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Alzheimer Disease Neuroimaging Initiative</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1718" to="1733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m">Imagenet-21k pretraining for the masses</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Watch your up-convolution: Cnn based generative deep neural networks are failing to reproduce spectral distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keuper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="7887" to="7896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Lr-cgan: Latent representation based conditional generative adversarial network for multi-modality mri synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">102457</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Adversarial Confidence Learning for Medical Image Segmentation and Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2494" to="2513" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Edge-preserving mri image synthesis via adversarial network with iterative multi-scale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Convolutional block design for learned fractional downsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09999</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Semi-supervised learning of mutually accelerated MRI synthesis without fully-sampled ground truths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U H</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?zbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>T?naz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ukur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14347</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
