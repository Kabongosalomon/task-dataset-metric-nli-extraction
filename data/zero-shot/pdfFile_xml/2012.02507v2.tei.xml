<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-to-Fine Entity Representations for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damai</forename><surname>Dai</surname></persName>
							<email>daidamai@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
							<email>zengs@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-to-Fine Entity Representations for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level Relation Extraction (RE) requires extracting relations expressed within and across sentences. Recent works show that graph-based methods, usually constructing a document-level graph that captures document-aware interactions, can obtain useful entity representations thus helping tackle document-level RE. These methods either focus more on the entire graph, or pay more attention to a part of the graph, e.g., paths between the target entity pair. However, we find that document-level RE may benefit from focusing on both of them simultaneously. Therefore, to obtain more comprehensive entity representations, we propose the Coarse-to-Fine Entity Representation model (CFER) that adopts a coarse-to-fine strategy involving two phases. First, CFER uses graph neural networks to integrate global information in the entire graph at a coarse level. Next, CFER utilizes the global information as a guidance to selectively aggregate path information between the target entity pair at a fine level. In classification, we combine the entity representations from both two levels into more comprehensive representations for relation extraction. Experimental results on two document-level RE datasets, DocRED and CDR, show that CFER outperforms existing models and is robust to the uneven label distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Extraction (RE) aims to extract semantic relations between named entities from plain text. It is an efficient way to acquire structured knowledge automatically, thus benefiting various natural language processing (NLP) applications, especially knowledge graph construction <ref type="bibr" target="#b9">[Luan et al., 2018]</ref>. Most of the previous RE works focus on the sentence level, i.e., they extract the relations within only a single sentence <ref type="bibr" target="#b15">[Zeng et al., 2014;</ref><ref type="bibr" target="#b17">Zhou et al., 2016]</ref>. However, in real-world scenarios, sentence-level RE models may omit  <ref type="figure">Figure 1</ref>: An example for document-level RE. Word spans with background colors denote the mentions. Mentions that refer to the same entity have the same background color. The solid lines denote intra-sentence relations. The dotted line denotes an inter-sentence relation. Document-level RE requires extracting both intra-and inter-sentence relations between all the target entity pairs. some inter-sentence relations while a considerable number of relations are expressed beyond a single sentence but across multiple sentences in a long document <ref type="bibr" target="#b14">[Yao et al., 2019]</ref>. Therefore, document-level RE has attracted much attention in recent years. <ref type="figure">Figure 1</ref> shows an example document and corresponding relational facts for document-level RE. In the example, to extract the relation between Benjamin Bossi and Columbia Records, two entities separated by several sentences, we need the following inference steps. First, we need to know that Benjamin Bossi is a member of Romeo Void. Next, we need to infer that Never Say Never is performed by Romeo Void, and released by Columbia Records. Based on these facts, we can draw a conclusion that Benjamin Bossi is signed by Columbia Records. The example indicates that, to tackle documentlevel RE, a model needs the ability to capture interactions between long-distance entities. In addition, since a document may have an extremely long text, a model also needs the ability to integrate global contextual information for words.</p><p>Recent works show that graph-based methods can obtain useful entity representations thus helping tackle documentlevel RE. These methods usually construct a document-level graph, which represents words as nodes and uses edges between them to capture document-aware interactions. To obtain entity representations for relation extraction, some of them use graph neural networks (GNN) <ref type="bibr" target="#b9">[Marcheggiani and Titov, 2017;</ref><ref type="bibr" target="#b2">Guo et al., 2019b]</ref> to integrate neighborhood information for each node <ref type="bibr" target="#b12">[Sahu et al., 2019;</ref><ref type="bibr" target="#b10">Nan et al., 2020]</ref>. Although they consider the entire graph structure, they may fail to model the interactions between long-distance entities  <ref type="figure">Figure 2</ref>: An illustration of a document-level graph corresponding to a two-sentence document. Each node in the graph corresponds to a word in the document. We design five categories of edges to connect nodes in the graph. For the simplicity of the illustration, we omit some self-loop edges and adjacent word edges. due to the inherent over-smoothing problem in GNN <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. Other works attempt to encode path information between the target entity pair in the graph <ref type="bibr" target="#b12">[Quirk and Poon, 2017;</ref><ref type="bibr" target="#b12">Christopoulou et al., 2019]</ref>. They have the ability to alleviate the problem of modeling long-distance entity interactions, but they may fail to capture global contextual information since they usually integrate only local contextual information for nodes in the graph. Therefore, to obtain more comprehensive entity representations, it is necessary to find a way to integrate global contextual information and model long-distance entity interactions simultaneously.</p><p>In this paper, we propose the Coarse-to-Fine Entity Representation model (CFER) to obtain comprehensive entity representations for document-level RE. More specifically, we first construct a document-level graph that captures rich document-aware interactions, reflected by syntactic dependencies, adjacent word connections, cross-sentence connections, and coreferential mention interactions. Based on the constructed graph, we design a coarse-to-fine strategy with two phases. First, we use Densely Connected Graph Convolutional Networks (DCGCN) <ref type="bibr" target="#b2">[Guo et al., 2019b]</ref> to integrate global contextual information in the entire graph at a coarse level. Next, we adopt an attention-based path encoding mechanism, which takes the global contextual information as a guidance, to selectively aggregate path information between the potentially long-distance target entity pair at a fine level. Given the entity representations from both two levels that feature global contextual information and long-distance interactions, respectively, we can obtain more comprehensive entity representations for relation extraction by combining them.</p><p>Our contributions are summarized as follows:</p><p>? We propose a novel document-level RE model called CFER. It uses a coarse-to-fine strategy to integrate global contextual information and model long-distance interactions between the target entities simultaneously, thus obtaining comprehensive entity representations.</p><p>? Experimental results on two popular document-level RE datasets, DocRED and CDR, show that CFER achieves better performance than existing models.</p><p>? Elaborate analysis validates the effectiveness of our coarse-to-fine strategy. Further, we highlight the robustness of CFER to the uneven label distribution and the ability of CFER to model long-distance interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formulation</head><p>Let D denote a document consisting of N sentences D =</p><formula xml:id="formula_0">{s i } N i=1 , where s i = {w j } M j=1</formula><p>denotes the i-th sentence containing M words denoted by w j . Let E denote an entity set</p><formula xml:id="formula_1">containing P entities E = {e i } P i=1 , where e i = {m j } Q j=1</formula><p>denotes the coreferential mention set of the i-th entity, containing Q word spans of corresponding mentions denoted by m j . Given D and E, document-level RE requires extracting all the relational facts in the form of triplets, i.e., extracting {(e i , r, e j )|e i , e j ? E, r ? R}, where R is a pre-defined relation category set. Since an entity pair may have multiple semantic relations, we formulate document-level RE as a multilabel classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document-level Graph Construction</head><p>Given a document, we first construct a document-level graph that captures rich document-aware interactions, reflected by syntactic dependencies, adjacent word connections, crosssentence connections, and coreferential mention interactions. <ref type="figure">Figure 2</ref> shows an example document-level graph corresponding to a two-sentence document. The graph regards words in the document as nodes and captures documentaware interactions by five categories of edges. These undirected edges are described as follows. Syntactic dependency edge: Syntactic dependency information is proved effective for document-level or cross-sentence RE in previous works <ref type="bibr" target="#b12">[Sahu et al., 2019]</ref>. Therefore, we use the dependency parser in spaCy 2 to parse the syntactic dependency tree for each sentence. After that, we add edges between all node pairs that have dependency relations. Adjacent word edge: <ref type="bibr" target="#b12">Quirk and Poon [2017]</ref> point out that adding edges between adjacent words can mitigate the dependency parser errors. Therefore, we add edges between all node pairs that are adjacent in the document. Self-loop edge: For a node, in addition to its neighborhood information, the historical information of the node itself is also essential in information integration. Therefore, we add a self-loop edge for each node. Adjacent sentence edge: To ensure that information can be integrated across sentences, for each adjacent sentence pair, we add an edge between their dependency tree roots. Coreferential mention edge: Coreferential mentions may share information captured from their respective contexts with each other. This could be regarded as global crosssentence interactions. Therefore, we add edges between the first words of all mention pairs that refer to the same entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Coarse-to-Fine Entity Representations</head><p>In this subsection, we describe our proposed Coarse-to-Fine Entity Representation model (CFER) in detail. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our model is composed of a text encoding module, a coarse-level representation module, a fine-level representation module, and a classification module. Text Encoding Module: This module aims to encode each word in the document as a vector with text contextual information. By default, CFER uses GloVe <ref type="bibr" target="#b10">[Pennington et al., 2014]</ref> embeddings and a Bi-GRU model <ref type="bibr">[Cho et al., 2014;</ref><ref type="bibr" target="#b12">Schuster and Paliwal, 1997]</ref> as the encoder. To improve the ability of contextual text encoding, we can also replace this module by Pre-Trained Models (PTM) such as BERT <ref type="bibr" target="#b0">[Devlin et al., 2019]</ref> or <ref type="bibr">RoBERTa [Liu et al., 2019]</ref>. This module finally outputs contextual word representations h i of each word in the document:</p><formula xml:id="formula_2">h i = Encoding(D),<label>(1)</label></formula><p>where h i ? R d h , and d h denotes the hidden dimension.</p><p>Coarse-level Representation Module: This module aims to integrate local and global contextual information in the entire document-level graph. As indicated by <ref type="bibr" target="#b2">Guo et al. [2019b]</ref>, Densely Connected Graph Convolutional Networks (DCGCN) have the ability to capture rich local and global contextual information. Therefore, we adopt DCGCN layers as the coarse-level representation module. DCGCN layers are organized into n blocks and the k-th block has m k sub-layers. At the l-th sub-layer in block k, the calculation for node i is defined as</p><formula xml:id="formula_3">h (k,l) i = ReLU ? ? j?N (i) W (k,l)? (k,l) j + b (k,l) ? ? , (2) h (k,l) j = [x (k) j ; h (k,1) j ; ...; h (k,l?1) j ],<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">x (k) j ? R d h is the block input of node j. h (k,l) i ? R d h /m k is the output of node i at the l-th sub-layer in block k.? (k,l) j ? R d h +(l?1)d h /m k is the neighborhood input, ob- tained by concatenating x (k) j</formula><p>and the outputs of node j from all previous sub-layers in the same block. W (k,l) and b (k,l) are trainable parameters. N (i) denotes the neighbor set of node i in the document-level graph. Finally, block k adds up the block input and the concatenation of all sub-layer outputs, and then adopts a fully connected layer to compute the block output o</p><formula xml:id="formula_5">(k) i ? R d h : o (k) i = FC x (k) i + [h (k,1) i ; ...; h (k,m k ) i ] .<label>(4)</label></formula><p>Finally, we take the output of the final block o (n) i as the output of the coarse-level representation module. Fine-level Representation Module: The coarse-level representations can capture rich contextual information, but may fail to model long-distance entity interactions. Taking the global contextual information as a guidance, the fine-level representation module aims to utilize path information between the target entity pair to alleviate this problem. This module adopts an attention-based path encoding mechanism based on a path encoder and an attention aggregator.</p><p>For a target entity pair (e 1 , e 2 ), we denote the numbers of their corresponding mentions as |e 1 | and |e 2 |, respectively. We first extract |e 1 | ? |e 2 | shortest paths between all mention pairs in a subgraph that contains only syntactic dependency and adjacent sentence edges. For the i-th path [w 1 , ..., w leni ], we then adopt a Bi-GRU model as the path encoder to compute the path-aware mention representations:</p><formula xml:id="formula_6">? ? ? p i,j = ???? GRU ???? p i,j?1 , o (n) wj ,<label>(5)</label></formula><formula xml:id="formula_7">? ? ? p i,j = ???? GRU ???? p i,j+1 , o (n) wj , (6) m (h) i = ? ? ? p i,1 , m (t) i = ???? p i,leni ,<label>(7)</label></formula><p>where ? ? ? p i,j , ? ? ? p i,j ? R d h are the forward and backward GRU hidden states of the j-th node in the i-th path, respectively. m</p><formula xml:id="formula_8">(h) i , m (t) i ? R d h</formula><p>are the path-aware representations of the head and tail mentions in the i-th path, respectively.</p><p>Since not all the paths contain useful information, we design an attention aggregator, which takes the global contextual information as a guidance, to selectively aggregate the path-aware mention representations:</p><formula xml:id="formula_9">h = 1 |e 1 | j?e1 o (n) j , t = 1 |e 2 | j?e2 o (n) j ,<label>(8)</label></formula><formula xml:id="formula_10">? i = Softmax i W a [ h; t; m (h) i ; m (t) i ] + b a ,<label>(9)</label></formula><formula xml:id="formula_11">h = i m (h) i ? ? i , t = i m (t) i ? ? i ,<label>(10)</label></formula><p>where h, t ? R d h are the coarse-level head and tail entity representations, respectively, which are computed by averaging their corresponding coarse-level mention representations. W a and b a are trainable parameters. h, t ? R d h are the pathaware fine-level representations of the head and tail entities, respectively. Classification Module: In this module, we combine the entity representations from both two levels to obtain comprehensive representations that capture both global contextual information and long-distance entity interactions. Next, we predict the probability of each relation by a bilinear scorer:</p><formula xml:id="formula_12">P (r|e 1 , e 2 ) = Sigmoid [ h; h] T W c [ t; t] + b c r ,<label>(11)</label></formula><p>where W c ? R 2d h ?nr?2d h and b c ? R nr are trainable parameters with n r denoting the number of relation categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization Objective</head><p>Considering that an entity pair may have multiple relations, we choose the binary cross entropy loss between the ground truth label y r and P (r|e 1 , e 2 ) as the optimization objective: </p><formula xml:id="formula_13">L = ? r y r ? log P (r|e 1 , e 2 ) + (1 ? y r ) ? log 1?P (r|e 1 , e 2 ) .<label>(12</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>We tune hyper-parameters on the development set. Generally, we use AdamW <ref type="bibr">[Loshchilov and Hutter, 2017]</ref> as the optimizer, and use the DCGCN consisting of two blocks with 4 sub-layers in each block. On CDR, we use a biomedical domain pre-trained model BioBERT <ref type="bibr" target="#b3">[Lee et al., 2020]</ref> as the text encoding module. Details of hyper-parameters such as learning rate, batch size, dropout rate, hidden dimension, and others for each version of CFER are shown in Appendix A.</p><p>Following previous works, for DocRED, we choose micro F1 and Ign F1 as evaluation metrics. For CDR, we choose micro F1, Intra-F1, and Inter-F1 as metrics. Ign F1 denotes F1 excluding relational facts that appear in both the training set and the development or testing set. Intra-and Inter-F1 denote F1 of relations expressed within and across sentences, respectively. We determine relation-specific thresholds ? r based on the micro F1 on the development set. With these thresholds, we classify a triplet (e 1 , r, e 2 ) as positive if P (r|e 1 , e 2 ) &gt; ? r or negative otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main Results</head><p>To obtain the main evaluation results, we run each version of CFER three times, take the median F1 on the development set to report, and test the corresponding model on the test set. <ref type="table">Table 1</ref> shows the main evaluation results on DocRED. We compare CFER with 21 existing models on three tracks: <ref type="formula" target="#formula_2">(1)</ref>   <ref type="table">Table 1</ref>, we observe that on all three tracks, CFER achieves the best performance and significantly outperforms most of the existing models. Further, we find that graph-based models generally have significant advantages over text-based models in document-level RE.   <ref type="table" target="#tab_3">Table 2</ref> shows the main evaluation results on CDR. Besides GCNN, LSR, EoG mentioned above, we compare two more models <ref type="figure">: BRAN [Verga et al., 2018]</ref>, a text-based method, and LSR w/o MDP Nodes, a modified version of LSR. From <ref type="table" target="#tab_3">Table 2</ref> we find that CFER outperforms all the baselines, especially on Inter-F1. This suggests that CFER has stronger advantages in modeling inter-sentence interactions. Note that the modified version of LSR performs better than full LSR. This may imply that LSR does not always work on all datasets, while CFER does not have this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Analysis</head><p>Ablation study: To verify the effectiveness of each module in CFER, we show ablation experiment results of CFER-GloVe on the development set of DocRED in <ref type="table" target="#tab_5">Table 3</ref>. Firstly, to explore the effectiveness of fine-level representations, we modify our model in three ways: (1) Replace the attention aggregator by a simple mean aggregator (denoted by -Attention Aggregator).</p><p>(2) Replace all used shortest paths by a random shortest path (denoted by -Multiple Paths).</p><p>(3) Remove the whole fine-level representations from the final classification (denoted by -Fine-level Repr.). F1 scores of these three modified models decrease by 1.48, 2.72, and 4.12, respectively. This verifies that our attention aggregator for multiple paths has the ability to selectively aggregate useful information thus producing effective fine-level representations. Secondly, to explore the effectiveness of coarse-level representations, we modify our model in two ways: (1) Remove the whole coarse-level representations from the final classification (denoted by -Coarse-level Repr.). (2) Remove DCGCN blocks (denoted by -DCGCN Blocks). F1 scores of these two modified models decrease by 1.14 and 2.08, respectively. This verifies that DCGCN has the ability to capture rich local and global contextual information, thus producing highquality coarse-level representations that benefit relation extraction. Finally, we remove both coarse-and fine-level representation modules (denoted by -Both-level Modules). This operation makes our model degenerate into a simple version similar to Bi-LSTM. As a result, this simple version achieves a similar performance to the Bi-LSTM baseline as expected. This suggests that our text encoding module is not much different from the Bi-LSTM baseline, and the performance improvement is totally introduced by our coarse-to-fine strategy.</p><p>Robustness to the uneven label distribution: To analyze our performance on different relations, we divide 96 relations in DocRED into 4 categories according to their ground-truth positive label numbers in the development set. We show micro F1 on each category achieved by two baseline models and CFER in <ref type="figure" target="#fig_4">Figure 5</ref>. As shown in the figure, compared to Bi-LSTM, BERT-Two-Step makes more improvement on relations with more than 20 positive labels (denoted by major relations), but less improvement (10.15%) on long-tail relations with less than or equal to 20 positive labels. By contrast, keeping the improvement on major relations, our model makes a much more significant improvement (86.18%) on long-tail relations. With high performance on long-tail relations, our model narrows the F1 gap between major and longtail relations from 46.05 (Bi-LSTM) to 38.12. This suggests that our model is more robust to the uneven label distribution. Ability to model long-distance interactions: To analyze our performance on entity pairs with different distances, we divide all entity pairs in CDR into 3 categories according to their sentence distance, i.e., the number of sentences that separate them. Specifically, the distance of an intra-sentence entity pair is 0. <ref type="table" target="#tab_6">Table 4</ref> shows the micro F1 on entity pairs with different sentence distances. From the table, we find that as the distance increases, the performance of CFER does not decrease much, and even increases from <ref type="bibr">[4,</ref><ref type="bibr">8)</ref> to <ref type="bibr">[8, )</ref>. This validates again the ability of CFER to model long-distance interactions between entity pairs. <ref type="figure" target="#fig_3">Figure 4</ref> shows an extraction case selected from the development set. In this case, CFER-GloVe (-Both-level Modules), a simple version similar to the Bi-LSTM baseline, extracts only two simple relational facts. CFER-GloVe (-Fine-level Repr.) extracts three more relational facts since it adopts DCGCN blocks to integrate richer local and global contextual information. CFER-GloVe (-DCGCN Blocks) makes use of path information to model long-distance entity interactions, and also extracts three more relational facts compared to CFER-GloVe (-Both-level Modules). CFER-GloVe (Full) combines all the advantages of its modified versions. As a result, it extracts the most relational facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>To reveal how our coarse-to-fine strategy works, we further analyze the relational fact (0, P140, 5), which is extracted by only CFER (Full). In <ref type="figure" target="#fig_3">Figure 4</ref>, for CFER (Full) and CFER (-DCGCN Blocks), we additionally show several high-weight paths along with their attention weights used for producing fine-level representations. From the shown case, we can find that CFER (Full) gives relatively smooth weights to its highweight paths. This enables it to aggregate richer path information from several useful paths. In fact, CFER (Full) successfully aggregates both intra-sentence (within sentence 7) and inter-sentence (across sentences 5, 6, and 7) information. By contrast, without the guidance of global contextual information, CFER (-DCGCN Blocks) learns extremely unbalanced weights and pays almost all its attention to a suboptimal path. As a result, it fails to extract the P140 relation. As for CFER-GloVe (-Fine-level Repr.) and CFER-GloVe (-Both-level Module), they do not consider any path information. Therefore, it is hard for them to achieve as good   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Most of existing document-level RE models are graphbased. They usually construct a document-level graph, which represents words as nodes and uses edges between them to capture document-aware interactions. <ref type="bibr" target="#b12">Quirk and Poon [2017]</ref> design feature templates to extract features from multiple paths for classification. Sahu et al.</p><p>[2019] use a labeled edge <ref type="bibr">GCN [Marcheggiani and Titov, 2017]</ref> to integrate information. <ref type="bibr" target="#b1">Guo et al. [2019a]</ref> and <ref type="bibr" target="#b10">Nan et al. [2020]</ref> apply graph convolutional networks to a complete graph with iteratively refined edges weights. <ref type="bibr" target="#b12">Christopoulou et al. [2019]</ref> propose an edge-oriented model that represents paths through a walkbased iterative inference mechanism.  propose a reconstructor to model path dependency between an entity pair.  propose to improve inter-sentence reasoning by characterizing interactions between sentences and relation instances. <ref type="bibr" target="#b7">Li et al. [2021]</ref> leverage knowledge graphs in document-level RE.  propose a context-enhanced model to capture global context information. <ref type="bibr" target="#b16">Zeng et al. [2020]</ref> design a double graph to cope with document-level RE. Besides graph-based methods, there are also text-based methods without constructing a graph. <ref type="bibr">Verga et al. [2018]</ref> adopt a transformer <ref type="bibr">[Vaswani et al., 2017]</ref> to encode the document. <ref type="bibr" target="#b12">Wang et al. [2019]</ref> adopt BERT <ref type="bibr" target="#b0">[Devlin et al., 2019]</ref> to encode the document and predict the existence of relations before predicting the specific relations. <ref type="bibr" target="#b12">Tang et al. [2020]</ref> design a hierarchical architecture to make full use of information from several levels. <ref type="bibr" target="#b6">Ye et al. [2020]</ref> attempt to explicitly capture relations between coreferential noun phrases to coherently comprehend the whole document. <ref type="bibr" target="#b11">Qin et al. [2020]</ref> use contrastive learning to obtain a deeper understanding of entities and relations in text.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose CFER with a coarse-to-fine strategy to learn comprehensive representations for document-level RE. Our model integrates global contextual information and models long-distance interactions between the target entity pair simultaneously, thus addressing the disadvantages that existing graph-based models suffer from. Experimental results on two document-level RE datasets, DocRED and CDR, show that CFER outperforms existing models. Further, elaborate analysis verifies the effectiveness of our coarse-to-fine strategy, and highlights the robustness of CFER to the uneven label distribution and the ability of CFER to model longdistance interactions. Note that our coarse-to-fine strategy is not limited to only the task of document-level RE. It has the potential to be applied to a variety of other NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Details of Hyper-parameters</head><p>We search the best hyper-parameters based on F1 on the development set. Generally, for all of CFER-GloVe, CFER-BERTBase, CFER-RoBERTaLarge and CFER for CDR, we use AdamW with ?1 = 0.9, ?2 = 0.999, = 1e ? 6, weight decay = 0.0001 as the optimizer, apply exponential moving average on all parameters with a decay rate of 0.9999, use ReLU as the activation function, and use the DCGCN consisting of two blocks with 4 sub-layers in each block. We adopt a slanted triangular scheduling strategy for learning rate, which first linearly increases the learning rate from 0 to the peak value in the first 10% steps (warm-up steps), and then linearly decreases it to 0 in remaining steps. For other key hyperparameters, we state the values tried and the finally selected value for four models separately as follows.</p><p>For CFER-GloVe: (1) We search the peak learning rate for all modules in {1e ? 3, 1e ? 4}, and finally choose 1e ? 3.</p><p>(2) We search the batch size in {8, 16, 32}, and finally select 16. (3) We search the dropout rate for DCGCN modules in {0.2, 0.4, 0.6}, and finally select 0.4. (4) We search the dropout rate for other modules in {0.2, 0.4, 0.6}, and finally select 0.2. (5) We set the embedding dimension to 300, the same as the dimension of used GloVe embeddings. (6) We search the hidden size in {100, 300, 512}, and finally select 300. (7) For each hyper-parameter configuration, we train 300 epochs and select the best F1 achieved during these 300 epochs to evaluate the performance under this configuration.</p><p>For CFER-BERTBase: (1) We search the peak learning rate for BERT modules in {1e ? 4, 5e ? 5, 1e ? 5}, and finally select 1e ? 5.</p><p>(2) We search the peak learning rate for the other modules in {1e ? 3, 5e ? 4, 1e ? 4}, and finally select 1e ? 3. (3) We search the batch size in {8, 16, 32}, and finally select 32. (4) We search the dropout rate for DCGCN modules in {0.2, 0.4, 0.6}, and finally select 0.6. (5) We search the dropout rate for other modules in {0.2, 0.4, 0.6}, and finally select 0.2. (6) We search the hidden size in {300, 512, 768}, and finally select 512. (7) For each hyperparameter configuration, we train 300 epochs and select the best F1 achieved during these 300 epochs to evaluate the performance under this configuration.</p><p>For CFER-RoBERTaLarge: (1) We search the peak learning rate for RoBERTa modules in {1e ? 4, 5e ? 5, 1e ? 5}, and finally select 1e ? 5.</p><p>(2) We search the peak learning rate for the other modules in {1e ? 3, 5e ? 4, 1e ? 4}, and finally select 1e ? 3. (3) We search the batch size in {8, 16, 32}, and finally select 32. (4) We search the dropout rate for DCGCN modules in {0.2, 0.4, 0.6}, and finally select 0.6. (5) We search the dropout rate for other modules in {0.2, 0.4, 0.6}, and finally select 0.2. (6) We search the hidden size in {512, 768, 1024}, and finally select 1024. (7) For each hyperparameter configuration, we train 300 epochs and select the best F1 achieved during these 300 epochs to evaluate the performance under this configuration.</p><p>For CFER for CDR: (1) We search the peak learning rate for BioBERT modules in {1e ? 4, 5e ? 5, 1e ? 5}, and finally select 1e ? 5.</p><p>(2) We search the peak learning rate for the other modules in {1e ? 3, 5e ? 4, 1e ? 4}, and finally select 1e ? 4. <ref type="formula" target="#formula_3">(3)</ref> We search the batch size in {4, 8, 16}, and finally select 4. (4) We search the dropout rate for DCGCN modules in {0.2, 0.4, 0.6}, and finally select 0.6. (5) We search the dropout rate for other modules in {0.2, 0.4, 0.6}, and finally select 0.2. (6) We search the hidden size in {512, 768, 1024}, and finally select 1024. (7) For each hyperparameter configuration, we train 100 epochs and select the best F1 achieved during these 100 epochs to evaluate the performance under this configuration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>consisted of saxophonist Benjamin Bossi ? (7) The success of their (0) Romeo Void was an American new wave band ? (1) The band primarily second release, a 4-song EP, Never Say Never resulted in a distribution deal with Columbia Records ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of our proposed model, CFER. It is composed of four modules: a text encoding module, a coarse-level representation module, a fine-level representation module, and a classification module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>non-PTM track: models on this track do not use Pre-Trained Models(PTM).GCNN [Sahu et al.,  2019], EoG [Christopoulou et al., 2019, GCGCN, GEDA, LSR<ref type="bibr" target="#b10">[Nan et al., 2020]</ref>, GAIN<ref type="bibr" target="#b16">[Zeng et al., 2020]</ref>, and HeterGSAN-Recon are graph-based models that leverage GNN to encode nodes. Bi-LSTM<ref type="bibr" target="#b14">[Yao et al., 2019]</ref> andHIN [Tang et al., 2020]  are text-based models without constructing a graph. (2) BERT track: models on this track are based on BERT Base<ref type="bibr" target="#b0">[Devlin et al., 2019]</ref>. GEDA-BERT Base , HIN-BERT Base , GCGCN-BERT Base , LSR-BERT Base , HeterGSAN-Recon-BERT Base , and GAIN-BERT Base are BERT-versions of corresponding non-PTM models. BERT-Two-Step<ref type="bibr" target="#b12">[Wang et al., 2019]</ref> and MIUK<ref type="bibr" target="#b7">[Li et al., 2021]</ref> are two text-based models. CorefBERT Base<ref type="bibr" target="#b6">[Ye et al., 2020]</ref> and ERICA-BERT Base<ref type="bibr" target="#b11">[Qin et al., 2020]</ref> are two pre-trained models.(3) RoBERTa track: models on this track are based on RoBERTa<ref type="bibr" target="#b14">[Liu et al., 2019]</ref>: ERICA-RoBERTa Base<ref type="bibr" target="#b11">[Qin et al., 2020]</ref> and CorefRoBERTa Large<ref type="bibr" target="#b6">[Ye et al., 2020]</ref>. From</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>An extraction case from the development set. We show the relational facts extracted by four versions of CFER-GloVe. For CFER-GloVe (Full) and CFER-GloVe (-DCGCN Blocks), we additionally show the paths along with their attention weights used for producing fine-level representations of entity 0 (Daniel Ajayi Adeniran) and entity 5 (Redeemed Christian Church).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Micro F1 on different categories of relations. CFER makes more improvement on long-tail relations with fewer positive labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>American new wave band from San Francisco California , formed in 1979.</figDesc><table><row><cell cols="2">Romeo Void was</cell><cell cols="2">adjacent word self-loop adjacent sentence an syntactic dependency coreferential mention</cell></row><row><cell></cell><cell>(root)</cell><cell></cell></row><row><cell>The</cell><cell cols="2">band primarily consisted</cell><cell>of saxophonist Benjamin Bossi , vocalist Debora Iyall , guitarist Peter Woods , and bassist Frank Zincavage .</cell></row><row><cell></cell><cell></cell><cell>(root)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>It contains 5, 053 documents, 132, 375 entities and 56, 354 relational facts divided into 96 relation categories. Among the annotated documents, 3, 053 are for training, 1, 000 are for development, and 1, 000 are for testing. Chemical-Disease Reactions (CDR) [Li et al., 2016] is a popular dataset in the biomedical domain containing 500 training examples, 500 development examples, and 500 testing examples.</figDesc><table><row><cell>)</cell></row><row><cell>3 Experiments and Analysis</cell></row><row><cell>3.1 Dataset</cell></row></table><note>We evaluate our model on two document-level RE datasets. DocRED [Yao et al., 2019] is a large-scale human-annotated dataset constructed from Wikipedia and Wikidata [Vrande- cic and Kr?tzsch, 2014], and is currently the largest human- annotated dataset for general domain document-level RE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main evaluation results on the biomedical dataset CDR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments on the development set of DocRED.</figDesc><table><row><cell>Distance</cell><cell>[0, 4)</cell><cell>[4, 8)</cell><cell>[8, )</cell></row><row><cell>Micro F1</cell><cell>66.3</cell><cell>59.8</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Micro F1 on entity pairs with different sentence distances.</figDesc><table /><note>performance as CFER (Full).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://spacy.io/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ajayi Adeniran experienced problems with alcohol , and in 1989 visited the Redeemed Christian Church of God across the street from his home near Ibadan . (3) In 1990 , he converted to the doctrines of the Redeemed Christian Church and was ordained through that denomination 1994 . (4) He moved to the United States in 1995 because of the political conditions under the dictator Sani Abacha in Nigeria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Ajayi Adeniran ; Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<meeting><address><addrLine>Nigeria; Makoto Miwa, and Sophia Ananiadou; Kenton Lee</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>NAACL-HLT 2019</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biocreative V CDR task corpus: a resource for chemical disease relation extraction</title>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
	</analytic>
	<monogr>
		<title level="m">AAAI 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph enhanced dual attention network for document-level relation extraction</title>
	</analytic>
	<monogr>
		<title level="m">COLING 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view inference for relation extraction with uncertain knowledge</title>
	</analytic>
	<monogr>
		<title level="m">AAAI 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="1907" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach. CoRR, abs/</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
	<note>Encoding sentences with graph convolutional networks for semantic role labeling</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Glove: Global vectors for word representation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ERICA: improving entity and relation understanding for pre-trained language models via contrastive learning</title>
		<idno>abs/2012.15022</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Sahu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<editor>Hong Wang, Christfried Focke, Rob Sylvester, Nilesh Mishra, and William Wang</editor>
		<imprint>
			<publisher>Vrandecic and Kr?tzsch</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
	<note>Communications of the ACM. Fine-tune bert for docred with two-step process. CoRR, abs/1909.11898</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Document-level relation extraction with reconstruction</title>
		<idno>abs/2012.11384</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Docred: A large-scale document-level relation extraction dataset</title>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<editor>Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Maosong Sun, and Zhiyuan Liu</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
	<note>Ye et al., 2020. Coreferential reasoning learning for language representation. CoRR, abs/2004.06870, 2020</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
	</analytic>
	<monogr>
		<title level="m">COLING 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for document-level relation extraction</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global context-enhanced graph convolutional networks for document-level relation extraction</title>
	</analytic>
	<monogr>
		<title level="m">COLING 2020</title>
		<editor>Zhou et al., 2020] Huiwei Zhou, Yibin Xu, Weihong Yao, Zhe Liu, Chengkun Lang, and Haibin Jiang</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
	<note>ACL 2016</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
