<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Generation of Continuous Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
							<email>iskorokhodov@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savva</forename><surname>Ignatyev</surname></persName>
							<email>savvaignatiev@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
							<email>mohamed.elhoseiny@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Generation of Continuous Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) -an MLP that predicts an RGB pixel value given its (x, y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex realworld data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In deep learning, images are typically represented as 2D arrays of pixels. However, there is another paradigm which views an image as a continuous function F (p) = v that maps a pixel's 2D coordinate p = (x, y) ? R 2 to its RGB value v = (r, g, b) ? R 3 . The advantage of such a representation is that it gives a true continuous version of the underlying 2D signal instead of its cropped quantized counterpart like pixel-based representations do. In practice, we almost never know the underlying function F (p) and thus have to work with its approximations. The most popular and expressive way to approximate F (p) is through a neural network F ? <ref type="bibr" target="#b82">[80,</ref><ref type="bibr" target="#b75">73]</ref>. It is called an implicit neural representation (INR) and is especially popular in 3D deep learning where working with voxels directly (i.e. pixels defined on a 3D grid) is too expensive <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b54">54]</ref>.</p><p>Building such a decoder has two severe difficulties: 1) since it is a hypernetwork, i.e. a network that produces parameters for another network <ref type="bibr" target="#b26">[26]</ref>, it is unstable to train and requires too many parameters in general <ref type="bibr" target="#b9">[9]</ref>; and 2) it is too costly to evaluate INRs for a dense high-resolution coordinates grid limiting their application to low-resolution images only. To alleviate these issues, we design two principled architectural techniques: factorized multiplicative modulation (FMM) layer for hypernetworks and multiscale INRs. We use these techniques to build INR-GAN: a state-of-the-art continuous image generator that generates pictures in their INR representations. Previous attempts to build such a model were only conducted on small MNIST-like datasets <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">50]</ref> and do not scale to complex real-world data. In our case, we managed to achieve FID <ref type="bibr" target="#b28">[28]</ref> scores of 5.09, 4.96 and 16.32 on LSUN Churches 256 2 , LSUN Bedrooms 256 2 and FFHQ 1024 2 , respectively, greatly reducing the gap between continuous image GANs and their pixel-based analogs. In their contemporary work, <ref type="bibr" target="#b0">[1]</ref> achieved even better results by employing a large-scale INR-based decoder with learnable coordinate embeddings.</p><p>In our paper, we also shed light on many interesting properties of the INR-based decoders:</p><p>? Extrapolating outside of image boundaries (see <ref type="figure">Fig. 2</ref>): an ability to generate a "zoomed-out" version of an image without being trained explicitly to do this.</p><p>? Geometric prior (see <ref type="figure">Fig. 4</ref>): better encoding of geometric properties of a dataset in the latent space.</p><p>? Accelerated low-resolution inference (see <ref type="figure">Fig. 9</ref>): an ability to generate a low-resolution version of an image in shorter time than an image of the corresponding training-time resolution.</p><p>? Meaningful image space interpolation (see <ref type="figure" target="#fig_3">Fig. 5</ref>).</p><p>? Out-of-the-box superresolution (see <ref type="figure">Fig. 3</ref>): an ability to produce a higher-resolution version of an image without being trained for this task at all.</p><p>We emphasize that these features come naturally to INRbased decoders and do not require any additional training. To summarize, our contributions are the following:</p><p>1. We propose a novel factorized multiplicative modulation (FMM) layer for hypernetworks. It makes it possible to generate INRs with a large number of parameters and stabilizes hypernetwork training.</p><p>2. We propose a novel multi-scale INR architecture. It makes it possible to represent high-resolution images in the INR-based form in a very efficient way.</p><p>3. Using the above two techniques we build INR-GAN: an INR-based GAN model that outperforms existing continuous image generators by several times on large real-world datasets. <ref type="bibr" target="#b3">4</ref>. We explore several exciting properties of INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and geometric prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Implicit Neural Representations. The original idea of augmenting neural networks with coordinates information <ref type="figure">Figure 2</ref>: Extrapolating outside of image boundaries. After training our INR-based GAN on LSUN 256 2 , we tried to evaluate it on a wider grid. During training, we used coordinates from [0, 1] 2 square, and here we evaluate it on coordinates from [?0.3, 1.3] 2 square. To our surprise, the model can produce meaningful extrapolation and generate the picture beyond the coordinates area it was trained on. Blue bounding box denotes [0, 1] 2 coordinates area -an image area the model was trained on. Very similar results were previously shown by <ref type="bibr" target="#b48">[48]</ref>.</p><p>was proposed in CPPN <ref type="bibr" target="#b79">[77]</ref> which is a neuroevolutionbased model that is trained to represent a 2D image. After that, there were several works that use coordinates as an additional source of information to neural networks <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b88">86,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b69">67,</ref><ref type="bibr" target="#b78">76]</ref>, but the largest popularity of implicit neural representations (INRs) is observed in 3D deep learning, where it provides a cheap and continuous way to represent a 3D shape compared to mesh/voxel/pointcloud-based ones <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b76">74]</ref>. They have also been used for other tasks, like representing textures <ref type="bibr" target="#b64">[62]</ref>, 3D shapes flow <ref type="bibr" target="#b59">[59]</ref>, scenes <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b77">75]</ref>, audios and differential equations <ref type="bibr" target="#b75">[73]</ref>, human grasps <ref type="bibr" target="#b40">[40]</ref> and other information <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b13">13]</ref>. Occupancy Networks <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b70">68,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b35">35</ref>] model a probability function of a voxel being occupied by a 3D shape and typically employ a coordinate-based decoder that operates on top of single-view images. They use the multi-resolution surface extraction method, which is similar in nature to our proposed multi-scale INR. However, in our case, we share computation between neighboring pixels while they use surface extraction to find regions to refine the predictions on. In this way, they conduct the full inference for each low-resolution coordinate which is the opposite of what we try to achieve with multi-scale INRs. DeepSDF <ref type="bibr" target="#b68">[66]</ref> models a signed distance function instead of the occupancy function and they additionally have an encoder, which transforms an image into a latent code. IM-NET <ref type="bibr" target="#b11">[11]</ref> proposes to train a generative model on top of these latent codes and conduct experiments not only on ShapeNet objects <ref type="bibr" target="#b8">[8]</ref>, but on MNIST images as well. DeepMeta <ref type="bibr" target="#b50">[50]</ref> models the occupancy function and predicts decoder parameters instead of the latent codes. A vital branch of research on INRs is concerned about the most efficient way to encode coordinates positions <ref type="bibr" target="#b85">[83,</ref><ref type="bibr" target="#b19">19]</ref>. Recent works show that using Fourier features greatly improves INR expressiveness <ref type="bibr" target="#b82">[80,</ref><ref type="bibr" target="#b75">73]</ref>, which we observe in our experiments as well.</p><p>GANs. held by generative adversarial networks (GANs) <ref type="bibr" target="#b23">[23]</ref>. Two key challenges in GAN training are instability and mode collapse, that is why a big part of research in recent years was devoted to finding stronger objective formulation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b47">47]</ref>, regularizers <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b57">57]</ref> and architectural designs <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b36">36]</ref> that would encourage stability, diversity and expressivity of the GAN-based models. Generative models + coordinates. There were attempts to combine generative modeling and INR-based representations prior to our work. IM-NET <ref type="bibr" target="#b11">[11]</ref> trains a generative model on top of latent codes of an occupancy autoencoder. In our case, instead of feeding a latent code to a coordinate-based decoder, we produce its parameters with a hypernetwork-based generator. Besides, their image generation experiments were limited to small-scale MNISTlike datasets <ref type="bibr" target="#b45">[45]</ref> only. CoordConv GAN <ref type="bibr" target="#b51">[51]</ref> concatenates coordinates to each representation in the DCGAN model, which endows it with geometric translating behavior during latent space interpolation. <ref type="bibr" target="#b92">[90]</ref> use CoordConv layers to perform superresolution. SBD <ref type="bibr" target="#b89">[87]</ref> augments a VAE <ref type="bibr" target="#b42">[42]</ref> decoder with coordinates information. SpatialVAE <ref type="bibr" target="#b2">[3]</ref> additionally models rotation and translation separately from the latent codes COCO-GAN <ref type="bibr" target="#b48">[48]</ref> generates images by patches given their spatial information and then assembles them into a single image. <ref type="bibr" target="#b94">[92]</ref> uses coordinates-based convolutions for sky replacement and video harmonization. In their con-  <ref type="figure">Figure 4</ref>: Predicting keypoints from latent codes. We train a linear model to predict face keypoints directly from the corresponding latent codes. Its performance shows how much geometric information is contained in a latent code and how accessible it is. For this benchmark, our model easily outperforms StyleGAN2 despite it being trained with the additional PPL loss <ref type="bibr" target="#b39">[39]</ref> that forces better latent codes conditioning in the decoder. That demonstrates better geometric prior of our model. The corresponding scores are presented in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>temporary work, <ref type="bibr" target="#b0">[1]</ref> builds an INR-based generator with learnable coordinate embeddings and achieves state-of-theart results on several large-scale datasets.</p><p>Hypernetworks. Hypernetworks or meta-models are models that generate parameters for other models <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b49">49]</ref>. Such parametrization provides higher expressivity <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> and compression due to weight sharing through a meta-model <ref type="bibr" target="#b26">[26]</ref>. Our factorized multiplicative modulation (FMM) is closely related to the squeeze-and-excitation mechanism <ref type="bibr" target="#b30">[30]</ref>. But in contrast to it, we modulate weights instead of hidden representations, similar to <ref type="bibr" target="#b10">[10]</ref>, where authors condition kernel weights on an input via attention. Hypernetworks are known to be unstable to train <ref type="bibr" target="#b84">[82]</ref> and <ref type="bibr" target="#b9">[9]</ref> proposed a principled initialization scheme to remedy the issue. In our case, we found it to be unnecessary since our FMM module successfully reduces the influence of hypernetwork initialization on signal propagation inside an INR. Hypernetworks found many applications in other areas like few-shot learning <ref type="bibr" target="#b3">[4]</ref>, continual learning <ref type="bibr" target="#b86">[84]</ref>, architecture search <ref type="bibr" target="#b93">[91]</ref> and others. In the case of generative modeling, <ref type="bibr" target="#b81">[79]</ref> built a character-level language model, and <ref type="bibr" target="#b72">[70]</ref> proposed a generative hypernetwork to produce parameters of neural classifiers. Similar to our FMM, <ref type="bibr" target="#b80">[78]</ref> proposed a low-rank modulation by parametrizing a target model's weight matrix as W = W s (A ? B), where W s is a shared component and A, B are rectangular matrices produced by a hypernetwork.</p><p>Hypernetworks + generative modeling. Combining hypernetworks and generative models is not new. In <ref type="bibr" target="#b72">[70]</ref> and <ref type="bibr" target="#b27">[27]</ref>, authors built a GAN model to generate parameters of a neural network that solves a regression or clas-sification task and demonstrate its favorable performance for uncertainty estimation. HyperVAE <ref type="bibr" target="#b58">[58]</ref> is designated to encode any target distribution by producing generative model parameters given distribution samples. HCNAF <ref type="bibr" target="#b65">[63]</ref> is a hypernetwork that produces parameters for a conditional autoregressive flow model <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b31">31]</ref>. <ref type="bibr" target="#b7">[7]</ref> trains a generator to produce a 3D object in the INR form and uses mFiLM <ref type="bibr" target="#b71">[69,</ref><ref type="bibr" target="#b14">14]</ref> to compress the output space.</p><p>Hypernetworks + INRs. There are also works that combine hypernetworks and INRs. <ref type="bibr" target="#b77">[75]</ref> represents a 3D scene as an INR, rendered by differentiable ray-marching, and trains a hypernetwork to learn the space of such 3D scenes. <ref type="bibr" target="#b44">[44]</ref> proposes to represent an image dataset using a hypernetwork and perform super-resolution by passing denser coordinate grid into it. DeepMeta <ref type="bibr" target="#b50">[50]</ref> builds an encoder that takes a single-view 3D shape image as input and outputs parameters of an INR. Authors also trained a model to encode a MNIST image into a temporal sequence of digits.</p><p>Computationally efficient models. Among other things, we demonstrate that our INR-based decoder enjoys faster inference speed compared to classical convolutional ones. Each INR layer can be seen as a 1 ? 1 convolution <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b74">72]</ref>, i.e. a convolution with the kernel size of 1. The core difference is that layer's weights are produced with a hypernetwork G. Since INR may have a lot of parameters, we factorize them with low-rank factorization <ref type="bibr" target="#b91">[89]</ref>. There is a vast literature on using low-rank matrix approximations to compress deep models or accelerate them <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b15">15]</ref>. In our case, we found it sufficient just to output a weight matrix as a product of two low-rank matrices W = A ? B without using any specialized techniques. SENet <ref type="bibr" target="#b30">[30]</ref> proposed to apply squeeze-and-excitation mechanism on the hidden representations, and we apply them on the INR weights to make the model more stable to train and faster to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image Meta Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model overview</head><p>We adopt the standard GAN training setup and replace a convolutional generator with our INR-based one, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We build upon the StyleGAN2 framework and keep every other component except for the generator untouched, including the discriminator, losses, optimizers, and the hyperparameters. The details are in Appendix A.</p><p>Our generator G is hypernetwork-based: it takes latent code z ? N (0, I) as input and generates parameters ? for an INR model F ? . To produce an actual image, we evaluate F ? at all the locations of a predefined coordinates grid, which size is determined by the dataset resolution. For example, for LSUN bedroom 256 2 we compute pixel values at 256 2 = 65536 grid locations of [0, 1] 2 square, that are positioned uniformly. We use recently proposed Fourier fea-  tures to embed the (x, y) coordinates <ref type="bibr" target="#b82">[80,</ref><ref type="bibr" target="#b75">73]</ref>. From the implementation perspective, these Fourier features is just a simple linear layer u = sin(?U p) with sine (or cosine) activation that maps a raw coordinate vector p = (x, y) into a feature vector u ? R d f . Note that our embedding matrix U is not kept fixed and shared but predicted by the generator from z. This gives the model flexibility to select feature frequencies that are the most appropriate for a given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Factorized Multiplicative Modulation (FMM)</head><p>A hypernetwork is a model that generates parameters for another model. In our case, we want to generate INR's parameters given the noise vector z. Imagine, that we need to produce the weights W ? R nin?nout , b ? R nout of the -th linear layer of F ? . A naive implementation would output them directly, but this is extremely inefficient: if the hypernetwork has the hidden dimensionality of size h, then its output projection matrix will have the size h ? (n in ? n out + n out ). Even for small n in , n out this is prohibitively expensive.</p><p>The main problem lies in generating W since it contains most of the weights, thus factorizing it via low-rank matrix decomposition W = A ? B might seem like a reasonable idea. However, our preliminary experiments showed that it severely decreases the performance because having low-rank weight matrices is equivalent to having a lot of zero singular values, leading to severe instabili-   <ref type="figure">Figure 6</ref>: (a) FMM linear layer for inputs x, shared matrix W s and output y; (b) INR-based generator with the FMM mechanism: its parameters are split into ? s (shared) and ? (predicted by the hypernetwork). This mechanism makes our architecture be similar to StyleGAN2 <ref type="bibr" target="#b39">[39]</ref>: the hypernetwork becomes a mapping network and the INR F ? becomes the synthesis network (decoder).</p><p>ties in GAN training <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b57">57]</ref>. That is why we change the parametrization so that the full rank is preserved while the hypernetwork output is still factorized. Namely, we first define a shared parameters matrix W s ? R n out ?n in , which is learnable and shared across all samples. Our hypernetwork produces two rectangular matrices A ? R n out ?r and B ? R r?n in , which are multiplied together to obtain a low-rank modulating matrix W h = A ? B . We compute the final weight matrix W as W = W s ?(W h ) where ? is sigmoid function. Bias vector b is produced directly since it is small. In all the experiments, we set r = 10 for all the layers of F ? except the first and the last ones which are small enough to be generated directly. We illustrate FMM in <ref type="figure">Figure 6</ref>.</p><p>The above modulation is very similar to the one proposed in <ref type="bibr" target="#b80">[78]</ref> with the difference that <ref type="bibr" target="#b80">[78]</ref> does not use the sigmoid activation. However, as our experiments in Section 4 demonstrate, using the activation is crucial since it bounds the activations and makes the training more stable. Note that the FMM-based INR-GAN becomes very close architecturally to StyleGAN2 <ref type="bibr" target="#b39">[39]</ref>, which uses a mapping network to predict the style vector used to modulate the convolutional weights of its decoder via multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-scale INRs</head><p>Scaling a traditional INR-based decoder to large image resolutions is too expensive: to produce a 1024 2 image, we have to input ?10 6 coordinates into F ? . Such a huge batch size makes it impossible to use large hidden layers' sizes due to excessive computation and memory consumption. To circumvent the issue, we propose a multi-scale INR architecture: we split F ? into K blocks, where each block operates on its own resolution and only the final block operates on the target one. Earlier blocks compute low-res features that are then replicated and passed to the next level. This process is illustrated on <ref type="figure" target="#fig_6">Fig. 7</ref> and is equivalent to using different grid sizes depending on the resolution and using the nearest neighbor interpolation for the hidden representations. For the multi-scale INRs, we use more neurons for lower resolutions and fewer ones for the more expensive high-resolution blocks. The use of multi-scale INRs makes our architecture very similar to classical convolutional decoders, which grow the resolution progressively with depth <ref type="bibr" target="#b63">[61]</ref>. In our case, one of the additional benefits of having the multi-scale architecture is that neighboring pixels get conditioned on the common context computed at a previous resolution.</p><p>We start with the resolution of 64 2 for the first block and increase it by 2 for each next one until we reach the target resolution. Each block contains 2-4 layers, and Fourier coordinates features are concatenated to a hidden representation at the beginning of each block. Replicating lowresolution features for each high-dimensional pixel is equivalent to upsampling the inner grid with nearest neighbors interpolation, and this is the way we implement it in practice. Further implementation details can be found in the supplementary material or the attached source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Standard GAN training</head><p>Datasets. We conduct experiments on four datasets: LSUN Bedrooms 128 2 , LSUN Bedrooms 256 2 , LSUN Churches 256 2 and FFHQ 1024 2 . LSUN Bedrooms and LSUN Churches consist on 3M and 125k images of in-thewild bedrooms and churches <ref type="bibr" target="#b90">[88]</ref>, respectively. FFHQ is a high-resolution dataset of 70k human faces <ref type="bibr" target="#b38">[38]</ref>. For all the datasets, we apply random horizontal flip for data augmentation.</p><p>Evaluation metrics. We evaluate the model using Frechet Inception Distance (FID) <ref type="bibr" target="#b28">[28]</ref> metric using 50k im- <ref type="table">Table 1</ref>: We start with the INR-based decoder conditioned on the latent code <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b2">3]</ref> and progressively improve it. O/M denotes "out-of-memory" error: we couldn't train the model even for a batch size of 1 on a 32GB NVidia V100 GPU.  Each block operates on a different resolution, determined by the granularity of an input grid. We increase the granularity with depth: this allows to share computation between neighbouring pixels and condition them on a common context. We depict the multi-scale mechanism without FMM not to clutter the illustration. In practice, we use both FMM and the multi-scale architecture for our INR-GAN. ages to compute the statistics. We also compute the number of parameters for a given model and the amount of multiplyaccumulate operations (MACs) -a standard measure for accessing the model's computational efficiency <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>Models and training details. All our models have equivalent training settings and differ only in the generator architecture. We use two existing architectures as our baseline:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A non-hypernetwork-based generator, which has</head><p>shared parameters for all INRs and each INR is conditioned on latent code w = G(z) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b68">66]</ref>. I.e., instead of producing parameters for F ? , we pass w into it as an additional input v = F ? (x, y, w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A hypernetwork-based generator, which produces parameters ? for F ? but does not use any factorization techniques to reduce the output matrix size <ref type="bibr" target="#b50">[50]</ref>.</p><p>We build upon the above baselines by incorporating Fourier positional embeddings of the coordinates <ref type="bibr" target="#b82">[80,</ref><ref type="bibr" target="#b75">73]</ref>, incorporating our FMM layer and incorporating our multi-scale INR architecture. In all the experiments, G is a 4-layer MLP with residual connections that takes z ? R 512 as input and produces INR parameters ? as output. For the first baseline, it produces the transformed latent codes instead. For hypernetwork-based models, we additionally apply a learned linear layer to w to obtained INR parameters ?.</p><p>In all the experiments, a ResNet-based discriminator from StyleGAN2 <ref type="bibr" target="#b39">[39]</ref> is used. However, since beating the scores is not the goal of the paper, we used its "small" version (corresponding to config-e). R 1 -regularization <ref type="bibr" target="#b53">[53]</ref> with the penalty weight of 10 is used. All the models are trained for 800k iterations on 4 NVidia V100 32GB GPUs.</p><p>Results. The results are reported in <ref type="table">Table 1</ref>. As we can see, the latent code conditioned baseline cannot fit training data at all since it lacks expressivity and cannot capture the whole image representation in the latent code. Its hypernetworks-based counterpart achieves much higher performance but is still not competitive. An attempt to fit the baselines on FFHQ 1024 2 resulted in out-of-memory errors even for a batch size of 1 for a 32GB GPU. The nonfactorized hypernetwork-based decoder <ref type="bibr" target="#b50">[50]</ref> was too expensive even for 256 2 resolution.</p><p>Our INR-based decoder with FMM layer and multiscale INR architecture achieves competitive FID scores and greatly reduces the gap between continuous and pixel-based image generation. It is still inferior to StyleGAN2 <ref type="bibr" target="#b39">[39]</ref> in terms of performance, but is three times more efficient. One should also note that we didn't use any of the numerous training tricks employed by StyleGAN2 and that our discriminator architecture is smaller (it corresponds to config-e <ref type="bibr" target="#b39">[39]</ref>). Besides, as we show in Section 4.2, our model naturally gives rise to a lot of additional properties that convolutional decoders lack. Our model also has three times more parameters than its convolution-based counterpart. The reason for it is the huge size of the output projection matrix of G, which has the dimensionality of ? 10 3 ? 10 5 occupying 90-99% of parameters of the entire model. Compressing this matrix is an ongoing hypernetworks research topic <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b15">15]</ref> and we leave this for future work. We emphasize that despite its enormous size, the output layer's inference time is almost negligible due to highly optimized matrixmatrix multiplications on modern GPUs. Increasing the rank beyond 1 gives a clear advantage, but it diminishes for the subsequent rank increase. "kimg" denotes the number of images seen by D. Ablating FMM rank. In all the experiments, we use an FMM rank of 10. We ablate over its importance for LSUN bedroom 256 2 and report the resulting convergence plots on <ref type="figure" target="#fig_7">Fig. 8</ref>. As one can see, the model benefits from the increased rank, but the advantage is diminishing for further rank increase. These results show that vanilla or "full-rank" hypernetworks are heavily overparameterized and there is no need to predict each parameter separately. On the other that is the evidence that there is much potential for "going further" than squeeze-and-excitation <ref type="bibr" target="#b30">[30]</ref> and AdaIN <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b38">38]</ref> approaches, i.e. predicting more than a single modulating value for each neuron.</p><p>Additional ablations. All the above experiments were conducted on top of a vanilla INR-based decoder which uses nearest neighbour interpolation (to make neighboring pixels be computed independently as the vanilla INR does). It also lacks important StyleGAN2's techniques which improve the performance, like equalized learning rate <ref type="bibr" target="#b37">[37]</ref>, style mixing, pixel normalization <ref type="bibr" target="#b37">[37]</ref> and noise injection <ref type="bibr" target="#b38">[38]</ref>. In all our experiments, we also used a small-size version of StyleGAN2's discriminator to make the training run faster. Incorporating the above design choices and giving up the nearest neighbour upsampling for the bilinear one (which might be reasonable for some applications) significantly improves our generator's performance. We conduct those experiments on LSUN Churches 256 2 and LSUN Bedrooms 256 2 and report the results in <ref type="table" target="#tab_2">Table 2</ref>. For the INR-based decoder with bilinear upsampling, we even surpass StyleGAN2's performance on Churches.</p><p>Another critical question is how important the activation in FMM is. As discussed in Section 3.2, it allows to stabilize training by bounding the magnitudes of the weights. We ablate its influence by replacing ? with the identity mapping and report the resulted FID in <ref type="table" target="#tab_2">Table 2</ref>. The drop in performance demonstrates that it carries a crucial influence on the image quality.</p><p>Performance on multi-class datasets. To test if the proposed architectural techniques improve the performance on diverse multi-class datasets, we also conduct experiments on LSUN-10 256 2 and MiniImageNet 128 2 . We provide the details of these experiments in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploring the properties</head><p>Extrapolating outside of image boundaries. At test time, we sample pixels beyond the coordinates grid that the model was trained on and present the results on <ref type="figure">Fig. 2</ref>. It shows that an INR-based decoder is capable of generating meaningful content outside of the grid boundaries, which is equivalent to a zooming-out operation. It is a surprising quality indicating that the coordinates features produced by the generator are exploited by an INR in a generalizable manner instead of just being simply memorized.</p><p>Meaningful interpolation. Image space interpolation is known for its poor behavior <ref type="bibr" target="#b22">[22]</ref>. However, when images are represented in the INR-based form and not the pixelbased one, the interpolation becomes reasonable. We illustrate the difference on <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>Keypoints prediction. Direct access to coordinates provides more fine-grained control over the geometrical properties of images during the generation process. Thus one can expect the INR-based decoder to better embed the geometric structure of an image into the latent space <ref type="bibr" target="#b51">[51]</ref>. To test this hypothesis, we perform the following experiment. We take 10k samples from our model trained on FFHQ 256 2 and extract face keypoints with a pre-trained model <ref type="bibr" target="#b6">[6]</ref>. After that, we fit a simple linear regression model to predict these keypoints coordinates given the corresponding latent code. We compute this model's test loss on the real-world images from FFHQ 256 2 and coin this metric Keypoints Prediction Loss (KPL). The above procedure is formally described in Algorithm 1 in Appendix B. The corresponding latent codes for the real-world images are obtained by projecting an image into the corresponding latent space through gradient descent optimization. We use the standard protocol from <ref type="bibr" target="#b39">[39]</ref> to do this for both models. KPL is computed for both Z-space and W-space. For our model, this corre- sponds to G input noise vectors z and penultimate hidden representations (i.e. hidden representations before the output projection into INR parameter space). For StyleGAN2, this corresponds to the mapping network's input and output space. We also compute KPL on top of random vectors to check that a better performance in keypoints prediction is not due to their reduced variability. The results of this experiment are provided in <ref type="table" target="#tab_3">Table 3</ref>. Accelerated inference of lower-resolution images. Our INR-GAN has a natural capability of generating a lowresolution sample faster because it can be directly evaluated at a low-resolution coordinates grid without performing the full generation. However, traditional convolutional decoders lack this property: to produce a lower-resolution image, one would need to perform the full inference and then downsample the resulted image with standard interpolation techniques. To state the claim rigorously, we compute an amount of multiply-accumulate (MAC) operations for our INR-based generator and StyleGAN2 generator trained on FFHQ 1024 2 for different lower-resolution image sizes. To produce the low-resolution image with StyleGAN2, we first produce the full-resolution image and them downsample it with nearest neighbour interpolation. For our model, we just evaluate it on a grid of the given resolution. The results are reported on <ref type="figure">Fig. 9</ref>. To the best of our knowledge, our work is the first one that explores the accelerated generation of lower-resolution images -an analog of earlyexit strategies <ref type="bibr" target="#b83">[81]</ref> for classifier models for image generation task.</p><p>Out-of-the-box superresolution. Our INR-based decoder is able to produce images of higher resolution than it was trained on. For this, we evaluate our model on a denser coordinates grid. To measure this quantitatively, we propose UpsampledFID: a variant of FID score where fake data statistics are computed based on upsampled lowresolution images. We compare our UpsampledFID score to three standard upsampling techniques: nearest neighbor, bilinear, and bicubic interpolation. <ref type="figure">Fig. 3</ref> demonstrates that INR-upsampling improves the score by up to 50%.  <ref type="figure">Figure 9</ref>: Accelerated low-resolution image generation. We measure a decoder's efficiency in terms of #MACs on generating an image of lower resolution compared to what it has been trained on. Since INR can do this by evaluating on a sparser grid, this allows it to save a lot of computation. Traditional convolutional decoders require performing a full inference first and then downsampling the produced image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional potential of INR-based decoders</head><p>we highlight their additional potential and leave its exploration for future work.</p><p>An ability to backpropagate through pixels positions. Since coordinate positions (x, y) are transformed via welldifferentiable operations, it gives a possibility to backpropagate through coordinate positions. This opens a large range of possible work like using spatial transformer layers <ref type="bibr" target="#b34">[34]</ref> in the generator and not only discriminator. Or producing a coordinates grid with the discriminator for zooming-in into specific parts of an image.</p><p>Faster inference speed. Since INRs do not use local context information during inference, it does not spend computation on aggregating it as convolutions do. This is equivalent to using convolutions with a kernel size of 1 and thus works much faster <ref type="bibr" target="#b29">[29]</ref>. <ref type="table">Table 1</ref> demonstrates that our model uses three times fewer MACs for its inference process, which is due to ignoring context information.</p><p>Parallel pixel computation Non-autoregressive models like Parallel WaveNet <ref type="bibr" target="#b66">[64,</ref><ref type="bibr" target="#b24">24]</ref> are valued by their ability to generate an arbitrarily long sequence in parallel, so their inference speed decreases linearly with more compute being added. INR-based decoders also have this property due to their independent pixel generation nature. Convolutional decoders, in contrast, are forced to use local context during the inference process, which limits their parallel inference.</p><p>Universal decoder architecture One can employ the same decoder architecture for training a decoder model in any domain: 2D images, 3D shapes, video, audio, etc. They would only differ at what coordinates are being passed as input to the corresponding INR. For 2D images, these would be (x, y)R 2 coordinates, for 2D video, this would be (x, y, t)R 2 ? R + with the additional timestep t ? R + input, for 3D shapes -(x, y, z) ? R 3 coordinates, etc. That has already been partially explored in <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b75">73]</ref>.</p><p>Biological plausbility. While convolutional encoders have a very intimate connection to how a human eye works <ref type="bibr" target="#b46">[46]</ref>, convolutional decoders do not have much resemblance to any human brain mechanism. In contrast, <ref type="bibr" target="#b73">[71]</ref> argue that hypernetworks have a very close relation to how a prefrontal cortex influences other brain parts. It does so by modulating the activity in several different areas at once, precisely how our hypernetwork-based generator applies modulation to different INR layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>The core limitation of INR-based decoders comes from their strengths: not using spatially common context between neighboring pixels. Multi-Scale INR architecture partially alleviates this issue by grounding them on the same low-resolution representation, but as our experiment with adding bilinear interpolation demonstrates (see <ref type="table" target="#tab_2">Table 2</ref>), it is not enough. Also, we noticed that the INR-based decoders might become too sensitive to high-frequency coordinates features. This may produce "texture" artifacts: a generated image having a random transparent texture spanned on it which becomes more noticeable for higher resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper explored the adversarial generation of continuous images represented in the implicit neural representations (INRs) form. We proposed two principled architectural techniques: factorized multiplicative modulation and multi-scale INRs, which allowed us to obtain solid state-ofthe-art results in continuous image generation. We explored several attractive properties of INR-based decoders and discussed their future potential and limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional implementation details</head><p>We build on top of the StyleGAN2 framework <ref type="bibr" target="#b39">[39]</ref> and change only its generator. All other settings, including the discriminator architecture D, optimizers, losses, training settings and other hyperparameters are kept untouched. This means that we use non-saturating logistic loss for training <ref type="bibr" target="#b23">[23]</ref> and use Adam optimizers with the parameters ? 1 = 0.0, ? 2 = 0.98 and = 1e ? 8. Our D is a small version of StyleGAN2 discriminator (i.e. config-e from the paper <ref type="bibr" target="#b39">[39]</ref>), regularized with zero-centered R 1 gradient penalty <ref type="bibr" target="#b53">[53]</ref> with weight ? = 10. We apply the regularization on each iteration instead of using the lazy setup, as done by StyleGAN2 <ref type="bibr" target="#b39">[39]</ref> who applies it only each 16-th iteration. We use the learning rate of 0.00001 for G, 0.0005 for the shared parameters of an INR (which is a part of G) and 0.003 for D. We also employ skip-connections for coordinates inside each multi-scale INR block <ref type="bibr" target="#b36">[36]</ref>. We apply them by concatenating the coordinates to inner representations.</p><p>For the main experiments, we didn't employ any ProGAN <ref type="bibr" target="#b37">[37]</ref>, StyleGAN <ref type="bibr" target="#b38">[38]</ref> or StyleGAN2 <ref type="bibr" target="#b39">[39]</ref> training tricks, like path regularization, progressive growing, equalized learning rate, noise injection, pixel normalization, style mixing, etc. For our additional ablations, we reimplemented our INR-based decoder on top of the StyleGAN2's generator and employed style mixing, equalized learning rate and pixel normalization for it. In terms of implementation, it was equivalent to replacing StyleGAN2's weight modulation-demodulation with our FMM mechanism, reducing kernel size from 3 to 1, concatenating coordinates information at each block and replacing its upfirdn2d upsampling with the nearest neighbour one. We found that for the nearest neighbour upsampling, the model learns to ignore spatial noise injection (by setting noise strengths to zero), because pixels cannot communicate the noise information between each other, making it meaningless and harmful to the generation process. Our G architecture is the same for all the experiments and consists on 3 non-linear layers with the hidden dimension of 1024 and residual connections. Our noise vector z has the dimensionality of 512.</p><p>For additional implementation details, we refer a reader to the accompanying source code. In all the experiments, we compute FID scores based on 50k images using the tensorflow script from BigGAN pytorch repo 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments details B.1. Zooming out</head><p>On <ref type="figure" target="#fig_0">Fig. 10</ref> we present additional examples of our zooming operation, but evaluating the model on [?1.5, 1.5] 2 grid instead of [?0.3, 1.3] 2 like we did for <ref type="figure">Fig. 2</ref> in the main body. This is done to demonstrate the extent to which the model can extrapolate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Keypoints prediction</head><p>As being said, we train a linear model to predict the keypoints from the latent codes. To train such a model, we first generate n = 10 4 latent codes w 1 , ..., w n for each model, then we decode them into images x 1 , ..., x n . After that, we predict keypoints vector y i for each image with Super-FAN model <ref type="bibr" target="#b6">[6]</ref>. Then we fit a linear regression model to predict y i from w i .</p><p>Measuring quality based on the synthesized images may be unfair since the variability in keypoints of each model can be different: imagine a generator that always produces a face with the same keypoints. This is why we compute the test quality by embedding real FFHQ images into each model. But to additionally demonstrate that the variability is equal for the both models, we fit a linear regression model on randomly permuted latent codes and check its score: if the prediction accuracy is high, than the variability in keypoints is low and the prediction task is much easier. We call this metric KPL (random) and depict the corresponding values on <ref type="table" target="#tab_3">Table 3</ref>. It clearly demonstrates that the both models have equal variability in terms of keypoints.</p><p>We project FFHQ images into a latent space using the latent space projection procedure from the official repo 2 . We used default hyperparameters except for it, except that we didn't optimize the injected noise since this would take away some of the information that is better to be stored in the latent code. We depict additional qualitative results for random samples of FFHQ on <ref type="figure" target="#fig_0">Fig. 11</ref>. We provide the algorithm on KPL computation in Algorithm 1. We use N tr = 10 4 and N ts = 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additional samples</head><p>On <ref type="figure" target="#fig_0">Fig. 13</ref>, we present additional samples with the truncation factor of 0.9 from our INR-based model trained on FFHQ1024. We perform the truncation in similar nature to StyleGAN2 <ref type="bibr" target="#b39">[39]</ref> by linearly interpolating an inner representation inside G to its averaged value. On <ref type="figure" target="#fig_0">Fig. 14,</ref> we present common artifacts found in the produced images. On <ref type="figure" target="#fig_0">Fig. 15</ref>, we present additional superresolution samples from our model trained on LSUN 128 2 . On <ref type="figure" target="#fig_0">Fig. 16</ref>, we present additional uncurated samples of our model trained on LSUN bedroom 256 2 .  <ref type="figure" target="#fig_0">Figure 11</ref>: Predicting keypoints from latent codes for random FFHQ images. The corresponding scores are presented in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Compute Keypoints Prediction Loss (KPL).</head><p>Input : Keypoints extractor K : x ? k ? R d k . Input : Generator model G : w ? x Input : Embedding procedure E : x ? w. Input : Collection of real face images X test = {x i } n i=1 of size N ts . Output: KPL score s ? [0, +?). Generate N tr latent codes W train = {w 1 , ..., w Ntr }; Generate a dataset of synthetic images </p><formula xml:id="formula_0">X train = {G(w i )|w i ? W train }; Extract keypoints K train = {K(x i ) | x i ? X train } and K test = {K(x j ) | x j ? X test }; Embed real images W test = {E(x j ) | x j ? X test }; Train a linear keypoints estimator (A * , b * ) = arg min A,b Ntr i=1 (Aw i + b) ? k i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Positional encoding of coordinates</head><p>Recent works <ref type="bibr" target="#b75">[73,</ref><ref type="bibr" target="#b82">80]</ref> demonstrate that using positional embeddings <ref type="bibr" target="#b85">[83]</ref> like Fourier features greatly increases the expressivity of a model, allowing to fit more complex data. Our positional encoding of coordinates follows <ref type="bibr" target="#b82">[80]</ref> design and consists on a linear matrix W ? R n?2 applied to raw coordinates vector p = (x, y) and followed by sine/cosine non-linearities and concatenated:  <ref type="figure" target="#fig_0">Figure 12</ref>: Projection results by projection images from 11. We use the original StyleGAN2's projection procedure <ref type="bibr" target="#b39">[39]</ref> to project FFHQ dataset images into the latent space of a generator. All low-frequency details, together with the keypoints are reconstructed well. In our case, the reconstruction quality is lower because we do not optimize for spatial noise as StyleGAN2 does because our vanilla INR-GAN architecture does not use spatial noise injection.</p><formula xml:id="formula_1">e(p) = sin(W p) cos(W p)<label>(1)</label></formula><p>Matrix W is produced by our generator G without any factorization since it has only 2 columns. Each row of this matrix corresponds to the parameters of the Fourier transform. The norm of a row corresponds to the frequency of the corresponding wave. We depict frequencies distribution learned by our generator on <ref type="figure" target="#fig_0">Figure 17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Geometric prior</head><p>Adding coordinates to network input induces powerful prior on the geometric shapes, since now different pixels, otherwise created equal are ordered through the euclidean (or any other) coordinate system. It is a well-known fact that complex geometric shapes could be compactly represented with the use of euclidean coordinates (for example one can write down an ellipse equation as (x?x0) 2 a 2 + (y?y0) 2 b 2 = 1 ). On the other hand without any form of prior one would hope to fit complex patterns with dedicated filters which would potentially consume much more parameters.</p><p>It is worthy to discuss the synergy between the coordinate representations and hypernetworks. Lets imagine a learnable system consisting of sequentially connected linear layer W ? R 2?2 , which is modulated by a hypernetwork, and INR, taking Euclidean coordinates as an input f (X), X ? R 2?1 The whole model then could be written as f (W (z) ? X). Now, it could be seen, that introducing the hypernetwork to the pipeline allows to apply linear transformation to the coordinates, rotating and zooming the image encoded by INR f . Thus, hypernetworks allow to easily perform transformations non-trivial for traditional deep learning systems. Finer control over form and placement Recent studies show that convolutional NN struggle with such simple and crucial tasks as accurately predicting the coordinates of a drawn point <ref type="bibr" target="#b51">[51]</ref> (and vice versa, drawing a point given the coordinates). One should expect, that such an important skill is necessary for the generative model for accurate placement of the different object parts and for precise representation of the object proportions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. FMM as a generalizaton of the common weight modulation schemes</head><p>In this section we show that Factorized Matrix Multiplication could be seen as a general framework for weight modulation, with Squeeze-and-Excitation, AdaIN and "vanilla" hypernetworks as its particular cases. Squeeze-and-Excitation Let us look at the l-th FMM layer of our network with the effective rank of 1. In this case A l and B l are matrices (actually vectors) of the sizes n in ? 1 and 1 ? n out respectively. Thus, following the rules of matrix multiplication, we get that W l h i,j = A l i ? B l j . On the other hand, lets look at the Squeeze-and-Excitation mechanism. Here we are modulating the output of the each neuron by multiplicating it by the predicted coefficient, which is equivalent to the multiplication of the corresponding weight matrix column by this coefficient. Using our notions and denoting the preactivation (before non-linearity) vector of modulation coefficients as A we get that in this case W l h i,j = A l i . While at the first glance it looks like our model is more expressive lets not forget about the fact that the next layer is by itself modulated with its own Squeeze-and-Excitation, from which (omitting relu non-linearity and the fact that it has its own sigmoid) we can get the B l j multiplier. Thus it could be seen that squeeze-and-excitation modulation is roughly equivalent to the FMM layer with the rank of 1. The same reasoning is applicable to the AdaIN case, though, with AdaIN obviously we have the additional normalization layer and do not have sigmoid non-linearity for the style vector which influences the learning dynamics in its own way. We can say that squeeze-and-excitation is the least powerful weight modulation scheme, which uses the matrix of the rank 1 to modulate the main shared weights, on the other hand it is cheap and simple. Hypernetworks Vanilla hypernetwork is perhaps the most straightforward (and the most expensive but flexible) approach to the weight modulation. It is as simple as predicting each weight as an output of MLP. So let's demonstrate that any weight To produce the plot, we sample 128 images in an INR-based form and computed the norms of the positional encoding layers, i.e. those layers which take raw coordinates as an input. As one can see, the model tries to use more high-frequent positional embeddings for the last layer since they are more important for drawing fine-grained details. Early layers determine the structure of an image and hence use smaller frequencies to operate on a larger scale. dynamic that can be modeled by hypernetwork could be fitted with the FMM of high enough rank. Let's assume that n in is larger than n out (which is our case, but not essential for the generality of the proof) and choose the FMM rank of n in . In this case A l and B l are matrices of the sizes n in ? n in and n in ? n out respectively. Since A is a square matrix we can set it to identity constant (which is a solution easily learnt by a NN just by setting bias) and get W l h i,j = B l i,j . In this case any hypernetwork could be "simulated" with FMM by fitting B l i,j = ? ?1 (? l i,j W l s i,j ), where? denotes the weight matrix predicted by the hypernetwork. While ? ?1 definitely imposes some restrictions, caused by the positiveness and the range, in our experiments adding sigmoid has not resulted in any harm, perhaps because of the flexible calibration of the W l s . We have shown that main weight modulation schemes could be seen as the boundary particular cases of our approach. Our approach to the weight modulation is somewhere in between of these two extremes, while reaping the benefits of the both of them. Studying the behaviour of this transmission is specially important for the shading light on the weight modulation at whole and going beyond straightforward approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance on multi-class datasets</head><p>In this section, we conduct experiments on two diverse datasets to demonstrate that our proposed architectural design improves performance in this scenario is well. For this, we employ two datasets: LSUN-10 256 2 and MiniImageNet-100 128 2 . LSUN-10 consists on 1M images of 10 LSUN scenes, where we take 100k images of each scene. MiniImageNet-100 consists on 100k images of 100 ImageNet classes 3 , where each class provides 1k images. We report the results for different models in <ref type="table" target="#tab_4">Table 4</ref>. They demonstrate that our proposed architectural design improves the performance for this setup as well. <ref type="bibr" target="#b2">3</ref> https://github.com/yaoyao-liu/mini-imagenet-tools </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison between a traditional convolutional generator (left) and an INR-based one (right). A traditional generator directly generates a pixel-based image representation given its latent code z. The INR-based one produces parameters of an MLP. The corresponding pixel-based representation is obtained by evaluating the INR at each coordinate location (x, y) of a specified grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Image interpolation in the pixel-based form.(b) Image interpolation in the INR-based form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Images have meaningful interpolation when represented in the INR-based form. To interpolate between F ?1 and F ?2 , we compute interpolation parameters ? = ?? 1 + (1 ? ?)? 2 and evaluate F ? for the provided coordinates grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>INR-based generator with FMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Multi-Scale INR-based GAN (without FMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>FID scores (in log scale) on LSUN Bedroom 256 2 for different rank values of the proposed FMM modulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>In Section 4 . 2 ,</head><label>42</label><figDesc>we explored several exciting properties of INR-based decoders and tested them in practice. Here,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>For our 256 ? 256 experiments (for both LSUN and FFHQ), we use 2 multi-scale INR blocks of resolutions 128 and 256. Each block contains 4 layers of 512 dimensions each. For FFHQ 1024 ? 1024, we used 4 multi-scale INR blocks of resolutions 128, 256, 512 and 1024. First 2 blocks had 3 layers of dimensionality of 512, 512 resolution had 2 layers of resolution 128, the final block had 2 layers of dimensionality of 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>After training our INR-based GAN on LSUN 256 ? 256 dataset, we feed larger coordinates grid [?0.5, 1.5] 2 into it. This is larger than onFigure 2to demonstrate the extent to which the model extrapolates. As one can see, extending the grid outside of [?0.4, 1.4] 2 makes the quality to decrease rapidly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 2 ;</head><label>22</label><figDesc>Evaluate its performance on the test set: s =Nts i=1 (Aw j + b) ? k j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) INR-GAN latent space projections. (b) StyleGAN2 latent space projections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 : 32 Figure 14 :</head><label>133214</label><figDesc>Random (uncurated) samples from our model trained on FFHQ 1024 ? 1024 dataset with the truncation factor of 0.9. FID: 16.Common artifacts found in our model's samples when sampling without truncation. As one can see, the most severe ones are "stains" and patterned texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Additional samples from our model to show superresolution properties. We trained the model on LSUN 128?128 and upsampled to 256 ? 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Random samples of our model on LSUN bedroom 256 2 dataset. FID: 6.27.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Frequencies distributions for different multi-scale INR blocks of our INR-based GAN trained on FFHQ 1024 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID scores for additional ablations of our multiscale INR-based GAN (INR-GAN). Removing ?(x) from FMM worsens the scores. Incorporating StyleGAN2's architecture and bilinear upsampling allows the INRbased generator to rival its convolution-based counterpart. Img/sec of G is measured on 1 NVidia V100 32GB.</figDesc><table><row><cell>Decoder type INR-GAN w/o FMM activation</cell><cell cols="3">Churches ? Bedrooms ? img/sec ? 10.35 11.73 267.3</cell></row><row><cell>INR-GAN</cell><cell>7.12</cell><cell>6.27</cell><cell>267.1</cell></row><row><cell>+ StyleGAN2 architecture</cell><cell>5.09</cell><cell>4.96</cell><cell>265.1</cell></row><row><cell>+ bilinear upsampling</cell><cell>3.12</cell><cell>3.41</cell><cell>203.4</cell></row><row><cell>StyleGAN2</cell><cell>3.86</cell><cell>2.65</cell><cell>85.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Keypoints Prediction Loss (KPL) metric for our INR-based decoder and StyleGAN2 generator computed on FFHQ 256 ? 256 dataset for different latent spaces. The qualitative difference is illustrated onFig. 4.</figDesc><table><row><cell>Generator</cell><cell cols="2">KPL (Z) KPL (W) KPL (random)</cell></row><row><cell cols="2">StyleGAN2 2.1 ? 10 ?4 7.6 ? 10 ?5 INR-GAN 6.0 ? 10 ?5 5.2 ? 10 ?5</cell><cell>4.6 ? 10 ?4 4.7 ? 10 ?4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>FID &amp; IS at 300k iterations on multi-class datasets.</figDesc><table><row><cell>Decoder type Basic INR decoder</cell><cell cols="2">LSUN-10 FID ? IS ? 216.8 1.0</cell><cell cols="2">MiniImageNet FID ? IS ? 271.5 1.03</cell></row><row><cell cols="4">+ Hypernetwork-based decoder OOM OOM 112.9</cell><cell>8.76</cell></row><row><cell>+ Fourier embeddings</cell><cell cols="3">OOM OOM 102.8</cell><cell>9.85</cell></row><row><cell>+ FMM</cell><cell>23.78</cell><cell>2.48</cell><cell>84.66</cell><cell>9.32</cell></row><row><cell>+ Multi-scale INR</cell><cell>12.47</cell><cell>3.02</cell><cell cols="2">59.63 11.29</cell></row><row><cell>StyleGAN2</cell><cell>8.99</cell><cell>3.18</cell><cell cols="2">52.94 12.32</cell></row><row><cell>Validation set</cell><cell>0.42</cell><cell>9.93</cell><cell>0.39</cell><cell>61.79</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ajbrock/BigGAN-PyTorch/blob/master/inception_tf13.py 2 https://github.com/NVlabs/stylegan2/blob/master/projector.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image generators with conditionally-independent pixel synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Ivan Anokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Demochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sterkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korzhenkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13775</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explicitly disentangling image content from translation and rotation with spatial-vae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Brignole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. Alche-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15409" to="15419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Super-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02765</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pi-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00926,2020.4</idno>
		<title level="m">Periodic implicit generative adversarial networks for 3d-aware image synthesis</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principled weight initialization for hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lampros</forename><surname>Flokas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rina</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David P</forename><surname>Woodruff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06730</idno>
		<title level="m">Algorithms for p low rank approximation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>?zg?n ? I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Feature-wise transformations. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://distill.pub/2018/feature-wise-transformations.4" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured multi-hashing for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yerlan</forename><surname>Idelbayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Comparing the parameter complexity of hypernetworks and the embedding-based alternative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Galanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10006,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the modularity of hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Galanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06126</idno>
		<title level="m">Deep structured implicit functions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning shape templates with structured implicit functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7154" to="7164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org.7" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Non-autoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximating the predictive distribution via adversarially-trained hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Johannes Von Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Carlo</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pascal</forename><surname>Surace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin F Grewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural autoregressive flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lowrank compression of neural nets: Learning the rank of each layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yerlan</forename><surname>Idelbayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jeruzalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03207</idno>
		<title level="m">Nasa: Neural articulated shape approximation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Msg-gan: Multi-scale gradients for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06-03" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korrawe</forename><surname>Karunratanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04451</idno>
		<title level="m">Grasping field: Learning implicit representations for human grasps</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial generation of continuous implicit shape representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><surname>Kleineberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00349</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hypernetwork functional image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Sylwester Klocek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Wo?czyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek?mieja</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="496" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cocogan: Generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">On the optimization dynamics of wide hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Galanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12193,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep meta functionals for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gidi</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Alex Sergeev, and Jason Yosinski. An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Implicit surface representations as layers in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Michalkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jhony</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08934</idno>
		<title level="m">Representing scenes as neural radiance fields for view synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Hypervae: A minimum description length variational hyper-encoding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuoc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santu</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu-Chi</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08482,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Occupancy flow: 4d reconstruction by learning particle dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5379" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Tensorizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry P</forename><surname>Vetrov</surname></persName>
		</author>
		<imprint>
			<pubPlace>In C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Texture fields: Learning texture representations in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4531" to="4540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hcnaf: Hyperconditioned neural autoregressive flow and its application for probabilistic occupancy map forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geunseob</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sebastien</forename><surname>Valois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">HyperGAN: A generative model for diverse, performant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neale</forename><surname>Ratzlaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep learning needs a prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Russin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bridging AI and Cognitive Science ICLR 2020 Workshop</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">P.h.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deepvoxels: Learning persistent 3d feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2437" to="2446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Scene representation networks: Continuous 3d-structure-aware neural scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Character-level language modeling with recurrent highway hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Language modeling with recurrent highway hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Branchynet: Fast inference via early exiting from deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surat</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Tsung</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2464" to="2469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Hypernetwork-based implicit posterior estimation and model averaging of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenya</forename><surname>Ukai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="176" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Continual learning with hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10152</idno>
		<title level="m">Solov2: Dynamic, faster and stronger</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Location augmentation for cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07044</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07017</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On compressing deep models by low rank and sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7370" to="7379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Efficient, lightweight, coordinate-based network for image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zafeirouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Castle in the sky: Dynamic sky replacement and harmonization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11800,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
