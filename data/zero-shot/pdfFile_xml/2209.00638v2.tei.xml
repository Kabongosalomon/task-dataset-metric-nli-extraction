<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Behrmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alireza Golestaneh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Understanding</term>
					<term>Action Segmentation</term>
					<term>Timestamp Supervised Learning</term>
					<term>Transformers</term>
					<term>Auto-Regressive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a unified framework for video action segmentation via sequence to sequence (seq2seq) translation in a fully and timestamp supervised setup. In contrast to current state-of-the-art frame-level prediction methods, we view action segmentation as a seq2seq translation task, i.e., mapping a sequence of video frames to a sequence of action segments. Our proposed method involves a series of modifications and auxiliary loss functions on the standard Transformer seq2seq translation model to cope with long input sequences opposed to short output sequences and relatively few videos. We incorporate an auxiliary supervision signal for the encoder via a frame-wise loss and propose a separate alignment decoder for an implicit duration prediction. Finally, we extend our framework to the timestamp supervised setting via our proposed constrained k-medoids algorithm to generate pseudo-segmentations. Our proposed framework performs consistently on both fully and timestamp supervised settings, outperforming or competing state-of-the-art on several datasets. Our code is publicly available at https://github.com/boschresearch/UVAST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to analyze, comprehend, and segment video content at a temporal level is crucial for many computer vision, video understanding, robotics, and surveillance applications. Recent state-of-the-art methods for action segmentation mainly formalize the task as a frame-wise classification problem; that is, the objective is to assign an action label to each frame, based upon the full sequence of video frames. We illustrate this general approach in <ref type="figure">Fig. 1 (a)</ref>. However, this formulation suffers several drawbacks, such as over-segmentation when trained on relatively small datasets (which typically need to consist of expensive frame-level annotations).</p><p>In this work, we propose an alternative approach to the action segmentation task. Our approach involves a transformer-based seq2seq architecture that aims to map from <ref type="bibr">(a)</ref> (b) (c) (d) (e) <ref type="figure">Fig. 1</ref>. Using Transformers for Action Segmentation. Instead of frame-level predictions, which are prone to over-segmentation (a), we propose a seq2seq transformer model for segment-level predictions (b). To provide more direct feedback to the encoder we apply a frame-wise loss (c); the resulting features enhance the decoder predictions. However, duration prediction still suffers, so we focus on transcript prediction (d) and use a separate alignment decoder to fuse encoder and decoder features to arrive at an implicit form of duration prediction (e).</p><p>the video frames directly to a higher-level sequence of action segments, i.e., a sequence of action label / duration pairs that describes the full predicted segmentation. The basic structure of our model follows traditional Transformer-based seq2seq models: the encoder branch takes as input a sequence of video frames and maps them to a set of features with the same length; the decoder branch then takes these features as input and generates a predicted sequence of high-level action segments in an autoregressive manner. This approach, illustrated in <ref type="figure">Fig. 1 (b)</ref>, is a natural fit for action segmentation because it allows the decoder to directly output sequences in the higherlevel description space. The main advantage over the frame-level prediction is that it is less prone to over-segmentation.</p><p>However, this seemingly natural approach does not immediately perform well on the action segmentation task by itself. In contrast to language translation, action segmentation typically involves long input sequences of very similar frames opposed to short output sequences of action segments. This difference together with the relatively small amount of training videos, makes it challenging for the encoder and decoder to keep track of the full information flow that is necessary to predict the high-level segmentation alone. For this reason, we incorporate several modifications and additional loss terms into our system, which together make this approach compete with or improve upon the state-of-the-art.</p><p>First, to provide more immediate feedback to the encoder, we employ a frame-wise loss that linearly classifies each frame with the corresponding action label given the encoder features, <ref type="figure">Fig. 1 (c)</ref>. As a result, the encoder performs frame-wise classification with high localization performance, i.e., high frame-wise accuracy, but low discrimination performance, i.e., over-segmentation with low Edit distance to the ground truth. Nonetheless, its features provide the decoder an informative signal to predict the sequence of actions more accurately. This immediate auxiliary supervision signal allows the decoder to learn more discriminative features for different actions. While the framewise loss improves the transcript prediction, the decoder still suffers from low localization performance for duration prediction. As the next step, we fuse the decoder predic-tions with the encoder, for which we propose two solutions. First, we propose to fuse the discriminative features of the decoder with the encoder features via a cross-attention mechanism in an alignment decoder, <ref type="figure">Fig. 1 (d,e)</ref>. Second, the high performance of our decoder on predicting transcripts and the high performance of our encoder on localizing actions allows us to effectively utilize the common post-processing algorithm such as FIFA <ref type="bibr" target="#b32">[33]</ref> and Viterbi <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Finally, we further extend our proposed framework when only a weaker form of timestamp supervision is available. As mentioned before, the frame-wise prediction is vital in our Transformer model to cope with small datasets and long sequences of frames. In this case, when the frame-level annotations are not fully available, we assign a label to each frame by a constrained k-medoids clustering algorithm that takes advantage of timestamp supervision. Our simple proposed clustering method achieves a frame-wise accuracy of up to 81% on the training set, which can be effectively used to train our seq2seq model. We further show that the clustering method can also be used in combination with frame-wise prediction methods such as ASFormer <ref type="bibr" target="#b41">[42]</ref>.</p><p>We evaluate our model on three challenging action segmentation benchmarks: 50Salads <ref type="bibr" target="#b34">[35]</ref>, GTEA <ref type="bibr" target="#b11">[12]</ref>, and Breakfast <ref type="bibr" target="#b18">[19]</ref>. While our method achieves competitive frame-wise accuracies compared to the state-of-the-art, our method substantially outperforms other approaches in predicting the action sequence of a video, which is measured by the Edit distance. By using Viterbi <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref> or FIFA <ref type="bibr" target="#b32">[33]</ref> as post-processing, our approach also achieves state-of-the-art results in terms of segmental F1 scores. To the best of our knowledge, this work is the first that utilizes Transformers in an autoregressive manner for action segmentation and is applicable to both the fully and timestamp supervised setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fully Supervised Action Segmentation. Early approaches for action segmentation are based on sliding window and non-maximum suppression <ref type="bibr">[31,</ref><ref type="bibr" target="#b17">18]</ref>. Other traditional approaches use hidden Markov Models (HMM) for high-level temporal modeling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. <ref type="bibr" target="#b27">[28]</ref> use a language and length model to model the probability of action sequences and convert the frame-wise probabilities into action segments using dynamic programming.</p><p>More recent approaches are based on temporal convolutions: <ref type="bibr" target="#b21">[22]</ref> propose temporal convolutional networks (TCN) with temporal pooling to capture long-range dependencies within the video. However, such temporal pooling operations struggle to maintain fine-grained temporal information. Therefore, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> use multi-stage TCNs, which maintain a high temporal resolution, with a smoothing loss and refinement modules. These methods solve the action segmentation task by predicting an action class for each frame, which is prone to over-segmentation and requires refinement modules and smoothing or expensive inference algorithms. <ref type="bibr" target="#b16">[17]</ref> address this issue by adding a boundary regression branch to detect action boundaries, which are used during inference to refine the segmentation. <ref type="bibr" target="#b15">[16]</ref> propose a graph-based temporal reasoning module that can be built on top of existing methods to refine predicted segmentations. In contrast, we aim to predict the high-level sequence of segments directly.</p><p>Weakly Supervised Action Segmentation. To avoid the costly frame-wise annotations, many methods have been proposed that rely on a weaker form of supervision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9]</ref>, such as transcript supervision <ref type="bibr" target="#b4">[5]</ref>: Here, only the ordered sequence of actions in the video are given. <ref type="bibr" target="#b14">[15]</ref> extend the connectionist temporal classification framework, originally introduced for speech recognition, to videos to efficiently evaluate all possible frame-to-action alignments. <ref type="bibr" target="#b9">[10]</ref> propose an iterative soft boundary assignment strategy to generate frame-wise pseudo-labels from transcripts. <ref type="bibr" target="#b29">[30]</ref> generate frame-wise pseudo-labels with the Viterbi algorithm. <ref type="bibr" target="#b23">[24]</ref> extend this work by adding a loss that discriminates between valid and invalid segmentations. <ref type="bibr" target="#b33">[34]</ref> use a two-branch neural network with a frame classification branch and a segment generation branch and enforce the two representations to be consistent via a mutual consistency loss. Similar to our method, their segment generation branch also predicts the transcript in an auto-regressive manner and achieves high Edit scores, validating our aspiration for segment-level predictions. While transcript supervision reduces the annotation cost significantly, the performance suffers. As an alternative, timestamp supervision <ref type="bibr" target="#b25">[26]</ref> has been proposed, where for each action segment a single frame is annotated. The annotation cost for such timestamps is comparable to transcript annotations <ref type="bibr" target="#b25">[26]</ref> but provides stronger supervision as it gives information about the rough location of the segments. Transformers. Transformers <ref type="bibr" target="#b37">[38]</ref> originally emerged in the field of natural language processing, and solely rely on the attention mechanism to capture contextual information from the entire sequence. Recently, Transformers have also seen wide adoption in vision-related tasks, e.g., image classification <ref type="bibr" target="#b10">[11]</ref>, segmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref> and action classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref>. Current standard Transformer-based models are unable to process very long sequences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44]</ref>. One reason for this is the self-attention operation, which scales quadratically with the sequence length. <ref type="bibr" target="#b2">[3]</ref> showed that using sliding window attention can reduce the time and memory complexity of the Transformer while preserving the performance. Recently, ASFormer <ref type="bibr" target="#b41">[42]</ref> leveraged multi-stage TCNs <ref type="bibr" target="#b0">[1]</ref> and transformer-based models for action segmentation. For each dilated temporal convolutional layer of MS-TCN, an additional self-attention block with instance normalization is added. The first stage is the encoder while the later stages are the decoders, which take the concatenated features of the encoder and the features at the end of the previous stage as input. While we use a similar encoder as ASFormer <ref type="bibr" target="#b41">[42]</ref>, our decoder is very different. While ASFormer and MS-TCN perform frame-level prediction as illustrated in <ref type="figure">Fig. 1</ref> (a), our decoder predicts the action segments in an auto-regressive manner as illustrated in <ref type="figure">Fig. 1 (d,e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce our Unified Video Action Segmentation model via Transformers (UVAST). The goal of action segmentation is to temporally segment long, untrimmed videos and classify each of the obtained segments. Current state-of-the-art methods are based on frame-level predictions -they assign an action label to each individual frame -which are prone to over-segmentation: The video is not accurately segmented into clean, continuous segments, but fragmented into many shorter pieces of alternating action classes. We challenge this view of frame-level predictions and pro-pose a novel approach that directly predicts the segments. By focusing on segment-level predictions -an alternative but equivalent representation of segmentations -our method overcomes the deep-rooted over-segmentation problem of frame-level predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer for Auto-Regressive Segment Prediction</head><p>In this work, we view action segmentation from a sequence-to-sequence (seq2seq) perspective: mapping a sequence of video frames to a sequence of action segments, e.g., as pairs of action label and segment duration. The Transformer model <ref type="bibr" target="#b37">[38]</ref> has emerged as a particularly powerful tool for seq2seq tasks and may seem like the natural fit. The vanilla Transformer model consists of an encoder module that captures long-range dependencies within the input sequence and a decoder module that translates the input sequence to the desired output sequence in an auto-regressive manner. In contrast to language translation tasks, action segmentation faces a strong mismatch between input and output sequence lengths, i.e., inputs are long and untrimmed videos with various sequence lengths, while outputs are relatively short sequences of action segments. Therefore, we incorporate several modifications to address these issues, which we will go over in more detail in the following. Notation. Given an input sequence of T frame-wise features x t , for frame t ? {1, . . . , T }, our goal is to temporally segment and classify the T frames. The groundtruth labels of a segmentation can be represented in two equivalent forms: 1) a sequence of frame-wise action labels? t ? C for frame t, where C is the set of action classes, 2) a sequence of segment-wise annotations, which consists of ground-truth segment action classes? i ? C (also known as transcript), and segment durations? i ? R + for each segment i ? {1, . . . , N }. Transformer Encoder. Our input sequence X ? R T ?d consists of T frame-wise features x t , where d denotes the feature dimension. We embed them using a linear layer and then feed them to the Transformer encoder, which consists of several layers and allows the model to capture long-range dependencies within the video via the self-attention mechanism. The output of the encoder, E ? R T ?d ? , is a sequence of frame-wise features e t , which will be used in the cross-attention module of the decoder. To provide direct feedback to the encoder, we apply a linear layer to obtain frame-level predictions for e t . This enables the encoder to accurately localize the action classes within the video and provides more informative features to the decoder. In practice, we use a modified version of the encoder proposed in <ref type="bibr" target="#b41">[42]</ref>, which locally restricts the self-attention mechanism and uses dilated convolutions (see supplemental material for more details). Transformer Decoder. Given a sequence of frame-wise features E ? R T ?d ? , we use a Transformer decoder to auto-regressively predict the transcript, i.e., the action labels of the segments. Starting with a start-of-sequence (sos) token, we feed the sequence of segments S ? R N ?d ? -embedded using learnable class tokens and positional encoding -up until segment i to the decoder. Via the cross-attention between the current sequence of segments and frame-wise features, the decoder determines the next segment i + 1 in the video. In principle, the decoder could predict the segment duration as well ( <ref type="figure">Fig. 1 (c)</ref>), however, in practice we found that the decoder's duration prediction suffers from low localization performance, see <ref type="table">Table 4</ref>. While it is sufficient to pick out a single or few frames in the cross-attention mechanism for predicting the correct action class of a segment, the duration prediction is more difficult since it requires to assign frames to a segment and count them. Since the number of segments is much smaller than the number of frames, the cross-attention mechanism tends to assign only a subset of the frames to the correct segment. To address this issue, we propose a separate decoder module, which fuses the discriminative decoder features with the highly localized encoder features to obtain a more accurate duration prediction, which we describe in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objective</head><p>Although our ultimate goal is segment-level predictions, we provide feedback to both the encoder and decoder model to make the best use of the labels. To that end, we apply a frame-wise cross-entropy loss on the frame-level predictions of the encoder:</p><formula xml:id="formula_0">L frame = ? 1 T T t=1 log(y t,? ),<label>(1)</label></formula><p>where y t,c denotes the predicted probability of label c at time t, and? denotes the ground-truth label of frame t. Analogously, we apply a segment-wise cross-entropy loss on the segment-level predictions of the decoder:</p><formula xml:id="formula_1">L segment = ? 1 N N i=1 log(a i,? ),<label>(2)</label></formula><p>where a i,c denotes the predicted probability of label c at segment i, and? denotes the ground-truth label of segment i. Regularization via Grouping. To regularize the encoder and decoder predictions, we additionally apply group-wise cross-entropy losses. To that end, we group the frames and segments by ground-truth labels L = {c ? C|c ? {? 1 , . . . ,? n }} that occur in the video: T c = {t ? {1, . . . , T }|? t = c} are the indices of frames with class c, and N c = {i ? {1, . . . , N }|? i = c} the indices of segments with class c. We apply a cross-entropy loss to the averaged prediction of each group:</p><formula xml:id="formula_2">L g-frame = ? 1 |L| c?L log ? ? 1 |T c | t?Tc y t,c ? ? (3) L g-segment = ? 1 |L| c?L log ? ? 1 |N c | i?Nc a i,c ? ? (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-Attention Loss</head><p>We utilize a loss through a cross-attention mechanism between the encoder and decoder features to allow further interactions between them. Let us assume that T video frames and corresponding N actions in the encoder and decoder are represented by their</p><formula xml:id="formula_3">features E ? R T ?d ? and D ? R N ?d ? ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>respectively. The cross-attention loss involves obtaining a cross-attention matrix</head><formula xml:id="formula_4">M = softmax( ED T ? ? ? d ? ),</formula><p>where ? ? is a stability temperature, and each row of M includes a probability vector that assigns each encoder feature (frame) to decoder features (actions). We then use M in the following crossentropy loss function:</p><formula xml:id="formula_5">L CA (M ) = ? 1 T t log(M t,n ),<label>(5)</label></formula><p>wheren is the ground-truth segment index to which frame t belongs. We use this loss in our transcript decoder (main decoder) and alignment decoder in the following. Cross-Attention Loss for the Transcript Decoder. The cross-attention loss, when applied to the transcript decoder, provides more intermediate feedback to the decoder about the action location in the input sequence, see <ref type="figure" target="#fig_3">Fig. 5</ref>. We found this loss function especially effective on smaller datasets such as 50Salads (see <ref type="table" target="#tab_3">Table 5</ref>). Our main objective for the encoder and the transcript decoder is:</p><formula xml:id="formula_6">L = L frame + L segment + L g-frame + L g-segment + L CA (M ),<label>(6)</label></formula><p>Cross-Attention Loss for the Alignment Decoder. While the transcript decoder generates the sequence of actions in a video, it does not predict the duration of each action. Although it is possible to predict the duration as well, as illustrated in <ref type="figure">Fig. 1 (c)</ref>, the transcript decoder still struggles to localize actions through direct duration prediction as shown in <ref type="table">Table 4</ref>. One reason for this could be the high mismatch between input and output sequence length and the relatively small number of training videos. While picking up a single segment frame is sufficient to predict the action class, the duration prediction effectively requires counting the number of frames in the segment, resulting in a more challenging task. Therefore, we design an alternative alignment decoder for predicting segment durations implicitly. The full flow of the complete model is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>A high Edit score of our decoder indicates that it has already learned discriminative features of the actions. The motivation for our alignment decoder is to align the encoder features to the highly discriminative features of the decoder, which can be further used for the duration prediction (see <ref type="figure">Fig 1 (e)</ref>). In essence, our proposed alignment decoder is a one-to-many mapping from the decoder features to the encoder features. The alignment decoder takes the encoder and decoder features E ? R T ?d ? and D ? R N ?d ? with positional encoding as input and generates the aligned features A ? R T ?d ? . Since the alignment decoder aims to explore the dependencies between the encoder features and the decoder features, we employ a cross-attention mechanism in its architecture similar to the transcript decoder. To this end, we compute an assignment matrix M ? R T ?N via cross-attention between the alignment decoder features (A) and positional encoded features of the transcript decoder (D) by M = softmax( AD T ? ) with a small value of ? . Note that with a small value of ? each row of M will be close to a one-hot-encoding indicating the segment index the frame is assigned to. The positional encoding for D resolves ambiguities if the same action occurs at several locations in the video.</p><p>In contrast to the decoder from the previous section, the alignment decoder is not auto-regressive since the full sequences of frame-wise and segment-wise features are Overview of our complete model. Our complete model consists of a Transformer encoder and an auto-regressive Transformer decoder, which we train for frame-level and segmentlevel predictions, respectively. For duration prediction we use an alignment decoder -followed by cross attention -on top of the encoder and decoder features to compute a frames-to-segment assignment, which is used to compute the durations of the segments. already available from the previous encoder and decoder. During inference, we compute the segment durations by taking the sum over the assignments:</p><formula xml:id="formula_7">u i = t M t,i ,<label>(7)</label></formula><p>where i ? {1, ..., n} and M t,i denotes whether frame t is assigned to segment i. We found that training the alignment decoder using only the loss for M (7) in a separate stage on top of the frozen encoder and decoder features results in a more robust model that suffers less from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Timestamp Supervision</head><p>In this section, we show how our proposed framework can be extended to the timestamp supervised setting. In this setting, we are given a single annotated frame for each segment in the video, i.e., frame annotations are reduced dramatically, and ground-truth segment durations are no longer available for all frames. As we extensively discussed before, our proposed framework relies on the frame-level supervisory signal on top of the encoder. However, it turns out that a noisy frame-level annotation provides a solid signal to the encoder. To obtain such frame-level annotations, we propose a constrained k-medoids algorithm that propagates the timestamp supervision to all frames. A typical k-medoids algorithm starts with random data points as the cluster centers. It iteratively updates the cluster centers chosen from the data points and the assignments based on their similarity to the cluster center. Having access to the timestamp supervision, we can use them as initialization and cluster the input features. However, in a standard k-medoids algorithm, a temporally continuous set of clusters are not taken for granted. We call our method constrained k-medoids because we force the clusters to be temporally continuous. This can be simply achieved by modifying the assignment step of the k-medoids algorithm. Instead of assigning pseudo-labels to each frame, we find the temporal boundaries of each cluster. In the assignment step, we update the boundaries such that the accumulative distance of each cluster to the current center is Algorithm 1: Constrained K-medoids to generate temporally continuous clusters. minimized. Alg. 1 summarizes the steps of our clustering method. In principle, we can apply k-medoids using the frame-wise input features x t , the encoder features e t , or a combination of both. In practice, we found that using input features alone gives surprisingly accurate segmentations, see <ref type="table" target="#tab_2">Table 3</ref> or supplemental material for more analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the performance of our proposed model extensively on three challenging action segmentation datasets (50Salads <ref type="bibr" target="#b34">[35]</ref>, GTEA <ref type="bibr" target="#b11">[12]</ref>, and Breakfast <ref type="bibr" target="#b18">[19]</ref>). We follow previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref> and perform 4-fold cross-validation on Breakfast and GTEA and 5-fold cross-validation on 50Salads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>For evaluation, following previous works, we report the frame-wise accuracy (Acc), segmental edit score (Edit), and the segmental F1 score at overlapping thresholds 10%, 25%, and 50%, denoted by F1@{10, 25, 50} <ref type="bibr" target="#b21">[22]</ref>. The overlapping threshold is determined based on the intersection over union (IoU) ratio. Although frame-wise accuracy is the most commonly used metric for action segmentation, it does not portray a thorough picture of the performance of action segmentation models. A major disadvantage of frame-wise accuracy is that long action classes have a higher impact than short action classes and dominate the results. Furthermore, over-segmentation errors have a relatively low impact on Acc, which is particularly problematic for applications such as video summarization. On the other hand, Edit and F1 scores establish more comprehensive measures of the quality of the segmentations <ref type="bibr" target="#b21">[22]</ref>; Edit measures the quality of the predicted transcript of the segmentation, while F1 scores penalize over-segmentation and are also insensitive to the duration of the action classes. Our proposed method performs particularly well on the Edit, and F1 scores on all datasets and in fully and timestamp supervised setups, achieving state-of-the-art results in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details and Training</head><p>We follow the standard training strategy from existing algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref> and train our main network (Section 3.1) end-to-end with batch size of 1. We train our model for at most 800 epochs using Adam optimizer with learning rate 0.0005 and the loss (6). In the cross-attention loss, Eq. <ref type="formula" target="#formula_7">(7)</ref>, we set ? = 1 during training to ensure training stability, and ? = 0.0001 during inference. As input for our model, we use the same I3D <ref type="bibr" target="#b5">[6]</ref> features that were used in many previous works. For the encoder, we used a modified version of the encoder proposed in <ref type="bibr" target="#b41">[42]</ref> 3 . For the decoder, we use a standard decoder architecture <ref type="bibr" target="#b37">[38]</ref>, with two layers and single head attention. Due to a strong imbalance in the segment durations, we propose a split-segment approach for improved training: longer action segments are split up into several shorter ones so that segment durations are more uniformly distributed; for details and ablations, see supplemental material. During the inference, we do not use any split-segment and use the entire video. For the alignment decoder (Section 3.3), we use a single layer, single head decoder. To train this model, we use similar hyper-parameters and optimizers while freezing the encoder-decoder model from Section 3.1 and only train the alignment decoder with our cross-attention loss. For positional encoding, we use the standard sinusoidal positional encoding <ref type="bibr" target="#b37">[38]</ref>. Furthermore, we use random dropping of the features as an augmentation method, where we randomly drop ? 1% of the features in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Evaluation</head><p>Here, we provide the overall performance comparison of our proposed method, UVAST, on three challenging action segmentation datasets with different levels of supervision. We demonstrate the effectiveness of our proposed method for both the fully supervised and timestamp supervised setup and achieve competitive results on both settings. We provide the results of our proposed model for four scenarios: Transcript prediction of our encoder-decoder architecture (referred to as "w/o duration") and three different approaches to obtain durations for the segments, namely alignment decoder from Section 3.3 ("+ alignment decoder"), Viterbi ("+ Viterbi"), and FIFA <ref type="bibr" target="#b32">[33]</ref> ("+ FIFA"). We only report the Edit score for "w/o duration", as it does not provide segment durations. A significant advantage of our method is that a predicted transcript is readily available and can be used in these inference algorithms instead of the previous methods, which need to iterate over the training transcripts. Furthermore, we can optionally use the predicted duration of the alignment decoder to initialize the segment lengths in FIFA. Fully Supervised Comparison. <ref type="table" target="#tab_0">Table 1</ref> shows the performance of our method in the fully supervised setting compared with state-of-the-art methods. At the bottom of <ref type="table" target="#tab_0">Table 1</ref> we provide the results of our proposed model for the four scenarios explained above. UVAST achieves significantly better Edit score on transcript prediction ("w/o duration") than all other existing methods on all three datasets, which demonstrates the effectiveness of our model to capture and summarize the actions occurring in the video. In the last three rows of <ref type="table" target="#tab_0">Table 1</ref>, we use three different approaches to compute the duration of the segments. Combining UVAST with the alignment decoder from Section 3.3 achieves competitive results. However, it is important to note that Transformers are very data-hungry and training them on small datasets can be challenging. We observe that UVAST with alignment decoder outperforms other methods in terms of Edit score. While the F1 scores are comparable to the state-of-the-art on the Breakfast dataset, the small size of the GTEA dataset hinders the training of the alignment decoder. Moreover, with frame-wise predictions and transcript prediction available, our method conveniently allows applying inference algorithms at test time, such as FIFA and Viterbi, without the need to expensively iterate over the training transcripts. Combining our method with Viterbi outperforms the existing methods on GTEA and 50Salads in terms of Edit and F1 scores, and achieves competitive results on Breakfast. We also provide the results of UVAST with FIFA, where we initialize the duration with the predicted duration. It achieves strong performance on Breakfast and 50Salads. Note that although FIFA is a fast approximation of Viterbi, it achieves better results on the Breakfast dataset. This is due to the fact that the objective function that is minimized by FIFA/Viterbi does not optimize the evaluation metrics directly, i.e., the global optimum of the Viterbi objective function does not guarantee the global optimum of the evaluation metrics. This observation is consistent with the results reported in <ref type="bibr" target="#b32">[33]</ref>.</p><p>The comparison to ASFormer <ref type="bibr" target="#b41">[42]</ref> is also interesting. While ASFormer performs like most other approaches frame-level prediction, <ref type="figure">Fig. 1 (a)</ref>, UVAST predicts the action <ref type="table">Table 2</ref>. Timestamp supervision results on all three datasets. UVAST, ASFormer <ref type="bibr" target="#b41">[42]</ref>, and MSTCN <ref type="bibr" target="#b0">[1]</ref> are trained via our constrained k-medoids pseudo-labels. Best result shown in bold.</p><p>UVAST outperforms SOTA on all datasets and metrics except for Acc on Breakfast. The performance in terms of Edit distance is significant, and is comparable to the fully supervised setup. segments in an autoregressive manner, <ref type="figure">Fig. 1 (d,e</ref>). As expected, ASFormer achieves in general a better frame-wise accuracy while UVAST achieves a better Edit score.</p><p>Since ASFormer uses a smoothing loss and multiple refinement stages to address oversegmentation similar to MS-TCN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>, it has ? 1.3M learnable parameters, whereas our proposed model has ? 1.1M parameters. Our approach with Viterbi achieves similar F1 scores on the Breakfast dataset, but higher F1 scores on the other datasets. For a more thorough and fair comparison with ASFormer, we additionally provide the results when combined with Viterbi or FIFA during inference. To that end, we extract the transcript to be used in Viterbi/FIFA from the frame-wise predictions of the model. Overall, we find that our method achieves strong performance in terms of Edit and F1 scores, while Acc is compared to the state-of-the-art lower on Breakfast. Note that Acc is dominated by long segments and less sensitive to over-segmentation errors. Lower Acc and higher Edit/F1 scores indicate that UVAST localizes action boundaries, which are difficult to annotate precisely, less accurately. It is therefore an interesting research direction to improve the segment boundaries, e.g., by using an additional refinement like ASFormer.</p><p>Timestamp Supervision Comparison. We use our proposed constrained k-medoids to generate pseudo-segmentation using the frame-wise input features and ground truth timestamps. The output consists of continuous segments, which can be identified with the transcript to yield a pseudo-segmentation. While this approach can be applied both to the input features and encoder features in principle, we find that using the input features already gives a surprisingly good performance; we report Acc and F1 scores in <ref type="table" target="#tab_2">Table 3</ref> averaged over all splits. Note that this is not a temporal segmentation method as it requires timestamp supervision as input. We use the resulting pseudo-segmentation as the auxiliary signal to our encoder during the training where we have access to the timestamp supervision.</p><p>In <ref type="table">Table 2</ref>, we compare our proposed timestamp model with the recently proposed method <ref type="bibr" target="#b25">[26]</ref> on the three action segmentation datasets. To the best of our knowledge, <ref type="bibr" target="#b25">[26]</ref> is the first work that proposed and applied timestamp supervision for the temporal action segmentation task. Although other weakly supervised methods exist, they are based on transcript supervision, a weaker form of supervision; therefore, we additionally train MS-TCN <ref type="bibr" target="#b0">[1]</ref> and ASFormer <ref type="bibr" target="#b41">[42]</ref> with our constrained k-medoids. To get more thorough and fair comparisons, we further show their performance when combined with Viterbi decoding or FIFA during inference. <ref type="table">Table 2</ref> shows that: I) our method largely outperforms the other methods by achieving the best performance on 13 out of 15 metrics. Analogously to the fully supervised case, we observe the strong performance of our alignment decoder in terms of Edit and F1 scores on Breakfast; with FIFA and Viterbi, we outperform the method of <ref type="bibr" target="#b25">[26]</ref> on 50Salads and GTEA. Notably, UVAST achieves significantly higher performance in terms of Edit distance, which is comparable to the fully supervised setup. II) ASFormer and MSTCN perform reasonably well in the timestamp supervision setup when trained on the pseudo-labels of our constrained k-medoids algorithm, which demonstrates one more time the effectiveness of our proposed constrained k-medoids algorithm. III) AS-Former and MSTCN do not benefit from the Viterbi algorithm in this case. This is due to the relatively lower Edit distance of these methods. Namely, Viterbi hurts MSTCN on Breakfast as it achieves significantly lower Edit distance compared to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Evaluation</head><p>We show qualitative results of two videos from the Breakfast dataset in the fully supervised and timestamp supervised setting in <ref type="figure">Fig. 4</ref>. We visualize the ground truth segmentations (first row) as well as the predicted segmentations of our encoder (second row) and decoder with alignment decoder, FIFA or Viterbi for duration prediction (last three rows). The encoder predictions demonstrate well the common problem of over-segmentation with frame-level predictions; the segment-level predictions of our decoder on the other hand yield coherent action segments. <ref type="figure">Fig. 4</ref>. Qualitative results. We show ground truth and predicted segmentation of fully supervised (left) and timestamp supervised (right) UVAST of two videos from the Breakfast dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablations Studies</head><p>Duration Prediction. As discussed in Section 1, the vanilla Transformer model, <ref type="figure">Fig. 1 (b)</ref>, does not generalize to the action segmentation task, see <ref type="table">Table 4</ref>. We train this model using L segment and MSE between predicted and ground truth durations, which are scaled to [0, 1] by dividing by the total number of frames T . Our first modification involves applying a frame-wise loss to the encoder features, which drastically improves the results. However, this explicit duration prediction still struggles to accurately localize the segments. Predicting duration implicitly via our alignment decoder instead, <ref type="figure">Fig. 1 (d)+(e)</ref>, on the other hand improves the localization, increasing Acc and F1.  <ref type="table">Table 4</ref>. Explicit duration prediction on Breakfast split 1. We show the results of different steps described in Section 1 from explicit duration prediction via the vanilla Transformer to implicit duration prediction with our alignment decoder and the impact of the full loss function, Eq. (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Loss F1@{10,25,50} Edit Acc Vanilla Transformer, <ref type="figure">Fig. 1</ref>  Impact of the Loss Terms. In <ref type="table" target="#tab_3">Table 5</ref> we investigate the impact of the different loss terms (Section 3.2) on split 1 of Breakfast and 50Salads. In the first row of <ref type="table" target="#tab_3">Table 5</ref>, we evaluate the encoder when trained only using the frame-wise loss, i.e., following the frame-wise prediction design as previous works. As expected, solely relying on the frame-wise loss leads to over-segmentation and poor performance. The rest of <ref type="table" target="#tab_3">Table 5</ref> shows the performance of our proposed model when using both encoder and decoder as explained in Sections 3.1 and 3.2, and reflect the key idea of our method to directly predict the segments. While the most basic version using the segment-wise loss (2) improves over frame-wise predictions, we observe that using both the frame-wise (1) and segment-wise (2) loss term increases the performance drastically. Moreover, we observe that adding the cross-attention loss (5) further improves the results, demonstrating its effectiveness for longer sequences with many action segments, such as 50Salads. While adding the group-wise loss terms (3) and (4) individually improves the performance moderately, the real benefit is revealed when combining them all together.</p><p>To shed more light on the contribution of our cross-attention loss we visualize its impact in <ref type="figure" target="#fig_3">Fig. 5</ref>. Given the ground truth segmentation, <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>, of a video,  we hypothesize that activations should be higher in areas that belong to the corresponding segment. <ref type="figure" target="#fig_3">Fig. 5 (c)</ref> shows the output of the cross-attention when using our cross-attention loss. We observe that this loss indeed guides the crossattention to have higher activations in the regions that belong to the related segment for an action. <ref type="figure" target="#fig_3">Fig. 5 (d)</ref> shows that lack of our cross-attention loss causes the attention map to be noisy; it's unclear which region is used for the segment classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented UVAST, a new unified design for fully and timestamp supervised temporal action segmentation via Transformers in a seq2seq style. While the segment-level predictions of our model effectively address the over-segmentation problem, this new design entails a new challenge: predicting the duration of segments explicitly does not work out of the box. Therefore, we proposed three different approaches to alleviate this problem, enabling our model to achieve competitive performance on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation Supplementary Material</head><p>The structure of this supplementary material is as follows. In Section 1, we provide more details regarding the datasets that we are using as well as highlighting the main differences among them in terms of number of videos, lengths, and segments. In Section 2, we provide the implementation details of our architecture; we investigate the impact of the encoder architecture, our Split-Segment approach, and the constrained K-Medoids algorithm. Furthermore, we provide the values for our hyper-parameters. Last but not least, in Section 3, we provide more insight on our proposed grouping losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Datasets</head><p>In <ref type="table">Table 6</ref>, we provide general information regarding the datasets (GTEA, 50Salads, and Breakfast) that we use for our experiments. Among these datasets Breakfast has the largest number of classes and videos. On the other hand, the GTEA and 50Salads dataset are much smaller in terms of number of samples. The 50Salads dataset has the longest video sequences among the three datasets, while GTEA contains videos with the highest number of action segments and repetitions of an action within a video. block (default pytorch implementation), the one proposed by <ref type="bibr" target="#b41">[42]</ref>, and our proposed modified version (last row in <ref type="table" target="#tab_4">Table 7</ref>) on split 1 of GTEA, 50Salad, and Breakfast. While the ASFormer encoder achieves drastic improvements over the simple encoder, our modified version further improves over the ASFormer encoder.  Impact of Split-Segment. Due to a strong imbalance in the duration of action segments, we propose a split-segment approach for improving the training of the network, where longer action segments are split up into several shorter ones, so that segment durations are more uniformly distributed; it is important to note that during the inference we do not use any split-segment and use the video as is. In the timestamp supervised setting, we do not use split-segment as we do not have access to the ground truth duration of segments. For the split-segment approach, we scale the durations to [0, 1] by dividing the absolute duration of a segment (i.e., the number of frames in the segment) by the total number of frames in the video. For instance, if the split-segment value is set to 0.1 it means that the duration of each action segment should be at most 0.1 and segments larger than 0.1 will be split up into smaller segments with maximum length of 0.1. During inference we merge the repeated actions into one: For example, if our model predicts an action sequence of (A, B, B, C, A, A, A) we convert it to <ref type="figure">(A, B, C, A)</ref>. The split-segment value is a hyper parameter that can be selected empirically for each dataset. In <ref type="table" target="#tab_5">Table 8</ref> we show the impact of using split-segment on split 1 of the Breakfast, 50Salads, and GTEA datasets, where we report the Edit scores. We can see that using the split-segment approach helps the model achieve better performances on all three datasets. Furthermore, in <ref type="table" target="#tab_6">Table 9</ref> we provide an ablation study regarding the impact of selecting different split-segment values on the split 1 of the Breakfast dataset.  Hyper-Parameters. <ref type="table" target="#tab_0">Table 10</ref> provides a summary of the values for our hyperparameters used for training our model. In <ref type="table" target="#tab_0">Table 10</ref>, Stage 1 and 2 refer to Section 3.1 and 3.3 of the main paper, where we train the Transformer for auto-regressive segment prediction and alignment Transformer for duration prediction, respectively. Furthermore, as shown in <ref type="table" target="#tab_0">Table 10</ref>, we use a cross-attention smoothing (average pooling on the cross-attention map along the T dimension) for the 50Salads dataset to reduce the noise in the cross-attention. Comparing to the Breakfast and GTEA datasets, 50Salads dataset has the longest videos (see <ref type="table">Table 6</ref>) with a very low number of training data which causes the cross-attention map to be noisy. Our proposed cross-attention loss along with cross-attention smoothing helps to reduce the noise and therefore leads to a better performance. Similar to previous methods and to have a fair comparison, we use sampling rate of 2 for the 50Salads dataset since it has higher FPS compared with the other two datasets <ref type="bibr" target="#b24">[25]</ref>. We state the hyper-parameters used in FIFA and Viterbi in <ref type="table" target="#tab_0">Table 11</ref>. K-Medoids. In the main paper, we propose a constrained k-medoids algorithm for generating pseudo-segmentations given frame-wise input features and timestamps. In contrast to the vanilla k-medoids clustering algorithm, our constrained version ensures temporal consistency of the clusters and the resulting temporally continuous clusters can be unambiguously identified with the class labels of the ground truth transcript.  This is a major advantage over an unconstrained clustering method, which may result in temporally fragmented clusters, making class label assignment ambiguous. We compare our constrained k-medoids with the unconstrained version, see <ref type="table" target="#tab_0">Table 12</ref>, where we assign each cluster the class label belonging to the timestamp it was initialized with. Note, that in this scenario the original ground truth timestamps may end up in completely different clusters and the class label assignment becomes noisy.</p><p>As expected the temporal fragmentation of the clusters leads to over-segmentation and correspondingly low Edit and F1 scores. However, even on Acc this unconstrained version suffers due to the noisy label assignment. Furthermore, we show two example videos from the Breakfast dataset in <ref type="figure">Fig. 7</ref>; again, we observe over-segmentation due to temporally fragmented clusters. Notably, we observe that the unconstrained k-medoids performs much better on GTEA compared with Breakfast and 50Salads. One reason for this can be the frequency of background classes, which are visually distinct to the action classes in the video and typically show very static scenes. The frequent background classes of highly similar features separate the action classes from one another. In contrast, Breakfast and 50Salads do not have a frequent background class and relatively static scenes are assigned to an action class, making it more difficult to separate them. <ref type="table" target="#tab_0">Table 12</ref>. K-Medoids. We compare our constrained k-medoids algorithm proposed in the main paper with the vanilla unconstrained version. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Grouping Loss Terms</head><p>The action segmentation datasets mentioned above are highly imbalanced in terms of the frequency of the actions that appear in the videos and the number of frames each action occupies, which influences the cross-entropy loss on top of the encoder and decoder. To cope with the imbalanced classes, we utilize two modified versions of the <ref type="figure">Fig. 7</ref>. K-Medoids. We compare our constrained and the unconstrained k-medoids algorithm qualitatively. Both algorithms cluster the frame-wise input features, using the ground truth timestamps t1, . . . , tn as initialization.</p><p>cross-entropy loss. The first loss involves averaging the probabilities of each class separately, and the second one involves averaging the logits of each class before passing them to Softmax.</p><p>To shed more light on the intuition behind the modifications, let us consider a classifier that classifies N frames x n into C classes. We denote the logits by a n and the corresponding probabilities by ? n = Softmax(a n ) where Softmax(a n ) c = e an,c C i=1 e a n,i . For grouping the frames by class label, we define: N c = {n|y n = c} for c ? {1, . . . , C},</p><formula xml:id="formula_8">? c = 1 |N c | n?Nc ? n ,<label>(8)</label></formula><p>? c = Softmax(?) c?c = 1 |N c | n?Nc a n,c .</p><p>We consider the following three loss terms, where the first is an element-wise crossentropy loss and the last two group-wise cross-entropy loss terms, taking the average outside and inside the Softmax, respectively: <ref type="table" target="#tab_0">Table 13</ref> summarizes the results for different choices of the grouping loss. We observe thatL works the best for the segment-wise loss on top of the decoder, andL works best for the frame-wise classification. Note that we report the results of the first stage training, i.e., transcript prediction only and therefore only report Edit score. <ref type="table" target="#tab_0">Table 13</ref>. Impact of different modifications on the group loss. Ablation study on split 1 of the 50Salads dataset regarding the impact of using different modifications of the group loss. We report Edit results for stage 1 training.</p><formula xml:id="formula_11">L = ? N n=1 C c=1 y nc log ? nc ,<label>(11)</label></formula><formula xml:id="formula_12">L = ? C c=1 log? c ,<label>(12)</label></formula><formula xml:id="formula_13">L = ? C c=1 log? c .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lg-frame</head><p>Lg-segment 50Salads L, Eq. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our complete model. Our complete model consists of a Transformer encoder and an auto-regressive Transformer decoder, which we train for frame-level and segmentlevel predictions, respectively. For duration prediction we use an alignment decoder -followed by cross attention -on top of the encoder and decoder features to compute a frames-to-segment assignment, which is used to compute the durations of the segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 9 for i = 1 , 1 Fig. 3 .</head><label>9113</label><figDesc>Input: T features xt, timestamps [t1, . . . , tn] 2 Init: mi = xt i # initialize medoids 3 repeat 4 Di,j = dist(mi, xj) # pairwise costs 5 b0 = 0; bn = T # compute boundaries 6 for i = 1, . . . , n ? 1 do 7 bi = argmin l ( l j=t i Di,j + . . . , n do 10 ti = argmin l ( b i j=b i?1 +1 dist(x l , xj)) 11 mi = xt i # new medoids 12 end 13 until until convergence; 14 return li = bi ? bi?Constrained K-medoids.Given frame-wise features and timestamps, k-medoids generates a pseudo-segmentation that guides the encoder during the training instead of ground truth frame-level labels in a fully supervised setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>+</head><label></label><figDesc>Frame-wise Loss, Fig. 1 (c) Eq. (2) + Eq. (1) + MSE 70.7 63.5 44.4 73.9 59.1 Eq. (6) + MSE 72.1 65.1 48.7 76.5 59.0 + Alignment Decoder, Fig. 1 (d)+(e) Eq. (2) + Eq. (1) + AD 73.5 68.3 54.3 75.2 67.7 Eq. (6) + AD 77.1 72.0 60.4 78.2 71.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 (</head><label>5</label><figDesc>b) shows our expected target activations (output of softmax) of the decoder's cross-attention map;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Impact of the Cross-Attention Loss for the Transcript Decoder. (a) A ground truth example of a video with 13150 frames and 12 segments from the 50Salads dataset. (b) The target cross-attention map after softmax with dimension 12 ? 13150. (c) and (d) show the zoomed-in segments of the cross-attention map of the decoder when using the cross-attention loss (top) or not using it (bottom). In (b-d) brighter color means higher values of the activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Modifications on Encoder Model. Comparison between the original encoder block proposed by [42] (a) and our modified version (b) with GELU activations instead of RELU and an additional dilated convolution at the end of the encoder block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Edit Acc {10 25 50} Edit Acc {10 25 50} Edit Acc Constrained k-medoids 95.5 87.5 70.0 100.0 76.9 97.5 90.4 75.6 100.0 81.3 99.8 97.7 83.0 100.0 75.3 Unconstrained k-medoids 8.4 6.6 3.8 12.3 53.8 3.8 2.5 1.0 2.3 52.9 71.0 68.2 52.9 59.8 69.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(12)L, Eq. (12) 78.4 L, Eq. (13)L, Eq. (13) 78.1 L, Eq. (12)L, Eq. (13) 79.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Fully supervised results on all three datasets. Best and second best results are shown in bold and underlined, respectively. With the assistance of Viterbi/FIFA our method outperforms state-of-the-art in terms of Edit and F1 scores on all datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Breakfast</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50Salads</cell><cell></cell><cell></cell><cell>GTEA</cell></row><row><cell></cell><cell></cell><cell cols="11">F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc</cell></row><row><cell>TDRN [23]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">72.9 68.5 57.2 66.0 68.1 79.2 74.4 62.7 74.1 70.1</cell></row><row><cell>SSA-GAN [13]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">74.9 71.7 67.0 69.8 73.3 80.6 79.1 74.2 76.0 74.4</cell></row><row><cell>MuCon [34]</cell><cell></cell><cell cols="5">73.2 66.1 48.4 76.3 62.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DTGRM [39]</cell><cell></cell><cell cols="11">68.7 61.9 46.6 68.9 68.3 79.1 75.9 66.1 72.0 80.0 87.3 85.5 72.3 80.7 77.5</cell></row><row><cell>Gao et al. [14]</cell><cell></cell><cell cols="11">74.9 69.0 55.2 73.3 70.7 80.3 78.0 69.8 73.4 82.2 89.9 87.3 75.8 84.6 78.5</cell></row><row><cell cols="2">MS-TCN++ [25]</cell><cell cols="11">64.1 58.6 45.9 65.6 67.6 80.7 78.5 70.1 74.3 83.7 88.8 85.7 76.0 83.5 80.1</cell></row><row><cell>BCN [41]</cell><cell></cell><cell cols="11">68.7 65.5 55.0 66.2 70.4 82.3 81.3 74.0 74.3 84.4 88.5 87.1 77.3 84.4 79.8</cell></row><row><cell>SSTDA [7]</cell><cell></cell><cell cols="11">75.0 69.1 55.2 73.7 70.2 83.0 81.5 73.8 75.8 83.2 90.0 89.1 78.0 86.2 79.8</cell></row><row><cell cols="2">Singhania et al. [32]</cell><cell cols="11">70.1 66.6 56.2 68.2 73.5 76.6 73.0 62.5 69.2 80.1 90.5 88.5 77.1 87.3 80.3</cell></row><row><cell>ASRF [17]</cell><cell></cell><cell cols="11">74.3 68.9 56.1 72.4 67.6 84.9 83.5 77.3 79.3 84.5 89.4 87.8 79.8 83.7 77.3</cell></row><row><cell>ASFormer [42]</cell><cell></cell><cell cols="11">76.0 70.6 57.4 75.0 73.5 85.1 83.4 76.0 79.6 85.6 90.1 88.8 79.2 84.6 79.7</cell></row><row><cell cols="2">ASFormer [42] + Viterbi</cell><cell cols="11">76.1 70.5 57.1 74.5 70.2 84.1 82.3 74.9 76.1 84.7 91.1 90.0 79.5 86.5 80.0</cell></row><row><cell cols="2">ASFormer [42] + FIFA</cell><cell cols="11">76.8 71.4 58.9 75.6 73.7 84.5 83.2 75.4 78.5 85.4 90.4 88.6 78.1 86.2 78.9</cell></row><row><cell></cell><cell>w/o duration</cell><cell>-</cell><cell>-</cell><cell cols="3">-76.9 -</cell><cell>-</cell><cell>-</cell><cell cols="3">-83.9 -</cell><cell>-</cell><cell>-</cell><cell>-92.1 -</cell></row><row><cell>UVAST (Ours)</cell><cell cols="12">+ alignment decoder 76.7 70.0 56.6 77.2 68.2 86.2 81.2 70.4 83.9 79.5 77.1 69.7 54.2 90.5 62.2 + Viterbi 75.9 70.0 57.2 76.5 66.0 89.1 87.6 81.7 83.9 87.4 92.7 91.3 81.0 92.1 80.2</cell></row><row><cell></cell><cell>+ FIFA</cell><cell cols="11">76.9 71.5 58.0 77.1 69.7 88.9 87.0 78.5 83.9 84.5 82.9 79.4 64.7 90.5 69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Constrained K-medoids results. We evaluate the pseudo-segmentations of our constrained k-medoids algorithm, Alg. 1, given the frame-wise input features and ground truth timestamps.</figDesc><table><row><cell cols="2">Dataset F1@{10,25,50} Acc</cell></row><row><cell cols="2">Breakfast 95.5 87.5 70.0 76.9</cell></row><row><cell cols="2">50Salads 97.5 90.4 75.6 81.3</cell></row><row><cell>GTEA</cell><cell>99.8 97.7 83.0 75.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Loss terms. Contribution of different loss terms on Breakfast and 50Salads (split 1).</figDesc><table><row><cell></cell><cell>Breakfast</cell><cell>50Salads</cell></row><row><cell></cell><cell cols="2">F1@{10,25,50} Edit F1@{10,25,50} Edit</cell></row><row><cell>Lframe</cell><cell cols="2">8.9 7.7 5.9 14.1 13.5 12.8 10.8 11.4</cell></row><row><cell>Lsegment</cell><cell cols="2">49.5 39.7 22.9 55.6 20.1 16.3 8.6 29.2</cell></row><row><cell>Lframe+Lsegment</cell><cell cols="2">71.8 66.3 52.6 73.4 55.0 52.4 37.5 45.3</cell></row><row><cell>Lframe+Lsegment+LCA</cell><cell cols="2">73.8 67.0 54.8 74.5 74.2 71.0 58.4 65.5</cell></row><row><cell>Lframe+Lsegment+Lg-frame</cell><cell cols="2">73.3 65.8 52.8 73.6 56.6 53.4 40.2 44.0</cell></row><row><cell>Lframe+Lsegment+Lg-segment</cell><cell cols="2">72.8 64.3 53.7 73.2 59.1 56.1 42.8 51.6</cell></row><row><cell>Lframe+Lsegment+Lg-frame+Lg-segment</cell><cell cols="2">73.5 67.9 55.0 73.1 57.0 54.5 40.4 42.4</cell></row><row><cell cols="3">Lframe+Lsegment+Lg-frame+Lg-segment+LCA 75.1 68.9 54.9 76.1 73.6 71.5 55.3 78.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Impact of Encoder Model. Quantitative comparison (Edit score) between the impact of different encoder blocks on split 1 of GTEA, 50Salads, and Breakfast.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Breakfast 50Salads GTEA</cell></row><row><cell></cell><cell>Simple Encoder</cell><cell>70.3</cell><cell>68.7</cell><cell>69.4</cell></row><row><cell>UVAST with</cell><cell cols="2">ASFormer [42] Encoder 74.6</cell><cell>73.8</cell><cell>88.6</cell></row><row><cell></cell><cell>Proposed Encoder</cell><cell>76.1</cell><cell>75.4</cell><cell>93.4</cell></row><row><cell>UVAST Timestamp</cell><cell cols="2">ASFormer [42] Encoder 72.9 Proposed Encoder 74.3</cell><cell>76.6 78.3</cell><cell>89.0 89.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Impact of Split-Segment.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Quantita-</cell></row><row><cell cols="4">tive comparison (Edit score) between the im-</cell></row><row><cell cols="4">pact of using split-segment versus not using it</cell></row><row><cell cols="4">on split 1 of GTEA, 50Salads, and Breakfast</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Breakfast 50Salads GTEA</cell></row><row><cell>No Split-Segment</cell><cell>75.0</cell><cell>74.2</cell><cell>88.2</cell></row><row><cell cols="2">With Split-Segment 76.1</cell><cell>75.4</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Impact of Split-Segment Values.</figDesc><table><row><cell>Ablation study on split 1 of the Breakfast</cell></row><row><cell>dataset regarding the impact of using differ-</cell></row><row><cell>ent values for the split-segment on the perfor-</cell></row><row><cell>mance (Edit score).</cell></row><row><cell>Split-Segment Values</cell></row><row><cell>0.05 0.1 0.15 0.17 0.2 0.3</cell></row><row><cell>Breakfast 74.9 75.6 76.1 76.1 76.1 75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>Hyper-Parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 .</head><label>11</label><figDesc>Hyper-Parameters used in FIFA and Viterbi.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Common Between Stage 1 and 2</cell><cell></cell><cell></cell><cell cols="3">Stage 1 (Encoder-Decoder)</cell><cell></cell><cell></cell><cell cols="3">Stage 2 (Alignment Decoder)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="13">Batch Optimizer LR Epoch sampling rate d d' ? ? Dropout Activation Split Segment # Layers in # Layers in Decoder Feedforward Cross-Attention Smoothing # Parameters</cell><cell># Layers in</cell><cell cols="2">Decoder Feedforward # Parameters</cell></row><row><cell></cell><cell>Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Encodeer Decoder</cell><cell>Dimension</cell><cell>Smoothing</cell><cell>Kernel</cell><cell></cell><cell>Alignment Decoder</cell><cell>Dimension</cell><cell></cell></row><row><cell cols="2">Breakfast 1</cell><cell>adam 0.0005 800</cell><cell>1</cell><cell>2048 64 0.001</cell><cell>?</cell><cell>GELU</cell><cell>0.17</cell><cell>10</cell><cell>2</cell><cell>2048</cell><cell>?</cell><cell>?</cell><cell>1.109M</cell><cell>1</cell><cell>1024</cell><cell>0.166M</cell></row><row><cell cols="2">50Salads 1</cell><cell>adam 0.0005 800</cell><cell>2</cell><cell>2048 64 0.001</cell><cell>?</cell><cell>GELU</cell><cell>0.15</cell><cell>10</cell><cell>2</cell><cell>2048</cell><cell>?</cell><cell>31</cell><cell>1.103M</cell><cell>1</cell><cell>1024</cell><cell>0.166M</cell></row><row><cell>GTEA</cell><cell>1</cell><cell>adam 0.0005 800</cell><cell>1</cell><cell>2048 64 0.001</cell><cell>?</cell><cell>GELU</cell><cell>0.17</cell><cell>10</cell><cell>2</cell><cell>2048</cell><cell>?</cell><cell>?</cell><cell>1.102M</cell><cell>1</cell><cell>1024</cell><cell>0.166M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell cols="2">FIFA</cell><cell></cell><cell>Viterbi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Epochs Sharpness Step-size frame sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Breakfast 3000</cell><cell>80</cell><cell></cell><cell>0.01</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">50Salads 3000</cell><cell>80</cell><cell></cell><cell>0.01</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GTEA</cell><cell>3000</cell><cell>80</cell><cell></cell><cell>0.1</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Please see the supplementary material for more details, hyper-parameters, and ablations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. JG has been supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) GA1927/4-2 (FOR 2535 Anticipating Human Behavior) and the ERC Starting Grant ARCA (677650).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2 Implementation Details All of the training and testing experiments were conducted on a single NVIDIA V100 GPU.</p><p>Impact of Encoder Model. In our proposed algorithm, we utilized a modified version of the encoder model proposed in <ref type="bibr" target="#b41">[42]</ref>. The encoder model proposed by <ref type="bibr" target="#b41">[42]</ref> takes advantage of window attention as well as hierarchical representation for the action segmentation task. While we find their proposed encoder model very effective and inspiring, we made small modifications to the architecture that further improved the performance. Particularly, we replace the RELU activation layers with GELU activation and add one more layer of dilated convolution at the end of each encoder block. <ref type="figure">Fig. 6</ref> shows the side by side comparison between the encoder block proposed by <ref type="bibr" target="#b41">[42]</ref> and our modified version. Moreover, <ref type="table">Table 7</ref> shows the Edit performance for using a simple encoder</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MS-TCN: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019) 3, 4, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action segmentation with joint selfsupervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fine-grained action segmentation using the semi-supervised action gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Global2local: Efficient structure search for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alleviating over-segmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV (2021) 3, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hybrid rnn-hmm approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Weakly supervised energy-based learning for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<title level="m">MS-TCN++: Multi-stage temporal convolutional network for action segmentation. TPAMI (2020) 3, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal action segmentation from timestamp supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nawrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tworkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyrolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13711</idno>
		<title level="m">Hierarchical transformers are more efficient language models</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">NeuralNetwork-Viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10859</idno>
		<title level="m">Coarse to fine multi-resolution temporal convolutional network</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FIFA: Fast inference approximation for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Souri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Despinoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR (2021)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast weakly supervised action segmentation using mutual consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Souri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Temporal relational modeling with self-supervision for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07508</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ASFormer: Transformer for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC (2021) 3, 4, 5, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
