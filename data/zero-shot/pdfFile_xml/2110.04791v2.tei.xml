<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stepwise-Refining Speech Separation Network via Fine-Grained Encoding in High-order Latent Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengwei</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Fanglin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Guangming</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Life Fellow, IEEE</roleName><forename type="first">David</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Stepwise-Refining Speech Separation Network via Fine-Grained Encoding in High-order Latent Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech separation</term>
					<term>high-order latent domain</term>
					<term>coarse-to-fine</term>
					<term>end-to-end</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The crux of single-channel speech separation is how to encode the mixture of signals into such a latent embedding space that the signals from different speakers can be precisely separated. Existing methods for speech separation either transform the speech signals into frequency domain to perform separation or seek to learn a separable embedding space by constructing a latent domain based on convolutional filters. While the latter type of methods learning an embedding space achieves substantial improvement for speech separation, we argue that the embedding space defined by only one latent domain does not suffice to provide a thoroughly separable encoding space for speech separation. In this paper, we propose the Stepwise-Refining Speech Separation Network (SRSSN), which follows a coarse-to-fine separation framework. It first learns a 1order latent domain to define an encoding space and thereby performs a rough separation in the coarse phase. Then the proposed SRSSN learns a new latent domain along each basis function of the existing latent domain to obtain a high-order latent domain in the refining phase, which enables our model to perform a refining separation to achieve a more precise speech separation. We demonstrate the effectiveness of our SRSSN by conducting extensive experiments, including speech separation in a clean (noise-free) setting on WSJ0-2/3mix datasets as well as in noisy/reverberant settings on WHAM!/WHAMR! datasets. Furthermore, we also perform experiments of speech recognition on separated speech signals by our model to evaluate the performance of speech separation indirectly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stepwise-Refining Speech Separation Network via</head><p>Fine-Grained Encoding in High-order Latent Domain Zengwei Yao, Wenjie Pei , Fanglin Chen, Member, IEEE, Guangming Lu , Member, IEEE, and David Zhang, Life Fellow, IEEE Abstract-The crux of single-channel speech separation is how to encode the mixture of signals into such a latent embedding space that the signals from different speakers can be precisely separated. Existing methods for speech separation either transform the speech signals into frequency domain to perform separation or seek to learn a separable embedding space by constructing a latent domain based on convolutional filters. While the latter type of methods learning an embedding space achieves substantial improvement for speech separation, we argue that the embedding space defined by only one latent domain does not suffice to provide a thoroughly separable encoding space for speech separation. In this paper, we propose the Stepwise-Refining Speech Separation Network (SRSSN), which follows a coarse-to-fine separation framework. It first learns a 1order latent domain to define an encoding space and thereby performs a rough separation in the coarse phase. Then the proposed SRSSN learns a new latent domain along each basis function of the existing latent domain to obtain a high-order latent domain in the refining phase, which enables our model to perform a refining separation to achieve a more precise speech separation. We demonstrate the effectiveness of our SRSSN by conducting extensive experiments, including speech separation in a clean (noise-free) setting on WSJ0-2/3mix datasets as well as in noisy/reverberant settings on WHAM!/WHAMR! datasets. Furthermore, we also perform experiments of speech recognition on separated speech signals by our model to evaluate the performance of speech separation indirectly.</p><p>Index Terms-Speech separation, high-order latent domain, coarse-to-fine, end-to-end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH separation aims to separate out the clean speech signals for each involved speaker from a mixture of speech signals. It plays an important role in speech processing <ref type="bibr" target="#b0">[1]</ref>, especially in the scenario of mixed and noisy speech environment. Speech separation, particularly under the single-channel condition, is still a highly challenging research problem due to the difficulty of encoding the mixed speech signal into an entirely separable embedding feature space. This paper focuses on single-channel speech separation.</p><p>A classical type of methods <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b7">[8]</ref> for single-channel speech separation is to transform the input mixture of temporal speech signals into the frequency domain employing the Short-Time Fourier Transform (STFT) <ref type="bibr" target="#b8">[9]</ref> and then perform speech separation in the frequency domain. Whilst this type of methods achieves great improvement on speech separation due to relatively mature techniques on time-frequency transformation and signal processing in frequency domain, it suffers from two limitations for speech separation: 1) most existing methods focus on reconstructing the magnitude of the signal in the frequency domain but ignore the modeling of the phase (which is the other crucial physical property of frequency signals) since there is no sufficiently effective way of modeling the phase yet; 2) performing speech separation in the frequency domain is an effective but not necessarily the optimal way, and it is still doubtful whether the frequency domain is able to provide entirely separable space for speech signals <ref type="bibr" target="#b9">[10]</ref>.</p><p>With great success of deep learning in many fields such as computer vision and machine learning by the powerful capability of feature representation, another research line of speech separation <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b19">[20]</ref> seeks to leverage deep convolutional neural networks (CNNs) to learn an embedding space that is separable for speech signals between different speakers. A remarkable benefit of such methods is that the whole separation procedures including encoding, separation and decoding can be integrated into an end-to-end model, which is in contrast to the sequentially individual steps in aforementioned STFT-based methods. A prominent example of such method is Time-domain Audio Separation Network (TASNET) <ref type="bibr" target="#b10">[11]</ref>, which employs a 1-D convolutional network consisting of multiple convolutional filters to transform the temporal signals within a time slot into a latent embedding space. This latent space can be considered as a 1-order latent domain with the convolutional filters as its basis functions. The input signals are first encoded into such embedding space defined by this latent domain and then the separation and decoding are performed subsequently in this embedding space. Many follow-up works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> focus on building better separators upon TASNET to further improve the performance of speech separation. While such TASNETbased methods have boosted the performance of speech separation substantially, we argue that whether the embedding space defined by only 1-order latent domain suffices to provide a thoroughly separable feature space, particularly in challenging scenarios where speakers with similar speech characteristics are involved in. An empirical investigation is conducted by ablation study both quantitatively and qualitatively in Sec-tion IV-B1 and the experimental results validate our doubt.</p><p>In this paper, we propose the Stepwise-Refining Speech Separation Network (SRSSN), which performs speech separation in a stepwise manner following a coarse-to-fine framework. In the coarse phase, it first conducts a rough separation in a coarse embedding space defined in a 1-order latent domain, which is similar as the typical way of TASNETbased methods <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref>. In the refining phase, our SRSSN learns a new latent domain along the basis functions of the existing domain in the coarse phase to form a high-order domain space. Then the coarse embedding space is further decomposed into a fine-grained embedding space defined by the constructed high-order domain. As a result, our proposed SRSSN is able to re-code the coarsely separated features in the fine-grained embedding space and achieve a more precise separation. To conclude, our SRSSN benefits from following advantages: ? We design a coarse-to-fine separation framework, which first conducts a rough speech separation and then performs the refining separation in the constructed fine-grained embedding space to achieve more precise separated results. ? A Fine-grained Encoding Mechanism is designed specifically to construct a fine-grained embedding space by learning a high-order latent domain space, which enables our SRSSN to refine the separation results. ? Our proposed model can be readily integrated into any of existing TASNET-based frameworks following the encodingseparation-decoding paradigm. In particular, we investigate the performance of our model by integrating it into two typical separator structures: DPRNN-TASNET <ref type="bibr" target="#b14">[15]</ref> based on RNN and DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref> based on Transformer <ref type="bibr" target="#b20">[21]</ref>. ? Extensive experiments validate the superiority of encoding in the learned high-order latent domain over in a 1order latent domain (as most existing methods do). It is demonstrated that our SRSSN compares favorably against state-of-the-art methods for speech separation in both the clean (noise-free) setting on WSJ0-2/3mix dataset and noisy/reverberant settings on WHAM!/WHAMR! datasets. Besides, we also perform experiments of speech recognition as an indirect evaluation way, in which a same automatic speech recognition (ASR) model achieves better performance on the separated speech signals by our model than on the separated signals by other models for speech separation.</p><p>II. RELATED WORK Frequency-domain-based methods. Frequency domainbased methods transform the mixed speech signal into frequency domain using the STFT <ref type="bibr" target="#b8">[9]</ref>. They perform separation in frequency domain and transform the separated representations back into speech signals employing the Inverse Shorttime Fourier Transform (iSTFT) <ref type="bibr" target="#b21">[22]</ref>. Hershey et al. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> devise a Deep Clustering (DPCL) method which learns a speaker-discriminative vector for each time-frequency bin of the mixture spectrogram and employs clustering to obtain speaker assignments for each time-frequency bin. Kolbaek et al. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> propose a Permutation Invariant Training (PIT) approach to align multiple estimates for different target speakers, which enumerates all source permutations and uses the permutation with the minimum error to update the network parameters. Wang et al. <ref type="bibr" target="#b6">[7]</ref> integrate the PITbased mask-inference network <ref type="bibr" target="#b4">[5]</ref> and DPCL <ref type="bibr" target="#b2">[3]</ref> regularizer into a multi-task learning framework for better separation performance. Inspired by the Computational Auditory Scene Analysis (CASA) <ref type="bibr" target="#b22">[23]</ref>, Liu et al. <ref type="bibr" target="#b7">[8]</ref> propose a Deep CASA approach, which first estimated spectra for each speaker at each frame and then groups the estimated frame-level spectra to different speakers employing clustering. Learnable-latent-domain-based methods. Learnable latent domain-based methods leverage CNN layers to learn a latent domain for speech separation. Luo et al. <ref type="bibr" target="#b10">[11]</ref> first propose the TASNET, which takes speech signal as input and reconstructs the separated sources in an end-to-end manner. Most variants of TASNET focus on building effective separation network to model the extremely long encoded sequence and thus to perform a precise representation separation. Luo et al. <ref type="bibr" target="#b9">[10]</ref> propose a fully-convolutional TASNET (Conv-TASNET), consisting of stacking 1-D dilated temporal convolutional layers similar to Wavenet <ref type="bibr" target="#b23">[24]</ref>. Shi et al. <ref type="bibr" target="#b12">[13]</ref> devise an improved version of Conv-TASNET equipped with dynamic gated mechanism. Tzinis et al. <ref type="bibr" target="#b16">[17]</ref> propose a computationally efficient backbone structure similar to U-Net <ref type="bibr" target="#b24">[25]</ref>, which extracts multiresolution temporal features through successive downsampling and upsampling. Luo et al. <ref type="bibr" target="#b14">[15]</ref> develop a Dual-Path Recurrent Neural Network (DPRNN), which performs local and global temporal modeling alternately, to capture long-time dependency over entire sequence instead of fixed receptive fields as in Conv-TASNET <ref type="bibr" target="#b9">[10]</ref>. Chen et al. <ref type="bibr" target="#b17">[18]</ref> devise a Dual-Path Transformer Network (DPTNET), which also adopt the dualpath strategy to handle the long sequence as in DPRNN <ref type="bibr" target="#b14">[15]</ref> and utilize improved transformer layers incorporating RNN layers to model the context in an order-aware manner. Subakan et al. <ref type="bibr" target="#b18">[19]</ref> propose the Sepformer, which is an extension of DPTNET <ref type="bibr" target="#b17">[18]</ref> by employing deeper and wider Transformer layers, achieves better performance than DPTNET at the cost of much larger model size.</p><p>Meanwhile, several studies explore to leverage the discriminative speaker representation to improve separation performance. Nachmani et al. <ref type="bibr" target="#b15">[16]</ref> introduce a task to minimize the distance between the speaker embeddings of the estimated signal and target signal, where the speaker embeddings are extracted from a separately trained speaker identification network. Zeghidour et al. <ref type="bibr" target="#b19">[20]</ref> propose the Wavesplit network consisting of a speaker stack and a separation stack, where the speaker stack extracts frame-level speaker-discriminative vectors and obtain speaker centroids employing clustering, and the separation stack estimates isolated speech signals conditioned the speaker centroids. Two-stage or cascaded architectures. Some studies adopt two-stage or cascaded strategies for speech separation and enhancement. Zhao et al. <ref type="bibr" target="#b25">[26]</ref> introduce a two-stage framework to enhance noisy-reverberant speech, where denoising and dereverberation are performed sequentially. Kavalerov et al. <ref type="bibr" target="#b26">[27]</ref> devise a iterative version of TASNET cascading of two same separation model, where the initial estimates obtained in the first model along with the mixture are fed into the second   <ref type="figure">Fig. 1</ref>: Architecture of our proposed SRSSN. It first performs a rough separation in the coarse phase, then the coarsely separated features are further encoded in a fine-grained embedding space to perform a more precise separation in the refining phase. The Fine-grained Encoding Mechanism is specifically designed to learn the fine-grained encoding space for refining separation by constructing a high-order latent domain upon the 1-order latent domain.</p><p>model for a more precise separation. Fan et al. <ref type="bibr" target="#b27">[28]</ref> propose to separate the mixture preliminarily in frequency domain and design an End-to-End Post-Filter (E2EPF) to leverage the similarity between the mixture and preliminary estimates to further improve the separation performance. Delfarah et al. <ref type="bibr" target="#b28">[29]</ref> introduce a two-stage deep CASA <ref type="bibr" target="#b7">[8]</ref> method to separate mixed speech in reverberant condition, where the first stage estimates reverberant separated speech and the second stage dereverberates the separated speech to obtain clean anechoic speech. Phan et al. <ref type="bibr" target="#b29">[30]</ref> utilize chained generators to gradually refine noisy speech in a stage-wise manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STEPWISE-REFINING SPEECH SEPARATION NETWORK</head><p>Given a mixture of single-channel speech signal from different speakers, we aim to separate and extract the clean signal for each involved speaker. The crux of this problem is how to encode the mixture signal into such a latent embedding space that different speech sources are entirely separable. To surmount this crux, our proposed Stepwise-Refining Speech Separation Network (SRSSN) performs fine-grained encoding in a high-order latent domain rather than in a 1-order latent domain (as most existing methods do) for signals in each time slot, to obtain more precise separation.</p><p>Specifically, our SRSSN performs speech separation in a stepwise-refining manner following a coarse-to-fine framework. It first conducts a rough separation in a coarse embedding space defined in a low-order (1-order) latent domain, then the coarse embedding space is further decomposed into a fine-grained embedding space defined in a high-order latent domain, which is achieved based on our designed Fine-grained Encoding Mechanism. As a result, the proposed SRSSN is able to perform a more precise speech separation. <ref type="figure">Figure 1</ref> illustrates the overall architecture of SRSSN. We will first present the coarse-to-fine separation framework, then we will elaborate on the Fine-grained Encoding Mechanism to explain how to construct the high-order latent domain for refining the result of speech separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coarse-to-fine Separation Framework</head><p>Our SRSSN consists of two separation phases: 1) the coarse phase for a rough separation of signals among speakers in a coarse embedding space defined by a 1-order latent domain, and 2) the refining phase in which a fine-grained embedding space is constructed in a high-order latent domain for a more precise separation. 1) Coarse phase: As shown in <ref type="figure">Figure 1</ref>, we build both the coarse and refining phases of our model based on the basic encoder-separator-decoder framework adopted by the classical TASNET-based models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Formally, given a mixture of signal x ? R 1?T of temporal length T , the coarse encoder E c of SRSSN encodes the signal into a coarse embedding space with N c basis functions by a nonlinear transformation:</p><formula xml:id="formula_0">F c = E c (x).<label>(1)</label></formula><p>Herein F c ? R N c ?T is the encoded feature representation and T is the encoded temporal length. Following the typical encoding scheme <ref type="bibr" target="#b17">[18]</ref>, we model the coarse encoder E c by a 1-D convolutional layer with N c filters of kernel size 1 ? K c , followed with the nonlinear activation layer ReLU. Here the N c convolutional filters can be viewed as the basis functions to form a latent domain, which presumably simulates the frequency domain due to the similar mathematical transformations between the encoder and the Short-Time Fourier Transformation <ref type="bibr" target="#b8">[9]</ref>. The encoded representations F c are then fed into the coarse separator S c of SRSSN to estimate the signal mask M c i ? R N c ?T , i = 1, . . . , D for each speaker, where D is the number of involved speakers in the input mixed signal x.</p><p>The mask values are constrained to be non-negative to indicate the content proportion for each speaker with respect to each basis function by applying a non-linear activation layer ReLU. Thus the separation in the coarse phase is performed as:</p><formula xml:id="formula_1">M c 1 , . . . , M c D = S c (F c ), F c i = F c M c i , i = 1, . . . , D,<label>(2)</label></formula><p>where F c i ? R N c ?T is the separated features for i-th speaker and denotes element-wise multiplication.</p><p>Modeling of Separator S c . The separator S c can be implemented in the same structure with any separator of existing TASNET-based models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. To investigate the effectiveness of our model extensively, we evaluate our model equipped with two typical separator structures respectively: the separator structure in DPRNN-TASNET <ref type="bibr" target="#b14">[15]</ref> and DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref> due to their excellent performance.</p><p>Both of the separator structures employ a same core block iteratively (for R times) for modeling temporal dependencies in a speech sequence. The main difference between these two types of separators lies in the structure of the core block: the separator in DPRNN-TASNET employs Bi-directional LSTM <ref type="bibr" target="#b30">[31]</ref> to construct its core block (DPRNN block), while the core block (named DPTNET block) of the separator in DPTNET-TASNET is designed based on Transformer <ref type="bibr" target="#b20">[21]</ref>. Specifically, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the encoded representation F c is firstly normalized by Layer-Normalization <ref type="bibr" target="#b31">[32]</ref> and fed into a linear layer to adjust the number of channels. Then it was equally segmented into successive chunks with temporal length of L and overlap of L 2 between adjacent chunks. Both the DPRNN block and the DPTNET block adopt a dual-path strategy to capture the long-term temporal dependencies. Each block consists of two paths: an intrapath used for capturing the intra-dependencies within each chunk and an inter-path for modeling the inter-dependencies among chunks. After temporal modeling by R core blocks, a nonlinear activation layer PReLU <ref type="bibr" target="#b32">[33]</ref> and a linear layer are used to expand the number of channels by D times to make the separated encoding representation F c i , i = 1, . . . , D keep the same feature dimension as the un-separated representation F c . Subsequently, the chunks are transformed back into sequential shape through the overlap-add operation <ref type="bibr" target="#b14">[15]</ref>. Finally, a linear layer followed by a activation layer ReLU is utilized to estimate the mask values.</p><p>The separated representations F c i are finally decoded into the speech sources? c i ? R 1?T , i = 1, . . . , D for each speaker by the coarse decoder D c :</p><formula xml:id="formula_2">s c i = D c (F c i ), i = 1, . . . , D.<label>(3)</label></formula><p>We model the decoder D c as a 1-D transposed convolutional layer which is the reversed operation of the encoder. Note that the decoding operation in the coarse phase is only performed in training for supervision. During inference, the separated latent representations F c i are fed into the refining phase for the fine-grained separation.</p><p>2) Refining phase: The performance of the whole model relies on the effectiveness of feature separation in the encoding embedding space. Thus, it is crucial that the speech sources can be precisely separable between different speakers in the encoded embedding space. However, we argue that the coarse embedding space encoded by only one latent domain does not suffice to provide a thoroughly separable feature space for all speakers, which is validated empirically by the ablation experiments in Section IV-B1. To tackle this limitation, we propose the Fine-grained Encoding Mechanism (presented in Section III-B) to construct a high-order latent domain and thereby define a fine-grained embedding space upon the coarse space to make a more precise separation.</p><p>Taken the separated representations in the coarse embedding space F c as inputs, the encoder E r in the refining phase re-codes the features for each speaker in the fine-grained embedding space:</p><formula xml:id="formula_3">F r i = E r (F c i ), i = 1, . . . , D,<label>(4)</label></formula><p>where the refining encoder E r will be described concretely in Section III-B.</p><p>Benefitting from the fine-grained feature decomposition from F c to F r , a refining separation can be performed on F r for each speaker by the separator S r in the refining phase, which is similar to the separation procedure in the coarse phase. For instance, the encoded feature for the i-th speaker F r i is further separated by S r and the component for the j-th speaker is F r i,j :</p><formula xml:id="formula_4">M r i,1 , . . . , M r i,D = S r (F r i ), F r i,j = F r i M r i,j j = 1, . . . , D.<label>(5)</label></formula><p>Accordingly, the refined encoded feature for j-th speaker is obtained by the summation over all j-th components (separated ingredients) from all speakers: </p><formula xml:id="formula_5">G r j = D i=1 F r i,j .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-D conv-filters ReLU Low order (1-order) latent domain</head><p>High-order (2-order) latent domain ? ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refining encoder</head><p>(a) Fine-grained encoding mechanism ( ) , ( ) , ( ) The decoder D r in the refining phase is then employed to decode the refined features into speech sources? r j ? R 1?T , j = 1, . . . , D:</p><formula xml:id="formula_6">, ( ) ( ) , ( ) , ( ) , ( ) ( ) , ( ) , ( ) , ( ) ? ? ? (b) Refining separator S r ( ) ( ) ( ) ( ) ( ) ( ) ? ? Speech signal ? ? (c) Refining decoder D r</formula><formula xml:id="formula_7">s r j = D r (G r j ), j = 1, . . . , D.<label>(7)</label></formula><p>The obtained speech sources? r are expected to be cleaner and more precise than the coarse version derived in the coarse phase due to the refining separation, which is verified in experiments of Section IV-B1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-grained Encoding Mechanism</head><p>The Fine-grained Encoding Mechanism is devised to construct a more fine-grained encoding space from the coarse encoding space for more precise and thorough separation among different speakers. Specifically, we learn a new latent domain along each basis function of the existing latent domain that defines the coarse embedding space. Consequently, the derived new latent domain and the old latent domain jointly form a high-order domain, which defines a fine-grained encoding space by decomposing the coarse embedding space along the newly learned latent domain.</p><p>The coarse encoder E c in the coarse phase transforms nonlinearly the input speech signal into an encoding space by learning N c 1-D convolutional filters together with ReLU. These N c convolutional filters can be viewed as basis functions to form a latent domain denoted as H c . As a result, the learned encoding space in the coarse phase can be considered to be defined by such latent domain H c comprising N c basis functions. We adopt the similar way to construct a new latent domain H r upon H c . Formally, we learn a 1-D convolutional layer composed of N r filters of kernel size 1 ? K r together with a nonlinear activation layer ReLU to construct a new latent domain with N r basis functions. The newly constructed latent domain H r is applied to each of N c basis functions of the old domain H c for a fine-grained decomposition. Consequently, the new domain H r enables one more order of partitions of the encoding space upon the old domain H c , and thus a high-order (2-order) latent domain H high is constructed. The N c basis functions of H c and the N r basis functions of H r correspond to basis functions in different orders of H high respectively: one may view H high as characterizing the encoding space by N c ? N r individual features to achieve a fine-grained encoding space.</p><p>Mathematical formulation. The whole procedure is mathematically formulated as:</p><formula xml:id="formula_8">N c basis functions of domain H c := N c conv-filters of E c , N r basis functions of domain H r := N r conv-filters of E r , high-order domain H high := H r ? H c , F r i (n) = E r (F c i (n)), i = 1, . . . , D; n = 1, . . . , N c . Feat n,m = F r i (n, m), i = 1, . . . , D; n = 1, . . . , N c ; m = 1, . . . , N r .<label>(8)</label></formula><p>Herein, H r ? H c denotes that the domain H r is applied to each basis function of H c in parallel. F c i (n) ? R 1?T is the encoded representations along the n-th basis function for the i-th speaker in domain H c in the coarse phase, while Feat n,m ? R 1?T is the corresponding encoded representations in fine-grained embedding space along the n-th basis function in the first order and the m-th basis function in the second order of the high-order domain H high (in the refining phase). It should be noted that all the basis feature functions in the old domain are decomposed by the same refining encoder E r to ensure that all decompositions are operated in the same new latent domain. To reduce the model complexity and avoid potential overfitting, in practice we apply the similar idea as group convolution <ref type="bibr" target="#b33">[34]</ref> to perform transformation in groupwise manner. Concretely, we first divide N c basis functions of the old domain into P groups and then apply the same refining encoder E r to each group</p><formula xml:id="formula_9">F c i (p) ? R N c P ?T to obtain a set of N r decomposed features F r i (p) ? R N r ?T : F r i (p) = E r (F c i (p)), i = 1,</formula><p>. . . , D; p = 1, . . . , P. <ref type="formula">(9)</ref> Consequently, the size of the resulting encoded representations in the new domain F r is N r ? P ? T . Physical interpretation. The rationale behind this design is that different basis functions correspond to different components in the latent domain, which is similar to different frequency bands in the frequency domain. Thus the speech signal for a speaker is characterized by the distribution of different components (along different basis functions) in a latent domain. Nevertheless, we argue that the capacity of feature representation in one latent domain is not sufficient to separate all speakers perfectly. For those mixtures of speech signal that cannot be thoroughly separated in one latent domain, our designed fine-grained embedding space encoded by a high-order latent domain enables more precise separation. Comparison with the modeling mechanism of scaling up the number of basis functions in the old 1-order domain. A straightforward way to expand the capacity of feature representation in the embedding space is to directly scale up the number of basis functions in the old domain H c . Whilst it seems plausible, it has two limitations compared to our proposed Fine-grained Encoding Mechanism: 1) This mechanism can only improve the feature representation in the same embedding space along the old latent domain which is prone to be saturated and overfitting. By contrast, our model exploits a larger embedding space by learning a new latent domain and applying it to each basis function of the old domain to form a high-order latent domain. Thus the feature representation power is increased quadratically instead of linearly. 2) This mechanism increases the parameter size of the encoder proportionally to the scaling factor. Nevertheless, the parameter size of the encoder in our model grows linearly to the scaling factor of the representation capacity, which benefits from the design that the same refining encoder E r encoding the new latent domain is applied to all the basis functions of the old domain. Refining separator S r in the fine-grained embedding space. As shown in <ref type="figure" target="#fig_3">Figure 3 (b)</ref>, the refining separator S r processes the encoded features in the fine-grained embedding space in parallel for all groups of basis functions of the old domain H c , i.e., the P feature blocks F r i (p) ? R N r ?T , p = 1, . . . , P for the i-th speaker, are processed in parallel:</p><formula xml:id="formula_10">M r i,j (p) = S r (F r i (p)), F r i,j (p) = F r i (p) M r i,j (p), p = 1, . . . , P.<label>(10)</label></formula><p>The refining separator S r shares the same model structure as the coarse separator S c : consisting of R stacked blocks of DPRNN <ref type="bibr" target="#b14">[15]</ref> or DPTNET <ref type="bibr" target="#b17">[18]</ref>. The refined separated representation for j-th speaker is obtained by the summation over all j-th components, namely the separated ingredient of j-th speaker, from all speakers in parallel along each of P feature blocks. Thus, Equation 6 is implemented as:</p><formula xml:id="formula_11">G r j (p) = D i=1</formula><p>F r i,j (p), p = 1, . . . , P.</p><p>Refining decoder D r in the fine-grained embedding space.</p><p>Since our fine-grained embedding space is constructed based upon the coarse embedding space, our refining decoder D r performs decoding reversely. Hence, the refining decoder D r is a two-stage decoder consisting of D r 1 and D r 2 . Specifically, the sub-decoder D r 1 decodes the feature representation from finegrained embedding space back to the coarse embedding space, which is applied to all feature groups in parallel. Then the subdecoder D r 2 decodes features from the coarse embedding space to final speech signal for each speaker. The whole decoding procedure is carried out as follows:</p><formula xml:id="formula_13">G c j (p) = D r 1 (G r j (p)), p = 1, . . . , P, s r j = D r 2 (G c j )<label>(12)</label></formula><p>As indicated in <ref type="figure" target="#fig_3">Figure 3</ref> (c), D r 1 is modeled by a 1-D transposed convolutional layer followed by a nonlinear activation layer ReLU, and D r 2 is modeled by a 1-D transposed convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. End-to-end Parameter Learning by Joint Supervision</head><p>We optimize the whole model of SRSSN in an end-to-end manner by performing supervised learning on both the coarse phase and the refining phase. Specifically, we employ the scale-invariant source-to-noise ratio (SI-SNR) <ref type="bibr" target="#b34">[35]</ref> as the loss function, which is widely used and validated effectively for end-to-end speech separation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Formally, given a target signal (groundtruth) s and the estimated signal?, the SI-SNR is defined as follows: </p><p>where s target is the projected correct ingredient of estimated signal? on the target signal and s noise is the residual noise ingredient of?. Following <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we adopt utterance-level PIT [5] to align multiple estimates for different target speakers in both the coarse phase and the refining phase. Consequently, our SRSSN is optimized by maximizing the SI-SNR scores in both the coarse phase and the refining phase:</p><formula xml:id="formula_15">L c = ? max ? c ?P 1 D D i=1 SI-SNR(s i ,? c ? c (i) ), L r = ? max ? r ?P 1 D D i=1</formula><p>SI-SNR(s i ,? r ? r (i) ),</p><formula xml:id="formula_16">L = L c + L r .<label>(14)</label></formula><p>where L c and L r are the loss functions used in the coarse phase and refining phase, respectively. ? c and ? r are permutations from the set P of all D! possible permutations among D speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To evaluate the effectiveness of our proposed SRSSN, we conduct three sets of experiments: 1) speech separation in the clean (noise-free) setting involving 2 and 3 speakers respectively, 2) speech separation between 2 speakers in noisy and reverberant settings, and 3) speech recognition on separated speech signals decoded by methods for speech separation to evaluate the performance of speech separation indirectly. We also perform ablation study on the task of speech separation in the clean setting to investigate the effect of each proposed functional technique in our proposed SRSSN. In each set of experiments, we evaluate the performance of our SRSSN adopting the separator structure of DPRNN-TASNET <ref type="bibr" target="#b14">[15]</ref> and DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref> respectively to evaluate the robustness of our model utilizing different classical separator structures. These two versions of our model are denoted as DPRNN-SRSSN and DPTNET-SRSSN respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Evaluation Metrics. For speech separation, we employ two standard metrics for evaluation, namely scale-invariant signalto-noise ratio improvement (?SI-SNR) <ref type="bibr" target="#b34">[35]</ref> and signal-todistortion ratio improvement (?SDR) <ref type="bibr" target="#b35">[36]</ref>. Higher value of ?SI-SNR or ?SDR indicates higher quality of the separated results. For speech recognition, we employ Word Error Rate (WER) of the predicted transcripts relative to the reference transcripts for evaluation. Lower value of WER implies better recognition result and higher quality of the separated speech. Implementation Details Our model is implemented in Pytorch framework <ref type="bibr" target="#b36">[37]</ref>. It is trained using Adam <ref type="bibr" target="#b37">[38]</ref> optimizer with a learning rate of 10 ?3 and a weight decay of 10 ?5 on 2-second temporal segments for 200 epochs. We clip all gradients to lie in the interval [?5, 5] to avoid potential gradient explosion. For the encoder E c in the coarse phase, the number of filters N c , kernel size K c , and stride size are set to be 256, 16, and 8, respectively. For the encoder E r in the refining phase, the number of filters N r , kernel size K r , stride size, and number of groups P are tuned to be 256, 2, 1, and 4, respectively. For both separators S c and S r , the numbers of core blocks R are set to 6 except for the experiment on ablation study (Section IV-B1), and the length of chunks L is set to 100. In the DPRNN blocks, each Bi-LSTM layer is equipped with 128 hidden units in each direction. In the DPTNET blocks, each improved Transformer layer <ref type="bibr" target="#b17">[18]</ref> consists of a 4-head self-attention layer with total embedding dimension of 64, a Bi-LSTM layer with 128 hidden units in each direction, and a linear layer with 64 hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speech Separation in Clean Setting</head><p>We first conduct experiments on speech separation in clean setting, i.e., no noise is contained in the mixture of speech signals except the signals of involved speakers to be separated. We first perform ablation study to investigate the effectiveness of the coarse-to-fine separation framework and the proposed Fine-grained Encoding Mechanism. Then we compare our SRSSN to the state-of-the-art methods for speech separation in this experimental setting. Dataset. We perform experiments on WSJ0-2mix and WSJ0-3mix <ref type="bibr" target="#b1">[2]</ref> in the clean setting, which is the reference mixed speech datasets for single-channel speech separation. WSJ0-2mix is generated from Wall Street Journal (WSJ) dataset <ref type="bibr" target="#b38">[39]</ref>, and consists of mixed speech utterances from two different speakers with random signal-to-noise ratio between 0 dB and 5dB. The data in WSJ0-2mix is split into three sets with duration of 30 hours, 10 hours and 5 hours for training, validation and test, respectively. WSJ0-3mix, containing threespeaker mixtures, is generated in a similar way as in WSJ0-2mix. We use the min version of speech data with sampling rate of 8kHz, the benchmark for speaker separation, in which the longer utterance is trimmed to align the shorter utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Ablation Study: We perform ablation experiments on nine variants of our SRSSN for both DPRNN-SRSSN and DPTNET-SRSSN on WSJ0-2mix [2]:</head><p>? Base model, which has only coarse phase and thus no Fine-grained Encoding Mechanism is used. As a result, the base model is equivalent to DPRNN-TASNET or DPTNET-TASNET, depending on the separator structure (based on DPRNN or DPTNET blocks). The stride size for Base model in both cases are set to be 8, which is consistent with all other models in ablation study. ? Base-expanded, which is similar to Base model with one difference: the encoding space in the coarse phase is expanded by scaling up the number of basis functions in the latent domain to exploit the limit of sufficiently large encoding space, namely using much more CNN filters (N c = 1024) for the encoder.  separation scheme. Specifically, the encoder is modeled by cascading the coarse encoder E c and refining encoder E r , to encode the speech signal into the fine-grained embedding space. The refining separator S r and the refining decoder D r are applied subsequently. ? Iterative, which adopts the iterative scheme used in iT-DCN++ <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Specifically, the separation procedure in Base model is repeated twice, where the mixed speech and the initial estimates in the first phase are fed into the second phase. Two phases are cascaded into an end-toend model. ? SRSSN-1D, which has both the coarse and refining phases but no Fine-grained Encoding Mechanism is used in the refining phase. The refining phase has the same model structure as the coarse phase. Thus both two phases encode features in a 1-order latent domain. Note that SRSSN-1D is different from Iterative in that the refining phase of SRSSN-1D accepts the coarsely separated latent representations as input whilst the second phase of Iterative takes the decoded estimated signals as the input. ? SRSSN-1D-expanded, which is similar to SRSSN-1D with one difference: the encoding space in the refining phase is expanded by scaling up the number of basis functions in the latent domain. Similar to Base-expanded, much larger number of CNN filters (N r = 1024) are used for the refining encoder. ? SRSSN-L r , which is the same as the proposed SRSSN, except that the model is trained with only loss function L r in the refining phase, discarding the loss function L c in the coarse phase. ? SRSSN, which is our intact model: the Coarse-to-fine framework is applied and the Fine-grained Encoding Mechanism is leveraged to construct a fine-grained encoding space defined by a learned high-order latent domain, which enables fine-grained separation.</p><p>Since the separator accounts for most of model parameters, the total number of core blocks (DPRNN or DPTNET) in the separator for each variant is kept consistent for a fair comparison. Specifically, for the variants with only one separation phase including Base model, Base-expanded, Base-deeper, and Base-high-order, the number of core blocks R is set to 6. For the variants with two separation phases, including Iterative, SRSSN-1D, SRSSN-1D-expanded, SRSSN-L r , and SRSSN, the number of blocks R in both phases is set to 3. <ref type="figure">Figure 4</ref> presents the experimental results of these nine variants of our model with two different separator structures (DPRNN-based and DPTNET-based) for ablation study.</p><p>It should be noted that the convolutional sampling rate (on the input signal) in the refining phase of our model are actually equal to the coarse phase when setting the stride size in the refining phase to be 1, because the refining phase is performed on the output of the coarse phase. In all our implementation, the stride size in the coarse phase and refining phase are set to be 8 and 1 respectively, thus the overall stride size (on the input signal) of our SRSSN after two phases is 8, which is equal to other methods in the ablation study. In such experimental settings, the comparisons between our model and other models in ablation study are fair. Effect of Coarse-to-fine framework. For both DPRNN-based and DPTNET-based separator structures, the performance is improved significantly in both metrics from Base model to SRSSN-1D, which manifests the remarkable advantages of our proposed Coarse-to-fine separation framework. Although the coarse phase and the refining phase of SRSSN-1D have the same model structure especially with the same encoding scheme, the learned encoding features of two phases are able to adapt to different separation stages under the guidance of the loss functions during training. The performance comparison between Iterative <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b39">[40]</ref> and SRSSN-1D demonstrates the benefit of our proposed Coarse-to-fine framework. The strategy of progressive separation through multiple phases is also adopted in Iterative, where the initial estimates from the first phase serve as prior speaker information to improve the separation performance in the second phase. However, it needs to learn a new encoding space for separation from scratch. In our proposed Coarse-to-Fine method, the separated representations from the first phase are further separated in the second phase, where the more thorough encoding space is constructed based on the existing coarse encoding space. The method SRSSN-L r trained with only loss function in the refining phase obtained much lower performance than the final version. The reason is that the performance of the coarse phase degenerates notably without direct supervision by the loss function on it.</p><formula xml:id="formula_17">/RZVLPLODULW\ 0HGLXPVLPLODULW\ +LJKVLPLODULW\ $OOVDPSOHV 6L615G%RI65661 6'5G%RI65661 6L615G%RI%DVHPRGHO 6'5G%RI%DVHPRGHO</formula><p>The performance gain from High-order to SRSSN also indicates the advantages of the progressive separation strategy through multiple phases defined in our proposed Coarse-tofine framework.</p><p>Effect of Fine-grained Encoding Mechanism. Our intact SRSSN outperforms SRSSN-1D substantially in both DPRNN-based and DPTNET-based cases, which indicates the effectiveness of the proposed Fine-grained Encoding Mechanism. Compared to the 1-order latent domain in SRSSN-1D, the constructed high-order latent domain space in SRSSN enables the model to perform separation in more fine-grained encoding space and achieve more precise separation results. To further explore the performance limit of 1-order latent domain and investigate the essential difference between 1order and high-order latent domain space, we scale up the number of basis functions in the latent domain of Base model and SRSSN-1D and compare our SRSSN with the expanded models Base-expanded and SRSSN-1D-expanded. <ref type="figure">Figure 4</ref> shows similar results in Base-expanded and SRSSN-1Dexpanded. Increasing the number of basis functions in 1-order latent space slightly improves the separation performance in the case of SRSSN-1D-expanded at the cost of proportionally increasing the parameters of encoder. In the case of DPTNET-SRSSN, the performance is even degenerated from Base model to Base-expanded due to potential overfitting in 1-order latent domain. Our model SRSSN performs distinctly better than these expanded models.</p><p>As shown in <ref type="figure">Figure 4</ref>, Base-deeper, which has deeper encoder and decoder than Base model, slightly improves the separation performance than Base model. However, it performs worse than Base-high-order, which reveals that the high-order encoding space modeled by our model is not equivalent to (but more powerful than) the deeper encoding space by Base-deeper. The high-order latent space constructed by the fine-grained encoding mechanism significantly improves the feature representation power with even less encoder parameters due to the constructed mechanism of high-order domain described in III-B.</p><p>Robustness of the proposed SRSSN. To evaluate the robustness of our SRSSN, we divide the test data into separate subsets according to the similarity between involved two speakers and compare the performance between our SRSSN and Base model. Typically, mixed speech with more similarities between speakers is harder to be separated. Specifically, we utilize a pre-trained speaker encoder model 1 <ref type="bibr" target="#b40">[41]</ref> to extract the embedding vector of each involved speaker and calculate their cosine similarity for each sample. Higher cosine similarity indicates higher similarity of speech characteristics of involved speakers. We sort the samples in test set according to the similarity, and divide them into three subsets with equal number of samples, namely Low similarity, Medium similarity and High similarity. <ref type="figure" target="#fig_6">Figure 5</ref> presents the results on these three subsets and all samples. As the speaker similarity increases, the performance decreases for both models, which is reasonable. Our SRSSN consistently outperforms Base model on all subsets in both metrics, which manifests the robustness of our SRSSN. Exploration of number of phases for stepwise separation.</p><p>Theoretically, our proposed Fined-grained Encoding Mechanism can be iteratively employed without limitation to constructed higher-order encoding space and perform more finegrained speech separation. However, more times of iterations inevitably lead to the increase of the model complexity and computational cost but diminishing marginal performance gain. We conduct experiments to investigate the scalability of our SRSSN with increasing separation phases: 1) 1-phase, which is the same as Base model; 2) 2-phase, which is the same as current version of our SRSSN; 3) 3-phase, which performs stepwise separation through 3 phases sequentially in the 1-order, 2-order, and 3-order encoding space, respectively. Due to the memory limit, we only perform experiments in the case of DPRNN separator and use fewer DPRNN blocks. For 1-phase, we use 4 blocks. For 2-phase, we use 2 blocks for both phases. For 3-phase, we use 2 blocks for the first phase and 1 phase for the later two phases. Similar to the construction of the 2-order embedding space, the 3-order embedding space is constructed by the decomposition of the 2-order embedding space using our proposed Fined-grained Encoding Mechanism. <ref type="table" target="#tab_3">Table I</ref> presents the separation performance of these versions of SRSSN. The performance is improved significantly from 1-phase to 2-phase in terms of both metrics, whilst the performance gain is negligible from 2-phase to 3-phase in terms of ?SI-SNR. Such results implie that the 2-order latent domain constructed in 2-phase suffices to provide a separable encoding space. Besides, the model size and GPU memory usage (on a single NVIDIA RTX 3090 when separating the 4second speech) increases as the increases of the iterating times. These results are consistent with above theoretical analysis. Qualitative Evaluation. To gain more insight into the effect of speech separation, we perform two sets of qualitative evaluation: 1) qualitative results by the coarse phase and refining phase and 2) qualitative comparison between our SRSSN and the base model. In the first set of experiments, we randomly select two samples from the test set for DPRNN-SRSSN and DPTNET-SRSSN, and employ the Librosa analysis toolkit <ref type="bibr" target="#b41">[42]</ref> to visualize the STFT power spectrums of the estimated speech sources in both phases in <ref type="figure" target="#fig_7">Figure 6</ref>. Comparing between the visualization of the groundtruth and estimates for each involved speaker, we observe that the separated results for one speaker in the coarse phase still contain residual ingredients from the other speaker, particularly in the regions indicated by bounding boxes. In contrast, the separated results in the refining phase is much better than that in the coarse phase: most of the incorrect residual ingredients are successfully removed. The estimates in the refining phase show more similar spectrum patterns as their groundtruth counterparts than that in the coarse phase. It indicates that the fine-grained embedding space defined by the high-order latent domain in refining phase enables a more precise separation.</p><p>In the second set of qualitative experiments, we randomly select two samples from the subset of High similarity (the most challenging subset) and compare between our SRSSN and Base model qualitatively. <ref type="figure" target="#fig_8">Figure 7</ref> visualizes the STFT power spectrums of the estimated speech sources. It is clearly shown that our SRSSN is able to perform a more precise speech separation than the base model.</p><p>2) Comparison with State-of-the-art Methods on WSJ0-2mix (involving 2 speakers): Next we conduct experiments to compare our model with state-of-the-art methods for speech separation on WSJ0-2mix dataset <ref type="bibr" target="#b1">[2]</ref>. In particular, we compare our model with 2 types of methods: 1) methods performing separation in the frequency domain, including DPCL++ <ref type="bibr" target="#b2">[3]</ref>, UPIT-Bi-LSTM-ST <ref type="bibr" target="#b4">[5]</ref>, Chimera++ <ref type="bibr" target="#b6">[7]</ref> and Deep CASA <ref type="bibr" target="#b7">[8]</ref>; 2) methods performing separation in a learnable latent domain in an end-to-end way, including Bi-LSTM-TASNET <ref type="bibr" target="#b11">[12]</ref>, Conv-TASNET <ref type="bibr" target="#b9">[10]</ref>, E2EPF <ref type="bibr" target="#b27">[28]</ref>, FurcaNeXt <ref type="bibr" target="#b12">[13]</ref>, DRPNN-TASNET <ref type="bibr" target="#b14">[15]</ref>, SuDoRM-RF <ref type="bibr" target="#b16">[17]</ref>, Nachmani et al. <ref type="bibr" target="#b15">[16]</ref>, DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref>, SepFormer <ref type="bibr" target="#b18">[19]</ref> and Wavesplit <ref type="bibr" target="#b19">[20]</ref>. We evaluate the performance of two versions of our SRSSN: DPRNN-SRSSN and DPTNET-SRSSN. The number of blocks in both coarse separator and refining separator R is set to 6.</p><p>Table II presents the experimental results of different models for speech separation on WSJ0-2mix dataset <ref type="bibr" target="#b38">[39]</ref> in terms of both ?SI-SNR and ?SDR. For the results without using data augmentation with dynamic mixing <ref type="bibr" target="#b19">[20]</ref>, our DPRNN-SRSSN outperforms all other methods except Wavesplit <ref type="bibr" target="#b19">[20]</ref> in terms of both metrics while DPTNET-SRSSN performs better than all other methods, which demonstrates advantages of our model. The methods which learn a separable encoding space defined by a latent domain, generally perform better than the other type of methods separating speech in frequency domain explicitly, which implies that the frequency domain is not necessarily the best separation space for speech as described in <ref type="bibr" target="#b9">[10]</ref>. It is worth noting that our DPRNN-SRSSN and DPTNET-SRSSN outperform the original methods DPRNN-TASNET and DPTNET-TASNET by a large margin, respectively.</p><p>SepFormer <ref type="bibr" target="#b18">[19]</ref>, which is extended over DPTNET <ref type="bibr" target="#b17">[18]</ref> by using deeper and wider Transformer layers, achieves better performance than DPTNET at the cost of much larger model size. Nachmani et al. <ref type="bibr" target="#b15">[16]</ref> conducts an additional task to minimize the distance of the learned speaker embeddings between the estimates and the groundtruth, which is extracted by a speaker recognition model. Such extra supervision further improves the separation performance whilst it relies on extra datasets to train the speaker recognition model. Thus it utilizes extra data for training than our model. Wavesplit <ref type="bibr" target="#b19">[20]</ref> learns speaker-discriminative vectors at each time step by leveraging the information of speaker identities in datasets to boost performance in the training process. Compare to Nachmani et al. <ref type="bibr" target="#b15">[16]</ref>, SepFormer <ref type="bibr" target="#b18">[19]</ref>, and Wavesplit <ref type="bibr" target="#b19">[20]</ref>, our method DPTNET-SRSSN achieves more superior performance without using additional information in a lightweight way. The techniques used in these models can be readily integrated into our SRSSN, leading to a more powerful speech separation system.</p><p>We also compare the memory consumption and inference time between two versions of our model (DPRNN-SRSSN and DPTNET-SRSSN) with their TASNET-based counterparts (DPRNN-TASNET and DPTNET-TASNET) using the same separator structure. <ref type="figure" target="#fig_9">Figure 8</ref> presents the comparison results in inference mode on the same GPU (a single NVIDIA RTX 3090) of four models when separating mixed speech of different speech duration (in second). DPRNN-SRSSN consumes slightly less memory than DPRNN-TASNET <ref type="bibr" target="#b14">[15]</ref>, while DPTNET-SRSSN reduces much more memory usage compared to DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref>, as the length of input speech increases. The major decrease of memory consumption lies in the larger stride size of encoder in our models than the TASNET-based methods. As reported in <ref type="bibr" target="#b14">[15]</ref>, smaller stride size of encoder leads to better performance. However, smaller stride size results in longer feature sequence, which requires more floating-point operations and memory usage. The stride size of encoder is tuned to be 1 in DPRNN-TASNET <ref type="bibr">[</ref>   set to 8 in our coarse encoder. Our method achieves much better performance with a larger stride than TASNET-based methods, which is favorable for devices with limited memory. Regarding the inference time, DPRNN-SRSSN and DPTNET-SRSSN both require slightly more inference time than their TASNET-based counterparts due to two separation phases in our SRSSN.</p><p>3) Comparison with State-of-the-art Methods on WSJ0-3mix (involving 3 speakers): Next we conduct experiments to compare our model with state-of-the-art methods for speech separation on WSJ0-3mix dataset <ref type="bibr" target="#b1">[2]</ref> involving 3 speakers, which is more challenging than the scenario with 2 speakers. In particular, we compare our model with 2 types of methods: 1) methods performing separation in the frequency domain, including DPCL++ <ref type="bibr" target="#b2">[3]</ref>, UPIT-Bi-LSTM-ST <ref type="bibr" target="#b4">[5]</ref>; 2) methods performing separation in a learnable latent domain, including E2EPF <ref type="bibr" target="#b27">[28]</ref>, Conv-TASNET <ref type="bibr" target="#b9">[10]</ref>, DRPNN-TASNET <ref type="bibr" target="#b14">[15]</ref>, DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref>, Nachmani et al. <ref type="bibr" target="#b15">[16]</ref>, Wavesplit <ref type="bibr" target="#b19">[20]</ref>, and SepFormer <ref type="bibr" target="#b18">[19]</ref>. We evaluate the performance of two versions of our SRSSN: DPRNN-SRSSN and DPTNET-SRSSN. The number of blocks in both coarse separator and refining separator R is set to 6.  <ref type="table" target="#tab_3">Table III</ref> presents the experimental results of different models for speech separation on WSJ0-3mix dataset <ref type="bibr" target="#b1">[2]</ref> in terms of both ?SI-SNR and ?SDR. Our DPRNN-SRSSN and DPTNET-SRSSN both outperform all other methods without using data augmentation by a large margin. DPTNET-SRSSN performs slightly worse than SepFormer using data augmentation <ref type="bibr" target="#b18">[19]</ref>, which manifests the significant advantages of our SRSSN. It is worth noting that our DPRNN-SRSSN and DPTNET-SRSSN both outperform their original methods DPRNN-TASNET and DPTNET-TASNET significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Speech separation in Noisy and Reverberant Settings</head><p>In this set of experiments, we conduct experiments in noisy and reverberant settings to validate the robustness of our proposed SRSSN. Datasets. We perform experiments on WSJ0 Hipster Ambient Mixtures (WHAM!) dataset <ref type="bibr" target="#b42">[43]</ref> and WHAMR! dataset <ref type="bibr" target="#b43">[44]</ref>, which are constructed based on WSJ0-2mix dataset <ref type="bibr" target="#b1">[2]</ref>. In WHAM!, each two-speaker utterance from WSJ0-2mix dataset is mixed with a unique noise sample, which is recorded in non-stationary ambient environments such as coffee shops, restaurants and bars. The random SNR value between the first (louder) speaker and the noise is sampled from a uniform distribution between ?6 dB and +3 dB. To separate the clean signals for involved speakers from such noisy speech data, the models are required to perform not only speech separation but also denoising. WHAMR! <ref type="bibr" target="#b43">[44]</ref> is an reverberant extension of WHAM!, in which synthetic reverberation noise is further fused into the input speech data. Thus dereverberation is also required for this data to perform thorough speech separation. We compare our proposed DPRNN-SRSSN and DPTNET-SRSSN with state-of-the-art methods for speech separation in noisy and reverberant settings: Chimera++ <ref type="bibr" target="#b42">[43]</ref>, Bi-LSTM-TASNET <ref type="bibr" target="#b43">[44]</ref>, Conv-TASNET <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, Learnable fbank <ref type="bibr" target="#b44">[45]</ref>, Cascaded-Bi-LSTM-TASNET <ref type="bibr" target="#b43">[44]</ref>, DPRNN-TASNET <ref type="bibr" target="#b15">[16]</ref>, DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref>, Nachmani et al. <ref type="bibr" target="#b15">[16]</ref> and Wavesplit <ref type="bibr" target="#b19">[20]</ref>. Note that Cascaded-Bi-LSTM-TASNET <ref type="bibr" target="#b43">[44]</ref> is specifically designed to adapt to the noisy and reverberant conditions. <ref type="table" target="#tab_3">Table IV</ref> shows that our DPRNN-SRSSN and DPTNET-SRSSN outperform other methods without using data augmentation by a large margin under noisy and reverberant conditions. In particular, our method performs distinctly better than the cascaded model Cascaded-Bi-LSTM-TASNET <ref type="bibr" target="#b43">[44]</ref>, which is equipped with the denoising and dereverberation functions. It manifests our model is generalized well to the noisy and reverberant conditions without specific design for adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Speech Recognition on Separated speech</head><p>We further conduct experiments of speech recognition on separated speech signals decoded by methods for speech separation to evaluate the performance of speech separation indirectly. To be specific, we first perform speech separation on a mixture of speech dataset, then we perform speech recognition using a standard Automatic Speech Recognition (ASR) model on the separated speech signals by different speech separation models, respectively. The achieved performance of speech recognition on the separation results can be considered as an indirect evaluation measurement for the corresponding model for speech separation. Dataset We conduct experiments on a recently released and fully open-source dataset Libri2Mix <ref type="bibr" target="#b45">[46]</ref> for speech recognition. Libri2Mix is generated based on the ASR dataset Lib-riSpeech <ref type="bibr" target="#b46">[47]</ref> by mixing randomly selected speech utterances from different speakers. We use the speech data in the clean condition with sampling rate of 8kHz. It consists of two modes min and max. In the min mode, the longer utterance is trimmed to align the shorter utterance. In the max mode, the shorter utterance is padded with zeros to align the longer utterance. We train the models for speech separation in the min mode of the train-100 subset, and perform test of speech separation in the max mode. The separated signals in the test phase are further used for performing experiments of speech recognition.</p><p>We use the standard hybrid DNN-HMM framework <ref type="bibr" target="#b47">[48]</ref> as the ASR model to perform speech recognition, implemented based on Kaldi open-source toolkit <ref type="bibr" target="#b48">[49]</ref>. The DNN acoustic model with p-norm non-linearities <ref type="bibr" target="#b49">[50]</ref> is trained on the top of fMLLR features and the forced alignment of the training data is produced by a GMM-HMM model <ref type="bibr" target="#b50">[51]</ref>. A 4-gram language model is utilized for rescoring. The ASR model is trained on the subset train-clean-100 of LirbiSpeech <ref type="bibr" target="#b46">[47]</ref>, following the official kaldi implementation 1 . We compare our proposed DPRNN-SRSSN and DPTNET-SRSSN with following state-of-the-art models for speech separation: Bi-LSTM-TASNET <ref type="bibr" target="#b11">[12]</ref>, Conv-TASNET <ref type="bibr" target="#b9">[10]</ref>, DPRNN-TASNET <ref type="bibr" target="#b14">[15]</ref>, DPTNET-TASNET <ref type="bibr" target="#b17">[18]</ref>. <ref type="table" target="#tab_8">Table V</ref> presents the performance of speech recognition. Besides, we also the report the experimental results of speech separation. Target signal and Mixed signal denote the ASR results on the target signals and original mixed (unseparated) signals respectively, which can be viewed as the upper bound and the lower bound for the speech recognition by the same ASR model. Our model achieves the best performance in both speech recognition and speech separation. Besides, it is shown that the performance of speech recognition is consistent with the performance of speech separation: better speech separated results lead to higher performance of speech recognition, which reveals the effectiveness of such indirect evaluation way, namely performing speech recognition on the separated speech signals.</p><p>V. CONCLUSION In this work, we have presented the Stepwise-Refining Speech Separation Network (SRSSN), which performs speech separation following a coarse-to-fine framework. SRSSN first conducts a rough speech separation by learning a 1-order latent domain to define the encoding space in the coarse phase, then performs refining in the constructed fine-grained embedding space to achieve more precise separation. In particular, we propose the Fine-grained Encoding Mechanism, which learns a new latent domain along each basis function of the existing latent domain that defines the coarse embedding space. Thus two latent domains jointly form a high-order domain and thereby define a fine-grained embedding space. Extensive experiments have demonstrated the effectiveness of the proposed SRSSN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Structures of the separator in DPRNN-TASNET and DPTNET-TASNET. Note that the coarse separator S c and the the refining separator S r share the same model structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fine-grained separation in the refining phase. (a) The proposed Fine-grained Encoding Mechanism learns a new latent domain H r along each basis function of the old latent domain H c that defines the coarse embedding space. The obtained new latent domain H r and the old latent domain H c jointly form a high-order domain, which defines a fine-grained embedding space for refining separation. (b) The refining separator processes the encoded representations in parallel for all groups of basis functions of the latent domain H c for each speaker. (c) The refining decoder performs two-stage decoding to obtain final speech signal: decoding from fine-grained embedding space to the coarse embedding space by D r 1 and decoding from the coarse embedding space to the output speech signal by D r 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>s</head><label></label><figDesc>noise =? ? s target , SI-SNR(?, s) := 10 log 10 s target 2 s noise 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Performance of using DPTNET as separator Performance of our SRSSN and Base model in terms of ? SI-SNR and ? SDR on different test subsets with different level of similarity between involved speakers in the mixed speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of STFT power spectrums of our DPRNN-SRSSN (left) and DPTNET-SRSSN (right) in both coarse and refining phases on two randomly selected samples from test set. The contrasting regions are highlighted in green and blue boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of STFT power spectrums of our SRSSN and Base model in case of DPRNN and DPTNET on two randomly selected samples from the subset of High similarity in test set. The contrasting regions are highlighted in green and blue boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of GPU memory usage and inference time as a function of input speech length at 8kHz sampling rate. The results are reported in inference mode on a single NVIDIA RTX 3090.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Wenjie Pei and Guangming Lu are corresponding authors. Zengwei Yao, Wenjie Pei, Fanglin Chen and Guangming Lu are with the Department of Computer Science, Harbin Institute of Technology at Shenzhen, Shenzhen 518057, China (e-mail: yaozengwei@outlook.com; wen-jiecoder@outlook.com; chenfanglin@hit.edu.cn; luguangm@hit.edu.cn) David Zhang is with the School of Science and Engineering, The Chinese University of Hong Kong at Shenzhen, Shenzhen 518172, China (e-mail: davidzhang@cuhk.edu.cn)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Coarse phase Refining phase Low-order (1-order) latent domain High-order (2-order) latent domain Fine-grained Encoding Mechanism Coarse encoder Coarse separator Coarse decoder Coarse decoder Refining encoder Refining encoder Refining separator Refining separator Refining decoder</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Base-deeper, which has deeper encoder and decoder than Base model, to investigate the effect of the convolutional depth of both the encoder and decoder.</figDesc><table><row><cell>%DVHPRGHO</cell><cell>6L6'5G% 6'5G%</cell><cell>%DVHPRGHO</cell><cell>6L6'5G% 6'5G%</cell></row><row><cell>%DVHH[SDQGHG</cell><cell></cell><cell>%DVHH[SDQGHG</cell><cell></cell></row><row><cell>%DVHGHHSHU</cell><cell></cell><cell>%DVHGHHSHU</cell><cell></cell></row><row><cell>%DVHKLJKRUGHU</cell><cell></cell><cell>%DVHKLJKRUGHU</cell><cell></cell></row><row><cell>,WHUDWLYH</cell><cell></cell><cell>,WHUDWLYH</cell><cell></cell></row><row><cell>65661'</cell><cell></cell><cell>65661'</cell><cell></cell></row><row><cell>65661'H[SDQGHG</cell><cell></cell><cell>65661'H[SDQGHG</cell><cell></cell></row><row><cell>65661 r</cell><cell></cell><cell>65661 r</cell><cell></cell></row><row><cell>65661</cell><cell></cell><cell>65661</cell><cell></cell></row><row><cell>(a) Performance of DPRNN-SRSSN</cell><cell></cell><cell cols="2">(b) Performance of DPTNET-SRSSN</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>In this variant,</cell></row><row><cell></cell><cell></cell><cell cols="2">we deepen the encoder and the decoder with 2, 3 and 4</cell></row><row><cell></cell><cell></cell><cell cols="2">convolutional layers (1-D) respectively, and select the best</cell></row><row><cell></cell><cell></cell><cell cols="2">performance as the optimal results w.r.t. the convolutional</cell></row><row><cell></cell><cell></cell><cell>depth.</cell><cell></cell></row></table><note>? Base-high-order, which directly encodes the mixed speech into high-order embedding space and perform speech separation in only one separation phase. Com- pared to our SRSSN, no rough separation is performed in the 1-order embedding space. Hence, this variant is proposed to validate the effectiveness of coarse-to-fine4: Performance of nine variants of our SRSSN in terms of ?SI-SNR and ?SDR for ablation study, using DPRNN and DPTNET as separator respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Performance of different number of phases of stepwise separation in terms of ?SI-SNR (dB) and ?SDR (dB) using DPRNN as separator.</figDesc><table><row><cell>1-phase</cell><cell>2.5M</cell><cell>1.76Gib</cell><cell>17.5</cell><cell>17.7</cell></row><row><cell>2-phase</cell><cell>2.7M</cell><cell>2.07Gib</cell><cell>19.0</cell><cell>19.3</cell></row><row><cell>3-phase</cell><cell>3.0M</cell><cell>3.86Gib</cell><cell>19.1</cell><cell>19.3</cell></row></table><note>Method Model size GPU Memory usage ?SI-SNR ? ?SDR ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Performance of different methods for speech separation on WSJ0-2mix in terms of ?SI-SNR (dB) and ?SDR (dB) in the clean setting.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="3">Model size ?SI-SNR ? ?SDR ?</cell></row><row><cell></cell><cell>DPCL++ [3]</cell><cell>13.6M</cell><cell>10.8</cell><cell>?</cell></row><row><cell>Frequency</cell><cell>UPIT-Bi-LSTM-ST [5]</cell><cell>92.7M</cell><cell>?</cell><cell>10.0</cell></row><row><cell>domain-based</cell><cell>Chimera++ [7]</cell><cell>32.9M</cell><cell>11.5</cell><cell>12.0</cell></row><row><cell></cell><cell>Deep CASA [8]</cell><cell>12.8M</cell><cell>17.7</cell><cell>18.0</cell></row><row><cell></cell><cell>Bi-LSTM-TASNET [12]</cell><cell>23.6M</cell><cell>13.2</cell><cell>13.6</cell></row><row><cell></cell><cell>Conv-TASNET [10]</cell><cell>5.1M</cell><cell>15.3</cell><cell>15.6</cell></row><row><cell></cell><cell>E2EPF [28]</cell><cell>?</cell><cell>16.9</cell><cell>17.3</cell></row><row><cell></cell><cell>FurcaNeXt [13]</cell><cell>51.4M</cell><cell>?</cell><cell>18.4</cell></row><row><cell></cell><cell>DPRNN-TASNET [15]</cell><cell>2.6M</cell><cell>18.8</cell><cell>19.0</cell></row><row><cell>Learnable latent</cell><cell>SuDoRM-RF [17]</cell><cell>2.6M</cell><cell>18.9</cell><cell>?</cell></row><row><cell>domain-based</cell><cell>Nachmani et al. [16]</cell><cell>7.5M</cell><cell>20.1</cell><cell>?</cell></row><row><cell></cell><cell>DPTNET-TASNET [18]</cell><cell>2.7 M</cell><cell>20.2</cell><cell>20.6</cell></row><row><cell></cell><cell>SepFormer [19]</cell><cell>26M</cell><cell>20.4</cell><cell>20.5</cell></row><row><cell></cell><cell>Wavesplit [20]</cell><cell>29M</cell><cell>21.0</cell><cell>21.2</cell></row><row><cell></cell><cell>DPRNN-SRSSN (ours)</cell><cell>7.5M</cell><cell>20.5</cell><cell>20.7</cell></row><row><cell></cell><cell>DPTNET-SRSSN (ours)</cell><cell>5.7M</cell><cell>21.2</cell><cell>21.4</cell></row><row><cell></cell><cell>Wavesplit + Data Augment [20]</cell><cell>29M</cell><cell>22.2</cell><cell>22.3</cell></row><row><cell></cell><cell>SepFormer + Data Augment [19]</cell><cell>26M</cell><cell>22.3</cell><cell>22.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Performance of different methods for speech separation on WSJ0-3mix in terms of ?SI-SNR (dB) and ?SDR (dB) in the clean setting.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="3">Model size ?SI-SNR ? ?SDR ?</cell></row><row><cell>Frequency</cell><cell>DPCL++ [3]</cell><cell>13.6M</cell><cell>7.1</cell><cell>?</cell></row><row><cell>domain-based</cell><cell>UPIT-Bi-LSTM-ST [5]</cell><cell>92.7M</cell><cell>?</cell><cell>7.7</cell></row><row><cell></cell><cell>E2EPF [28]</cell><cell>?</cell><cell>12.5</cell><cell>13.0</cell></row><row><cell></cell><cell>Conv-TASNET [10]</cell><cell>5.1M</cell><cell>12.7</cell><cell>13.1</cell></row><row><cell></cell><cell>DPRNN-TASNET [15]</cell><cell>2.6M</cell><cell>15.7</cell><cell>16.0</cell></row><row><cell></cell><cell>DPTNET-TASNET [18]</cell><cell>2.7 M</cell><cell>16.2</cell><cell>16.5</cell></row><row><cell></cell><cell>Nachmani et al. [16]</cell><cell>7.5M</cell><cell>16.9</cell><cell>?</cell></row><row><cell>Learnable latent domain-based</cell><cell>Wavesplit [20]</cell><cell>29M</cell><cell>17.3</cell><cell>17.6</cell></row><row><cell></cell><cell>SepFormer [19]</cell><cell>26M</cell><cell>17.6</cell><cell>17.9</cell></row><row><cell></cell><cell>DPRNN-SRSSN (ours)</cell><cell>7.5M</cell><cell>18.8</cell><cell>19.0</cell></row><row><cell></cell><cell>DPTNET-SRSSN (ours)</cell><cell>5.7M</cell><cell>19.4</cell><cell>19.6</cell></row><row><cell></cell><cell>Wavesplit + Data Augment [20]</cell><cell>29M</cell><cell>17.8</cell><cell>18.1</cell></row><row><cell></cell><cell>SepFormer + Data Augment [19]</cell><cell>26M</cell><cell>19.5</cell><cell>19.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of Speech Separation by different methods in terms of ?SI-SNR (dB) and ?SDR (dB) on WHAM! and WHAMR! in noisy and reverberant settings.</figDesc><table><row><cell>Method</cell><cell cols="4">WHAM! ?SI-SNR ? ?SDR ? ?SI-SNR ? ?SDR ? WHAMR!</cell></row><row><cell>Chimera++ [43]</cell><cell>9.9</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Bi-LSTM-TASNET [44]</cell><cell>12.0</cell><cell>?</cell><cell>9.2</cell><cell>?</cell></row><row><cell>Conv-TASNET [44], [45]</cell><cell>12.7</cell><cell>?</cell><cell>8.3</cell><cell>?</cell></row><row><cell>Learnable fbank [45]</cell><cell>12.9</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Cascaded-Bi-LSTM-TASNET [44]</cell><cell>12.9</cell><cell>?</cell><cell>10.8</cell><cell>?</cell></row><row><cell>DPRNN-TASNET [16]</cell><cell>13.9</cell><cell>?</cell><cell>10.3</cell><cell>?</cell></row><row><cell>DPTNET-TASNET</cell><cell>14.9</cell><cell>15.3</cell><cell>12.1</cell><cell>11.1</cell></row><row><cell>Nachmani et al. [16]</cell><cell>15.2</cell><cell>?</cell><cell>12.2</cell><cell>?</cell></row><row><cell>Wavesplit [20]</cell><cell>15.4</cell><cell>15.8</cell><cell>12.0</cell><cell>11.1</cell></row><row><cell>DPRNN-SRSSN (ours)</cell><cell>15.7</cell><cell>16.1</cell><cell>12.3</cell><cell>11.4</cell></row><row><cell>DPTNET-SRSSN (ours)</cell><cell>16.1</cell><cell>16.5</cell><cell>12.3</cell><cell>11.3</cell></row><row><cell>Wavesplit + Data Augment [20]</cell><cell>16.0</cell><cell>16.5</cell><cell>13.2</cell><cell>12.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Performance of speech recognition as well as speech separation on Libri2Mix dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">?SI-SNR (dB) ? ?SDR (dB) ?</cell><cell>WER (%) ?</cell></row><row><cell>Bi-LSTM-TASNET</cell><cell>13.5</cell><cell>13.9</cell><cell>30.8</cell></row><row><cell>Conv-TASNET</cell><cell>14.4</cell><cell>14.7</cell><cell>27.4</cell></row><row><cell>DPRNN-TASNET</cell><cell>16.1</cell><cell>16.6</cell><cell>23.8</cell></row><row><cell>DPTNET-TASNET</cell><cell>16.7</cell><cell>17.1</cell><cell>22.4</cell></row><row><cell>DPRNN-SRSSN (ours)</cell><cell>17.3</cell><cell>17.7</cell><cell>22.1</cell></row><row><cell>DPTNET-SRSSN (ours)</cell><cell>18.3</cell><cell>18.6</cell><cell>20.6</cell></row><row><cell>Target signal</cell><cell>?</cell><cell>?</cell><cell>15.6</cell></row><row><cell>Mixed signal</cell><cell>?</cell><cell>?</cell><cell>95.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/resemble-ai/Resemblyzer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Singlechannel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Short term spectral analysis, synthesis, and modification by discrete fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="238" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Real-time single-channel dereverberation and separation with time-domain audio separation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Furcanext: End-toend monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sudo rm-rf: Efficient networks for universal audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 30th International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13154</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computational auditory scene analysis: Principles, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley-IEEE press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stage deep learning for noisyreverberant speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end post-filter for speech separation with deep attention fusion features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1303" to="1314" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Talker-independent speaker separation in reverberant conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delfarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving gans for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1700" to="1704" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving universal sound separation using sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalized end-to-end loss for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Wham!: Extending speech separation to noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Whamr!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Filterbank design for end-to-end speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context-dependent pretrained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A compact model for speaker-adaptive training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anastasakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP&apos;96</title>
		<meeting>eeding of Fourth International Conference on Spoken Language essing. ICSLP&apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1137" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
