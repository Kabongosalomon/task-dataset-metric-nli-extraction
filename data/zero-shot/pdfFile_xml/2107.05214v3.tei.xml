<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split, Embed and Merge: An accurate table structure recognizer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202230-02-01">February 1, 2022 30 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenrong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengren</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Split, Embed and Merge: An accurate table structure recognizer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202230-02-01">February 1, 2022 30 Jan 2022</date>
						</imprint>
					</monogr>
					<note>Preprint submitted to Journal of L A T E X Templates</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Table structure recognition</term>
					<term>Self-regression</term>
					<term>Attention mechanism</term>
					<term>Encoder-decoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table structure</ref> <p>recognition is an essential part for making machines understand tables. Its main task is to recognize the internal structure of a table. However, due to the complexity and diversity in their structure and style, it is very difficult to parse the tabular data into the structured format which machines can understand, especially for complex tables. In this paper, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer. SEM is mainly composed of three parts, splitter, embedder and merger. In the first stage, we apply the splitter to predict the potential regions of the table row/column separators, and obtain the fine grid structure of the table. In the second stage, by taking a full consideration of the textual information in the table, we fuse the output features for each table grid from both vision and text modalities.</p><p>Moreover, we achieve a higher precision in our experiments through providing additional textual features. Finally, we process the merging of these basic table grids in a self-regression manner. The corresponding merging results are learned through the attention mechanism. In our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR dataset which outperforms other methods by a large margin. We also won the first place of complex tables and third place of all tables in Task-B of ICDAR 2021 Competition on Scientific Literature Parsing. Extensive experiments on other publicly available datasets further demonstrate the effectiveness of our proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this age of knowledge and information, documents are a very important source of information for many different cognitive processes such as knowledge database creation, optical character recognition (OCR), graphic understanding, document retrieval, etc. Automatically processing the information embedded in these documents is crucial. Numerous efforts have been made in the past to automatically extract the relevant information from documents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. As a particular entity, the tabular structure is very commonly encountered in documents. These tabular structures convey some of the most important information in a very concise form. Therefore, they are extremely prevalent in domains like finance, administration, research, and even archival documents. <ref type="table" target="#tab_0">Table structure</ref> recognition (TSR) aims to recognize the table internal structure to the machine readable data mainly presented in two formats: logical structure and physical structure <ref type="bibr" target="#b3">[4]</ref>. More concretely, logical structure only contains every cell's row and column spanning information, while the physical one additionally contains bounding box coordinates of cells. As a result, table structure recognition as a precursor to contextual table understanding will be useful in a wide range of applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.  <ref type="figure" target="#fig_0">Figure 1</ref>. Although significant efforts have been made in the past to recognize the internal structure of tables through an automated process <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, most of these methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> only focus on simple tables and are hard to accurately recognize the structure of complex tables. The spanning cells usually contain more important semantic information than other simple cells, because they are more likely to be table headers in a table. The table header is crucial to understand the table. Therefore, more needs to be done for recognizing the structure of complex tables. is shown in (c), and (d) is its real structure. Note that in (d), the cells with the contents of "System" and "TEDS" occupy multiple rows or multiple columns, so it is a complex table.</p><p>using visual and textual representations in a joint framework. However, most previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref> in table structure recognition only use the spatial or visual features without considering the textual information of each table cell.</p><p>The structures of some tables have a certain ambiguity from the visual appearance, especially for table cells which contain multi-line contents. Therefore, to recognize the table structure accurately, it is inevitable to take advantage of the cross-modality nature of visually-rich table images, where visual and textual information should be jointly modeled. In our work, we design vision module and text module in our embedder to extract visual features and textual features, respectively, and achieve a higher recognition accuracy.</p><p>Most existing literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> on table structure recognition depends on extraction of meta-information from the pdf document or the OCR models to extract low-level layout features from the image. Nevertheless, these methods fail to extend to scanned documents due to the absence of meta-information or errors made by the OCR, when there is a wide variety in table layouts and text organization. In our work, we address the problem of table structure recognition by directly operating over table images with no dependency on meta-information or OCR.</p><p>In this study, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. <ref type="table">Considering that the table is composed of a set of table cells and each table cell</ref>   <ref type="bibr" target="#b12">[13]</ref>, is first applied to predict the fine grid structure of the table as shown in the upper-right of <ref type="figure" target="#fig_1">Figure 2</ref>. The embedder as a feature extractor embeds vision and plain text contained in a table grid into a feature vector. More specifically, we use the RoIAlign <ref type="bibr" target="#b13">[14]</ref> to extract the visual features from the output of the backbone, and extract textual features using the off-the-shelf recognizer <ref type="bibr" target="#b14">[15]</ref> and the pretrained BERT <ref type="bibr" target="#b15">[16]</ref> model. Finally, the merger which is a Gated Recurrent Unit (GRU) decoder will predict the gird merged results step by step based on the grid-level features extracted by the embedder. For each predicted merged result, the attention mechanism built into the merger scans the entire grid elements and predicts which grids should be merged at the current step. The proposed method can not only process simple tables well, but also complex tables. The ambiguity problem of the table structure recognition based on visual appearance can be alleviated through our embedder. Moreover, SEM is able to directly operate over table images, which enhances the applicability of the system (to both PDFs and images).</p><p>The main contributions of this paper are as follows:</p><p>? We present an accurate ? Based on our proposed method, we won the first place of complex tables and the third place of all tables in Task-B of ICDAR 2021 Competition on scientific literature parsing. In addition, we also achieved the results with an average F1-Measure of 97.11% and 95.72% in SciTSR and SciTSR-COMP datasets, respectively, demonstrating the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Table Structure Recognition</head><p>Analyzing tabular data in unstructured documents mainly focuses on three problems: i) table detection: localizing the bounding boxes of tables in documents <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, ii) table structure recognition: parsing only the structural (row and column layout) information of tables <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>, and iii) table recognition: parsing both the structural information and content of table cells <ref type="bibr" target="#b4">[5]</ref>. In this study, we mainly focus on table structure recognition. Most early proposed methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> are based on heuristics. While these methods were primarily dependent on hand-crafted features and heuristics (horizontal and vertical ruling lines, spacing and geometric analysis).</p><p>Due to the rapid development of deep learning and the massive amounts of tabular data in documents on the Web, many deep learning-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>, which are robust to the input type (whether being scanned images or native digital), have also been presented to understand table structures. These also do not make any assumptions about the layouts, are datadriven, and are easy to fine-tune across different domains. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> utilize recently published insights from semantic segmentation <ref type="bibr" target="#b12">[13]</ref> research for identifying rows, columns, and cell positions within tables to recognize table structures. However, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> do not consider the complex tables containing spanning cells, so that they cannot handle the structure recognition of complex tables well. GraphTSR <ref type="bibr" target="#b9">[10]</ref> proposes a novel graph neural network for recognizing the table structure in PDF files and can recognize the structure of complex tables. GraphTSR takes the table cells as input which means that it fails to generalize well due to the absence of meta-information or errors made by the OCR. EDD <ref type="bibr" target="#b4">[5]</ref> treats table structure recognition as a task similar to img2latex <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>. EDD directly generates the HTML tags that define the structure of the table through an attention-based structure decoder. <ref type="bibr" target="#b5">[6]</ref> presents the TabStructNet for table structure recognition that combines cell detection and interaction modules to localize the cells and predict their row and column associations with other detected cells. Compared with the aforementioned methods, our method SEM not only takes table images as input, but also can recognize the structure of complex tables well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanisms</head><p>Given a query element and a set of key elements, an attention function can adaptively aggregate the key contents according to attention weights, which measure the compatibility of query-key pairs. The attention mechanisms as an integral part of models enable neural networks to focus more on relevant elements of the input than on irrelevant parts. They were first studied in natural language processing (NLP), where encoder-decoder attention modules were developed to facilitate neural machine translation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. In particular, self-attention, also called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, and textual entailment.</p><p>The landmark work, Transformer <ref type="bibr" target="#b26">[27]</ref>, presents the transduction model relying entirely on self-attention to compute representations of its input and output, and substantially surpasses the performance of past work.</p><p>The success of attention modeling in NLP <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> has also led to its adoption in computer vision such as object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, semantic seg-mentation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, image captioning <ref type="bibr" target="#b31">[32]</ref> and text recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>, etc. DETR <ref type="bibr" target="#b27">[28]</ref> completes the object detection by adoptting an encoder-decoder architecture based on transformers <ref type="bibr" target="#b26">[27]</ref> to directly predict a set of object bounding boxes. In order to capture contextual information, especially in the long range, <ref type="bibr" target="#b30">[31]</ref> proposes the point-wise spatial attention network (PSANet) to aggregate long-range contextual information in a flexible and adaptive manner. Mask TextSpotter v2 <ref type="bibr" target="#b32">[33]</ref> applies a spatial attentional module for text recognition, which alleviates the problem of character-level annotations and improves the performance significantly. In our work, we apply the transformers to capture the long-range dependencies on grid-level featuers and build attention mechanisms into our merger to predict which gird elements should be merged together to recover table cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multimodality</head><p>Several joint learning tasks such as image captioning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9]</ref>, visual question answering <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref>, and document semantic structure extraction <ref type="bibr" target="#b0">[1]</ref> have demonstrated the significant impact of using visual and textual representations in a joint framework. <ref type="bibr" target="#b8">[9]</ref> aligned parts of visual and language modalities through a common, multimodal embedding, and used the inferred alignments to learn to generate novel descriptions of image regions. <ref type="bibr" target="#b7">[8]</ref> proposed a novel model, Multimodal Multi-Copy Mesh (M4C), for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images and achieved the state-of-the-art. <ref type="bibr" target="#b0">[1]</ref> considered document semantic structure extraction as a pixel-wise segmentation task, and presented a unified model, Multimodal Fully Convolutional Network (MFCN). MFCN classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. In our work, we take a full consideration of the plain text contained in table images, and design the embedder to extract both visual and textual features at the same time. The experiments also prove that more accurate results will be obtained when providing additional textual information on visual clues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall pipeline of our system is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The modified ResNet-34 <ref type="bibr" target="#b36">[37]</ref> with FPN <ref type="bibr" target="#b37">[38]</ref> as our backbone is first applied to the input table image to extract multi-level feature maps. The splitter takes the output of the backbone as input and predicts the fine grid structure of the table. The table grid structure is in the form of row and column separators that span the entire image as shown in the upper-right of <ref type="figure" target="#fig_1">Figure 2</ref>. The following embedder extracts the feature representation of each basic table grid. Finally, based on the grid-level features extracted by the embedder, the merger with the attention mechanism will predict which grids should be merged step by step. The table structure can be recovered based on the merged results. In the following subsections, three main components in our system, namely, the splitter, the embedder and the merger, will be elaborated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Splitter</head><p>Different from the method in <ref type="bibr" target="#b4">[5]</ref>, performing table structure prediction on the image-level features, we believe that using table grids as the basic processing units will be more reasonable, so we design the splitter to predict the basic table grid pattern. Inspired by the segmentation-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> in the field of text detection and the FCN <ref type="bibr" target="#b12">[13]</ref> in image segmentation, we refer to the potential regions of the table row/column separators as the foreground and design the splitter which contains two separate row/column segmenters to predict the table row/column separator maps? row /? col as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><formula xml:id="formula_0">S row /? col ? R H?W and H ? W is the size of the input image.</formula><p>Each segmenter is actually the fully convolutional network which contains a convolutional layer, ReLU and a convolutional layer. As some table row/column separator regions are quite slender, it is important to ensure segmentation results have a high resolution. The kernel size and the stride of each convolutional layer in the segmenter is set to 3 ? 3 and 1, respectively, which keeps the same spatial resolution of the input and the output. Moreover, we modify the ResNet-34 by setting the stride of the first convolutional layer with 7 ? 7 kernel size to 1, and remove the adjacent pooling layer to guarantee the resolution of the lowest-level feature map is consistent with the input image. We strongly believe that rich semantics extracted by deeper layers can help with obtaining more accurate segmentation results, so we add a top-down path <ref type="bibr" target="#b37">[38]</ref> in our backbone to enrich semantics in feature maps. Finally, the backbone generates a feature pyramid with four feature maps {P2, P3, P4, P5}, whose final output strides are 1, 2, 4, 8, respectively. The number of channels in the feature maps is D. We take P2 as the input of the splitter.</p><p>The loss function is defined as follows:</p><formula xml:id="formula_1">L row s = H j=1 W i=1 L(? row i,j , S row i,j ) H j=1 W i=1 S row i,j<label>(1)</label></formula><formula xml:id="formula_2">L col s = H j=1 W i=1 L(? col i,j , S col i,j ) H j=1 W i=1 S col i,j<label>(2)</label></formula><p>in which</p><formula xml:id="formula_3">L(x, y) = ?(y log(?(x)) + (1 ? y) log(1 ? ?(x)))<label>(3)</label></formula><p>where S row /S col denotes the ground-truth of the table row/column separator map. S row i,j /S col i,j is 1 if the pixel in i th column and j th row belongs to the table row/column separator region, otherwise 0. The ? is the sigmoid function.</p><p>The goal of our post-processing is to extract table row/column lines from the table row/column separator map as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Specifically, we first apply the sigmoid function to the predicted segmentation map? row /? col and average them by column/row size to obtain theS row /S col as illustrated in Eq. <ref type="bibr" target="#b3">4</ref>  </p><formula xml:id="formula_4">= 1 W W i ? ? row i,j<label>(4)</label></formula><formula xml:id="formula_5">S col i = 1 H H j ? ? col i,j<label>(5)</label></formula><p>We can easily obtain a set of bounding boxes G of represents the position of the lower right. The embedder aims to extract the feature representations of each grid. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> have demonstrated the effectiveness of taking advantage of the multi-modality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Embedder</head><formula xml:id="formula_6">E v ? R (M ?N )?D , E t ? R (M ?N )?D and E ? R (M ?N )?D , where D represents</formula><p>the number of feature channels.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the vision module takes the image-level feature map P2 from the FPN and the well-divided table grids G obtained from the splitter as input. It applies the RoIAlign <ref type="bibr" target="#b13">[14]</ref> to pool a fixed size R ? R feature map? v i for each table grid.</p><formula xml:id="formula_7">E v i = RoIAlign R?R (P2, G i ) ?i = {1, ..., M ? N }<label>(6)</label></formula><p>where? v i ? R R?R?D . The final visual features E v i are obtained according to:</p><formula xml:id="formula_8">E v i = FFN(? v i ) ?i = {1, ..., M ? N }<label>(7)</label></formula><p>in which</p><formula xml:id="formula_9">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2<label>(8)</label></formula><p>where FFN <ref type="bibr" target="#b26">[27]</ref> is actually two linear transformations with a ReLU activation</p><formula xml:id="formula_10">in between. x ? R din , W 1 ? R din?d ff , b 1 ? R d ff , W 2 ? R d ff ?dout , b 2 ? R dout .</formula><p>The dimensionality of input and output is d in and d out , and the inner-layer has dimensionality d ff . Here we set d ff = d out in default.</p><p>The table image is both visually-rich and textual-rich, so it is necessary to make full use of the textual information in the table to achieve a more accurate table structure recognizer. As shown in the text module of <ref type="figure" target="#fig_3">Figure 4</ref>, we apply the off-the-shelf recognizer <ref type="bibr" target="#b14">[15]</ref> to obtain a sequence of M ? N contents for all features E t are obtained by applying FFN again to fine-tune the extracted textual features? t to make it more suitable for our network.</p><formula xml:id="formula_11">E t i = FFN(? t i ) ?i = {1, ..., M ? N }<label>(9)</label></formula><p>The blender module in <ref type="figure" target="#fig_3">Figure 4</ref> is to fuse the visual features E v and textual features E t , and its specific process is as follows:</p><p>1) For each basic table grid, we first obtain the intermediate results? i</p><formula xml:id="formula_12">according to :? i = FFN( ? ? E v i E t i ? ? ) ?i ? [1, ..., M ? N ]<label>(10)</label></formula><p>where [?] is the concatenation operation. The input and output dimensionality of the FFN is 2D and D, respectively.</p><p>2) So far, the features of each basic table grid are still independent of each other, especially for textual features. Therefore, we introduce the transformer <ref type="bibr" target="#b26">[27]</ref> to capture long-range dependencies on table grid elements. We take the features? as query, key and value which are required by the transformer.</p><p>The output of the transformer as final grid-level features E have a global receptive field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Merger</head><p>The merger is an RNN that takes the grid-level features E as input and produces a sequence of merged maps M as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Here we choose the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b40">[41]</ref>, an improved version of simple RNN.</p><formula xml:id="formula_13">M = {m 1 , m 2 , ..., m C }<label>(11)</label></formula><p>where C is the length of a predicted sequence. Each merged map m t is a (M ? N )-dimension vector, the same size as the element of E, and the value of each grid element m ti is 1 or 0, indicating whether the i th grid element belongs to the t th cell or not. The cells that span multiple rows or columns can be recovered according to M. Inspired by the successful applications of attention mechanism in img2latex <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>, text recognition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, machine translation <ref type="bibr" target="#b26">[27]</ref>, etc., we build the attention mechanism into our merger and achieve promising results. For the merged map m t decoding, we compute the prediction of current hidden state? t from previous context vector c t?1 and its hidden state h t?1 :</p><formula xml:id="formula_14">h t = GRU(c t?1 , h t?1 )<label>(12)</label></formula><p>Then we employ an attention mechanism with? t as the query and grid-level features E as both key and the value:</p><formula xml:id="formula_15">m t = f att (E,? t )<label>(13)</label></formula><formula xml:id="formula_16">c t = m t m t 1 E<label>(14)</label></formula><p>where || ? || 1 is the vector 1-norm. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, we design f att function as follows:</p><formula xml:id="formula_17">F = Q * t?1 l=1 m l (15) m ti = ? T tanh(W att?t + U att e i + U F f i )<label>(16)</label></formula><formula xml:id="formula_18">m ti = Binarize(m ti )<label>(17)</label></formula><p>in which</p><formula xml:id="formula_19">Binarize(x) = ? ? ? ? ? 1 if ?(x) &gt; 0.5 0 otherwise<label>(18)</label></formula><p>where * denotes a convolution layer, t?1 l=1 m l denotes the sum of past determined grids,m ti denotes the output energy, f i denotes the element of F, which is used to help append the history information into standard attention mechanism. It's worth noting that the attention mechanism is completed on the grid-level features. For each cell, it is quite clear which grid elements belong to it. Therefore, unlike the previous methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> using the softmax to obtain the attention probability, we use the Binarize Eq. 18 to calculate. Moreover, we find that the model is difficult to converge when using the softmax.</p><p>With the context vector c t , we compute the current hidden state:</p><formula xml:id="formula_20">h t = GRU(c t ,? t )<label>(19)</label></formula><p>The training loss of the merger is defined as follows:</p><formula xml:id="formula_21">L m = t i L(m ti , y ti ) C y t 1<label>(20)</label></formula><p>where function L has been defined in Eq. 3, C is the length of a predicted sequence and y ti denotes the ground-truth of cell's grid elements. y ti is 1 if the i th grid element belong to the cell of time step t, otherwise 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Post-Processing</head><p>Through the merger, we can obtain the spanning of each boxes along with cell spanning information and its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use the publicly available table structure datasets -SciTSR <ref type="bibr" target="#b9">[10]</ref>, SciTSR-COMP <ref type="bibr" target="#b9">[10]</ref> and PubTabNet <ref type="bibr" target="#b4">[5]</ref> to evaluate the effectiveness of our model. Statistics of these datasets are listed in <ref type="table" target="#tab_8">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Label Generation</head><p>Label of Splitter We use the annotation, namely the text content with position being aligned to each table cell, to generate the ground-truth of the table row/column separator map S row /S col for the splitter. The S row /S col is designed to maximize the size of the separator regions without intersecting any non-spanning cell content, as shown in <ref type="figure" target="#fig_6">Figure 7</ref>. Different from traditional notion of cell separators, which for many tables are thin lines with only a few pixels. Predicting small regions is more difficult than predicting large regions.</p><p>In the case of unlined tables, the exact location of the cell separator is ill-defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label of Merger</head><p>Since we obtain the label of the splitter, we could divide the table into a set of basic table grids as shown in the upper-right of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The <ref type="table" target="#tab_0">original table structure annotation can provide which grids each table cell   occupies in the basic table grid pattern. We arrange the table cells in a top-</ref>to-bottom and left-to-right way and use the grids occupied by each cell as the prediction target for a certain time step of the merger. The red mask is the table row/column separator region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Metric</head><p>We use both F1-Measure <ref type="bibr" target="#b44">[45]</ref> and Tree-Edit-Distance-based Similarity (TEDS) metric <ref type="bibr" target="#b4">[5]</ref>, which are commonly adopted in The TEDS metric was recently proposed in <ref type="bibr" target="#b4">[5]</ref>. While using the TEDS metric, we need to present tables as a tree structure in the HTML format.</p><p>Finally, TEDS between two trees is computed as:</p><formula xml:id="formula_22">TEDS(T a , T b ) = 1 ? EditDist(T a , T b ) max(|T a |, |T b |)<label>(21)</label></formula><p>where T a and T b are the tree structure of tables in the HTML formats. EditDist represents the tree-edit distance <ref type="bibr" target="#b45">[46]</ref>, and |T | is the number of nodes in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>The modified ResNet-34 <ref type="bibr" target="#b36">[37]</ref> as our backbone is pre-trained on ImageNet <ref type="bibr" target="#b46">[47]</ref>.</p><p>The number of FPN channels is set to D = 256. The pool size R?R of RoIAlign in vision module is set to 3 ? 3. The recognizer <ref type="bibr" target="#b14">[15]</ref> is pre-trained on 35M table cell images, which are cropped from 500k table images in the PubTabNet dataset <ref type="bibr" target="#b4">[5]</ref>, and the success rate of word predictions per table cell reaches 94.1%.</p><p>The BERT <ref type="bibr" target="#b15">[16]</ref> we used is from the transformers package 2 . The hidden state dimension in the merger is set to 256.</p><p>The training objective of our model is to minimize the segmentation loss (Eq. 1, Eq. 2) and the cell merging loss (Eq. 20). The objective function for optimization is shown as follows:</p><formula xml:id="formula_23">O = ? 1 L row s + ? 2 L col s + ? 3 L m<label>(22)</label></formula><p>In our experiments, we set ? 1 = ? 2 = ? 3 = 1. We employ the ADADELTA algorithm <ref type="bibr" target="#b47">[48]</ref> for optimization, with the following hyper parameters: ? 1 = 0.9, ? 2 = 0.999 and ? = 10 ?9 . We set the learning rate using the cosine annealing schedule <ref type="bibr" target="#b48">[49]</ref> as follows:</p><formula xml:id="formula_24">? t = ? min + 1 2 (? max ? ? min )(1 + cos( T cur T max ?))<label>(23)</label></formula><p>where ? t is the updated learning rate. ? min and ? max are the minimum learning rate and the initial learning rate, respectively. T cur and T max are the current number of iterations and the maximum number of iterations, respectively. Here we set ? min = 10 ?6 and ? max = 10 ?4 .</p><p>Our model SEM is trained and evaluated with table images in original size.</p><p>We use the NVIDIA TESLA V100 GPU with 32GB RAM memory for our experiments and the batch-size of 8. The whole framework was implemented using PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>In this section, we visualize the segmentation results of the spliter and show how the merger recovers the <ref type="table">table cells from the table grid elements through</ref> attention visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Splitter</head><p>We refer the potential regions of the table row (column) separators as the foreground as shown in <ref type="figure" target="#fig_6">Figure 7</ref>, and design the splitter which is actually a fully convolutional network (FCN) to predict the foreground in table images. As shown in the first two rows of <ref type="figure" target="#fig_7">Figure 8</ref>, we can obtain accurate segmentation results through the splitter. The fine grid structure of the table can be obtained by post-processing as shown in the third row of <ref type="figure" target="#fig_7">Figure 8</ref>. It is worth noting that the example table in <ref type="figure" target="#fig_7">Figure 8 (a)</ref> is the simple table, while others are complex tables. We can find that the structure of the simple table has been recovered correctly through the splitter from the third row of <ref type="figure" target="#fig_7">Figure 8</ref>. However, the structure of complex tables is not complete and still needs to be processed. Therefore, we design the following embedder  and merger to recover the structure of complex tables based on the outputs of the splitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Merger</head><p>In order to recover the table cells, we build the attention mechanism into our merger to predict which grid elements should be merged step by step. The merged result in each step is a binary map, and the table cell can be recovered by merging the elements that are 1 in the binary map. Taking the table of <ref type="figure" target="#fig_7">Figure 8</ref> (b) as a example, the attention mechanism is visualized in <ref type="figure" target="#fig_9">Figure 9</ref>. The cell with the content of "Number of modules"</p><p>in <ref type="figure" target="#fig_9">Figure 9</ref> occupies the first row of basic table grids. Our merger correctly predicts the structure of this cell through the attention mechanism as shown in the first time step of <ref type="figure" target="#fig_9">Figure 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>In order to investigate the effect of each component, we conduct ablation experiments through several designed systems as shown in <ref type="table" target="#tab_11">Table 2</ref>. The model is not modified except the component being tested. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Splitter</head><formula xml:id="formula_25">Embedder Merger VM TM T1 - - - T2 - T3 - T4</formula><p>The Number of Transformer Layers We measure the performance of T1-T4 with different numbers of transformer layers in the embedder. We try from 0 to 3 as shown in <ref type="figure" target="#fig_0">Figure 10</ref>. When Num = 0 in <ref type="figure" target="#fig_0">Figure 10</ref>, it means the transformer layer is removed. In the T3 configuration, only the vision module (VM) in the embedder is used to extract the visiual features to represent each grid element. Also there is not much gap whether the transformer layer is added or not. Through a series of convolutional layers, the backbone features P2 already has a certain receptive field. Therefore, it is not significant to add the transformer layers while the VM has pooled each grid features from P2. It is worth noting that when the Num is greater than 0, the performance of the designed system T2 outperforms the model without the transformer layer. This is because the transformer layer here can capture the semantical dependencies among all table grid elements. As our final system, T4 achieves the best result when Num = 1, so we set Num = 1 in subsequent experiments by default. The Effectiveness of the Merger In   <ref type="table" target="#tab_12">Table 3</ref>. Compared with T4, systems T2 and T3 that only use TM or VM are sub-optimal.</p><p>When both TM and VM are used, the system (T4) performance reaches the best. As shown in <ref type="figure" target="#fig_0">Figure 11</ref>, although the predictions of table grid structure from the splitter in both T3 and T4 are the same, the T3 system which only uses VM is more unstable comparing with T4 which uses both VM and TM in the embedder.</p><p>The Efficiency of Each Component In order to investigate the efficiency of each component, we compare the frames per second (FPS) of T1-T4 systems as shown in <ref type="table" target="#tab_12">Table 3</ref>. From T1 to T2-T4 systems, the speed of the systems is much slower. This is because as the number of table cells increases, the decoding steps of the merger increases. The reason why T2 and T4 are slower than T3</p><p>is that the former uses a recognizer to recognize the content in the basic table grid and applies the BERT to extract the corresponding textual features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison with State-of-the-art Methods</head><p>We compare our method with other state-of-the-art methods on both SciTSR and SciTSR-COMP datasets. The results are shown in <ref type="table" target="#tab_15">Table 4</ref>. Our model is    <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53]</ref>, we use the RoIAlign to pool the features of table cells and append an attention-based recognizer <ref type="bibr" target="#b14">[15]</ref> to recognize the content in table cells. Note that the modified models are trained in an end-to-end manner. The single model results of our methods are shown in <ref type="table" target="#tab_17">Table 5</ref>. Based on the configuration of T3 with a recognizer, we divide our model into three sub-networks, splitter, merger and newly added recognizer, adopting multi-model fusion for each sub-network. Finally, we combine the training set with the validation set for training. The results of the competition are shown in <ref type="table" target="#tab_18">Table 6</ref>. Our team is named USTC-NELSLIP, and we won the first place of complex tables and third place of all tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Error Analysis</head><p>In this section, we show some incorrect table structure recognition results of the SEM as shown in <ref type="figure" target="#fig_0">Figure 13</ref>. Our splitter occasionally misses or overcuts the basic table grids when the blank space between cells is too large. In the training phase, the merger predicts the spanning information of cells on the correct basic table grid pattern. Therefore, in the inference phase, once the splitter predicts incorrect results, it is difficult for the merger to fix them.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we proposed a new method for the table structure recognition, SEM. The proposed method takes images as input with no dependency on metainformation or OCR. It mainly contains three components including splitter, embedder and merger. We first split table images into a set of basic table grids.</p><p>Then the embedder is used to extract the feature representations of each grid element. Finally, we use the merger with the attention mechanism to predict which grid elements should be merged to recover the table cells. The final table structure can be obtained by parsing all table cells. The method can not only process simple tables well, but also the complex tables. We demonstrate through visualization and experiment results that the attention mechanism built in the merger performs well in predicting which grid elements belong to each cell. To our best knowledge, this is the first time to take a full consideration of the textual information in table images and design the embedder to extract both the visual and the textual features. The ablation studies prove the effectiveness of our embedder. Our method achieves state-of-the-art on both SciTSR and SciTSR-COMP datasets. Based on our method, we won the first place of complex tables and third place of all tables in Task-B of ICDAR 2021 Competition on Scientific Literature Parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An intuitive comparison between simple and complex tables. The example of the simple table is shown in (a), and (b) is its real structure. The example of the complex table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>SEM pipeline The backbone is applied to extract the feature maps from the table image. The splitter uses the backbone features to predict a set of basic table grids. The embedder extracts the region features corresponding to each basic table gird. The merger predicts which grid elements need to be merged to recover the table cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The illustration of the splitter. The splitter takes a feature map as input and predicts the potential regions of the table row/column separators, which are the green masks in the table images. The following post-processing is used to extract the basic table grids according to the segmentation results from the segmenters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The illustration of the embedder. It is composed of vision module (VM), text module (TM) and blender module (BM). The embedder extracts the gird-level visual and textual features from VM and TM, respectively. Finally, the BM fuses the both features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The illustration of the merger. The yellow masks in lower part indicate which table grid elements should be merged in each time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The illustration of the attention mechanism. The prediction of current hidden stat? ht and the grid-level features E is used as query and key, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Example of the ground-truth of table row/column separator map for the splitter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>The visualization results from our system on table images of the SciTSR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>First Row:</head><label></label><figDesc>the green masks are the segmentation results of the row segmenter in the splitter. Second Row: the green masks are the segmentation results of the column segmenter in the splitter. Third Row: the blue lines indicate the boundaries of the basic table grids which are extracted through post-processing from both row and column segmentation results. Fourth Row: the blue lines indicate the boundaries of the table cells which are the merged results from the merger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>The visualization of the attention mechanism in the merger on the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Performance by varying number of transformer layers in T2, T3, T4 on the SciTSR test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>The comparison results between designed system T3 and T4. The fisrt column is the results on the SciTSR dataset. The second column is the results on the PubTabNet dataset. First Row: the predictions of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Some structure recognition results of our and other methods on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Some incorrect table structure recognition results of our method. First Column: the row segmentation results of the splitter. Second Column: the column segmentation results of the splitter. Third Column: the predictions of the table grid structure from the splitter. Fourth Column: the final prediction results of our method. The red dash boxes denote the incorrect prediction results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table structure</head><label>structure</label><figDesc>recognition is a challenging problem due to the complex structure and high variability in table layouts. A spanning cell is a table cell that occupies at least two rows or columns. If a table contains spanning cells, it is called a complex table, as shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is composed of one or more basic table grids, we deem table grids as the basic processing units in our framework. Therefore, we design the pipeline of SEM as follows: 1) divide table into basic table grids 2) merge them to recover the table cells. The final table structure can be obtained by parsing all table cells. As a consequence, SEM mainly has three</figDesc><table /><note>components: splitter, embedder and merger. The splitter, which is actually a fully convolutional network (FCN)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>table structure recognizer, Split, Embed and Merge (SEM), to recognize the table structure. The designed merger can accurately predict table structure based on the fine grid structure of the table. This proposed new method can not only process simple tables well, but also complex tables. ? We demonstrate that jointly modeling the visual and textual information in the table will further boost model performance. Through visualization in experiments, the ambiguity problem of the table structure recognition can be alleviated based on our multimodality features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>5, whereS row ? R H?1 andS col ? R 1?W . Then we binarize theS row /S col intoS row /S col ,S = 1 indicating this row/column is a potential table line. For the block that is equal to 1 inS row /S col , we select the row/column with the maximum value of the corresponding block inS row /S col as the final table row/column line.S</figDesc><table><row><cell>row j /S col i</cell></row><row><cell>row</cell></row><row><cell>j</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>table grids from the table row/column lines. G ? R (M ?N )?4 , where M , N are the number of rows and columns occupied by the table grid structure, respectively. More specifically, each bounding box can be precisely defined by (x 1 , y 1 , x 2 , y 2 ), where (x 1 , y 1 ) corresponds to the position of the upper left in the bounding box, and (x 2 , y 2 )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Different from the previous table structure recognition methods<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref> which mostly recover the table structure based on the visual modality, we fuse the output features for each basic table grid from both visual and textual modalities.</figDesc><table /><note>Therefore, we design the vision module and text module in the embedder to extract visual features E v and textual features E t , respectively, and fuse both features to produce the final grid-level features E through the blender module.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>table grids</head><label>grids</label><figDesc></figDesc><table /><note>, and embed contents into corresponding feature vectors? t using a pretrained BERT model [16].? t ? R (M ?N )?B , where B is the feature vector dimension of the BERT. It's worth noting that both the recognizer and the BERT do not update the parameters during the training phase. The final textual</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>table cell along the rows and columns. By combining the spanning information and the prediction results of the splitter, which contains the table row/column lines information, the bounding box coordinates of each table cell can be obtained. We match the text content with position to the table cells according to the IOU. The output for every table image finally contains coordinates of predicted cell bounding</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used for our experiments. It's worth noting that SciTSR provides the text contents with positions for each table image, but not with being aligned with the table cells. However, in our model, we need the text position in each table cell to generate the labels of splitter. Therefore, we apply the data preprocessing 1 to align the text information with the table cells. 2) PubTabNet [5] contains over 500k training samples and 9k validation samples. PubTabNet [5] annotates each table image with information about both the structure of table and the text content with position of each nonempty table cell. Moreover, nearly half of them are complex tables which have spanning cells in PubTabNet.</figDesc><table><row><cell>Dataset</cell><cell>SciTSR</cell><cell>SciTSR-COMP</cell><cell>PubTabNet</cell></row><row><cell>Train</cell><cell>12k</cell><cell>-</cell><cell>500k</cell></row><row><cell>Validation</cell><cell>-</cell><cell>-</cell><cell>9k</cell></row><row><cell>Test</cell><cell>3k</cell><cell>716</cell><cell>9k</cell></row></table><note>1) SciTSR [10] is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format as well as their corresponding high quality structure labels obtained from LaTeX source files. SciTSR splits 12, 000 for training and 3, 000 for testing. Furthermore, to reflect the model's ability of recognizing complex tables, [10] extracts all the 716 complex tables from the test set as a test subset, called SciTSR-COMP.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>table structure</head><label>structure</label><figDesc></figDesc><table><row><cell>recognition literature</cell></row><row><cell>and competitions, to evaluate the performance of our model for recognition of</cell></row><row><cell>the table structure.</cell></row><row><cell>In order to use the F1-Measure, the adjacency relationships among the table</cell></row><row><cell>cells need to be detected. F1-Measure measures the percentage of correctly</cell></row><row><cell>detected pairs of adjacent cells, where both cells are segmented correctly and</cell></row><row><cell>identified as neighbors.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>Comparison among systems from T1 to T4.</figDesc><table><row><cell>Attributes for comparison include: 1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 ,</head><label>3</label><figDesc>we show the F1-Measure of systems T1-T4 on SciTSR and SciTSR-COMP datasets. Almost 76.3% of the tables are simple tables in SciTSR test dataset, and all are complex tables in the SciTSR-COMP dataset. The performance gap between T1 and other systems (T2-T4) is remarkable on the SciTSR dataset, but the gain is more significant on the SciTSR-COMP dataset, e.g., almost 6% in F1-Measure from T1 to T4. This is because all table cells have only one table grid in the simple table, which means that the table grid structure is the table structure. However there are</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 :</head><label>3</label><figDesc>Comparison of F1-Measure among different systems inTable 2on SciTSR and SciTSR-COMP datasets. have more than one table grids in the complex table. Therefore, the designed system T1 can only process simple tables well by using splitter to predict the fine grid structure of table, and T2-T4 have the ability to recover the structure of the complex tables through the merger. The comparison of T1 with T2, T3, T4 on the SciTSR-COMP dataset demonstrates the effectiveness of the merger.</figDesc><table><row><cell></cell><cell></cell><cell>SciTSR</cell><cell></cell><cell cols="3">SciTSR-COMP</cell><cell></cell></row><row><cell>System</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FPS</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell></row><row><cell>T1</cell><cell>96.69</cell><cell>94.15</cell><cell>95.40</cell><cell>93.81</cell><cell>96.06</cell><cell>89.77</cell><cell>16.47</cell></row><row><cell>T2</cell><cell>96.63</cell><cell>94.36</cell><cell>95.48</cell><cell>94.15</cell><cell>88.04</cell><cell>90.99</cell><cell>2.00</cell></row><row><cell>T3</cell><cell>97.40</cell><cell>95.97</cell><cell>96.68</cell><cell>96.52</cell><cell>93.82</cell><cell>95.15</cell><cell>3.65</cell></row><row><cell>T4</cell><cell>97.70</cell><cell>96.52</cell><cell>97.11</cell><cell>96.80</cell><cell>94.67</cell><cell>95.72</cell><cell>1.94</cell></row><row><cell cols="8">some table cells Vision and Language Modalities In order to evaluate the effect of each</cell></row><row><cell cols="8">modality, we design the systems T2, T3, T4 as shown in Table 2. Each system</cell></row><row><cell cols="8">uses vision module (VM), text module (TM) or both in the embedder. The</cell></row><row><cell cols="7">experiment results on SciTSR and SciTSR-COMP are shown in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>table grid structure from the splitter. Second Row: the predictions of the table structure from the T3 which only uses the vision module in the embedder. Third Row: the predictions of the table structure from the T4 which uses both the vision module and text module in the embedder. Note that the predictions of table grid structure in systems T3 and T4 are the same, and the predictions of table structure in the third row are all totally correct. The red dash boxes denote the different predictions between T3 and T4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 4 :</head><label>4</label><figDesc>A performance comparison between our method and other state-of-the-art methods on the SciTSR and SciTSR-COMP datasets. ], our method achieves state-of-the-art. Similar to DeepDeSRT<ref type="bibr" target="#b2">[3]</ref>, T1 is actually a segmentation model. We take full consideration of the extremely unbalanced number of foreground and background pixels in the segmentation masks and design a more reasonable segmentation loss to penalize the model during training in Eq. 1 2, which makes the performance of T1 significantly better than DeepDeSRT. It is worth noting that GraphTSR<ref type="bibr" target="#b9">[10]</ref> needs the text position in table cells during both the training and testing stage, while our method only takes table images as input during inference. Although the comparison between GraphTSR and our method is not fair, our method still outperforms it to a certain extend. The TabStruct-Net<ref type="bibr" target="#b5">[6]</ref> applies a detection network<ref type="bibr" target="#b13">[14]</ref> to detect individual cells in a table image, however, it fails to capture empty cells accurately due to the absence of cell content. Our SEM obtains the bounding boxes of cells based on table lines detection, which makes the prediction of empty cells less difficult. In addition, the embedder makes full use of the visual and textual modalities, and the merger enables the model to process complex tables, which ultimately makes our method achieve state-of-the-art. Some table structure recognition results of our and other methods are shown inFigure 12.</figDesc><table><row><cell></cell><cell></cell><cell>SciTSR</cell><cell></cell><cell cols="3">SciTSR-COMP</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FPS</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell></row><row><cell>Adobe [3]</cell><cell>82.9</cell><cell>79.6</cell><cell>81.2</cell><cell>79.6</cell><cell>73.7</cell><cell>76.5</cell><cell>-</cell></row><row><cell>TabbyPDF [50]</cell><cell>91.4</cell><cell>91.0</cell><cell>91.2</cell><cell>86.9</cell><cell>84.1</cell><cell>85.5</cell><cell>-</cell></row><row><cell>DeepDeSRT [3]</cell><cell>89.8</cell><cell>89.7</cell><cell>89.7</cell><cell>81.1</cell><cell>81.3</cell><cell>81.2</cell><cell>20.88</cell></row><row><cell>GraphTSR [10]</cell><cell>93.6</cell><cell>93.1</cell><cell>93.4</cell><cell>94.3</cell><cell>92.5</cell><cell>93.4</cell><cell>-</cell></row><row><cell>TabStruct-Net [6]</cell><cell>92.7</cell><cell>91.3</cell><cell>92.0</cell><cell>90.9</cell><cell>88.2</cell><cell>89.5</cell><cell>0.77</cell></row><row><cell>T1</cell><cell cols="7">96.69 94.15 95.40 93.81 96.06 89.77 16.47</cell></row><row><cell>SEM</cell><cell cols="6">97.70 96.52 97.11 96.80 94.67 95.72</cell><cell>1.94</cell></row><row><cell cols="8">trained and tested with default configuration. Comparing with other meth-</cell></row><row><cell>ods [10, 3, 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>table images of the SciTSR dataset. The blue lines denote the prediction of table structure. First Row: the results of the DeepDeSRT. Second Row: the intermediate cell detection results of the TabStruct-Net. Third Row: the results of our method. The predictions of table structure in the third row are all correct. 4.8. ICDAR 2021 Competition on Scientific Literature Parsing, Task-B ICDAR 2021 Competition on Scientific Literature Parsing, Task-B 3 is organized by the IBM company in conjunction with IEEE ICDAR 2021. This competition aims to drive the advances in table recognition. Different from the table structure recognition task, we need to recognize not only the structure of the table, but also the content within each cell. Through our method, we can not only predict the structure of the table, but also obtain the position of each cell. Inspired by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 5 :</head><label>5</label><figDesc>The performance of table recognition on PubTabNet validation set.</figDesc><table><row><cell></cell><cell></cell><cell>TEDS</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>FPS</cell></row><row><cell></cell><cell>Simple</cell><cell>Complex</cell><cell>All</cell><cell></cell></row><row><cell>T3 + Recognizer</cell><cell>94.7</cell><cell>92.1</cell><cell>93.4</cell><cell>1.81</cell></row><row><cell>T4 + Recognizer</cell><cell>94.8</cell><cell>92.5</cell><cell>93.7</cell><cell>1.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 6 :</head><label>6</label><figDesc>Table recognition competition results on PubTabNet final evaluation data set.</figDesc><table><row><cell>Team Name</cell><cell>Simple</cell><cell>TEDS Complex</cell><cell>All</cell></row><row><cell>Davar-Lab-OCR</cell><cell>97.88</cell><cell>94.78</cell><cell>96.36</cell></row><row><cell>VCGroup</cell><cell>97.90</cell><cell>94.68</cell><cell>96.32</cell></row><row><cell>USTC-NELSLIP</cell><cell>97.60</cell><cell>94.89</cell><cell>96.27</cell></row><row><cell>YG</cell><cell>97.38</cell><cell>94.79</cell><cell>96.11</cell></row><row><cell>DBJ</cell><cell>97.39</cell><cell>93.87</cell><cell>95.66</cell></row><row><cell>TAL</cell><cell>97.30</cell><cell>93.93</cell><cell>95.65</cell></row><row><cell>PaodingAI</cell><cell>97.35</cell><cell>93.79</cell><cell>95.61</cell></row><row><cell>anyone</cell><cell>96.95</cell><cell>93.43</cell><cell>95.23</cell></row><row><cell>LTIAYN</cell><cell>97.18</cell><cell>92.40</cell><cell>94.84</cell></row><row><cell>EDD</cell><cell>91.20</cell><cell>85.40</cell><cell>88.30</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ZZR8066/SciTSR</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://aieval.draco.res.ibm.com/challenge/40/overview</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decnt: Deep deformable cnn for table detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2880211</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="74151" to="74161" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2017.192</idno>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of table recognition: Models, observations, transformations, and inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cordy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10032-004-0120-9</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-based table recognition: Data, model, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="564" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<title level="m">Table structure recognition using topdown and bottom-up cues</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2020</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation for table structure recognition in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00225</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1397" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iterative answer prediction with pointer-augmented multimodal transformers for textvqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01001</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9989" to="9999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2598339</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Mao</surname></persName>
		</author>
		<title level="m">Complicated table structure recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00125</idno>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
	<note>Res2tim: Reconstruct syntactic structures from table images</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<title level="m">Rethinking table recognition using graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298965</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.06.017</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2017.06.017" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="196" to="206" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orsi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2013.292</idno>
		<title level="m">2013 International Conference on Document Analysis and Recognition (IC-DAR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1449" to="1453" />
		</imprint>
	</monogr>
	<note>Icdar 2013 table competition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>D?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00243</idno>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1510" to="1515" />
		</imprint>
	</monogr>
	<note>Icdar 2019 competition on table detection and recognition (ctdar)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00027</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itonori</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.1993.395625</idno>
		<idno>doi:10.1109/ ICDAR.1993.395625</idno>
		<title level="m">Table structure recognition based on textblock arrangement and ruled line position</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
	<note>International Conference on Document Analysis and Recognition (ICDAR)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Co?asnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lemaitre</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-85729-859-1_20</idno>
		<title level="m">Recognition of Tables and Forms</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kieninger</surname></persName>
		</author>
		<title level="m">Table structure recognition based on robust block segmentation</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tree-structured decoder for image-to-markup generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11076" to="11085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Relation networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00326</idno>
		<idno>doi:10. 1109/CVPR.2019.00326</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="532" to="548" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense semantic embedding network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.01.028</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2019.01.028" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="285" to="296" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accuracy vs. complexity: A trade-off in visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2021.108106</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2021.108106" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">108106</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linguistically-aware attention for reducing the semantic gap in vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107812</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2020.107812" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">107812</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.106</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Textmountain: Accurate scene text detection via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107336</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2020.107336" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107336</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-toend trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Track, attend, and parse (tap): An end-toend framework for online handwritten mathematical expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2018.2844689</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2848939</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01354</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13525" to="13534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A constraint-based approach to table structure derivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Document Analysis and Recognition</title>
		<meeting>the Seventh International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">911</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tree edit distance: Robust and memory-efficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pawlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Augsten</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.is.2015.08.004</idno>
		<ptr target="https://doi.org/10.1016/j.is.2015.08.004" />
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<idno>doi:10.1109/ CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shigarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikhailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Paramonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cherkashin</surname></persName>
		</author>
		<title level="m">Tabbypdf: Web-based system for pdf table extraction</title>
		<editor>R. Dama?evi?ius, G. Vasiljevien?</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="257" to="269" />
		</imprint>
	</monogr>
	<note>Information and Software Technologies</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Abcnet: Real-time scene text spotting with adaptive bezier-curve network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00983</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9806" to="9815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-toend trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mask textspotter v3: Segmentation proposal network for robust scene text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
