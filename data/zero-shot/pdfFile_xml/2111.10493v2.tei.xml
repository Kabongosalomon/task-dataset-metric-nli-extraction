<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DISCRETE REPRESENTATIONS STRENGTHEN VISION TRANSFORMER ROBUSTNESS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
							<email>lujiang@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
							<email>dehghani@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
							<email>vondrick@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>sukthankar@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
							<email>irfanessa@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Insitute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">DISCRETE REPRESENTATIONS STRENGTHEN VISION TRANSFORMER ROBUSTNESS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vectorquantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite their high performance on in-distribution test sets, deep neural networks fail to generalize under real-world distribution shifts <ref type="bibr" target="#b3">(Barbu et al., 2019)</ref>. This gap between training and inference poses many challenges for deploying deep learning models in real-world applications where closedworld assumptions are violated. This lack of robustness can be ascribed to learned representations that are overly sensitive to minor variations in local texture and insufficiently adept at representing more robust scene and object characteristics, such as the shape.</p><p>Vision Transformer (ViT) <ref type="bibr">(Dosovitskiy et al., 2020)</ref> has started to rival Convolutional Neural Networks (CNNs) in many computer vision tasks. Recent works found that ViTs are more robust than CNNs <ref type="bibr">(Paul &amp; Chen, 2021;</ref><ref type="bibr">Mao et al., 2021b;</ref><ref type="bibr" target="#b5">Bhojanapalli et al., 2021)</ref> and generalize favorably on a variety of visual robustness benchmarks <ref type="bibr">(Hendrycks et al., 2021b;</ref><ref type="bibr">Hendrycks &amp; Dietterich, 2019)</ref>. These work suggested that ViTs' robustness comes from the self-attention architecture that captures a globally-contextualized inductive bias than CNNs.</p><p>However, though self-attention can model the shape information, we found that ImageNet-trained ViT is still biased to textures than shape <ref type="bibr">(Geirhos et al., 2019)</ref>. We hypothesize that this deficiency in robustness comes from the high-dimensional, individually informative, linear tokenization which biases ViT to minimize empirical risk via local signals without learning much shape information.</p><p>In this paper, we propose a simple yet novel input layer for vision transformers, where image patches are represented by discrete tokens. To be specific, we discretize an image and represent an image patch as a discrete token or "visual word" in a codebook. Our key insight is that discrete tokens capture important features in a low-dimensional space <ref type="bibr">(Oord et al., 2017)</ref> preserving shape and structure of the object (see <ref type="figure">Figure 2</ref>). Our approach capitalizes on this discrete representation to promote the robustness of ViT. Using discrete tokens drives ViT towards better modeling of spatial interactions between tokens, given that individual tokens no longer carry enough information to  <ref type="figure">Figure 1</ref>: Overview of the proposed ViT using discrete representations. In addition to the pixel embeddings (orange), we introduce discrete tokens and embeddings (pink) as the input to the standard Transformer Encoder of the ViT model <ref type="bibr">(Dosovitskiy et al., 2020)</ref>.</p><p>depend on. We also concatenate a low dimensional pixel token to the discrete token to compensate for the potentially missed local details encoded by discrete tokens, especially for small objects.</p><p>Our approach only changes the image patch tokenizer to improve generalization and robustness, which is orthogonal to all existing approaches for robustness, and can be integrated into architectures that extend vision transformer. We call the ViT model using our Discrete representation or Dr. ViT.</p><p>Our experiments and visualizations show that incorporating discrete tokens in ViT significantly improves generalization accuracy for all seven out-of-distribution ImageNet benchmarks: Stylized-ImageNet by up to 12%, ImageNet-Sketch by up to 10%, and ImageNet-C by up to 10%. Our method establishes the new state-of-the-art on four benchmarks without any dataset-specific data augmentation. The success of discrete representation has been limiting to image generation <ref type="bibr">(Ramesh et al., 2021;</ref><ref type="bibr">Esser et al., 2021)</ref>. Our work is the first to connect discrete representations to robustness and demonstrate consistent robustness improvements. We hope our work helps pave the road to a joint vision transformer for both image classification and generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Vision Transformer (ViT) <ref type="bibr">(Dosovitskiy et al., 2020)</ref>, inspired by the Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> in NLP, is the first CNN-free architecture that achieves state-of-the-art image classification accuracy. Since its inception, numerous works have proposed improvements to the ViT architecture <ref type="bibr" target="#b8">(Wang et al., 2021;</ref><ref type="bibr" target="#b8">Chu et al., 2021;</ref><ref type="bibr">Liu et al., 2021;</ref><ref type="bibr">d'Ascoli et al., 2021)</ref>, objective <ref type="bibr" target="#b6">(Chen et al., 2021a)</ref>, training strategy <ref type="bibr">(Touvron et al., 2021)</ref>, etc.. Given the difficulty to study all existing ViT models, this paper focuses on the classical ViT model <ref type="bibr">(Dosovitskiy et al., 2020)</ref> and its recent published versions. <ref type="bibr">Specifically, Steiner et al. (2021)</ref> proposed ViT-AugReg that applies stronger data augmentation and regularization to the ViT model. <ref type="bibr">Tolstikhin et al. (2021)</ref> introduced MLP-Mixer to replace self-attention in ViT with multi-layer perceptions (MLP).</p><p>We select the above ViT model family <ref type="bibr">(Dosovitskiy et al., 2020;</ref><ref type="bibr">Steiner et al., 2021;</ref><ref type="bibr">Tolstikhin et al., 2021)</ref> in our robustness study for three reasons. First, they represent both the very first and one-of-the-best vision transformers in the literature. Second, these models demonstrated competitive performance when pre-trained on sufficiently large datasets such as ImageNet-21K and JFT-300M. Finally, unlike other Transformer models, they provide architectures consisting of solely Transformer layers as well as a hybrid of CNN and Transformer layers. These properties improve our understanding of robustness for different types of network layers and datasets.</p><p>Robustness. Recent works established multiple content robustness datasets to evaluate the out-ofdistribution generalization of deep models <ref type="bibr" target="#b3">(Barbu et al., 2019;</ref><ref type="bibr">Hendrycks et al., 2021b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b3">Wang et al., 2019;</ref><ref type="bibr">Geirhos et al., 2019;</ref><ref type="bibr">Hendrycks &amp; Dietterich, 2019;</ref><ref type="bibr">Recht et al., 2019)</ref>. In this paper, we consider 7 ImageNet robustness benchmarks of real-world test images (or proxies) where deep models trained on ImageNet are shown to suffer from notable performance drop. Existing works on robustness are targeted at closing the gap in a subset of these ImageNet robustness benchmarks and were extensively verified with the CNNs. Among them, carefully-designed data augmentations <ref type="bibr">(Hendrycks et al., 2021a;</ref><ref type="bibr" target="#b10">Cubuk et al., 2018;</ref><ref type="bibr">Steiner et al., 2021;</ref><ref type="bibr">Mao et al., 2021b</ref>;a), model regularization <ref type="bibr" target="#b3">(Wang et al., 2019;</ref><ref type="bibr">Huang et al., 2020b;</ref><ref type="bibr">Hendrycks et al., 2019)</ref>, and multitask learning <ref type="bibr" target="#b12">(Zamir et al., 2020)</ref> are effective to address the issue.</p><p>More recently, a few studies <ref type="bibr">(Paul &amp; Chen, 2021;</ref><ref type="bibr" target="#b5">Bhojanapalli et al., 2021;</ref><ref type="bibr">Naseer et al., 2021;</ref><ref type="bibr">Shao et al., 2021)</ref> suggest that ViTs are more robust than CNNs. Existing works mainly focused on analyzing the cause of superior generalizability in the ViT model. As our work focuses on discrete token input, we train the models using the same data augmentation as the ViT-AugReg baseline <ref type="bibr">(Steiner et al., 2021)</ref>. While tailoring data augmentation <ref type="bibr">(Mao et al., 2021b</ref>) may further improve our results, we leave it out of the scope of this paper.</p><p>Discrete Representation was used as a visual representation prior to the deep learning revolution, such as in bag-of-visual-words model <ref type="bibr">(Sivic &amp; Zisserman, 2003;</ref><ref type="bibr" target="#b9">Csurka et al., 2004)</ref> and VLAD model <ref type="bibr" target="#b1">(Arandjelovic &amp; Zisserman, 2013)</ref>. <ref type="bibr">Recently, (Oord et al., 2017;</ref><ref type="bibr">Vahdat et al., 2018)</ref> proposed neural discrete representation to encode an image as integer tokens. Recent works used discrete representation mainly for image synthesis <ref type="bibr">(Ramesh et al., 2021;</ref><ref type="bibr">Esser et al., 2021)</ref>. To the best of our knowledge, our work is the first to demonstrate discrete representations strengthening robustness. The closest work to ours is BEiT <ref type="bibr" target="#b2">(Bao et al., 2021)</ref> that pretrains the ViTs to predict the masked tokens. However, the tokens are discarded after pretraining, where the ViT model can still overfit the non-robust nuisances in the pixel tokens at later finetuning stage, undermining its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>3.1 PRELIMINARY ON VISION TRANSFORMER Vision Transformer (Dosovitskiy et al., 2020) is a pure transformer architecture that operates on a sequence of image patches. The 2D image x ? R H?W ?C is flattened into a sequence of image patches, following the raster scan, denoted by x p ? R L?(P 2 ?C) , where L = H?W P 2 is the effective sequence length and P 2 ? C is the dimension of image patch. A learnable classification token x class is prepended to the patch sequence, then the position embedding E pos is added to formulate the final input embedding h 0 .</p><formula xml:id="formula_0">h 0 = [x class ; x 1 p E; x 2 p E; ? ? ? ; x L p E] + E pos , E ? R (P 2 ?C)?D , E pos ? R (L+1)?D (1) h = MSA(LN(h ?1 )) + h ?1 , = 1, . . . , L f (2) h = MLP(LN(h )) + h , = 1, . . . , L f (3) y = LN(h 0 L ),<label>(4)</label></formula><p>The architecture of ViT follows that of the Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>, which alternates layers of multi-headed self-attention (MSA) and multi-layer perceptron (MLP) with LayerNorm (LN) and residual connections being applied to every block. We denote the number of blocks as L f . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ARCHITECTURE</head><p>Existing ViTs represent an image patch as a sequence of pixel tokens, which are linear projections of flattened image pixels. We propose a novel architecture modification to the input layer of the vision transformer, where an image patch x p is represented by a combination of two embeddings. As illustrated in <ref type="figure">Fig. 1</ref>, in addition to the original pixel-wise linear projection, we discretize an image patch into an discrete token in a codebook V ? R K?dc , where K is the codebook size and d c is the dimension of the embedding. The discretization is achieved by a vector quantized (VQ) encoder p ? that produces an integer z for an image patch x as:</p><formula xml:id="formula_1">p ? (z = k|x) = 1(k = arg min j=1:K z e (x) ? V j 2 ),<label>(5)</label></formula><p>where z e (x) denotes the output of the encoder network and 1(?) is the indicator function.</p><p>The encoder is applied to the patch sequence x p ? R L?(P 2 ?C) to obtain an integer sequence z d ? {1, 2, ..., K} L . Afterward, we use the embeddings of both discrete and pixel tokens to construct the input embedding to the ViT model. Specifically, the input embedding in Equation 1 is replaced by:</p><formula xml:id="formula_2">h 0 = [x class ; f (V z 1 d , x 1 p E); f (V z 2 d , x 2 p E); ? ? ? ; f (V z L d , x L p E)] + E pos ,<label>(6)</label></formula><p>where f is the function, embodied as a neural network layer, to combine the two embeddings. We empirically compared four network designs for f and found that the simplest concatenation works 4 1 2 8 9 7 5 9 2 decode t abby c at t abby c at v ul t ur e f l ami ngo f l ami ngo c out i nous t ok ens ( i mage pat c h) di s c r et e t ok ens Comparison of pixel and discrete embeddings: Pixel and discrete embeddings represent different aspects of the input image. Discrete embeddings capture important features in a low-dimension space <ref type="bibr">(Oord et al., 2017)</ref> that preserves the global structure of an object but lose local details. <ref type="figure">Fig. 2</ref> compares the original image (top) and the reconstructed images decoded from the discrete embeddings of our model (bottom). As shown, the decoded images from discrete embeddings reasonably depict the object shape and global context. Due to the quantization, the decoder hallucinates the local textures, e.g., in the cat's eye, or the text in the "vulture" and "flamingo" images. It is worth noting that the VQ encoder/decoder is only trained on ImageNet 2012 but they can generalize to out-of-distribution images. Please see more examples in Appendix A.</p><p>On the flip side, pixel embeddings capture rich details through the linear projection from raw pixels. However, given the expressive power of transformers, ViTs can spend capacity on local textures or nuance patterns that are often circumferential to robust recognition. Since humans recognize images primarily relying on the shape and semantic structure, this discrepancy to human perception undermines ViT's generalization on out-of-distribution data. Our proposed model leverages the power of both embeddings to promote the interaction between modeling global and local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING PROCEDURE</head><p>The training comprises two stages: pretraining and finetuning. First, we pretrain the VQ-VAE encoder and decoder (Oord et al., 2017) on the given training set. We do not use labels in this step. In the finetuning stage, as shown in <ref type="figure" target="#fig_0">Fig. 3a</ref>  shows the first 28 principal components. Our filters capture more structural and shape patterns. Now we discuss the objective that the proposed ViT model optimizes. Let q ? (z|x) denote the VQ encoder, parameterized by network weights ?, to represent an image x into a sequence of integer tokens z. The decoder models the distribution p ? (x|z) over the RGB image generated from discrete tokens. p ? (y|x, z) stands for the proposed vision transformer shown in <ref type="figure">Fig. 1</ref>.</p><p>We factorize the joint distribution of image x, label y, and the discrete token z by p(x, y, z) = p ? (x|z)p ? (y|x, z)p(z). Our overall training procedure is to maximize the evidence lower bound (ELBO) on the joint likelihood:</p><formula xml:id="formula_3">log p(x, y) ? E q ? (z|x) [log p ? (x|z)] ? D KL [q ? (z|x) p ? (y|x, z)p(z)]<label>(7)</label></formula><p>In the first stage, we maximize the ELBO with respect to ? and ?, which corresponds to learning the VQ-VAE encoder and decoder. Following <ref type="bibr">(Oord et al., 2017)</ref>, we assume a uniform prior for both p ? (y|x, z) and p(z). Given that q ? (z|x) predicts a one-hot output, the regularization term (KL divergence in Equation 7) will be a constant. Note that the DALL-E model <ref type="bibr">(Ramesh et al., 2021)</ref> uses a similar assumption to stabilize the training. Our implementation uses VQ- <ref type="bibr">GAN (Esser et al., 2021)</ref> which adds a GAN loss and a perceptual loss. We also include results with VQ-VAE <ref type="bibr">(Oord et al., 2017)</ref> in Appendix A.8 for reference.</p><p>In the finetuning stage, we optimize ? while holding ? and ? fixed, which corresponds to learning a ViT and finetuning the discrete embeddings. This can be seen by rearranging the ELBO:</p><formula xml:id="formula_4">log p(x, y) ? E q ? (z|x) [log p ? (y|x, z)] + E q ? (z|x) [log p ? (x|z)p(z) q ? (z|x) ],<label>(8)</label></formula><p>where p ? (y|x, z) is the proposed ViT model. The first term denotes the likelihood for classification that is learned by minimizing the multi-class cross-entropy loss. The second term becomes a constant given the fixed ? and ?. It is important to note that the discrete embeddings are learned end-to-end in both pretraining and finetuning. Details are discussed in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>Datasets.</p><p>In the experiments, we train all the models, including ours, on ImageNet 2012 or ImageNet-21K under the same training settings, where we use identical training data, batch size, and learning rate schedule, etc. Afterward, the trained models are tested on the ImageNet robustness benchmarks to assess their robustness and generalization capability.</p><p>In total, we evaluate the models on nine benchmarks. ImageNet and ImageNet-Real are two indistribution datasets. ImageNet Seven out-of-distribution (OOD) datasets are considered. <ref type="figure">Fig. 4</ref>  Implementation details On ImageNet, the models are trained with three ViT model variants, i.e. Ti, S, and B, from small to large. The codebook size is K = 1024, and the codebook embeds the discrete token into d c = 256 dimensions. On ImageNet-21K, the quantizer model is a VQGAN and is trained on ImageNet-21K only with codebook size K = 8, 192. All models including ours use the same augmentation (RandAug and Mixup) as in the ViT baseline <ref type="bibr">(Steiner et al., 2021)</ref>. More details can be found in the Appendix A.12.  <ref type="table" target="#tab_5">Table 1</ref> shows the results of the models trained on ImageNet. All models are trained under the same setting with the same data augmentation from (Steiner et al., 2021) except for the ViT-B Vanilla row, which uses the data augmentation from (Dosovitskiy et al., 2020). Our improvement is entirely attributed to the proposed discrete representations. By adding discrete representations, all ViT variants, including Ti, S, and B, improve robustness across all eight benchmarks. When only using discrete representations as denoted by "(discrete only)", we observe a larger margin of improvement (10%-12%) on the datasets depicting object shape: Rendition, Sketch, and Stylized-ImageNet. It is worth noting that it is a challenging for a single model to obtain robustness gains across all the benchmarks as different datasets may capture distinct types of data distributions. Our approach scales as more training data is available. <ref type="table" target="#tab_6">Table 2</ref> shows the performance for the models pretrained on ImageNet-21K and finetuned on ImageNet. Training on sufficiently large datasets inherently improves robustness <ref type="bibr" target="#b5">(Bhojanapalli et al., 2021)</ref> but also renders further improvement even more challenging. Nevertheless, our model consistently improves the baseline ViT-B model across all robustness benchmarks (by up to 10% on ImageNet-C). The results in <ref type="table" target="#tab_5">Table 1 and Table 2</ref> validate our model as a generic approach that is highly effective for the models pretrained on the sufficiently large ImageNet-21k dataset.   <ref type="table" target="#tab_8">Table 3c</ref> studies the OOD generalization setting " <ref type="bibr">IN-SIN" used in (Geirhos et al., 2019)</ref>, where the model is trained only on the ImageNet (IN) and tested on the Stylized ImageNet (SIN) images with conflicting shape and texture information, e.g. the images of a cat with elephant texture. The Stylized-ImageNet dataset is designed to measure the model's ability to recognize shapes rather than textures, and higher performance in <ref type="table" target="#tab_8">Table 3c</ref> is a direct proof of our model's efficacy in recognizing objects by shape. While ViT outperforms CNN on the task, our discrete representations yield another 10+% gain. However, if the discrete token is replaced with the global, continuous CNN features, such robustness gain is gone (cf. <ref type="table" target="#tab_13">Table 8</ref>). This substantiates the benefit of discrete representations in recognizing object shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS</head><p>Additionally, we conduct the "shape vs. texture biases" analysis following <ref type="bibr">(Geirhos et al., 2019)</ref> under the OOD setting "IN-SIN" (cf. Appendix A.5). <ref type="figure">Figure.</ref> 5a compares shape bias between  humans and three models: ResNet-50, ViT-B, and Ours. It shows the scores on 16 categories along their average denoted by the colored horizontal line. Humans are highly biased towards shape with an average fraction of 0.96 to correctly recognize an image by shape. ViT (0.42) is more shapebiased than CNN ResNet-50 (0.20), which is consistent with the prior studies <ref type="bibr">(Naseer et al., 2021)</ref>.</p><p>Adding discrete representation (0.62) greatly shrinks the gap between the ViT (0.42) and human baseline (0.96). Such behavior is not observed when adding global CNN features whose average fraction of shape decisions is 0.3, lower than the baseline ViT, hence is not displayed in the figure.</p><p>Finally, we validate discrete representation's ability in modeling shape information via position embedding. Following <ref type="bibr" target="#b7">(Chen et al., 2021b)</ref>, we compare training ViT with and without using position embedding. As position embedding is the only vehicle to equip ViT with shape information, its contribution suggests to what degree the model makes use of shape for recognition. As shown in <ref type="table" target="#tab_10">Table 5</ref>, removing position embedding from the ViT model only leads to a marginal performance drop (2.8%) on ImageNet, which is consistent with <ref type="bibr" target="#b7">(Chen et al., 2021b)</ref>. However, without position embedding, our model accuracy drops by 29%, and degrades by a significant 36%-94% on the robustness benchmarks. This result shows that spatial information becomes crucial only when discrete representation is used, which also suggests our model relies on more shape information for recognition. Qualitative results. First, following (Dosovitskiy et al., 2020), we visualize the learned pixel embeddings, called filters, of the ViT-B model and compare them with ours in <ref type="figure" target="#fig_0">Fig. 3b</ref> with respect to (1) randomly selected filters (the top row in <ref type="figure" target="#fig_0">Fig. 3b) and</ref> (2) the first principle components (the bottom).</p><p>We visualize the filters of our default model here and include more visualization in Appendix A.1.3. The evident visual differences suggest that our learned filters capture structural patterns.</p><p>Second, we compare the attention from the classification tokens in <ref type="figure" target="#fig_2">Fig. 5b</ref>, where (i) visualizes the individual images and (ii) averages attention values of all the image in the same mini-batch. Our model attends to image regions that are semantically relevant for classification and captures more global contexts relative to the central object.</p><p>Finally, we investigate what information is preserved in discrete representations. Specifically, we reconstruct images by the VQ-GAN decoder from the discrete tokens and finetuned discrete embeddings. <ref type="figure">Fig. 4</ref> visualizes representative examples from ImageNet and the seven robustness benchmarks. By comparing the original and decoded images in <ref type="figure">Fig. 4</ref>, we find the discrete representation preserves object shape but can perturb unimportant local signals like textures. For example, the decoded image changes the background but keeps the dog shape in the last image of the first row. Similarly, it hallucinates the text but keeps the bag shape in the first image of the third row. In the first image of ImageNet-C, the decoding deblurs the bird image by making the shape sharper. Note that the discrete representation is learned only on ImageNet, but it can generalize to the other out-of-distribution test datasets. We present more results with failure cases in Appendix A.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDIES</head><p>Model capacity vs. robustness. Does our robustness come from using larger models? <ref type="figure" target="#fig_3">Fig. 6</ref> shows the robustness vs. the number of parameters for the ViT baselines (blue) and our models (orange). We use ViT variants Ti, S, B, and two hybrid variants Hybrid-S and Hybrid-B, as baselines. We use "+Our" to denote our method and "+Small" to indicate that we use a smaller version of quantization encoder (cf. Appendix A.11.1). With a similar amount of model parameters, our model outperforms the ViT and Hybrid-ViT in robustness.</p><p>Codebook size vs. robustness. In <ref type="table" target="#tab_11">Table 6</ref>, we conduct an ablation study on the size of the codebook, from 1024 to 8192, using the small variant of our quantized encoder trained on ImageNet. A larger codebook size gives the model more capacity, making the model closer to a continuous one. Overall, Stylized-ImageNet benefits the most when the feature is highly quantized. With a medium codebook size, the model strikes a good balance between quantization and capacity, achieving the best overall performance. A large codebook size learned on ImageNet can hurt performance.  Discrete vs. Continuous global representation. In Appendix A.4, instead of using our discrete token, we study whether concatenate continuous CNN representations with global information can improve robustness. Results show that concatenating global CNN representation performs no better than the baseline, which demonstrates the necessity of discrete representations.</p><p>Network designs for combining the embeddings. In Appendix A.9, we experiment with 4 different designs, including addition, concatenation, residual gating (Vo et al., 2019), and cross-attention, to combine the discrete and pixel embeddings. Among them, the simple concatenation yields the best overall result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper introduces a simple yet highly effective input representations for vision transformers, in which an image patch is represented as the combined embeddings of pixel and discrete tokens. The results show the proposed method is generic and works with several vision transformer architectures, improving robustness across seven out-of-distribution ImageNet benchmarks. Our new findings connect the robustness of vision transformer to discrete representation, which hints towards a new direction for understanding and improving robustness as well as a joint vision transformer for both image classification and generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>We thank the discussions and feedback from Han Zhang, Hao Wang, and Ben Poole.</p><p>Ethics statement: The authors attest that they have reviewed the ICLR Code of Ethics for the 2022 conference and acknowledge that this code applies to our submission. Our explicit intent with this research paper is to improve the state-of-the-art in terms of accuracy and robustness for Vision Transformers. Our work uses well-established datasets and benchmarks and undertakes detailed evaluation of these with our novel and improved approaches to showcase state-of-the-art improvements. We did not undertake any subject studies or conduct any data-collection for this project. We are committed in our work to abide by the eight General Ethical Principles listed at ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics).</p><p>Reproducibility statement Our approach is simple in terms of implementation, as we only change the input embedding layer for the standard ViT while keeping everything else, such as the data augmentation, the same as the established ViT work. We use the standard, public available training and testing dataset for all our experiments. We include flow graph for our architecture in <ref type="figure">Fig. 1</ref>, pseudo JAX code in <ref type="figure" target="#fig_0">Fig. 3a</ref>, and implementation details in Section 4.1 and Appendix A.11. We will release our model and code to assist in comparisons and to support other researchers in reproducing our experiments and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>The Appendix is organized as follows. Appendix A.2 extends the discussion on learning objective. Appendix A.1 presents more figures to visualize the attention, the decoded images, and the decoding failure cases. Appendix A.9 compares designs for combing the pixel and discrete embeddings. In the end, Appendix A.11 discusses the implementation details.  <ref type="figure" target="#fig_4">Fig. 7</ref> shows more examples of reconstructed images decoded from the finetuned discrete embedding. Generally, the discrete representation reasonably preserves object shape but can perturb local signals.</p><p>Besides <ref type="figure" target="#fig_4">, Fig 7 also</ref> shows the distribution diversity in the experimented robustness benchmarks, e.g., the objects in ImageNet-A are much smaller. Note that while the discrete representation is learned only on the ImageNet 2012 training dataset (the first row in <ref type="figure" target="#fig_4">Fig. 7)</ref>, it can generalize to the other out-of-distribution test datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 LIMITATIONS OF DISCRETE REPRESENTATION</head><p>We find four failure cases that the discrete representation are unable to capture: text, human faces, parallel lines, and small objects or parts. The example images are illustrated in <ref type="figure" target="#fig_5">Fig. 8</ref>. Without prior knowledge, the discrete representation has difficulty to reconstruct text in image. On the other hand, it is interesting to find the decoder can model animal faces but not human faces. This may be a result of lacking facial images in the training data of ImageNet and ImageNet-21K.</p><p>The serious problem we found for recognition is its failure to capture small objects or parts. Though this can be beneficial sometimes, e.g. force the model to recognize the tire without using the "BMW" logo (see the first image in the last row of <ref type="figure" target="#fig_5">Fig. 8)</ref>, it can cause problems in many cases, e.g. recognizing the small whales in the last image. As a result, our proposed model leverages the power of both embeddings to promote the interaction between modeling global and local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 FILTERS</head><p>In <ref type="figure" target="#fig_0">Fig. 3b</ref> of the main paper, we illustrate the visual comparison of the learned pixel embeddings between the standard ViT and our best-performing model. In this subsection, we extend the visualization to our models with a varying number of pixel dimensions when concatenating the discrete and pixel embedding, where their classification performances are compared in <ref type="table" target="#tab_5">Table 13</ref>. As the dimension of pixel embedding grows, the filters started to pick up more high-frequency signals. However, the default setting in the paper using only 32 pixel dimensions (pixel dim=32), which seems to induce an inductive bias by limiting the budget of pixel representation, turns out to be the best performing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 ATTENTIONS</head><p>We visualize the attention following the steps in <ref type="bibr">(Dosovitskiy et al., 2020)</ref>. To be specific, to compute maps of the attention from the output token to the input space, we use Attention Rollout <ref type="bibr" target="#b0">(Abnar &amp; Zuidema, 2020)</ref>. Briefly, we average attention weights of ViT-B and ViT-B+Ours across all heads and then recursively multiply the weight matrices of all layers. Both ViT and ViT+Ours are trained on ImageNet.</p><p>In <ref type="figure" target="#fig_0">Fig. 11, Fig. 12 and Fig. 13</ref>, we visualize the attention of individual images in the first mini-batch of each test set. As shown, our model attends to image regions that are semantically relevant for  <ref type="table" target="#tab_5">Table 13</ref>. Ours (pixel dim=32) works the best and is used as the default model in the main paper. classification and captures more global contexts relative to the central object. This can be better seen from <ref type="figure" target="#fig_7">Fig. 10</ref>, where the heat map averages the attention values of all images in the first minibatch of each test set. As found in prior works <ref type="bibr">(Dosovitskiy et al., 2020;</ref><ref type="bibr">Naseer et al., 2021)</ref>, ViTs put much attentions on the corners of the image, our attentions are more global and do not exhibit the same defect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DETAILS ON LEARNING OBJECTIVE</head><p>Let x denote an image with the class label y, and z represent the discrete tokens z ? Z. For notational convenience, we use a single random variable to represent the discrete latent variables whereas our implementation actually represents an image as a flattened 1-D sequence following the raster scan.</p><p>? q ? (z|x) denotes the VQ-VAE encoder, parameterized by ?, to represent an image x into the discrete token z.</p><p>? p ? (x|z) is the decoder that models the distribution over the RGB image generated from discrete tokens. ? p ? (y|x, z) stands for the vision transformer model shown in <ref type="figure">Fig. 1</ref> of the main paper.</p><p>We model the joint distribution of image x, label y, and the discrete token z using the factorization p(x, y, z) = p ? (x|z)p ? (y|x, z)p(z). Our overall procedure can be viewed as maximizing the evidence lower bound (ELBO) on the joint likelihood, which yields:</p><formula xml:id="formula_5">D KL [q ? (z|x), p(z|x, y))] = ? z q ? (z|x) log p(z|x, y) q ? (z|x) (9) = ? z q ? (z|x) log p(x, y, z) p(x, y)q ? (z|x)<label>(10)</label></formula><p>Since the f -divergence is non-negative, we have:</p><formula xml:id="formula_6">? z q ? (z|x) log p(x, y, z) q ? (z|x) + log p(x, y)[ z q ? (z|x)] ? 0 (11) log p(x, y) ? z q ? (z|x) log p(x, y, z) q ? (z|x)<label>(12)</label></formula><p>Using the factorization, we have:</p><formula xml:id="formula_7">log p(x, y) ? E q ? (z|x) [log p ? (y|x, z)p ? (x|z)p(z) q ? (z|x) ]<label>(13)</label></formula><p>Equation 13 yields the ELBO:</p><formula xml:id="formula_8">log p(x, y) ? E q ? (z|x) [log p ? (x|z)] + E q ? (z|x) [log p ? (y|x, z)p(z) q ? (z|x) ] (14) ? E q ? (z|x) [log p ? (x|z)] ? D KL [q ? (z|x) p ? (y|x, z)p(z)]<label>(15)</label></formula><p>In the first stage, we maximize the ELBO with respect to ? and ?, which corresponds to learning the VQ-VAE encoder and decoder. We assume a uniform prior for both p ? (y|x, z) and p(z). Given that q ? (z|x) predicts a one-hot output, the regularization term (KL divergence) will be a constant log K, where K is the size of the codebook V. As in the VQ-VAE model, the distribution p ? is deterministic. We note a similar assumption as in the DALL-E model <ref type="bibr">(Ramesh et al., 2021)</ref> is used to stabilize the training, which means the transformer is not learned at this stage. We have the same observation as in DALL-E that this strategy works better than jointly training with p ? (y|x, z).</p><p>In addition to the reconstruction loss, the VQ-VAE's training objective adds a dictionary learning loss and a commitment loss, calculated from:</p><formula xml:id="formula_9">L VQ-VAE = log p(x|z q (x)) + sg[z e (x)] ? v 2 + ? z e (x) ? sg[v] ,<label>(16)</label></formula><p>where z e (x) and z q (x) represent the encoder output and the decoder input, respectively. v denotes the discrete embedding for the image x. sg(?) is the stop gradient function. The training used straight-through estimation which just copies gradients from z q (x) to z e (x). Notice that our implementation uses VQ- <ref type="bibr">GAN (Esser et al., 2021)</ref> which adds an additional GAN loss:</p><formula xml:id="formula_10">L VQ-GAN = log p(x|z q (x)) + sg[z e (x)] ? v 2 + ? z e (x) ? sg[v] + ?L GAN ,<label>(17)</label></formula><p>where ? = 0.25 and ? = 0.1 are used to balance different loss terms. L VQ-GAN also has a perceptual loss <ref type="bibr">(Johnson et al., 2016)</ref>.</p><p>In the second stage of finetuning, we optimize ? while holding ? and ? fixed, which corresponds to learning a vision transformer for classification and finetuning the discrete embedding V. By rearranging Equation 14, we have:</p><formula xml:id="formula_11">log p(x, y) ? E q ? (z|x) [log p ? (y|x, z)] + E q ? (z|x) [log p ? (x|z)p(z) q ? (z|x) ],<label>(18)</label></formula><p>where p ? (y|x, z) is the proposed ViT model as illustrated in <ref type="figure">Fig. 1</ref> of the main paper. The first term denotes the likelihood for classification that is learned by minimizing the multi-class cross-entropy loss. The second term becomes a constant given the fixed ? and ?.</p><p>In the finetuning stage, two sets of parameters are updated: the vision transformer ? is learned from scratch and the discrete embedding V is finetuned. We fix the encoder and stop the gradient propagating back to the encoder considering the instability of straight-through estimation and the stop gradient operation in the dictionary learning loss, i.e. Equation 16. Empirically, we also compared the proposed pretraining-finetuning strategy with the joint learning strategy in which all parameters are optimized simultaneously. We find the joint learning significantly underperforms the proposed learning strategy. As shown in <ref type="table" target="#tab_12">Table 7</ref>, our method achieves the best performance on the robustness benchmarks while is marginally worse than the best baseline with state-of-the-art data augmentation strategy (CutMix + RandAug) on the ImageNet in-distribution validation set. Incorporating state-of-the-art data augmentation in our model is worthy of future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 THE IMPORTANCE OF DISCRETE INFORMATION</head><p>This subsection compares to a new baseline that directly concatenates the global feature extracted by CNN to the input token of ViT. The only difference between this baseline and our model is that ours concatenates discrete embeddings rather than global CNN features. In <ref type="table" target="#tab_13">Table 8</ref>, we extensively search the concatenation dimension of the CNN features in <ref type="bibr">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">384,</ref><ref type="bibr">512,</ref><ref type="bibr">640]</ref>. However, none of the models improve robustness. Our results show that concatenating the global CNN feature performs no better than the ViT baseline and significantly worse than concatenating discrete representations, demonstrating the necessity of discrete representations for robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 TEXTURE VS. SHAPE STUDY</head><p>In <ref type="figure" target="#fig_2">Fig. 5a</ref> of the main paper, we performs the "shape vs. texture biases" analysis following (Geirhos et al., 2019) (see <ref type="figure">Figure 4</ref> in their paper). It uses the score "fraction of shape decision" to quantify a model's shape-bias between 0 and 1. To compute the score, we follow the steps on their Github 1 as follows: 1) evaluate the model on all 1280 images in Stylized ImageNet; 2) map the model decision to 16 classes; 3) exclude images without a cue conflict; 4) take the subset of "correctly" classified images (either shape or texture category correctly predicted); (5) compute "shape bias" as the fraction between correct shape decisions and (correct shape decisions + correct texture decisions). <ref type="figure" target="#fig_2">Fig. 5a</ref> in the main paper presents the scores on 16 categories along their average denoted by the colored horizontal line. We compute the scores for three models trained on ImageNet, i.e. ResNet-50, ViT-B/16, and ours, and quote human scores from <ref type="bibr">(Geirhos et al., 2019)</ref>. We also compute the score for the concatenating global CNN features baseline which is about 0.3. Note that the OOD generalization setting " <ref type="bibr">IN-SIN" (Geirhos et al., 2019)</ref> is used, where the model is trained only on the ImageNet <ref type="figure">(IN)</ref> and tested on the Stylized ImageNet (SIN) images with conflicting shape and texture cue.</p><p>A.6 THE EFFECT OF CODEBOOK ENCODER CAPACITY ON ROBUSTNESS We analyze how the capacity of the discrete encoder affects the model's robustness. We train a small encoder and a standard encoder and compare their performance. We describe the VQ encoder's architecture configuration in <ref type="table" target="#tab_5">Table 17</ref>, where the small encoder has only 13% of the #parameters of the standard VQ encoder. We run experiments in <ref type="table" target="#tab_14">Table 9</ref>. Our results show that using the standard VQ-encoder is better but the small VQ-encoder comparatively yields reasonable results considering the reduced model capacity.</p><p>A.7 THE EFFECT OF PRETRAINING CODEBOOK WITH MORE DATA ON ROBUSNTESS We analyze the effect of pre-training codebook on sufficiently large data. We use the same model architecture, but pretrain two codebooks (and encoders) on the ImageNet ILSVRC 2012 and the ImageNet21K (about 11x larger), respectively. Using the pre-trained codebook, we train ViTs on ImageNet21K and compare the results in <ref type="table" target="#tab_5">Table 10</ref>, where it shows pretraining on large data improves the robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 ADDITIONAL MODELS FOR LEARNING DISCRETE REPRESENTATION</head><p>The results show that the ability of discrete representations to improve ViT's robustness is general which is not limited to specific VQ-GAN models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 COMPARING DESIGNS FOR COMBINING EMBEDDINGS</head><p>In this subsection, we compare different designs to combine the pixel and discrete embeddings. See the combination operation in <ref type="figure">Fig. 1</ref> and in Equation 6 of the main paper. The results are used verify our design of using a simple concatenation presented in the paper. Four designs are considered and discussed below. Their performances are compared in <ref type="table" target="#tab_5">Table 12</ref>. Addition. We first use linear projection matrix E to obtain the pixel embedding that shares the same dimension as the discrete embedding which is 256. We then add the two embedding, and project the resulting embedding to the dimension required by the ViT if needed. Formally, we compute</p><formula xml:id="formula_12">f (V z d , x p E) = (V z d + x p E)E 2 ,<label>(19)</label></formula><p>where V z d and x p E represent the discrete and pixel embeddings, respectively, and E 2 is a new MLP projection layer that is needed when the resulting dimension is different to the ViT input.</p><p>Concatenation. We concatenate the discrete embedding with the pixel embedding, and then feed the resulting embedding into the transformer encoder. By default, we use a dimension of 32 for the pixel embedding, a dimension of 256 for the discrete embedding, thus we pad 0 to the vector if the input dimension of the ViT is higher than 288. Residual Gating. As pixel embeddings may contain nuisances, inspired by <ref type="bibr">(Vo et al., 2019)</ref>, we learn a gate from both embeddings, and then multiply it with the pixel embeddings to filter out details that are unimportant to classification. Specifically, we calculate the gate by a 2-layer MLP, and apply a softmax to the output from the last layer.</p><formula xml:id="formula_13">f (V z d , x p E) = [V z d ; x p E; 0],<label>(20)</label></formula><formula xml:id="formula_14">G = Softmax(MLP(V z d ; x p E))<label>(21)</label></formula><p>Then the pixel embeddings are gated by:</p><formula xml:id="formula_15">P = G x p E<label>(22)</label></formula><p>Then we concatenate the gated embedding with the discrete embedding:</p><formula xml:id="formula_16">f (V z d , x p E) = [V z d ; P; 0]<label>(23)</label></formula><p>Cross-Attention. We use the discrete embedding as the query, and the pixel token as the key and value in a standard multi-head cross-attention module (MCA).</p><formula xml:id="formula_17">A = MCA(V z d , x p E, x p E)<label>(24)</label></formula><p>We then concatenate the attended feature output with the discrete embedding. We also pad 0 if the ViT requires larger input dimensionality.</p><formula xml:id="formula_18">f (V z d , x p E) = [V z d ; A; 0]<label>(25)</label></formula><p>A.9.1 THE ROBUSTNESS EFFECT OF USING CONTINUOUS REPRESENTATION WITH DISCRETE REPRESENTTION.</p><p>The above study shows that concatenation is the most effective way to combine the discrete and continuous representations. As we fixed the dimensionality for the discrete token to be 256, we can study the optimal ratio between continuous and discrete representations by changing the dimensionality on the pixel token. <ref type="table" target="#tab_5">Table 13</ref>: The robust performance under different pixel token dimension in concatenation combining method. By increasing the dimension of the pixel embeddings, the model uses more continuous representations than discrete representations. There is an inherent trade off between the out of distribution robustness performance. The more continuous representations the model use, the lower robustness on the out-of-distribution set that depicting object shape, but also aciheves higher accuracy on the in-distribution ImageNet and out of distribution variants ImageNet-A that requires detailed information. However, using only continuous embeddings also hurt robustness. Thus we choice to use dimension 32, which balance the trade-off and achieves the best overall robustness. In addition to the analysis in the main paper, we also investigate whether it is the convolutional operation in the vector quantized encoder that makes the model use the spatial information better. We remove the position embedding from the Hybrid-B model, whose input is also produced by a convolution encoder. Our results show that removing the positional information from the Hybrid model with a convolutional encoder does not decrease the performance. However, for both our models, the discrete only and the combined one, removing positional embedding causes a large drop in performance. This shows that our robustness gain via the spatial structure is from our discrete design, not convolution.  We use three variants of ViT transformer encoder backbone in our experiments. We show the configuration details and the number of parameters in <ref type="table" target="#tab_5">Table 15</ref>. We also use hybrid model with ResNet50 as the tokenization backbone. We show the configuration in <ref type="table" target="#tab_5">Table 16</ref>.</p><p>We use B-16 for the MLPMixer Tolstikhin et al. <ref type="bibr">(2021)</ref>.</p><p>We use two kinds of VQ Encoder in our experiment: Small VQ Encoder and VQ Encoder. The VQ encoder uses the same encoder architecture as the VQGAN encoder <ref type="bibr">(Esser et al., 2021)</ref>. For Small VQ Encoder, we decrease both the number of the resisual blocks and the number for filter channel by half, which is a lightweight model that decrease the model size by around 8 times. We show the configuration detail in <ref type="table" target="#tab_5">Table 17</ref>.</p><p>A.12 TRAINING Implementation details We implement our model in Jax and optimize all models with Adam (Kingma &amp; Ba, 2014). Unless specified otherwise, the input images are resized to 224x224, trained with a batch size of 4,096, with a weight decay of 0.1. We use a linear learning rate warmup and cosine decay. On ImageNet, the models are trained for 300 epochs using three ViT model variants, i.e. Ti, S, and B, from small to large. The VQ-GAN model is also trained on ImageNet for 100K steps using a batch size of 256. The codebook size is K = 1024, and the codebook embeds the discrete token into d c = 256 dimensions. For discrete ViT-B, we use average pooling on the final features. On ImageNet-21K, we train 90 epochs. We finetune on a higher resolution on ImageNet, with Adam and 1e-4 learning rate, batch size 512, for 20K iterations. The quantizer model is a VQ-GAN and is trained on ImageNet-21K only with codebook size K = 8, 192. All models including ours use the same augmentation (RandAug and Mixup) as in the ViT baseline <ref type="bibr">(Steiner et al., 2021)</ref>. More details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head><p>Training We train our model with the Adam (Kingma &amp; Ba, 2014) optimizer. Our batchsize is 4096 and trained on 64 TPU cores. We start from a learning rate of 0.001 and train 300 epoch.  We use linear warm up for 10k iterations and then a cosine annealing for the learning rate schedule. We use 224 ? 224 resolution for the input image. We conduct experiment on three variants of the ViT model, Ti, S, and B, from small to large. In addition, we also experiment on the newly released MLPMixer, which also uses image patch as the token embeddings. We denote our approach as using both representations, and we also run a discrete only variant where we only uses the discrete embeddings without pixel embeddings. Due to the small input dimension of Ti, we only use the code representation without concatenate the pixel representation, and we project the 256 dimension to 192 via a linear projection layer. For the other variants, we concatenate both representations, and pad zeros if additional dimension required for the transformer. We use the same augmentation and model regularization as <ref type="bibr">(Steiner et al., 2021)</ref>, where we use RandAug with hyper-parameter (2,15) and mixup with ? = 0.5, and we apply a dropout rate of 0.1 and stochastic block dropout of 0.1. We download pretrained VQGAN model, which is trained on only ImageNet. We use VQ-GAN's encoder only with a codebook size of 1,024. The codebook integer is embedded into a 256 dimension vector. For the ViT-B discrete, we find training 800 epoch instead of 300 epoch can give another 0.1-1% gain over the test set, while training others for longer results in decreased performance.</p><p>ImageNet-21K Training Our models use learning rate of 0.001, weight decay of 0.03, and train 90 epoch. We use linear warm up for 10k iterations and then a cosine annealing for the learning rate schedule. We use 224?224 resolution for the input image. We use the same augmentation as <ref type="bibr">(Steiner et al., 2021)</ref>. For the quantizer model, we use VQGAN that is trained on unlabeled ImageNet-21K only. We use a codebook size of 8,192, and the codebook integer is embedded into a 256 dimension vector. To evaluate on the standard test set, we further finetune the pretrained ImageNet-21K model on ImageNet. We use Adam and optimize for 20k with 500 steps of warm up. We use a learning rate of 0.0001. We use a larger resolution 384 ? 384 and 512 ? 512. We only experiment the ViT-B variant.</p><p>For training VQ Encoder and Decoder, we train with batchsize 256 until it converges. We use perceptual loss with weight of 0.1, adversarial loss with weight 0.1. We use L1 gradient penalty of 10 during the optimization. The model is trained with resolution of 256 ? 256. The model can scale to different image resolution without finetuning. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(a) Pseudo code for training the proposed ViT model. (b) Comparing visualized pixel embeddings of the ViT and our model. Top row shows the randomly selected filters and Bottom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Deng et al., 2009) is the standard validation set of ILSVRC2012. ImageNet-Real (Beyer et al., 2020) corrects the label errors in the ImageNet validation set (Northcutt et al., 2021), which measures model's generalization on different labeling procedures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>We show the fraction of shape decisions on Stylized-ImageNet in Figure (a), and attention on OOD images in Figure (b), where (i) is the attention map, and (ii) is the heat map of averaged attention from images in a mini-batch. See Appendix A.1.4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The robustness vs. #model-parameters on 4 robust test set. Our models (orange) achieve better robustness with a similar model capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of eight evaluation benchmarks. Each image consists of the original test image (Left) and the decoded image (Right) from the finetuned discrete embeddings. The encoder and decoder are trained only on ImageNet 2012 data but generalize on out-of-distribution datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>tFigure 8 :</head><label>8</label><figDesc>ex t ( l et t er s or di gi t s ) human f ac es par al l el l i nes or c ompl ex t ex t ur es s mal l obj ec t s or par t s Visualization of failure cases for the decoded images. Each image consists of the original test image (Left) and the decoded image (Right) from the finetuned discrete embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Comparing visualized pixel embeddings of the ViT and our model. The top row shows the randomly selected filters and the Bottom shows the first 28 principal components. Our models with varying pixel dimensions are shown, where their classification performances are compared in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of average attention of the ViT (top row) and the proposed model (bottom row) on four validation datasets: ImageNet 2012, ImageNet-R, Stylized-ImageNet, and ObjectNet. The heat map averages the attention values of all images in the first mini-batch of each test set. The results show our attention capture more global context relative to the central object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A. 3</head><label>3</label><figDesc>COMPARING TO DATA AUGMENTATIONS Since our model only changes the token representation in the ViT architecture, it is conceptually different and complementary to data augmentation. Nevertheless, this subsection compares our method with seven combinations of recent data augmentation strategies, including CutOut (DeVries &amp; Taylor, 2017), CutMix (Yun et al., 2019), RandAug (Cubuk et al., 2020), and Mixup<ref type="bibr" target="#b13">(Zhang et al., 2018)</ref>. As discussed following(Steiner et al., 2021), the ViT baseline and our model both use the default augmentation (RandAug + Mixup).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>where ";" indicates the concatenation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :Figure 13 :</head><label>1113</label><figDesc>Attention comparison of the ViT and the proposed model on ImageNet 2012. (a) ViT-B/16 (Sketch) (b) Ours ViT-B/16 (Sketch) (c) ViT-B/16 (ObjectNet) (d) Ours ViT-B/16 (ObjectNet) Attention comparison of the VIT and the proposed model on ImageNet Sketch and ObjectNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.01, 0.2, 0.53, ?, 0.04] 1 [0.6, 0.22, 0.83, ?, 0.01] 1024 [0.26, 0.75, 0.27, ?, 0.98]</figDesc><table><row><cell>Input Image</cell><cell cols="2">3 2 Linear Projection 4 8 0 3 3 Fine-tuned CodeBook 0 Vector Quantized Model Discrete Token 1 2 0 5 5 1 9 5 [0Position 5 Discrete Embedding Image * Class Token Reconstructed Transformer Prediction Encoder Classification Decode</cell></row><row><cell>Pixel Token</cell><cell>Pixel Embedding</cell><cell>Embedding</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Model performance trained on ImageNet. All columns indicate the top-1 classification accuracy except the last column ImageNet-C which indicates the mean Corruption Error (the lower the better). The bold number indicates higher accuracy than the corresponding baseline. The box highlights the best accuracy.</figDesc><table><row><cell>Out of Distribution Robustness Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Model performance when pretrained on ImageNet21K and finetuned on ImageNet with 384x384 resolution. See the caption of Table 1 for more description.</figDesc><table><row><cell>Out of Distribution Robustness Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Comparison to the State-of-the-Art We compare our model with the state-of-the-art results, in-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure 4: Visualization of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>eight evaluation benchmarks.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Each image consists of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>original test image (Left) and the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>decoded image from the finetuned</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>discrete embeddings (Right).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Note that the encoder and decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>are trained only on ImageNet</cell></row><row><cell></cell><cell></cell><cell>car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2012 data but generalize on out-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of-distribution datasets. See more</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>examples in Appendix A.1.1.</cell></row><row><cell>or i gi nal</cell><cell>decoded</cell><cell>or i gi nal</cell><cell>decoded</cell><cell>or i gi nal</cell><cell>decoded</cell><cell>or i gi nal</cell><cell>decoded</cell></row></table><note>cluding ViT with CutOut (DeVries &amp; Taylor, 2017) and CutMix (Yun et al., 2019), on four datasets in Table 3 and Table 4. It is noteworthy that different from the compared approaches that are tai- lored to specific datasets, our method is generic and uses the same data augmentation as our ViT baseline (Steiner et al., 2021), i.e. RandAug and Mixup, for all datasets. On ImageNet-Rendition, we compare with the leaderboard numbers from (Hendrycks et al., 2021a). Trained on ImageNet, our approach beats the state-of-the-art DeepAugment+AugMix approach by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the state-of-the-art classification accuracy on three ImageNet robustness datasets. Notice that the augmentation we used-RandAug+MixUp-is 17% worse than the DeepAug-ment+AugMix on ResNets. Our performance can be further improved by another 6% when pretrained on ImageNet-21K. On ImageNet-Sketch, we surpass the state-of-the-art (Hermann et al., 2020) by 8%. On Stylized-ImageNet, our approach improves 18% top-5 accuracy by switching to discrete representation. On ImageNet-C, our approach slashes the prior mCE by up to 10%. Note that most baselines are trained with CNN architectures that have a smaller capacity than ViT-B. The comparison is fair for the bottom entries that are all built on the same ViT-B backbone.4.3 IN-DEPTH ANALYSISIn this subsection, we demonstrate, both quantitatively and qualitatively, that discrete representations facilitate ViT to better capture object shape and global contexts.</figDesc><table><row><cell>Model</cell><cell>Rendition</cell><cell>Model</cell><cell>Sketch</cell><cell>Model</cell><cell>Stylized</cell></row><row><cell>ResNet 50</cell><cell>36.1</cell><cell>Huang et al. (2020a)</cell><cell>16.1</cell><cell></cell><cell>Top5</cell></row><row><cell>ResNet 50 *21K</cell><cell>37.2</cell><cell>RandAug + Mixup</cell><cell>17.7</cell><cell>BagNet-9</cell><cell>1.4</cell></row><row><cell>DeepAugment</cell><cell>42.2</cell><cell>Xu et al. (2020)</cell><cell>18.1</cell><cell>BagNet-17</cell><cell>2.5</cell></row><row><cell>DeepAugment + AugMix</cell><cell>46.8</cell><cell>Mishra et al. (2020)</cell><cell>24.5</cell><cell>BagNet-33</cell><cell>4.2</cell></row><row><cell>RandAug + Mixup</cell><cell>29.6</cell><cell>Hermann et al. (2020)</cell><cell>30.9</cell><cell>ResNet-50</cell><cell>16.4</cell></row><row><cell>ViT-B</cell><cell>38.2</cell><cell>ViT-B</cell><cell>23.3</cell><cell>ViT-B</cell><cell>22.2</cell></row><row><cell>ViT-B + CutOut</cell><cell>38.1</cell><cell>ViT-B + CutOut</cell><cell>26.9</cell><cell>ViT-B + CutOut</cell><cell>24.7</cell></row><row><cell>ViT-B + CutMix</cell><cell>38.4</cell><cell>ViT-B + CutMix</cell><cell>27.5</cell><cell>ViT-B + CutMix</cell><cell>22.7</cell></row><row><cell>Our ViT-B (discrete only)</cell><cell>48.8</cell><cell>Our ViT-B (discrete only)</cell><cell>39.1</cell><cell>ViT-B *21K</cell><cell>31.3</cell></row><row><cell>Our ViT-B *21K</cell><cell>55.3</cell><cell>Our ViT-B (discrete only) *21K</cell><cell>44.7</cell><cell>Our ViT-B (discrete only)</cell><cell>40.3</cell></row><row><cell cols="2">(a) ImageNet-Rendition</cell><cell>(b) ImageNet-Sketch</cell><cell></cell><cell cols="2">(c) Stylized-ImageNet</cell></row><row><cell>2%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Quantitative results. Result in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>State-of-the-art mean Corruption Error (mCE) ? rate on ImageNet-C (the smaller the better).</figDesc><table><row><cell>Model</cell><cell>mCE</cell><cell>?</cell><cell cols="15">Gauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG</cell></row><row><cell>Resnet152 (He et al., 2016)</cell><cell cols="13">69.27 72.5 73.4 76.3 66.9 81.4 65.7 74.5 70.7 67.8 62.1 51.0</cell><cell>67.1</cell><cell cols="3">75.6 68.9 65.1</cell></row><row><cell cols="14">+Stylized (Geirhos et al., 2019) 64.19 63.3 63.1 64.6 66.1 77.0 63.5 71.6 62.4 65.4 59.4 52.0</cell><cell>62.0</cell><cell cols="3">73.2 55.3 62.9</cell></row><row><cell>+GenInt (Mao et al., 2021a)</cell><cell cols="13">61.70 59.2 60.2 62.4 60.7 70.8 59.5 69.9 64.4 63.8 58.3 48.7</cell><cell>61.5</cell><cell cols="3">70.9 55.2 60.0</cell></row><row><cell>DA (Hendrycks et al., 2021a)</cell><cell cols="2">53.60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT-B</cell><cell cols="13">52.71 43.0 47.2 44.4 63.2 73.4 55.1 70.8 51.6 45.6 35.2 44.2</cell><cell>41.3</cell><cell cols="3">61.6 54.0 59.4</cell></row><row><cell>ViT-B + Ours</cell><cell cols="13">46.22 36.9 38.6 36.0 54.6 57.4 53.4 63.2 45.4 38.7 34.1 40.9</cell><cell>39.6</cell><cell cols="3">56.6 45.0 53.0</cell></row><row><cell>ViT-B *21K</cell><cell cols="13">49.62 47.0 48.3 46.0 55.7 65.6 46.7 54.0 40.0 40.6 32.2 42.3</cell><cell>42.0</cell><cell cols="3">57.1 63.1 63.6</cell></row><row><cell>ViT-B + Ours *21K</cell><cell cols="13">38.74 30.5 30.6 29.2 47.3 54.3 44.4 49.4 34.5 31.7 25.7 34.7</cell><cell>33.1</cell><cell cols="3">52.9 39.5 43.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Contribution of position embedding for robust recognition as measured by the relative performance drop when the position embedding is removed. Position embedding is much more crucial in our model.</figDesc><table><row><cell>Out of Distribution Robustness Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The impact of codebook size for robustness.</figDesc><table><row><cell>CodeBook</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Out of Distribution Robustness Test</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>ImageNet</cell><cell>Real</cell><cell>Rendition</cell><cell>Stylized</cell><cell>Sketch</cell><cell>ObjectNet</cell><cell>V2</cell><cell>A</cell><cell>C ?</cell></row><row><cell>1024</cell><cell>76.63</cell><cell>77.69</cell><cell>45.92</cell><cell>21.95</cell><cell>35.02</cell><cell>26.03</cell><cell>64.18</cell><cell>10.68</cell><cell>58.64</cell></row><row><cell>4096</cell><cell>77.31</cell><cell>78.17</cell><cell>47.04</cell><cell>21.33</cell><cell>35.71</cell><cell>27.72</cell><cell>64.79</cell><cell>11.37</cell><cell>57.26</cell></row><row><cell>8192</cell><cell>77.04</cell><cell>77.92</cell><cell>46.58</cell><cell>20.94</cell><cell>35.72</cell><cell>27.54</cell><cell>65.23</cell><cell>11.00</cell><cell>57.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Model performance trained on ImageNet. We compared with several recent data augmentations, including CutOut (DeVries &amp; Taylor, 2017) and CutMix Yun et al. (2019), on the ViT-B model. All columns indicate the top-1 classification accuracy except the last column ImageNet-C which indicates the mean Corruption Error (lower the better). The bold number indicates higher accuracy than the corresponding baseline.The box highlights the best accuracy. While the state-of-the-art data augmentation (CutMix+RandAug) can improve Imagenet in-distribution accuracy, they are worse than ours on the OOD test accuracy.</figDesc><table><row><cell>Out of Distribution Robustness Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>We replace our discrete token representation with global continuous (GC) features from the CNN model that has the same CNN architecture as our VQGAN encoder. We denote the dimension for the global features as GF-Dim. We vary the dimension of the pixel token to concatenate with the global continuous CNN features. Simply concatenating the global feature extracted from CNN does not improve robustness.</figDesc><table><row><cell></cell><cell>GF-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Out of Distribution Robustness Test</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Dim</cell><cell>ImageNet</cell><cell>Real</cell><cell>Rendition</cell><cell>Stylized</cell><cell>Sketch</cell><cell>ObjectNet</cell><cell>V2</cell><cell>A</cell><cell>C ?</cell></row><row><cell>GC CNN + ViT</cell><cell>640</cell><cell>78.48</cell><cell>83.27</cell><cell>34.75</cell><cell>9.38</cell><cell>24.85</cell><cell>28.37</cell><cell>65.65</cell><cell>16.13</cell><cell>58.71</cell></row><row><cell>GC CNN + ViT</cell><cell>512</cell><cell>78.59</cell><cell>83.24</cell><cell>34.31</cell><cell>10.23</cell><cell>25.50</cell><cell>27.50</cell><cell>65.35</cell><cell>16.48</cell><cell>57.66</cell></row><row><cell>GC CNN + ViT</cell><cell>384</cell><cell>78.51</cell><cell>83.44</cell><cell>34.66</cell><cell>10.47</cell><cell>25.14</cell><cell>28.35</cell><cell>65.86</cell><cell>15.97</cell><cell>57.64</cell></row><row><cell>GC CNN + ViT</cell><cell>256</cell><cell>78.44</cell><cell>83.09</cell><cell>34.96</cell><cell>9.61</cell><cell>25.20</cell><cell>27.66</cell><cell>65.95</cell><cell>15.96</cell><cell>58.35</cell></row><row><cell>GC CNN + ViT</cell><cell>128</cell><cell>78.29</cell><cell>82.96</cell><cell>34.32</cell><cell>9.22</cell><cell>24.90</cell><cell>26.91</cell><cell>65.28</cell><cell>15.73</cell><cell>58.71</cell></row><row><cell>GC CNN + ViT</cell><cell>64</cell><cell>78.34</cell><cell>82.98</cell><cell>34.35</cell><cell>9.19</cell><cell>25.06</cell><cell>27.07</cell><cell>65.48</cell><cell>15.40</cell><cell>58.58</cell></row><row><cell>ViT</cell><cell>0</cell><cell>78.73</cell><cell>84.85</cell><cell>38.15</cell><cell>10.39</cell><cell>28.60</cell><cell>28.71</cell><cell>67.34</cell><cell>16.92</cell><cell>53.51</cell></row><row><cell>Ours (discrete only)</cell><cell>-</cell><cell>78.67</cell><cell>84.28</cell><cell>48.82</cell><cell>22.19</cell><cell>39.10</cell><cell>30.27</cell><cell>66.52</cell><cell>14.77</cell><cell>55.21</cell></row><row><cell>Ours</cell><cell>-</cell><cell>79.48</cell><cell>84.86</cell><cell>44.77</cell><cell>19.38</cell><cell>34.59</cell><cell>30.55</cell><cell>68.05</cell><cell>17.20</cell><cell>46.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>The impact of codebook encoder capacity for robustness.</figDesc><table><row><cell>CodeBook</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Out of Distribution Robustness Test</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>ImageNet</cell><cell>Real</cell><cell>Rendition</cell><cell>Stylized</cell><cell>Sketch</cell><cell>ObjectNet</cell><cell>V2</cell><cell>A</cell><cell>C ?</cell></row><row><cell>Small VQ Encoder</cell><cell>76.63</cell><cell>77.69</cell><cell>45.92</cell><cell>21.95</cell><cell>35.02</cell><cell>26.03</cell><cell>64.18</cell><cell>10.68</cell><cell>58.64</cell></row><row><cell>VQ Encoder</cell><cell>78.67</cell><cell>84.28</cell><cell>48.82</cell><cell>22.19</cell><cell>39.10</cell><cell>30.27</cell><cell>66.52</cell><cell>14.77</cell><cell>55.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>The impact of pretraining codebook on sufficiently large data for robustness. The codebook is pretrained on the standard ImageNet ILSVRC 2012 or the ImageNet21K dataset (about 11x larger). Using the codebook, we train ViT models on ImageNet21K.</figDesc><table><row><cell>Codebook</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Out of Distribution Robustness Test</cell><cell></cell><cell></cell></row><row><cell>Pretrained Data</cell><cell>ImageNet</cell><cell>Real</cell><cell>Rendition</cell><cell>Stylized</cell><cell>Sketch</cell><cell>ObjectNet</cell><cell>V2</cell><cell>A</cell><cell>C ?</cell></row><row><cell>ImageNet</cell><cell>83.32</cell><cell>88.36</cell><cell>51.56</cell><cell>14.77</cell><cell>41.10</cell><cell>42.50</cell><cell>72.57</cell><cell>33.51</cell><cell>55.31</cell></row><row><cell>ImageNet21K</cell><cell>83.40</cell><cell>88.44</cell><cell>55.26</cell><cell>19.69</cell><cell>44.72</cell><cell>43.92</cell><cell>72.68</cell><cell>36.64</cell><cell>45.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Model architecture for discrete representation. We bold the numbers if they improve robustness over the baseline ViT model. The numbers are boxed where VQ-VAE further improves robustness than VQ-GAN under apple-to-apple comparison.</figDesc><table><row><cell>Codebook</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Out of Distribution Robustness Test</cell><cell></cell><cell></cell></row><row><cell>Pretrained Model</cell><cell>ImageNet</cell><cell>Real</cell><cell>Rendition</cell><cell>Stylized</cell><cell>Sketch</cell><cell>ObjectNet</cell><cell>V2</cell><cell>A</cell><cell>C ?</cell></row><row><cell>Baseline (No discrete)</cell><cell>78.73</cell><cell>84.85</cell><cell>38.15</cell><cell>10.39</cell><cell>28.60</cell><cell>28.71</cell><cell>67.34</cell><cell>16.92</cell><cell>53.51</cell></row><row><cell>VQ-VAE (Discrete Only)</cell><cell>78.36</cell><cell>84.35</cell><cell>46.22</cell><cell>23.36</cell><cell>35.17</cell><cell>29.34</cell><cell>66.24</cell><cell>13.61</cell><cell>52.20</cell></row><row><cell>VQ-GAN (Discrete Only)</cell><cell>78.67</cell><cell>84.28</cell><cell>48.82</cell><cell>22.19</cell><cell>39.10</cell><cell>30.27</cell><cell>66.52</cell><cell>14.77</cell><cell>55.21</cell></row><row><cell>VQ-VAE</cell><cell>78.51</cell><cell>83.68</cell><cell>41.54</cell><cell>17.50</cell><cell>30.91</cell><cell>27.43</cell><cell>65.74</cell><cell>15.61</cell><cell>50.47</cell></row><row><cell>VQ-GAN</cell><cell>79.48</cell><cell>84.86</cell><cell>44.77</cell><cell>19.38</cell><cell>34.59</cell><cell>30.55</cell><cell>68.05</cell><cell>17.20</cell><cell>46.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>The accuracy using different combination methods. While directly adding pixel token to discrete token improves the most ImageNet and ImageNet-A accuracy, concatenation method improves the overall robustness.</figDesc><table><row><cell>Combining</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Out of Distribution Robustness Test</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ImageNet</cell><cell>Real</cell><cell>Rendition</cell><cell>Stylized</cell><cell>Sketch</cell><cell>ObjectNet</cell><cell>V2</cell><cell>A</cell><cell>C ?</cell></row><row><cell>Addition</cell><cell>79.67</cell><cell>84.89</cell><cell>40.68</cell><cell>13.59</cell><cell>30.89</cell><cell>29.18</cell><cell>67.19</cell><cell>17.68</cell><cell>50.85</cell></row><row><cell>Concatenation</cell><cell>79.48</cell><cell>84.86</cell><cell>44.77</cell><cell>19.38</cell><cell>34.59</cell><cell>30.55</cell><cell>68.05</cell><cell>17.20</cell><cell>46.22</cell></row><row><cell>Residual Gating</cell><cell>74.59</cell><cell>79.66</cell><cell>41.06</cell><cell>17.03</cell><cell>29.56</cell><cell>25.34</cell><cell>62.37</cell><cell>9.33</cell><cell>59.88</cell></row><row><cell>Cross-Attention</cell><cell>79.18</cell><cell>84.53</cell><cell>43.75</cell><cell>18.67</cell><cell>34.19</cell><cell>29.71</cell><cell>66.73</cell><cell>15.81</cell><cell>50.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Contribution of position embedding for robust recognition as measured by the relative performance drop when the position embedding is removed. We experiment on ViT, Ours, Hybrid-ViT, and Ours with discrete token only. Note that Hybrid uses the continuous, global feature from a ResNet-50 CNN as the input token. A larger relative drop indicates the model relies on spatial information more. Adding discrete token input representation drives position information and spatial structure more crucial in our model.</figDesc><table><row><cell>Out of Distribution Robustness Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Model configuration for the Transformer Encoder.</figDesc><table><row><cell>Model</cell><cell cols="5">Layers Hidden size MLP size Heads #Params</cell></row><row><cell>Tiny (Ti)</cell><cell>12</cell><cell>192</cell><cell>768</cell><cell>3</cell><cell>5.8M</cell></row><row><cell>Small (S)</cell><cell>12</cell><cell>384</cell><cell>1546</cell><cell>6</cell><cell>22.2M</cell></row><row><cell>Base (B)</cell><cell>12</cell><cell>768</cell><cell>3072</cell><cell>12</cell><cell>86M</cell></row><row><cell>Large (L)</cell><cell>24</cell><cell>1024</cell><cell>4096</cell><cell>16</cell><cell>307M</cell></row><row><cell cols="2">A.11 IMPLEMENTATION DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.11.1 ARCHITECTURE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 :</head><label>16</label><figDesc>Model configuration for ResNet+ViT hybrid models.</figDesc><table><row><cell>Model</cell><cell cols="3">Resblocks Patch-size #Params</cell></row><row><cell>Hybrid-S</cell><cell>[3,4,6,3]</cell><cell>1</cell><cell>46.1</cell></row><row><cell>Hybrid-B</cell><cell>[3,4,6,3]</cell><cell>1</cell><cell>111</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 17 :</head><label>17</label><figDesc>Model configuration for the VQ encoders.</figDesc><table><row><cell>Model</cell><cell cols="4">Resblocks Filter Channel Embedding dimension #Params</cell></row><row><cell cols="2">Small VQ Encoder [1,1,1,1,1]</cell><cell>64</cell><cell>256</cell><cell>3.0M</cell></row><row><cell>VQ Encoder</cell><cell>[2,2,2,2,2]</cell><cell>128</cell><cell>256</cell><cell>23.7M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/rgeirhos/texture-vs-shape</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Xiaohua Zhai, and A?ron van den Oord. Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding robustness of transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">When vision transformers outperform resnets without pretraining or strong data augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01548</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust learning through cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Cheerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Attention comparison of the ViT and the proposed model on ImageNet-R</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
