<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ludwig Maximilian University of Munich &amp; IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<country>Germany Runway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ludwig Maximilian University of Munich &amp; IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<country>Germany Runway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ludwig Maximilian University of Munich &amp; IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<country>Germany Runway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ludwig Maximilian University of Munich &amp; IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<country>Germany Runway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ludwig Maximilian University of Munich &amp; IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<country>Germany Runway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers <ref type="bibr" target="#b67">[66,</ref><ref type="bibr" target="#b68">67]</ref>. In contrast, the promising results of GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref> have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models <ref type="bibr">[82]</ref>, which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive <ref type="bibr">Figure 1</ref>. Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K <ref type="bibr" target="#b0">[1]</ref> validation set, evaluated at 512 2 px. We denote the spatial downsampling factor by f . Reconstruction FIDs <ref type="bibr" target="#b28">[29]</ref> and PSNR are calculated on ImageNet-val. <ref type="bibr" target="#b11">[12]</ref>; see also <ref type="bibr">Tab. 8.</ref> results in image synthesis <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b86">85]</ref> and beyond <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b57">57]</ref>, and define the state-of-the-art in class-conditional image synthesis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref> and super-resolution <ref type="bibr" target="#b73">[72]</ref>. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization <ref type="bibr" target="#b86">[85]</ref> or stroke-based synthesis <ref type="bibr" target="#b53">[53]</ref>, in contrast to other types of generative models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b70">69]</ref>. Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models <ref type="bibr" target="#b68">[67]</ref>. Democratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b74">73]</ref>. Although the reweighted variational objective <ref type="bibr" target="#b29">[30]</ref> aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 -1000 V100 days in <ref type="bibr" target="#b14">[15]</ref>) and repeated evaluations on a noisy version of the input space render also inference expensive, so that producing 50k samples takes approximately 5 days <ref type="bibr" target="#b14">[15]</ref> on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint <ref type="bibr" target="#b66">[65,</ref><ref type="bibr" target="#b87">86]</ref>. Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 -1000 steps in <ref type="bibr" target="#b14">[15]</ref>).</p><p>To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.</p><p>Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: <ref type="figure" target="#fig_7">Fig. 2</ref> shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis.</p><p>Following common practice <ref type="bibr">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b67">66,</ref><ref type="bibr" target="#b68">67,</ref><ref type="bibr" target="#b97">96]</ref>, we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66]</ref>, we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs).</p><p>A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks <ref type="bibr" target="#b82">[81]</ref>. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM's UNet backbone <ref type="bibr" target="#b72">[71]</ref> and enables arbitrary types of token-based conditioning mechanisms, see Sec. <ref type="bibr">3.3.</ref> In sum, our work makes the following contributions: (i) In contrast to purely transformer-based approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66]</ref>, our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see <ref type="figure">Fig. 1</ref>) and (b) can be efficiently <ref type="bibr">Figure 2</ref>. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from <ref type="bibr" target="#b29">[30]</ref>. applied to high-resolution synthesis of megapixel images.</p><p>(ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs.</p><p>(iii) We show that, in contrast to previous work <ref type="bibr" target="#b94">[93]</ref> which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.</p><p>(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of ? 1024 2 px.</p><p>(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.</p><p>(vi) Finally, we release pretrained latent diffusion and autoencoding models at https : / / github . com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs <ref type="bibr" target="#b82">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Models for Image Synthesis The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) <ref type="bibr" target="#b26">[27]</ref> allow for efficient sampling of high resolution images with good perceptual quality <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref>, but are diffi-cult to optimize <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">54]</ref> and struggle to capture the full data distribution <ref type="bibr" target="#b55">[55]</ref>. In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved. Variational autoencoders (VAE) <ref type="bibr" target="#b46">[46]</ref> and flow-based models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> enable efficient synthesis of high resolution images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b93">92]</ref>, but sample quality is not on par with GANs. While autoregressive models (ARM) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b95">94,</ref><ref type="bibr" target="#b96">95]</ref> achieve strong performance in density estimation, computationally demanding architectures <ref type="bibr" target="#b98">[97]</ref> and a sequential sampling process limit them to low resolution images. Because pixel based representations of images contain barely perceptible, high-frequency details <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b74">73]</ref>, maximum-likelihood training spends a disproportionate amount of capacity on modeling them, resulting in long training times. To scale to higher resolutions, several two-stage approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b68">67,</ref><ref type="bibr" target="#b102">101,</ref><ref type="bibr" target="#b104">103]</ref> use ARMs to model a compressed latent image space instead of raw pixels.</p><p>Recently, Diffusion Probabilistic Models (DM) [82], have achieved state-of-the-art results in density estimation <ref type="bibr" target="#b45">[45]</ref> as well as in sample quality <ref type="bibr" target="#b14">[15]</ref>. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b72">71,</ref><ref type="bibr" target="#b86">85]</ref>. The best synthesis quality is usually achieved when a reweighted objective <ref type="bibr" target="#b29">[30]</ref> is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b85">84]</ref> and hierarchical approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b94">93]</ref>, training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-Stage Image Synthesis</head><p>To mitigate the shortcomings of individual generative approaches, a lot of research <ref type="bibr">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b68">67,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b102">101,</ref><ref type="bibr" target="#b104">103]</ref> has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs <ref type="bibr" target="#b68">[67,</ref><ref type="bibr" target="#b102">101]</ref> use autoregressive models to learn an expressive prior over a discretized latent space. <ref type="bibr" target="#b67">[66]</ref> extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, <ref type="bibr" target="#b71">[70]</ref> uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b104">103]</ref> employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66]</ref>, limit the overall performance of such ap-proaches and less compression comes at the price of high computational cost <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66]</ref>. Our work prevents such tradeoffs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing highfidelity reconstructions (see <ref type="figure">Fig. 1</ref>).</p><p>While approaches to jointly <ref type="bibr" target="#b94">[93]</ref> or separately <ref type="bibr" target="#b81">[80]</ref> learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities <ref type="bibr">[11]</ref> and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms <ref type="bibr" target="#b29">[30]</ref>, they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.</p><p>We propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see <ref type="figure" target="#fig_7">Fig. 2</ref>). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity.</p><p>Such an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture <ref type="bibr" target="#b72">[71]</ref>, which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66]</ref>. (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Perceptual Image Compression</head><p>Our perceptual compression model is based on previous work <ref type="bibr" target="#b22">[23]</ref> and consists of an autoencoder trained by combination of a perceptual loss <ref type="bibr" target="#b107">[106]</ref> and a patch-based <ref type="bibr" target="#b32">[33]</ref> adversarial objective <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b104">103]</ref>. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as L 2 or L 1 objectives.</p><p>More precisely, given an image x ? R H?W ?3 in RGB space, the encoder E encodes x into a latent representa-tion z = E(x), and the decoder D reconstructs the image from the latent, givingx = D(z) = D(E(x)), where z ? R h?w?c . Importantly, the encoder downsamples the image by a factor f = H/h = W/w, and we investigate different downsampling factors f = 2 m , with m ? N.</p><p>In order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, KL-reg., imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b70">69]</ref>, whereas VQ-reg. uses a vector quantization layer <ref type="bibr" target="#b97">[96]</ref> within the decoder. This model can be interpreted as a VQGAN <ref type="bibr" target="#b22">[23]</ref> but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space z = E(x), we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66]</ref>, which relied on an arbitrary 1D ordering of the learned space z to model its distribution autoregressively and thereby ignored much of the inherent structure of z. Hence, our compression model preserves details of x better (see Tab. 8). The full objective and training details can be found in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Diffusion Models</head><p>Diffusion Models [82] are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T . For image synthesis, the most successful models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b73">72]</ref> rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching <ref type="bibr" target="#b86">[85]</ref>. These models can be interpreted as an equally weighted sequence of denoising autoencoders ? (x t , t); t = 1 . . . T , which are trained to predict a denoised variant of their input x t , where x t is a noisy version of the input x. The corresponding objective can be simplified to (Sec. B)</p><formula xml:id="formula_0">L DM = E x, ?N (0,1),t ? ? (x t , t) 2 2 ,<label>(1)</label></formula><p>with t uniformly sampled from {1, . . . , T }. Generative Modeling of Latent Representations With our trained perceptual compression models consisting of E and D, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space. Unlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">66,</ref><ref type="bibr" target="#b104">103]</ref>, we can take advantage of image-specific inductive biases that our model offers. This  <ref type="figure">Figure 3</ref>. We condition LDMs either via concatenation or by a more general cross-attention mechanism. See Sec. <ref type="bibr">3.3</ref> includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads</p><formula xml:id="formula_1">L LDM := E E(x), ?N (0,1),t ? ? (z t , t) 2 2 . (2)</formula><p>The neural backbone ? (?, t) of our model is realized as a time-conditional UNet <ref type="bibr" target="#b72">[71]</ref>. Since the forward process is fixed, z t can be efficiently obtained from E during training, and samples from p(z) can be decoded to image space with a single pass through D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conditioning Mechanisms</head><p>Similar to other types of generative models <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b84">83]</ref>, diffusion models are in principle capable of modeling conditional distributions of the form p(z|y). This can be implemented with a conditional denoising autoencoder ? (z t , t, y) and paves the way to controlling the synthesis process through inputs y such as text <ref type="bibr" target="#b69">[68]</ref>, semantic maps <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b62">61]</ref> or other image-to-image translation tasks <ref type="bibr" target="#b33">[34]</ref>.</p><p>In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels <ref type="bibr" target="#b14">[15]</ref> or blurred variants of the input image <ref type="bibr" target="#b73">[72]</ref> is so far an under-explored area of research.</p><p>We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism <ref type="bibr" target="#b98">[97]</ref>, which is effective for learning attention-based models of various input modalities <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. To pre-process y from various modalities (such as language prompts) we introduce a domain specific encoder ? ? that projects y to an intermediate representation ? ? (y) ? R M ?d? , which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing</p><formula xml:id="formula_2">Attention(Q, K, V ) = softmax QK T ? d ? V , with Q = W (i) Q ? ? i (z t ), K = W (i) K ? ? ? (y), V = W (i) V ? ? ? (y).</formula><p>Here, ? i (z t ) ? R N ?d i denotes a (flattened) intermediate representation of the UNet implementing ? and W</p><formula xml:id="formula_3">(i) V ? 4 CelebAHQ FFHQ</formula><p>LSUN-Churches LSUN-Beds ImageNet <ref type="figure">Figure 4</ref>. Samples from LDMs trained on CelebAHQ <ref type="bibr" target="#b38">[39]</ref>, FFHQ <ref type="bibr" target="#b40">[41]</ref>, LSUN-Churches <ref type="bibr" target="#b103">[102]</ref>, LSUN-Bedrooms <ref type="bibr" target="#b103">[102]</ref> and classconditional ImageNet <ref type="bibr" target="#b11">[12]</ref>, each with a resolution of 256 ? 256. Best viewed when zoomed in. For more samples cf . the supplement. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b98">97]</ref>. See <ref type="figure">Fig. 3</ref> for a visual depiction.</p><formula xml:id="formula_4">R d?d i , W (i) Q ? R d?d? &amp; W (i) K ? R d?d? are learnable pro- jection matrices</formula><p>Based on image-conditioning pairs, we then learn the conditional LDM via</p><formula xml:id="formula_5">L LDM := E E(x),y, ?N (0,1),t ? ? (z t , t, ? ? (y)) 2 2 ,<label>(3)</label></formula><p>where both ? ? and ? are jointly optimized via Eq. 3. This conditioning mechanism is flexible as ? ? can be parameterized with domain-specific experts, e.g. (unmasked) transformers <ref type="bibr" target="#b98">[97]</ref> when y are text prompts (see Sec. 4.3.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>LDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that LDMs trained in VQregularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of VQregularized first stage models slightly fall behind those of their continuous counterparts, cf . Tab. 8. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions &gt; 256 2 can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">On Perceptual Compression Tradeoffs</head><p>This section analyzes the behavior of our LDMs with different downsampling factors f ? {1, 2, 4, 8, 16, 32} (abbreviated as LDM-f , where LDM-1 corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.</p><p>Tab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com-pared in this section. <ref type="figure" target="#fig_1">Fig. 6</ref> shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet <ref type="bibr" target="#b11">[12]</ref> dataset. We see that, i) small downsampling factors for LDM-{1,2} result in slow training progress, whereas ii) overly large values of f cause stagnating fidelity after comparably few training steps. Revisiting the analysis above ( <ref type="figure">Fig. 1</ref> and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. LDM-{4-16} strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID <ref type="bibr" target="#b28">[29]</ref> gap of 38 between pixel-based diffusion (LDM-1) and LDM-8 after 2M training steps.</p><p>In <ref type="figure" target="#fig_2">Fig. 7</ref>, we compare models trained on CelebA-HQ <ref type="bibr" target="#b38">[39]</ref> and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler <ref type="bibr" target="#b85">[84]</ref> and plot it against FID-scores <ref type="bibr" target="#b28">[29]</ref>. LDM-{4-8} outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based LDM-1, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, LDM-4 and -8 offer the best conditions for achieving high-quality synthesis results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Generation with Latent Diffusion</head><p>We train unconditional models of 256 2 images on CelebA-HQ <ref type="bibr" target="#b38">[39]</ref>, FFHQ <ref type="bibr" target="#b40">[41]</ref>, LSUN-Churches and -Bedrooms <ref type="bibr" target="#b103">[102]</ref> and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID <ref type="bibr" target="#b28">[29]</ref> and ii) Precision-and-Recall <ref type="bibr" target="#b50">[50]</ref>. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM <ref type="bibr" target="#b94">[93]</ref> where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space Text-to-Image Synthesis on LAION. 1.45B Model.</p><p>'A street sign that reads "Latent Diffusion" ' 'A zombie in the style of Picasso'</p><p>'An image of an animal half mouse half octopus'</p><p>'An illustration of a slightly conscious neural network' 'A painting of a squirrel eating a burger' 'A watercolor painting of a chair that looks like an octopus' 'A shirt with the inscription: "I love generative models!" ' <ref type="figure">Figure 5</ref>. Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION <ref type="bibr" target="#b79">[78]</ref> database. Samples generated with 200 DDIM steps and ? = 1.0. We use unconditional guidance <ref type="bibr" target="#b31">[32]</ref> with s = 10.0.  We outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM <ref type="bibr" target="#b14">[15]</ref>, despite utilizing half its parameters and requiring 4-times less train resources (see Appendix E.3.5).  Moreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In <ref type="figure">Fig. 4</ref> we also show qualitative results on each dataset. </p><formula xml:id="formula_6">CelebA-HQ 256 ? 256 FFHQ 256 ? 256 Method FID ? Prec. ? Recall ? Method FID ? Prec. ? Recall ? DC-VAE [63] 15.8 - - ImageBART [21] 9.57 - - VQGAN+T. [23] (k=400) 10.2 - - U-Net GAN (+aug) [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Conditional Latent Diffusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Transformer Encoders for LDMs By introducing cross-attention based conditioning into</head><p>LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M <ref type="bibr" target="#b79">[78]</ref>. We employ the BERT-tokenizer <ref type="bibr" target="#b13">[14]</ref> and implement ? ? as a transformer <ref type="bibr" target="#b98">[97]</ref> to infer a latent code which is mapped into the UNet via (multi-head) crossattention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf . <ref type="figure" target="#fig_3">Fig. 8</ref> and 5.</p><p>For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO <ref type="bibr" target="#b51">[51]</ref> validation set, where our model improves upon powerful AR <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b67">66]</ref> and GAN-based <ref type="bibr" target="#b110">[109]</ref> methods, cf . Tab. 2. We note that applying classifier-free diffusion guidance <ref type="bibr" target="#b31">[32]</ref> greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR <ref type="bibr" target="#b25">[26]</ref> and diffusion models <ref type="bibr" target="#b59">[59]</ref> for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages <ref type="bibr" target="#b49">[49]</ref>, and finetune on COCO <ref type="bibr" target="#b3">[4]</ref>, see <ref type="figure" target="#fig_3">Fig. 8</ref>  <ref type="bibr" target="#b31">[32]</ref>, s = 1.5 <ref type="table">Table 3</ref>. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on ImageNet <ref type="bibr" target="#b11">[12]</ref>. A more detailed comparison with additional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes classifier-free guidance with a scale s as proposed in <ref type="bibr" target="#b31">[32]</ref>.</p><p>purpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b62">61]</ref> and concatenate downsampled versions of the semantic maps with the latent image representation of a f = 4 model (VQ-reg., see Tab. 8). We train on an input resolution of 256 2 (crops from 384 2 ) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner (see <ref type="figure" target="#fig_4">Fig. 9</ref>). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between 512 2 and 1024 2 . For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. D.1 we illustrate this when learning an LDM on (i) the latent space as provided by a f = 4 model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation. The latter, in combination with classifier-free guidance <ref type="bibr" target="#b31">[32]</ref>, also enables the direct synthesis of &gt; 256 2 images for the text-conditional LDM-KL-8-G as in <ref type="figure">Fig. 13</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Super-Resolution with Latent Diffusion</head><p>LDMs can be efficiently trained for super-resolution by diretly conditioning on low-resolution images via concatenation (cf . Sec. 3.3). In a first experiment, we follow SR3 bicubic LDM-SR SR3 <ref type="figure">Figure 10</ref>. ImageNet 64?256 super-resolution on ImageNet-Val.</p><p>LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from <ref type="bibr" target="#b73">[72]</ref>.</p><p>[72] and fix the image degradation to a bicubic interpolation with 4?-downsampling and train on ImageNet following SR3's data processing pipeline. We use the f = 4 autoencoding model pretrained on OpenImages (VQ-reg., cf . Tab. 8) and concatenate the low-resolution conditioning y and the inputs to the UNet, i.e. ? ? is the identity. Our qualitative and quantitative results (see <ref type="figure">Fig. 10</ref> and Tab. 5) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception <ref type="bibr" target="#b107">[106]</ref> and favor blurriness over imperfectly aligned high frequency details <ref type="bibr" target="#b73">[72]</ref>. Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3 <ref type="bibr" target="#b73">[72]</ref> where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 4 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a post-hoc guiding mechanism <ref type="bibr" target="#b14">[15]</ref> and we implement this image-based guider via a perceptual loss, see Sec. D.6.  Since the bicubic degradation process does not generalize well to images which do not follow this pre-processing, we also train a generic model, LDM-BSR, by using more diverse degradation. The results are shown in Sec. D.6.1.   <ref type="table">Table 6</ref>. Assessing inpainting efficiency. ? : Deviations from <ref type="figure" target="#fig_2">Fig. 7</ref> due to varying GPU settings/batch sizes cf . the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SR on ImageNet Inpainting on Places</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inpainting with Latent Diffusion</head><p>Inpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa <ref type="bibr" target="#b89">[88]</ref>, a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions <ref type="bibr" target="#b7">[8]</ref>. The exact training &amp; evaluation protocol on Places <ref type="bibr" target="#b109">[108]</ref> is described in Sec. E.2.2.</p><p>We first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQ-LDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution 256 2 and 512 2 , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7? between pixel-and latent-based diffusion models while improving FID scores by a factor of at least 1.6?.</p><p>The comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of <ref type="bibr" target="#b89">[88]</ref>. LPIPS between the unmasked images and our samples is slightly higher than that of <ref type="bibr" target="#b89">[88]</ref>. We attribute this to <ref type="bibr" target="#b89">[88]</ref> only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf . <ref type="figure" target="#fig_7">Fig. 21</ref>. Additionally in a user study (Tab. 4) human subjects favor our results over those of <ref type="bibr" target="#b89">[88]</ref>.</p><p>Based on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the VQregularized first stage without attention. Following <ref type="bibr" target="#b14">[15]</ref>, the UNet of this diffusion model uses attention layers on three levels of its feature hierarchy, the BigGAN instead of 215M. After training, we noticed a discrepancy in the quality of samples produced at resolutions 256 2 and 512 2 , which we hypothesize to be caused by the additional attention modules. However, fine-tuning the model for half an epoch at resolution 512 2 allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting (big, w/o attn, w/ ft in Tab. 7, <ref type="figure" target="#fig_5">Fig. 11</ref>.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations &amp; Societal Impact</head><p>Limitations While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our f = 4 autoencoding models (see <ref type="figure">Fig. 1</ref>), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.</p><p>Societal Impact Generative models for media like imagery are a double-edged sword: On the one hand, they enable various creative applications, and in particular approaches like ours that reduce the cost of training and inference have the potential to facilitate access to this technology and democratize its exploration. On the other hand, it also means that it becomes easier to create and disseminate manipulated data or spread misinformation and spam.</p><p>In particular, the deliberate manipulation of images ("deep fakes") is a common problem in this context, and women in particular are disproportionately affected by it <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. Generative models can also reveal their training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b91">90]</ref>, which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.</p><p>Finally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b92">91]</ref>. While diffusion models achieve better coverage of the data distribution than e.g. GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.</p><p>For a more general, detailed discussion of the ethical considerations of deep generative models, see e.g. [13].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our crossattention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.</p><p>This work has been supported by the German Federal Ministry for Economic Affairs and Energy within the project 'KI-Absicherung -Safe AI for automated driving' and by the German Research Foundation (DFG) project 421703927.  <ref type="figure">Figure 13</ref>. Combining classifier free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter text-to-image model can be used for rendering images larger than the native 256 2 resolution the model was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Changelog</head><p>Here we list changes between this version (https://arxiv.org/abs/2112.10752v2) of the paper and the previous version, i.e. https://arxiv.org/abs/2112.10752v1.</p><p>? We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B parameters). This also includes a new comparison to very recent competing methods on this task that were published on arXiv at the same time as ( <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b110">109]</ref>) or after ( <ref type="bibr" target="#b25">[26]</ref>) the publication of our work.</p><p>? We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by retraining the model with a larger batch size. The corresponding qualitative results in <ref type="figure" target="#fig_1">Fig. 26</ref> and <ref type="figure" target="#fig_2">Fig. 27</ref> were also updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance <ref type="bibr" target="#b31">[32]</ref> as a measure to increase visual fidelity.</p><p>? We conducted a user study (following the scheme suggested by Saharia et al <ref type="bibr" target="#b73">[72]</ref>) which provides additional evaluation for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).</p><p>? Added <ref type="figure">Fig. 5</ref> to the main paper, moved <ref type="figure" target="#fig_3">Fig. 18</ref> to the appendix, added <ref type="figure">Fig. 13</ref> to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed Information on Denoising Diffusion Models</head><p>Diffusion models can be specified in terms of a signal-to-noise ratio SNR(t) = </p><formula xml:id="formula_7">q(x t |x 0 ) = N (x t |? t x 0 , ? 2 t I)<label>(4)</label></formula><p>with the Markov structure for s &lt; t:</p><formula xml:id="formula_8">q(x t |x s ) = N (x t |? t|s x s , ? 2 t|s I) (5) ? t|s = ? t ? s (6) ? 2 t|s = ? 2 t ? ? 2 t|s ? 2 s<label>(7)</label></formula><p>Denoising diffusion models are generative models p(x 0 ) which revert this process with a similar Markov structure running backward in time, i.e. they are specified as</p><formula xml:id="formula_9">p(x 0 ) = z p(x T ) T t=1 p(x t?1 |x t )<label>(8)</label></formula><p>The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as</p><formula xml:id="formula_10">? log p(x 0 ) ? KL(q(x T |x 0 )|p(x T )) + T t=1 E q(xt|x0) KL(q(x t?1 |x t , x 0 )|p(x t?1 |x t ))<label>(9)</label></formula><p>The prior p(x T ) is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on the final signal-to-noise ratio SNR(T ). To minimize the remaining terms, a common choice to parameterize p(x t?1 |x t ) is to specify it in terms of the true posterior q(x t?1 |x t , x 0 ) but with the unknown x 0 replaced by an estimate x ? (x t , t) based on the current step x t . This gives <ref type="bibr" target="#b45">[45]</ref> p</p><formula xml:id="formula_11">(x t?1 |x t ) := q(x t?1 |x t , x ? (x t , t)) (10) = N (x t?1 |? ? (x t , t), ? 2 t|t?1 ? 2 t?1 ? 2 t I),<label>(11)</label></formula><p>where the mean can be expressed as</p><formula xml:id="formula_12">? ? (x t , t) = ? t|t?1 ? 2 t?1 ? 2 t x t + ? t?1 ? 2 t|t?1 ? 2 t x ? (x t , t).<label>(12)</label></formula><p>In this case, the sum of the ELBO simplify to</p><formula xml:id="formula_13">T t=1 E q(xt|x0) KL(q(x t?1 |x t , x 0 )|p(x t?1 ) = T t=1 E N ( |0,I) 1 2 (SNR(t ? 1) ? SNR(t)) x 0 ? x ? (? t x 0 + ? t , t) 2<label>(13)</label></formula><p>Following <ref type="bibr" target="#b29">[30]</ref>, we use the reparameterization</p><formula xml:id="formula_14">? (x t , t) = (x t ? ? t x ? (x t , t))/? t<label>(14)</label></formula><p>to express the reconstruction term as a denoising objective,</p><formula xml:id="formula_15">x 0 ? x ? (? t x 0 + ? t , t) 2 = ? 2 t ? 2 t ? ? (? t x 0 + ? t , t) 2<label>(15)</label></formula><p>and the reweighting, which assigns each of the terms the same weight and results in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Guiding Mechanisms</head><p>Samples 256 2 Guided Convolutional Samples 512 2 Convolutional Samples 512 2 <ref type="figure">Figure 14</ref>. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures (see column 2). L2-guiding with a low resolution image can help to reestablish coherent global structures.</p><p>An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">82,</ref><ref type="bibr" target="#b86">85]</ref>. In particular, <ref type="bibr" target="#b14">[15]</ref> presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset with a classifier log p ? (y|x t ), trained on each x t of the diffusion process. We directly build on this formulation and introduce post-hoc image-guiding:</p><p>For an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in <ref type="bibr" target="#b14">[15]</ref> reads:</p><formula xml:id="formula_16">? ? (z t , t) + 1 ? ? 2 t ? zt log p ? (y|z t ) .<label>(16)</label></formula><p>This can be interpreted as an update correcting the "score" ? with a conditional distribution log p ? (y|z t ).</p><p>So far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution p ? (y|T (D(z 0 (z t )))) as a general purpose image-to-image translation task given a target image y, where T can be any differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling operation or similar.</p><p>As an example, we can assume a Gaussian guider with fixed variance ? 2 = 1, such that</p><formula xml:id="formula_17">log p ? (y|z t ) = ? 1 2 y ? T (D(z 0 (z t ))) 2 2 (17)</formula><p>becomes a L 2 regression objective. <ref type="figure">Fig. 14 demonstrates</ref> how this formulation can serve as an upsampling mechanism of an unconditional model trained on 256 2 images, where unconditional samples of size 256 2 guide the convolutional synthesis of 512 2 images and T is a 2? bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the L 2 objective with the LPIPS <ref type="bibr" target="#b107">[106]</ref> metric, see Sec. 4.4. As discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space (i.e. Var(z)/? 2 t ) significantly affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KLregularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the latents as described in Sec. G, the SNR is descreased. We illustrate the effect on convolutional sampling for semantic image synthesis in <ref type="figure" target="#fig_8">Fig. 15</ref>. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Full List of all First Stage Models</head><p>We provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Layout-to-Image Synthesis</head><p>Here we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We train a model on the COCO <ref type="bibr" target="#b3">[4]</ref> and one on the OpenImages <ref type="bibr" target="#b49">[49]</ref> dataset, which we subsequently additionally finetune on COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the art models in layout-toimage synthesis, when following their training and evaluation protocol <ref type="bibr" target="#b90">[89]</ref>. When finetuning from the OpenImages model, we surpass these works. Our OpenImages model surpasses the results of Jahn et al <ref type="bibr" target="#b36">[37]</ref> by a margin of nearly 11 in terms of FID. In <ref type="figure" target="#fig_1">Fig. 16</ref> we show additional samples of the model finetuned on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Class-Conditional Image Synthesis on ImageNet</head><p>Tab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires significantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar to previous work, we can further boost the performance by training a classifier on each noise scale and guiding with it,  layout-to-image synthesis on the COCO dataset see Sec. C. Unlike the pixel-based methods, this classifier is trained very cheaply in latent space. For additional qualitative results, see <ref type="figure" target="#fig_1">Fig. 26</ref> and <ref type="figure" target="#fig_2">Fig. 27</ref>    For the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is additionally provided in <ref type="figure" target="#fig_2">Fig. 17, showing</ref>   <ref type="table" target="#tab_0">Table 11</ref>. ?4 upscaling results on ImageNet-Val. (256 2 ); ? : FID features computed on validation split, ? : FID features computed on train split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs of additional training compared to the former results.</p><formula xml:id="formula_18">20 f |Z| c R-FID ? R-IS ? PSNR ? PSIM ? SSIM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6. Super-Resolution</head><p>For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by comparing a diffusion model trained for the same number of steps and with a comparable number 1 of parameters to our LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better performance while allowing for significantly faster sampling. A qualitative comparison is given in <ref type="figure" target="#fig_7">Fig. 20</ref> which shows random samples from both LDM and the diffusion model in pixel space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation bicubic</head><p>LDM-SR LDM-BSR <ref type="figure" target="#fig_3">Figure 18</ref>. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a classconditional LDM (image cf . <ref type="figure">Fig. 4</ref>) to 1024 2 resolution. In contrast, using a fixed degradation process (see Sec. 4.4) hinders generalization.</p><p>To evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly downsampled conditioning as in <ref type="bibr" target="#b73">[72]</ref>, does not generalize well to images which do not follow this pre-processing. Hence, to obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the degration pipeline from <ref type="bibr" target="#b106">[105]</ref>. The BSR-degradation process is a degradation pipline which applies JPEG compressions noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a random order to an image. We found that using the bsr-degredation process with the original parameters as in <ref type="bibr" target="#b106">[105]</ref> leads to a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https: //github.com/CompVis/latent-diffusion). <ref type="figure" target="#fig_3">Fig. 18</ref> illustrates the effectiveness of this approach by directly comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed preprocessing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in <ref type="figure" target="#fig_4">Fig. 19</ref>    <ref type="table" target="#tab_0">-2048  8192  16384  16384  16384  Diffusion steps  1000  1000  1000  1000  1000  1000  Noise Schedule  linear  linear  linear  linear  linear  linear  Model Size  270M  265M  274M  258M  260M  258M  Channels  192  192  224  256  256  256  Depth  2  2  2  2  2  2  Channel Multiplier  1</ref>  <ref type="table" target="#tab_0">Table 14</ref>. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in <ref type="figure" target="#fig_2">Fig. 7</ref>. All models trained on a single NVIDIA A100. * : All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.  </p><p>for i = 1, . . . , N :</p><formula xml:id="formula_20">? 1 ? LayerNorm(?) (19) ? 2 ? MultiHeadSelfAttention(? 1 ) + ? (20) ? 3 ? LayerNorm(? 2 ) (21) ? ? MLP(? 3 ) + ? 2 (22) ? ? LayerNorm(?)<label>(23)</label></formula><p>With ? available, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in <ref type="figure">Fig. 3</ref>. We modify the "ablated UNet" <ref type="bibr" target="#b14">[15]</ref> architecture and replace the self-attention layer with a shallow (unmasked) transformer consisting of T blocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer; see Tab. <ref type="bibr" target="#b15">16</ref>. Note that without (ii) and (iii), this architecture is equivalent to the "ablated UNet".</p><p>While it would be possible to increase the representational power of ? ? by additionally conditioning on the time step t, we do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modification to future work.</p><p>For the text-to-image model, we rely on a publicly available 3 tokenizer <ref type="bibr" target="#b100">[99]</ref>. The layout-to-image model discretizes the spatial locations of the bounding boxes and encodes each box as a (l, b, c)-tuple, where l denotes the (discrete) top-left and b the bottom-right position. Class information is contained in c. See Tab. 17 for the hyperparameters of ? ? and Tab. 13 for those of the UNet for both of the above tasks.</p><p>Note that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where ? ? is a single learnable embedding layer with a dimensionality of 512, mapping classes y to ? ? R 1?512 . <ref type="table" target="#tab_0">Table 16</ref>. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard "ablated UNet" architecture <ref type="bibr" target="#b14">[15]</ref>. Here, n h denotes the number of attention heads and d the dimensionality per head.</p><formula xml:id="formula_22">input R h?w?c LayerNorm R h?w?c Conv1x1 R h?w?d?n h Reshape R h?w?d?n h ?T ? ? ? ? ? SelfAttention MLP CrossAttention R h?w?d?n h R h?w?d?n h R h?w?d?n h Reshape R h?w?d?n h Conv1x1 R h?w?c</formula><p>Text-to-Image Layout-to-Image </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.2 Inpainting</head><p>For our experiments on image-inpainting in Sec. 4.5, we used the code of <ref type="bibr" target="#b89">[88]</ref> to generate synthetic masks. We use a fixed set of 2k validation and 30k testing samples from Places <ref type="bibr" target="#b109">[108]</ref>. During training, we use random crops of size 256 ? 256 and evaluate on crops of size 512 ? 512. This follows the training and testing protocol in <ref type="bibr" target="#b89">[88]</ref> and reproduces their reported metrics (see ? in Tab. 7). We include additional qualitative results of LDM-4, w/ attn in <ref type="figure" target="#fig_7">Fig. 21</ref> and of LDM-4, w/o attn, big, w/ ft in <ref type="figure" target="#fig_7">Fig. 22</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Evaluation Details</head><p>This section provides additional details on evaluation for the experiments shown in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis</head><p>We follow common practice and estimate the statistics for calculating the FID-, Precision-and Recall-scores <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">50]</ref> shown in Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating FID scores we use the torch-fidelity package <ref type="bibr" target="#b60">[60]</ref>. However, since different data processing pipelines might lead to different results <ref type="bibr" target="#b65">[64]</ref>, we also evaluate our models with the script provided by Dhariwal and Nichol <ref type="bibr" target="#b14">[15]</ref>. We find that results mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76 (torch-fidelity) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by Nichol and Dhariwal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.2 Text-to-Image Synthesis</head><p>Following the evaluation protocol of <ref type="bibr" target="#b67">[66]</ref> we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset <ref type="bibr" target="#b51">[51]</ref>. FID and Inception Scores are computed with torch-fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.3 Layout-to-Image Synthesis</head><p>For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common practice <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b90">89]</ref> and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split.</p><p>To obtain better comparability, we use the exact same samples as in <ref type="bibr" target="#b36">[37]</ref>. For the OpenImages dataset we similarly follow their protocol and use 2048 center-cropped test images from the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.4 Super Resolution</head><p>We evaluate the super-resolution models on ImageNet following the pipeline suggested in <ref type="bibr" target="#b73">[72]</ref>, i.e. images with a shorter size less than 256 px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity <ref type="bibr" target="#b60">[60]</ref>, and we produce samples on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5 and Tab. 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.5 Efficiency Analysis</head><p>For efficiency reasons we compute the sample quality metrics plotted in <ref type="figure" target="#fig_1">Fig. 6, 17</ref> and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf . Tab. 13 and 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.6 User Study</head><p>For the results of the user study presented in Tab. 4 we followed the protocoll of <ref type="bibr" target="#b73">[72]</ref> and and use the 2-alternative force-choice paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was generated by using the middle image as conditioning. For SuperResolution subjects were asked: 'Which of the two images is a better high quality version of the low resolution image in the middle?'. For Inpainting we asked 'Which of the two images contains more realistic inpainted regions of the image in the middle?'. In Task-2, humans were similarly shown the lowres/masked version and asked for preference between two corresponding images generated by the two competing methods. As in <ref type="bibr" target="#b73">[72]</ref> humans viewed the images for 3 seconds before responding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Requirements</head><p>Method  In Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided numbers, cf . <ref type="bibr" target="#b14">[15]</ref>. As they report their used compute in V100 days and we train all our models on a single NVIDIA A100 GPU, we convert the A100 days to V100 days by assuming a ?2.2 speedup of A100 vs V100 <ref type="bibr" target="#b75">[74]</ref>  <ref type="bibr" target="#b3">4</ref> . To assess sample quality, we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as StyleGAN2 <ref type="bibr" target="#b41">[42]</ref> and ADM <ref type="bibr" target="#b14">[15]</ref> while significantly reducing the required compute resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Details on Autoencoder Models</head><p>We train all our autoencoder models in an adversarial manner following <ref type="bibr" target="#b22">[23]</ref>, such that a patch-based discriminator D ? is optimized to differentiate original images from reconstructions D(E(x)). To avoid arbitrarily scaled latent spaces, we regularize the latent z to be zero centered and obtain small variance by introducing an regularizing loss term L reg . We investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between q E (z|x) = N (z; E ? , E ? 2 ) and a standard normal distribution N (z; 0, 1) as in a standard variational autoencoder <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b70">69]</ref>, and, (ii) regularizing the latent space with a vector quantization layer by learning a codebook of |Z| different exemplars <ref type="bibr" target="#b97">[96]</ref>. To obtain high-fidelity reconstructions we only use a very small regularization for both scenarios, i.e. we either weight the KL term by a factor ? 10 ?6 or choose a high codebook dimensionality |Z|.</p><p>The full objective to train the autoencoding model (E, D) reads: (ii) For a VQ-regularized latent space, we extract z before the quantization layer and absorb the quantization operation into the decoder, i.e. it can be interpreted as the first layer of D.</p><formula xml:id="formula_23">L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Additional Qualitative Results</head><p>Finally, we provide additional qualitative results for our landscapes model ( <ref type="figure" target="#fig_7">Fig. 12, 23, 24 and 25</ref>), our class-conditional ImageNet model ( <ref type="figure" target="#fig_1">Fig. 26 -27</ref>) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets ( <ref type="figure" target="#fig_3">Fig. 28 -31</ref>). Similar as for the inpainting model in Sec. 4.5 we also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on 512 2 images and depict qualitative results in <ref type="figure" target="#fig_7">Fig. 12</ref> and <ref type="figure" target="#fig_7">Fig. 23</ref>. For our those models trained on comparably small datasets, we additionally show nearest neighbors in VGG <ref type="bibr" target="#b80">[79]</ref> feature space for samples from our models in <ref type="figure" target="#fig_7">Fig. 32</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>42</head><p>Nearest Neighbors on the CelebA-HQ dataset <ref type="figure" target="#fig_7">Figure 32</ref>. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 <ref type="bibr" target="#b80">[79]</ref>. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>43</head><p>Nearest Neighbors on the FFHQ dataset <ref type="figure">Figure 33</ref>. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 <ref type="bibr" target="#b80">[79]</ref>. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>44</head><p>Nearest Neighbors on the LSUN-Churches dataset <ref type="figure">Figure 34</ref>. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 <ref type="bibr" target="#b80">[79]</ref>. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Analyzing the training of class-conditional LDMs with different downsampling factors f over 2M train steps on the Im-ageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [84] and ? = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Comparing LDMs with varying compression on the CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate {10, 20, 50, 100, 200} sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM-{4-8}. FID scores assessed on 5000 samples. All models were trained for 500k (CelebA) / 2M (ImageNet) steps on an A100.and avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, seeFig. 1-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Layout-to-image synthesis with an LDM on COCO [4], see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>A LDM trained on 256 2 resolution can generalize to larger resolution (here: 512?1024) for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 .</head><label>11</label><figDesc><ref type="bibr" target="#b2">[3]</ref> residual block for up-and downsampling and has 387M parameters 8 input result Qualitative results on object removal with our big, w/ ft inpainting model. For more results, seeFig. 22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>9 '</head><label>9</label><figDesc>A painting of the last supper by Picasso.' 'An oil painting of a latent space.' 'An epic painting of Gandalf the Black summoning thunder and lightning in the mountains.' 'A sunset over a mountain range, vector image.'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>? 2 t ? 2 t</head><label>2</label><figDesc>consisting of sequences (? t ) T t=1 and (? t ) T t=1 which, starting from a data sample x 0 , define a forward diffusion process q as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 .</head><label>15</label><figDesc>KL-reg, w/o rescaling KL-reg, w/ rescaling VQ-reg, w/o rescaling Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See Sec. 4.3.2 and Sec. D.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 .</head><label>16</label><figDesc>More samples from our best model for layout-to-image synthesis, LDM-4, which was trained on the OpenImages dataset and finetuned on the COCO dataset. Samples generated with 100 DDIM steps and ? = 0. Layouts are from the COCO validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 .</head><label>17</label><figDesc>For completeness we also report the training progress of class-conditional LDMs on the ImageNet dataset for a fixed number of 35 V100 days. Results obtained with 100 DDIM steps<ref type="bibr" target="#b85">[84]</ref> and ? = 0. FIDs computed on 5000 samples for efficiency reasons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>L</head><label></label><figDesc>rec (x, D(E(x))) ? L adv (D(E(x))) + log D ? (x) + L reg (x; E, D) (25) DM Training in Latent Space Note that for training diffusion models on the learned latent space, we again distinguish two cases when learning p(z) or p(z|y) (Sec. 4.3): (i) For a KL-regularized latent space, we sample z = E ? (x)+E ? (x)?? =: E(x), where ? ? N (0, 1). When rescaling the latent, we estimate the component-wise varianc? ? 2 = 1 bchw b,c,h,w (z b,c,h,w ??) 2 from the first batch in the data, where? = 1 bchw b,c,h,w z b,c,h,w . The output of E is scaled such that the rescaled latent has unit standard deviation, i.e. z ? ? ? = E(x) ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 19 .Figure 20 .Figure 21 .Figure 22 . 33 SemanticFigure 23 .</head><label>192021223323</label><figDesc>LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN-Cows dataset to 1024 2 resolution. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace. Evaluated on imagenet validation-set after same amount of training steps. Qualitative results on image inpainting. In contrast to<ref type="bibr" target="#b89">[88]</ref>, our generative approach enables generation of multiple diverse samples for a given input. More qualitative results on object removal as inFig. 11. Synthesis on Flickr-Landscapes<ref type="bibr" target="#b22">[23]</ref> (512 2 finetuning) Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 512 2 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 24 . 35 SemanticFigure 25 .Figure 26 .Figure 27 .Figure 28 .Figure 29 .Figure 30 .Figure 31 .</head><label>243525262728293031</label><figDesc>A LDM trained on 256 2 resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2. Synthesis on Flickr-Landscapes<ref type="bibr" target="#b22">[23]</ref> When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during training. Although this model was trained on inputs of size 256 2 it can be used to create high-resolution samples as the ones shown here, which are of resolution 1024 ? 384.36 Random class conditional samples on the ImageNet dataset Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale s = 5.0 and 200 DDIM steps with ? = 1.0. 37 Random class conditional samples on the ImageNet dataset Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale s = 3.0 and 200 DDIM steps with ? = 1.0. 38 Random samples on the CelebA-HQ dataset Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and ? = 0 (FID = 5.15). 39 Random samples on the FFHQ dataset Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and ? = 1 (FID = 4.98). 40 Random samples on the LSUN-Churches dataset Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and ? = 0 (FID = 4.48). 41 Random samples on the LSUN-Bedrooms dataset Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and ? = 1 (FID = 2.95).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from<ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b101">100]</ref>, FFHQ from<ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</figDesc><table><row><cell>77] 10.9 (7.6)</cell><cell>-</cell><cell>-</cell></row></table><note>LSUN-Churches 256 ? 256 LSUN-Bedrooms 256 ? 256? : N -s refers to N sampling steps with the DDIM [84] sampler.* : trained in KL-regularized latent space. Additional re- sults can be found in the supplementary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of text-conditional image synthesis on the 256 ? 256-sized MS-COCO<ref type="bibr" target="#b51">[51]</ref> dataset: with 250 DDIM<ref type="bibr" target="#b85">[84]</ref> steps our model is on par with the most recent diffusion<ref type="bibr" target="#b59">[59]</ref> and autoregressive<ref type="bibr" target="#b25">[26]</ref> methods despite using significantly less parameters.</figDesc><table /><note>? /* :Numbers from [109]/ [26]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. See Sec. D.3 for the quantitative evaluation and implementation details.Lastly, following prior work<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, we evaluate our best-performing class-conditional ImageNet models with f ? {4, 8} from Sec. 4.1 in Tab. 3,Fig. 4and Sec. D.4. Here we outperform the state of the art diffusion model ADM<ref type="bibr" target="#b14">[15]</ref> while significantly reducing computational requirements and parameter count, cf . Tab 18.4.3.2 Convolutional Sampling Beyond 256 2By concatenating spatially aligned conditioning information to the input of ? , LDMs can serve as efficient general-</figDesc><table><row><cell>Method</cell><cell>FID?</cell><cell>IS?</cell><cell cols="2">Precision? Recall?</cell><cell>Nparams</cell><cell></cell></row><row><cell>BigGan-deep [3]</cell><cell>6.95</cell><cell>203.6?2.6</cell><cell>0.87</cell><cell>0.28</cell><cell>340M</cell><cell>-</cell></row><row><cell>ADM [15]</cell><cell>10.94</cell><cell>100.98</cell><cell>0.69</cell><cell>0.63</cell><cell>554M</cell><cell>250 DDIM steps</cell></row><row><cell>ADM-G [15]</cell><cell>4.59</cell><cell>186.7</cell><cell>0.82</cell><cell>0.52</cell><cell>608M</cell><cell>250 DDIM steps</cell></row><row><cell>LDM-4 (ours)</cell><cell>10.56</cell><cell>103.49?1.24</cell><cell>0.71</cell><cell>0.62</cell><cell>400M</cell><cell>250 DDIM steps</cell></row><row><cell>LDM-4-G (ours)</cell><cell>3.60</cell><cell>247.67?5.59</cell><cell>0.87</cell><cell>0.48</cell><cell>400M</cell><cell>250 steps, c.f.g</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>(ours, big, 100 steps) 2.4 ? /4.3 ?</figDesc><table><row><cell>Method</cell><cell>FID ?</cell><cell>IS ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>Nparams</cell><cell>[ samples s</cell><cell>](  *  )</cell></row><row><cell>Image Regression [72]</cell><cell>15.2</cell><cell>121.1</cell><cell>27.9</cell><cell>0.801</cell><cell>625M</cell><cell cols="2">N/A</cell></row><row><cell>SR3 [72]</cell><cell>5.2</cell><cell>180.1</cell><cell>26.4</cell><cell>0.762</cell><cell>625M</cell><cell cols="2">N/A</cell></row><row><cell>LDM-4 (ours, 100 steps)</cell><cell>2.8  ? /4.8  ?</cell><cell>166.3</cell><cell>24.4?3.8</cell><cell>0.69?0.14</cell><cell>169M</cell><cell cols="2">4.62</cell></row><row><cell cols="3">emphLDM-4 174.9</cell><cell>24.7?4.1</cell><cell>0.71?0.15</cell><cell>552M</cell><cell cols="2">4.5</cell></row><row><cell>LDM-4 (ours, 50 steps, guiding)</cell><cell>4.4  ? /6.4  ?</cell><cell>153.7</cell><cell>25.8?3.7</cell><cell>0.74?0.12</cell><cell>184M</cell><cell cols="2">0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>?4 upscaling results on ImageNet-Val. (256 2 ); ? : FID features computed on validation split, ? : FID features computed on train split;</figDesc><table><row><cell></cell><cell cols="3">train throughput sampling throughput  ?</cell><cell>train+val</cell><cell>FID@2k</cell></row><row><cell>Model (reg.-type)</cell><cell>samples/sec.</cell><cell>@256</cell><cell cols="3">@512 hours/epoch epoch 6</cell></row><row><cell>LDM-1 (no first stage)</cell><cell>0.11</cell><cell>0.26</cell><cell>0.07</cell><cell>20.66</cell><cell>24.74</cell></row><row><cell>LDM-4 (KL, w/ attn)</cell><cell>0.32</cell><cell>0.97</cell><cell>0.34</cell><cell>7.66</cell><cell>15.21</cell></row><row><cell>LDM-4 (VQ, w/ attn)</cell><cell>0.33</cell><cell>0.97</cell><cell>0.34</cell><cell>7.04</cell><cell>14.99</cell></row><row><cell>LDM-4 (VQ, w/o attn)</cell><cell>0.35</cell><cell>0.99</cell><cell>0.36</cell><cell>6.66</cell><cell>15.95</cell></row></table><note>* : Assessed on a NVIDIA A100</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Comparison of inpainting performance on 30k crops of size 512 ? 512 from test images of Places [108]. The column 40-50% reports metrics computed over hard examples where 40-50% of the image region have to be inpainted. ? recomputed on our test set, since the original test set used in<ref type="bibr" target="#b89">[88]</ref> was not available.</figDesc><table><row><cell></cell><cell cols="2">40-50% masked</cell><cell cols="2">All samples</cell></row><row><cell>Method</cell><cell>FID ?</cell><cell cols="2">LPIPS ? FID ?</cell><cell>LPIPS ?</cell></row><row><cell>LDM-4 (ours, big, w/ ft)</cell><cell>9.39</cell><cell>0.246? 0.042</cell><cell>1.50</cell><cell>0.137? 0.080</cell></row><row><cell>LDM-4 (ours, big, w/o ft)</cell><cell cols="2">12.89 0.257? 0.047</cell><cell cols="2">2.40 0.142? 0.085</cell></row><row><cell>LDM-4 (ours, w/ attn)</cell><cell cols="2">11.87 0.257? 0.042</cell><cell cols="2">2.15 0.144? 0.084</cell></row><row><cell>LDM-4 (ours, w/o attn)</cell><cell cols="2">12.60 0.259? 0.041</cell><cell cols="2">2.37 0.145? 0.084</cell></row><row><cell>LaMa [88]  ?</cell><cell>12.31</cell><cell>0.243? 0.038</cell><cell>2.23</cell><cell>0.134? 0.080</cell></row><row><cell>LaMa [88]</cell><cell>12.0</cell><cell>0.24</cell><cell>2.21</cell><cell>0.14</cell></row><row><cell>CoModGAN [107]</cell><cell>10.4</cell><cell>0.26</cell><cell>1.82</cell><cell>0.15</cell></row><row><cell>RegionWise [52]</cell><cell>21.3</cell><cell>0.27</cell><cell>4.75</cell><cell>0.15</cell></row><row><cell>DeepFill v2 [104]</cell><cell>22.1</cell><cell>0.28</cell><cell>5.20</cell><cell>0.16</cell></row><row><cell>EdgeConnect [58]</cell><cell>30.5</cell><cell>0.28</cell><cell>8.37</cell><cell>0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>?1.07 17.45 ?2.90 2.58 ?0.48 0.41 ?0.18 ?4.83 29.13 ?3.46 0.38 ?0.13 0.90 ?0.05 ?5.16 32.47 ?4.19 0.20 ?0.09 0.93 ?0.04</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell cols="3">16 VQGAN [23] 16384 256</cell><cell>4.98</cell><cell>-</cell><cell>19.9 ?3.4</cell><cell>1.83 ?0.42</cell><cell>0.51 ?0.18</cell></row><row><cell>16 VQGAN [23]</cell><cell>1024</cell><cell>256</cell><cell>7.94</cell><cell>-</cell><cell>19.4 ?3.3</cell><cell>1.98 ?0.43</cell><cell>0.50 ?0.18</cell></row><row><cell>8 DALL-E [66]</cell><cell>8192</cell><cell>-</cell><cell>32.01</cell><cell>-</cell><cell>22.8 ?2.1</cell><cell>1.95 ?0.51</cell><cell>0.73 ?0.13</cell></row><row><cell cols="5">32 40.40 16 16384 16 31.83 16384 8 5.15 144.55 ?3.74</cell><cell cols="2">20.83 ?3.61 1.73 ?0.43</cell><cell>0.54 ?0.18</cell></row><row><cell>8</cell><cell>16384</cell><cell>4</cell><cell>1.14</cell><cell>201.92 ?3.97</cell><cell cols="2">23.07 ?3.99 1.17 ?0.36</cell><cell>0.65 ?0.16</cell></row><row><cell>8</cell><cell>256</cell><cell>4</cell><cell>1.49</cell><cell>194.20 ?3.87</cell><cell cols="2">22.35 ?3.81 1.26 ?0.37</cell><cell>0.62 ?0.16</cell></row><row><cell>4</cell><cell>8192</cell><cell>3</cell><cell>0.58</cell><cell>224.78 ?5.35</cell><cell cols="2">27.43 ?4.26 0.53 ?0.21</cell><cell>0.82 ?0.10</cell></row><row><cell>4  ?</cell><cell>8192</cell><cell>3</cell><cell>1.06</cell><cell>221.94 ?4.58</cell><cell cols="2">25.21 ?4.17 0.72 ?0.26</cell><cell>0.76 ?0.12</cell></row><row><cell>4</cell><cell>256</cell><cell>3</cell><cell>0.47</cell><cell>223.81 ?4.58</cell><cell cols="3">26.43 ?4.22 0.62 ?0.24 0.80 ?0.11</cell></row><row><cell>2</cell><cell>2048</cell><cell>2</cell><cell>0.16</cell><cell>232.75 ?5.09</cell><cell cols="3">30.85 ?4.12 0.27 ?0.12 0.91 ?0.05</cell></row><row><cell cols="5">2 226.62 32 64 2 0.40 KL 64 2.04 189.53 ?3.68</cell><cell cols="2">22.27 ?3.93 1.41 ?0.40</cell><cell>0.61 ?0.17</cell></row><row><cell>32</cell><cell>KL</cell><cell>16</cell><cell>7.3</cell><cell>132.75 ?2.71</cell><cell cols="2">20.38 ?3.56 1.88 ?0.45</cell><cell>0.53 ?0.18</cell></row><row><cell>16</cell><cell>KL</cell><cell>16</cell><cell>0.87</cell><cell>210.31 ?3.97</cell><cell cols="2">24.08 ?4.22 1.07 ?0.36</cell><cell>0.68 ?0.15</cell></row><row><cell>16</cell><cell>KL</cell><cell>8</cell><cell>2.63</cell><cell>178.68 ?4.08</cell><cell cols="2">21.94 ?3.92 1.49 ?0.42</cell><cell>0.59 ?0.17</cell></row><row><cell>8</cell><cell>KL</cell><cell>4</cell><cell>0.90</cell><cell>209.90 ?4.92</cell><cell cols="2">24.19 ?4.19 1.02 ?0.35</cell><cell>0.69 ?0.15</cell></row><row><cell>4</cell><cell>KL</cell><cell>3</cell><cell>0.27</cell><cell>227.57 ?4.89</cell><cell cols="2">27.53 ?4.54 0.55 ?0.24</cell><cell>0.82 ?0.11</cell></row><row><cell>2</cell><cell>KL</cell><cell>2</cell><cell>0.086</cell><cell>232.66</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table /><note>Complete autoencoder zoo trained on OpenImages, evaluated on ImageNet-Val. ? denotes an attention-free autoencoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>. 21 COCO256 ? 256 OpenImages 256 ? 256 OpenImages 512 ? 512</figDesc><table><row><cell>Method</cell><cell>FID?</cell><cell>FID?</cell><cell>FID?</cell></row><row><cell>LostGAN-V2 [87]</cell><cell>42.55</cell><cell>-</cell><cell>-</cell></row><row><cell>OC-GAN [89]</cell><cell>41.65</cell><cell>-</cell><cell>-</cell></row><row><cell>SPADE [62]</cell><cell>41.11</cell><cell>-</cell><cell>-</cell></row><row><cell>VQGAN+T [37]</cell><cell>56.58</cell><cell>45.33</cell><cell>48.11</cell></row><row><cell>LDM-8 (100 steps, ours)</cell><cell>42.06  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>LDM-4 (200 steps, ours)</cell><cell>40.91  *</cell><cell>32.02</cell><cell>35.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Quantitative comparison of our layout-to-image models on the COCO<ref type="bibr" target="#b3">[4]</ref> and OpenImages<ref type="bibr" target="#b49">[49]</ref> datasets.</figDesc><table><row><cell>Method</cell><cell>FID?</cell><cell>IS?</cell><cell cols="3">Precision? Recall? Nparams</cell><cell></cell></row><row><cell>SR3 [72]</cell><cell>11.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>625M</cell><cell>-</cell></row><row><cell>ImageBART [21]</cell><cell>21.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.5B</cell><cell>-</cell></row><row><cell>ImageBART [21]</cell><cell>7.44</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.5B</cell><cell>0.05 acc. rate  *</cell></row><row><cell>VQGAN+T [23]</cell><cell>17.04</cell><cell>70.6?1.8</cell><cell>-</cell><cell>-</cell><cell>1.3B</cell><cell>-</cell></row><row><cell>VQGAN+T [23]</cell><cell>5.88</cell><cell>304.8?3.6</cell><cell>-</cell><cell>-</cell><cell>1.3B</cell><cell>0.05 acc. rate  *</cell></row><row><cell>BigGan-deep [3]</cell><cell>6.95</cell><cell>203.6?2.6</cell><cell>0.87</cell><cell>0.28</cell><cell>340M</cell><cell>-</cell></row><row><cell>ADM [15]</cell><cell>10.94</cell><cell>100.98</cell><cell>0.69</cell><cell>0.63</cell><cell>554M</cell><cell>250 DDIM steps</cell></row><row><cell>ADM-G [15]</cell><cell>4.59</cell><cell>186.7</cell><cell>0.82</cell><cell>0.52</cell><cell>608M</cell><cell>250 DDIM steps</cell></row><row><cell>ADM-G,ADM-U [15]</cell><cell>3.85</cell><cell>221.72</cell><cell>0.84</cell><cell>0.53</cell><cell>n/a</cell><cell>2 ? 250 DDIM steps</cell></row><row><cell>CDM [31]</cell><cell>4.88</cell><cell>158.71?2.26</cell><cell>-</cell><cell>-</cell><cell>n/a</cell><cell>2 ? 100 DDIM steps</cell></row><row><cell>LDM-8 (ours)</cell><cell>17.41</cell><cell>72.92?2.6</cell><cell>0.65</cell><cell>0.62</cell><cell>395M</cell><cell>200 DDIM steps, 2.9M train steps, batch size 64</cell></row><row><cell>LDM-8-G (ours)</cell><cell>8.11</cell><cell>190.43?2.60</cell><cell>0.83</cell><cell>0.36</cell><cell>506M</cell><cell>200 DDIM steps, classifier scale 10, 2.9M train steps, batch size 64</cell></row><row><cell>LDM-8 (ours)</cell><cell>15.51</cell><cell>79.03?1.03</cell><cell>0.65</cell><cell>0.63</cell><cell>395M</cell><cell>200 DDIM steps, 4.8M train steps, batch size 64</cell></row><row><cell>LDM-8-G (ours)</cell><cell>7.76</cell><cell>209.52?4.24</cell><cell>0.84</cell><cell>0.35</cell><cell>506M</cell><cell>200 DDIM steps, classifier scale 10, 4.8M train steps, batch size 64</cell></row><row><cell>LDM-4 (ours)</cell><cell>10.56</cell><cell>103.49?1.24</cell><cell>0.71</cell><cell>0.62</cell><cell>400M</cell><cell>250 DDIM steps, 178K train steps, batch size 1200</cell></row><row><cell>LDM-4-G (ours)</cell><cell>3.95</cell><cell>178.22?2.43</cell><cell>0.81</cell><cell>0.55</cell><cell>400M</cell><cell>250 DDIM steps, unconditional guidance [32] scale 1.25, 178K train steps, batch size 1200</cell></row><row><cell>LDM-4-G (ours)</cell><cell>3.60</cell><cell>247.67?5.59</cell><cell>0.87</cell><cell>0.48</cell><cell>400M</cell><cell>250 DDIM steps, unconditional guidance [32] scale 1.5, 178K train steps, batch size 1200</cell></row></table><note>? : Training from scratch on COCO;* : Finetuning from OpenImages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on the ImageNet<ref type="bibr" target="#b11">[12]</ref> dataset.</figDesc><table /><note>* : Classifier rejection sampling with the given rejection rate as proposed in [67].D.5. Sample Quality vs. V100 Days (Continued from Sec. 4.1)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>qualitatively similar results.</figDesc><table><row><cell>Method</cell><cell>FID ?</cell><cell>IS ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell></row><row><cell>Image Regression [72]</cell><cell>15.2</cell><cell>121.1</cell><cell>27.9</cell><cell>0.801</cell></row><row><cell>SR3 [72]</cell><cell>5.2</cell><cell>180.1</cell><cell>26.4</cell><cell>0.762</cell></row><row><cell>LDM-4 (ours, 100 steps)</cell><cell>2.8  ? /4.8  ?</cell><cell>166.3</cell><cell>24.4?3.8</cell><cell>0.69?0.14</cell></row><row><cell>LDM-4 (ours, 50 steps, guiding)</cell><cell>4.4  ? /6.4  ?</cell><cell>153.7</cell><cell>25.8?3.7</cell><cell>0.74?0.12</cell></row><row><cell>LDM-4 (ours, 100 steps, guiding)</cell><cell>4.4  ? /6.4  ?</cell><cell>154.1</cell><cell>25.7?3.7</cell><cell>0.73?0.12</cell></row><row><cell>LDM-4 (ours, 100 steps, +15 ep.)</cell><cell>2.6  ? / 4.6  ?</cell><cell>169.76?5.03</cell><cell>24.4?3.8</cell><cell>0.69?0.14</cell></row><row><cell>Pixel-DM (100 steps, +15 ep.)</cell><cell>5.1  ? / 7.1  ?</cell><cell>163.06?4.67</cell><cell>24.1?3.3</cell><cell>0.59?0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>It is not possible to exactly match both architectures since the diffusion model operates in the pixel space 23 E. Implementation Details and Hyperparameters E.1. Hyperparameters We provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15. CelebA-HQ 256 ? 256 FFHQ 256 ? 256 LSUN-Churches 256 ? 256 LSUN-Bedrooms 256 ? 256</figDesc><table><row><cell>f</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>4</cell></row><row><cell>z-shape</cell><cell>64 ? 64 ? 3</cell><cell>64 ? 64 ? 3</cell><cell>-</cell><cell>64 ? 64 ? 3</cell></row><row><cell>|Z|</cell><cell>8192</cell><cell>8192</cell><cell>-</cell><cell>8192</cell></row><row><cell>Diffusion steps</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Noise Schedule</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell></row><row><cell>N params</cell><cell>274M</cell><cell>274M</cell><cell>294M</cell><cell>274M</cell></row><row><cell>Channels</cell><cell>224</cell><cell>224</cell><cell>192</cell><cell>224</cell></row><row><cell>Depth</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Channel Multiplier</cell><cell>1,2,3,4</cell><cell>1,2,3,4</cell><cell>1,2,2,4,4</cell><cell>1,2,3,4</cell></row><row><cell>Attention resolutions</cell><cell>32, 16, 8</cell><cell>32, 16, 8</cell><cell>32, 16, 8, 4</cell><cell>32, 16, 8</cell></row><row><cell>Head Channels</cell><cell>32</cell><cell>32</cell><cell>24</cell><cell>32</cell></row><row><cell>Batch Size</cell><cell>48</cell><cell>42</cell><cell>96</cell><cell>48</cell></row><row><cell>Iterations  *</cell><cell>410k</cell><cell>635k</cell><cell>500k</cell><cell>1.9M</cell></row><row><cell>Learning Rate</cell><cell>9.6e-5</cell><cell>8.4e-5</cell><cell>5.e-5</cell><cell>9.6e-5</cell></row></table><note>.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 .</head><label>12</label><figDesc>Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA A100. 256 ? 3 128 ? 128 ? 2 64 ? 64 ? 3 32 ? 32 ? 4 16 ? 16 ? 8 88 ? 8 ? 32</figDesc><table><row><cell></cell><cell cols="2">LDM-1</cell><cell>LDM-2</cell><cell>LDM-4</cell><cell>LDM-8</cell><cell>LDM-16</cell><cell>LDM-32</cell></row><row><cell cols="2">z-shape 256 ? |Z|</cell><cell>-</cell><cell>2048</cell><cell>8192</cell><cell>16384</cell><cell>16384</cell><cell>16384</cell></row><row><cell>Diffusion steps</cell><cell cols="2">1000</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Noise Schedule</cell><cell cols="2">linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell></row><row><cell>Model Size</cell><cell cols="2">396M</cell><cell>391M</cell><cell>391M</cell><cell>395M</cell><cell>395M</cell><cell>395M</cell></row><row><cell>Channels</cell><cell cols="2">192</cell><cell>192</cell><cell>192</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>Depth</cell><cell></cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Channel Multiplier</cell><cell cols="2">1,1,2,2,4,4</cell><cell>1,2,2,4,4</cell><cell>1,2,3,5</cell><cell>1,2,4</cell><cell>1,2,4</cell><cell>1,2,4</cell></row><row><cell>Number of Heads</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Batch Size</cell><cell></cell><cell>7</cell><cell>9</cell><cell>40</cell><cell>64</cell><cell>112</cell><cell>112</cell></row><row><cell>Iterations</cell><cell cols="2">2M</cell><cell>2M</cell><cell>2M</cell><cell>2M</cell><cell>2M</cell><cell>2M</cell></row><row><cell>Learning Rate</cell><cell cols="2">4.9e-5</cell><cell>6.3e-5</cell><cell>8e-5</cell><cell>6.4e-5</cell><cell>4.5e-5</cell><cell>4.5e-5</cell></row><row><cell>Conditioning</cell><cell cols="2">CA</cell><cell>CA</cell><cell>CA</cell><cell>CA</cell><cell>CA</cell><cell>CA</cell></row><row><cell>CA-resolutions</cell><cell cols="2">32, 16, 8</cell><cell>32, 16, 8</cell><cell>32, 16, 8</cell><cell>32, 16, 8</cell><cell>16, 8, 4</cell><cell>8, 4, 2</cell></row><row><cell>Embedding Dimension</cell><cell cols="2">512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>Transformers Depth</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 .</head><label>13</label><figDesc>Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1. All models trained on a single NVIDIA A100. Implementations of ? ? for conditional LDMsFor the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner ? ? as an unmasked transformer which processes a tokenized version of the input y and produces an output ? := ? ? (y), where ? ? R M ?d? . More specifically, the transformer is implemented from N transformer blocks consisting of global self-attention layers, layer-normalization and position-wise MLPs as follows<ref type="bibr" target="#b1">2</ref> : 256 ? 3 128 ? 128 ? 2 64 ? 64 ? 3 32 ? 32 ? 4 16 ? 16 ? 8 88 ? 8 ? 32 |Z|</figDesc><table><row><cell>E.2. Implementation Details</cell></row><row><cell>E.2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 .</head><label>15</label><figDesc>Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting model which was trained on eight V100.</figDesc><table /><note>? ? TokEmb(y) + PosEmb(y)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17 .</head><label>17</label><figDesc>Hyperparameters for the experiments with transformer encoders in Sec. 4.3.</figDesc><table><row><cell>seq-length</cell><cell>77</cell><cell>92</cell></row><row><cell>depth N</cell><cell>32</cell><cell>16</cell></row><row><cell>dim</cell><cell>1280</cell><cell>512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 18 .</head><label>18</label><figDesc>Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute during training in V100-days, numbers of competing methods taken from<ref type="bibr" target="#b14">[15]</ref> unless stated differently;</figDesc><table /><note>* : Throughput measured in sam- ples/sec on a single NVIDIA A100;? : Numbers taken from [15] ;? : Assumed to be trained on 25M train examples; ? ? : R-FID vs. ImageNet validation set</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">adapted from https://github.com/lucidrains/x-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This factor corresponds to the speedup of the A100 over the V100 for a U-Net, as defined inFig. 1in<ref type="bibr" target="#b75">[74]</ref> </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wasserstein gan</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="1920" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulfar</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast fourier convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<idno>abs/2011.10650, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diagnosing and enhancing VAE models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ethical considerations of generative ai. AI for Content Creation Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno>abs/2105.05233</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Musings on typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cogview: Mastering text-toimage generation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/2105.13290</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="658" to="666" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/2108.08827</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A note on data biases in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02516</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/2012.09841</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sex, lies, and videotape: Deep fakes and free speech delusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">Ezra</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Md. L. Rev</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">892</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clipdraw: Exploring text-to-drawing synthesis through languageimage encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">B</forename><surname>Soros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Witkowski</surname></persName>
		</author>
		<idno>abs/2106.14843, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Make-a-scene: Scenebased text-to-image generation with human priors. CoRR, abs/2203.13131</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelly</forename><surname>Sheynin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. CoRR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cascaded diffusion models for high fidelity image generation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Perceiver IO: A general architecture for structured inputs &amp;outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<idno>abs/2107.14795, 2021. 4</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Highresolution complex scene synthesis with transformers. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Jahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/2105.06458</idno>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niharika</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Olmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sailik</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lydia</forename><surname>Manikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Score matching model for unbounded data score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno>abs/2106.05527</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Variational diffusion models. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno>abs/2107.00630</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno>abs/2106.00132, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
		<idno>abs/1811.00982</idno>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improved precision and recall metric for assessing generative models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Region-wise generative adversarial imageinpainting for large missing areas. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Hancock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Sdedit: Image synthesis and editing with stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>abs/2108.01073, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On the convergence properties of GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<idno>abs/1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<idno>abs/2103.16091, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><forename type="middle">Z</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno>abs/1901.00212</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">GLIDE: towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2112.10741</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis Yu-Jing</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">High-fidelity performance metrics for generative models in pytorch</title>
		<idno type="DOI">10.5281/zen-odo.4957738</idno>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dual contradistinctive generative autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="823" to="832" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11222</idno>
		<title level="m">On buggy resizing libraries and surprising subtleties in fid calculation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Carbon emissions and large neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/2104.10350</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A?ron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Network-to-network translation with conditional invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI (3)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement. CoRR, abs/2104.07636</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>abs/1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Salvator</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia Developer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blog</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/blog/getting-immediate-speedups-with-a100-tf32" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/2104.02600, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Projected gans converge faster. CoRR, abs/2111.01007, 2021. 6</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A unet based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sch?nfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<idno>2020. 6</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8204" to="8213" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
	<note>Int. Conf. Learn. Represent</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">D2C: diffusion-denoising models for few-shot conditional generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>abs/2106.06819, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Alien Dreams: An Emerging Art Scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<ptr target="https://ml.berkeley.edu/blog/posts/clip-art/" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Online; accessed November-2021</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>abs/1503.03585</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net, 2021. 3, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Scorebased generative modeling through stochastic differential equations. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for modern deep learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13693" to="13696" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Learning layout and style reconfigurable gans for controllable image synthesis. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Resolution-robust large mask inpainting with fourier convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mashikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naejin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshith</forename><surname>Goka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv, abs/2109.07161, 2021. 8, 9</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Object-centric image generation from layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1920" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">This face does not exist... but it might be yours! identity leakage in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tinsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Czajka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Scorebased generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>CoRR, abs/2106.05931, 2021. 2, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<ptr target="https://twitter.com/RiversHaveWings/status/1478093658716966912,2022.6" />
		<title level="m">Rivers Have Wings. Tweet on Classifier-free guidance for autoregressive models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">VAEBM: A symbiosis between variational autoencoders and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using VQ-VAE and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno>abs/2104.10157, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">LSUN: construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1506.03365</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Vector-quantized image modeling with improved vqgan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><forename type="middle">Yu</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Designing a practical degradation model for deep blind image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno>abs/2103.14006, 2021. 23</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Large scale image completion via co-modulated generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno>abs/2103.10428, 2021. 9</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">LAFITE: towards language-free training for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/2111.13792</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
