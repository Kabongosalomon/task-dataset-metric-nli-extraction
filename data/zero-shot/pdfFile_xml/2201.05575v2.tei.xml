<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cheng</surname></persName>
							<email>chengxu@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Improving the Government&apos;s Governance Capability Big Data Application Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Improving the Government&apos;s Governance Capability Big Data Application Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS ? Information systems ? Information extraction KEYWORDS Pre-trained Language Models, Knowledge Graph Completion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they struggle to reason rare or emerging unseen entities. In this paper, we propose kNN-KGE, a new knowledge graph embedding approach with pre-trained language models, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge Graphs (KGs) organize facts in a structured way as triples in the form of &lt;subject, predicate, object&gt;, abridged as ( , , ), where and denote entities and builds relations between entities. Most KGs are far from complete due to emerging entities and their relations in real-world applications; hence KG completion-the problem of extending a KG with missing triples-has <ref type="figure">Figure 1</ref>: Our NN-KGE not only leverages entity prediction from softmax (MEM head in PLMs) but also retrieves the entities from the knowledge store constructed from entity descriptions and training triples. appeal to researchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. Most previous KG completion methods, such as TransE <ref type="bibr" target="#b1">[2]</ref>, ComplEx <ref type="bibr" target="#b29">[30]</ref>, and RotatE <ref type="bibr" target="#b27">[28]</ref>, are knowledge embedding techniques that embed the entities and relations into a vector space and then obtain the predicted triples by leveraging a pre-defined scoring function to those vectors. Those methods encode all of the relational knowledge implicitly in the weights of the parametric neural network via end-to-end training. However, a major limitation of these approaches is that they can hardly reason through rare entities evolving in a few triples or emerging entities unseen during training.</p><p>Note that human reasoning is facilitated by complex systems interacting together, for example, integration of current knowledge and retrieval from memory. Recent progress in memory-augmented neural networks has given rise to the design of modular architectures that separate computational processing and memory storage. Those memory-based approaches (or non/semi-parametric methods) have been applied to tasks such as language modeling <ref type="bibr" target="#b14">[15]</ref> and question answering <ref type="bibr" target="#b13">[14]</ref>, which are expressive and adaptable.</p><p>Inspired by this, we propose NN-KGE, an approach that extends knowledge graph embedding by linearly interpolating its entity distribution with a -nearest neighbors ( NN) model. As shown in <ref type="figure">Figure 1</ref>, we construct a knowledge store of entities with pretrained language models (PLMs) and retrieve nearest neighbors according to distance in the entity embedding space. Given a triple with a head or tail entity missing, we utilize the representation of [MASK] output as the predicted anchor entity embedding to find the nearest neighbor in the knowledge store and interpolate the nearestneighbor distribution with the masked entity prediction. Thus, rare entities or emerging triples can be memorized explicitly, which makes reasoning through memorization rather than implicitly in model parameters. Experimental results on two datasets (FB15k-237</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Entity Vocabulary Expansion</head><p>Plato was an Athenian philosopher.</p><p>[CLS]</p><p>[SEP] <ref type="bibr">[MASK]</ref> [SEP]   <ref type="bibr" target="#b19">[20]</ref>.</p><p>[29] and WN18RR <ref type="bibr" target="#b7">[8]</ref>) in both transductive and inductive reasoning demonstrate the effectiveness of our approach. Qualitatively, we observe that our approach is particularly beneficial for low-resource knowledge graph embedding, which might be easier to access via explicit memory. Our contributions can be summarized as follows:</p><p>? To the best of our knowledge, this is the first semi-parametric approach for knowledge graph embedding. Our work may open up new avenues for improving knowledge graph reasoning through explicit memory. ? We introduce NN-KGE that can explicitly memorize rare or emerging entities, which is essential in practice since KGs are evolving. ? Experimental results on two benchmark datasets show that our model can yield better performance than baselines and is particularly beneficial for low-resource reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY 2.1 Preliminary</head><p>Knowledge Graph. We define a knowledge graph with entity descriptions as a tuple G = (E, R, T , D), where E represents a set of entities, R represents relation types, T represents a set of triples and D represents the entity descriptions. For each triple in T , it has the form ( , , ) where , ? E is the head and tail entity respectively. For each entity ? E, there exists a text to describe . To complete missing triples in knowledge graphs, link prediction is proposed, which aims at predicting the tail entity given the head entity and the query relation, denoted by ( , , ?) <ref type="bibr" target="#b1">2</ref> , where the answer is supposed to be always within the KG.</p><p>Masked Language Model (MLM). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the origin vocabulary id of the masked token based only on its context <ref type="bibr" target="#b8">[9]</ref>. Pre-trained language models mostly use MLM as the pre-training task.</p><p>[MASK], [CLS] and [SEP] are the special tokens pre-defined in the pre-trained language models (PLMs). We can easily get the output embedding of the special tokens in the input sequence by the encoder of PLMs. MEM Head. Like the 'word embedding layer' (MLM Head) in the pre-trained language model that maps contextualized token representation into probability distribution of tokens in the vocabulary, MEM Head which consists of entity embeddings, maps contextualized entity representation to the probability distribution of the entity in the knowledge graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework</head><p>In this section, we introduce the general framework of the proposed approach. We first propose masked entity modeling and entity vocabulary expansion in section 2.3, which converts link prediction into an entity prediction task. To address the issue of rare or unknown entities, in section 2.4, we construct a knowledge store for retrieving the entity by the anchor embedding (the representation of [MASK] output). Lastly, in section 2.5, we provide the details of inference which makes reasoning through memorization rather than implicitly in model parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contextualized KG Representation</head><p>In this subsection, we treat the BERT model as the entity predictor because we convert the link prediction task to a masked entity modeling task, which uses the structural information or text description to predict the missing entity.</p><p>Masked Entity Modeling. For link prediction, given an incomplete triple ( , , ?), previous studies utilize KG embeddings or textual encoding to represent triple and leverage a pre-defined scoring function to those vectors. With pre-trained encoders, those approaches such as KG-BERT <ref type="bibr" target="#b33">[34]</ref> are generalizable enough and robust to the incompleteness; however, they have to iterate through all possible triples to in the inference stage. In this paper, we simply leverage masked entity modeling for link prediction, which makes the model predict the correct entity like the Masked Language Model (MLM) task. Concretely, masked entity modeling can mitigate scoring all triples with incorrect entities, sophisticated score function designing, and free of negative sampling. In other words, the model only needs to predict the missing entity at the tail or head.</p><p>Specifically, given a querying triple ( , , ?) and the description to the entity , we concatenate this triple and the entity description to obtain the input sequence to predict the entity as follows:</p><formula xml:id="formula_0">= [CLS] [SEP] [SEP] [MASK] [SEP].</formula><p>By masked entity modeling, the model can obtain the correct entity by ranking the probability of each entity in the knowledge graph with MEM .</p><formula xml:id="formula_1">MEM ( | ) = ([MASK] = | ; ?),<label>(1)</label></formula><p>where ? represents the parameters of the pre-trained language models. Note that the procedure of masked entity modeling is simple yet effective, and the one-pass inference speed is faster than the previous BERT-based model like StAR <ref type="bibr" target="#b30">[31]</ref>. A detailed comparison of inference time can be found in <ref type="table" target="#tab_1">Table 1</ref>. The same as the masked language modeling task, we must predict the original value of each [MASK] token. To do so, we use the same loss function to optimize our masked entity models.</p><formula xml:id="formula_2">L MEM = ? 1 |E | ?? ? E 1 = log ([MASK] = | ; ?) ,<label>(2)</label></formula><p>where |E | is the number of total entities E and ? represents the parameters of the model.</p><p>Since PLMs usually leverage byte-pair encoding to transform sequences into subwords units, making entity representation is separated with tokens and challenging to inference. To address this issue, we propose entity vocabulary expansion to utilize embeddings for each unique entity.</p><p>Entity Vocabulary Expansion. Since it is non-trivial to utilize subwords for entity inference, we directly utilize embeddings for each unique entity as common knowledge embedding methods <ref type="bibr" target="#b1">[2]</ref> do. Specifically, we represent the entities ? E as special tokens in language model's vocabulary; thus, knowledge graph reasoning can reformulate as a masked entity prediction task as shown in <ref type="figure">Figure 1</ref> and <ref type="figure" target="#fig_0">Figure 2</ref>. To obtain those entity embeddings (special tokens in vocabulary), we utilize entity description to design prompts such as "The description of [MASK] is that" to obtain entity embeddings. Given an entity description = ( 1 , ..., ) to the entity , we add tokens [CLS] and [SEP] to the beginning and end of the description to construct the input sequence as follows:</p><formula xml:id="formula_3">= [CLS] prompt([MASK]) [SEP] [SEP]</formula><p>We optimize those entity embeddings (random initialization) by predicting the entity at the masked position with the other parameters fixed. Formally, we have:</p><formula xml:id="formula_4">L = ? 1 |E | ?? ?E 1 = log ([MASK] = | ; ?)<label>(3)</label></formula><p>where |E | is the number of total entities E and ? represents the parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Knowledge Store</head><p>Inspired by recent progress in memory-augmented neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, we construct knowledge store to explicit memorize entities.</p><p>To be specific, we construct the knowledge store mainly from two parts: entity descriptions that represent semantic information and triples of the entity that represents structural knowledge. Notably, we use the model trained by contextualized KG representation to construct our knowledge store from two aspects, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>; thus, rare or emerging entities can be explicitly represented in the external memory, which is flexible and adaptable. We introduce the details of construction as follows:</p><p>From Descriptions D. Let (?) be the function that maps the entity in the input to a fixed-length vector representation computed by PLM. We use pre-designed prompts to obtain entity embedding from entity descriptions. Like the same procedure in section 2.3, we use the prompt input to obtain the entity embedding and add them to the knowledge store. Thus, we can construct the knowledge store (K, V) D from descriptions (the set of descriptions D of all entities E in G).</p><formula xml:id="formula_5">(K, V) D = {( ( ) , ) | ( , ) ? G} (4)</formula><p>From Triples T . Since different relations focus on different aspects of the same entity, it is intuitive to utilize different triples to represent entities. For example, given the triple (Plato, lives, ?), the model can reason through the triple of (Plato, nationality, Greece) in KGs to obtain Greece. Thus, we also construct a knowledge store from triples. We add all training triples containing entities to the knowledge store to represent a different aspect of the same entity. To obtain the entity embedding from triples T , we follow the same procedure in section 2.3 to obtain the entity embedding . Finally, we can construct the knowledge store (K, V) T from triples (the set of all triples T in G).</p><formula xml:id="formula_6">(K, V) T = {( ( ) , ) | ( , ) ? G} (5)</formula><p>Implementation. The knowledge store contains entity embedding constructed from descriptions and triples, which can be up to millions of entity embeddings. Concretely, we define a key-value pair ( , ) for the knowledge store inference, where the key is the vector representation of the entity embedding ( ) and the value is the target entity where is the sequence built on descriptions and triples to entity . We utilize the open-source library FAISS <ref type="bibr" target="#b11">[12]</ref> for fast nearest-neighbor retrieval in high dimensional spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Memorized Inference</head><p>For knowledge graph completion tasks, given a triple with the head or tail entity missing, we use the representation of [MASK] output as the predicted anchor entity embedding to find the nearest neighbor in the knowledge store. To be specific, the model queries the knowledge store with the final hidden state h <ref type="bibr">[MASK]</ref> corresponding to the special token [MASK] to obtain the distribution of NN according to a distance function d(?, ?).</p><formula xml:id="formula_7">d(h , h ) = |h , h | 2 ,<label>(6)</label></formula><p>where || 2 refers to the Euclidean distance. Thus, we can obtain the probability distribution over neighbors based on a softmax of ? nearest neighbors. For each entity retrieved from the knowledge store, we choose only one nearest embedding in the knowledge store to represent the entity since there are several entity embeddings corresponding to the same entity <ref type="bibr" target="#b2">3</ref> .</p><formula xml:id="formula_8">kNN ( | ) ? ?? ( , ) ?N 1 = exp (?d ( , ( )))<label>(7)</label></formula><p>To obtain the target entity, we interpolate the nearest neighbor distribution NN with the model entity prediction MEM which can be seen in eq. (1) and using a hyper-parameter to produce the final NN-KGE distribution: <ref type="bibr" target="#b2">3</ref> We set zero probability for the entities that do not appear in the retrieved targets </p><formula xml:id="formula_9">( | ) = NN ( | ) + (1 ? ) MEM ( | )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>Datasets. We evaluate our method on FB15k-237 <ref type="bibr" target="#b28">[29]</ref> and WN18RR <ref type="bibr" target="#b7">[8]</ref> in both transductive and inductive settings, which are widely used in the link prediction literature. In FB15k-237, descriptions of entities are obtained from the introduction section of the Wikipedia page of each entity. In WN18RR, each entity corresponds to a word sense, and description is the word definition. More details about datasets are listed in the Appendix.</p><p>Metrics. Given the score for each entity in the candidate set, we sort them to obtain a ranked list. Let head be the position of the correct entity in the rank. The reciprocal rank is calculated by 1/ head . This procedure is repeated by predicting the tail entity to obtain the reciprocal rank 1/ tail . The Mean Reciprocal Rank (MRR) is the mean of these two values, averaged across all triples in the knowledge graph. The Mean Rank (MR) is the mean of head and tail averaged across all triples in the knowledge graph. The Hits at 1 metric (Hits@1) is obtained by counting the number of times the correct triple appears at position 1, and averaging for the MRR. The Hits@3 and Hits@10 are computed similarly, considering the first 3 and 10 positions, respectively. When scoring candidates for a given triple, we consider the filtered setting <ref type="bibr" target="#b1">[2]</ref> where candidates are in the set? minus those that would result in a correct triple, according to the training, validation, and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transductive Experiments</head><p>Setting. We adopt BERT-base for the encoder of the model for a fair comparison with other PLM-based KGC methods. Following StAR <ref type="bibr" target="#b30">[31]</ref>, we choose the graph embedding approach and textual encoding approach as our baseline models in the transductive setting. We run a grid search over the datasets and report the results on the test set with hyperparameters of the best performance on the validation set. The detailed hyperparameters are listed in the Appendix for reproducibility.</p><p>Evaluation. We evaluate the test set of triples T ? disjoint from the set of training triples T . For a test triple ( , , ), we make an inference for the target by predicting the entity in the candidate set E. Ideally, the probability of incorrect candidates should be lower <ref type="table">Table 3</ref>: Link prediction results on WN18RR and FB15k-237. ?Resulting numbers are reported by <ref type="bibr" target="#b22">[23]</ref> and others are taken from the original papers. NN-KGE (w/o KS) represents we only use masked entity modeling without knowledge store. The bold numbers denote the best results in each genre while the underlined ones are the second-best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k-237  <ref type="table">Table 4</ref>: Link prediction results on WN18RR and FB15k-237 in inductive setting. ?Resulting numbers are reported by <ref type="bibr" target="#b6">[7]</ref>. We use WN18RR and FB15k-237 in inductive setting (see text for more details). The bold numbers denote the best results in each genre while the underlined ones are the second-best performance. Following <ref type="bibr" target="#b6">[7]</ref> we choose baseline models as BERT for Link Prediction (BLP) followed by the employed relational model (e.g. BLP-TransE) and BERT-based (BE) as the encoder for textual models. than the probability of the correct entity. In the transductive setting, the entities in the test triple are assumed to be seen in the set of training entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR ind</head><p>Main Results. As shown in <ref type="table">Table 3</ref>, our proposed NN-KGE can obtain state-of-the-art or competitive performance on all these datasets, especially significant improvement (0.443 ? 0.525 on WN18RR and 0.252 ? 0.280 on FB15k-237) in terms of Hits@1. We hold that the improvement is mainly attributed to better retrieval by our masked entity modeling and knowledge store. Note that our approach can memorize rare entities in KGs, thus, promoting performance. Further analysis can be seen in the section 3.4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inductive Experiments</head><p>Setting. We also adopt BERT-base for the encoder of our NN-KGE in the inductive setting. We reproduce the Bag-Of-Words (BOW) baseline of Daza et al. <ref type="bibr" target="#b6">[7]</ref>, in which an entity is encoded as the average of embeddings of words. Notably, for the entities in the validation and test set, we construct the knowledge store only from their descriptions as shown in section 2.3; thus, those triples are unknown during training.</p><p>Evaluation. In the inductive setting, we construct a new test graph G ? = (E ? , R ? , T ? , D ? ) where the sets of entities E ? are disjoint from E. While for the relations ? R ? , we assume that R ? ? R.</p><p>Main Results. From <ref type="table">Table 4</ref>, we observe that our NN-KGE method can yield better or comparable performance compared with the previous state-of-the-art method, which demonstrates the effectiveness of the knowledge store. Compared to the baseline models that use BERT solely as the text description encoder, our approach makes full use of the self-attention mechanism of the transformer-based model to predict the missing entities. We notice that NN-KGE reaches the best Hits@1, which further validates the advantages of reasoning through memorization. Moreover, our approach is flexible for inductive settings since emerging entities can be directly added to the knowledge store, which is especially relevant in practice since KGs may be extended with triples describing new objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study with Less Training Data</head><p>As shown in section 3.1, retrieving neighbors from the knowledge store can improve the model's performance. When facing emerging entities, the translation-based model (such as TransE) have to add new entities or new triples to the models and retrain the entire model, which requires huge computation resource. Thus, we further evaluate our model with only a few training triples and incrementally add new triples step by step, simulating the appearance of new entities. Specifically, we use a fixed untrained knowledge store constructed from all training samples <ref type="bibr" target="#b3">4</ref> and optimize our NN-KGE with from 10% to 90% training triples. We conducted an ablation study compared with the state-of-the-art model TuckER <ref type="bibr" target="#b0">[1]</ref> (red line) and our approach without retrieving knowledge store (w/o KS). From <ref type="figure" target="#fig_1">Figure 3</ref>, we observe that with 10% training triples, our approach can achieve improvement (MRR 0.18 ? 0.279) when added with nearest neighbors retrieval over knowledge store. Moreover, we notice that our approach with 70% samples can yield comparable performance with the previous SoTA model in MRR. This result suggests that our method can efficiently handle rare and emerging entities via reasoning through memorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>In this section, we first conduct a case study and show the improvements in tail entities by the knowledge store. Second, we analyze the effect of hyperparameters on retrieving the entities in the knowledge store. Then, we visualize the entity embeddings in the <ref type="bibr" target="#b3">4</ref> All entity embeddings are fixed.  knowledge store with a query triple. We further study the impact of different pre-trained language models with different pre-trained tasks on the performance of NN-KGE. Lastly, we summarize the size of the knowledge store and the speed of reasoning on two datasets: WN18RR and FB15k-237.</p><p>Improvements in Tail Entities. We report the baseline model Ro-tatE and NN-KGE performance in the dataset FB15k-237 group by different frequency of the entities and illustrate the long-tail distribution of the FB15k-237 dataset. Note that the construction of the knowledge store is to improve the model's performance on the less frequent entities. We find that those tail entities in long-tailed distributions can achieve performance improvement. We can observe from the <ref type="figure" target="#fig_3">Figure 4</ref> that only 18% entities show up more than 50 times and NN-KGE obtain improvements (0.118 ? 0.183) with the entities occur less than 20 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Number of Neighbors.</head><p>To validate the impact of the number of neighbors, we conduct experiments with the different numbers of nearest neighbor entities to retrieve from the knowledge store. From <ref type="figure" target="#fig_4">Figure 5</ref>, we find that the model performance continues to improve as increases until it converges when reaching a threshold ( = 64). We think this is because those entities retrieved from the knowledge store are far from the anchor embedding, thus, having a low influence on the knowledge graph reasoning results.  Impact of Interpolation. Since we use a parameter to interpolate between the BERT model distribution and the retrieved distribution from the NN search over the dataset, we further conduct experiments to analyze the interpolation. From <ref type="figure" target="#fig_5">Figure 6</ref>, we notice that = 0.2 is optimal on FB15k-237 dataset. The suitable can help correct the model from inferring the wrong entities, which can be seen in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>Visualization of Entity Embeddings in Knowledge Store. Since we build our knowledge store with different entity embeddings from different aspects of the same entity, we visualize those embeddings in the knowledge store. Specific, we random sample an input triple (?, nationality, America), retrieve and visualize the 20 nearest neighbor entities with their embeddings in the knowledge store. From <ref type="figure" target="#fig_7">Figure 8</ref>, we notice that those retrieved entities clustered together (we can treat the anchor embedding ? as the aspect of "American" in the knowledge store) with similar semantic concepts while other entities outside the top-20 distance (black circle) focus on other concepts like profession. example, BERT is trained on a combination of BOOKCORPUS <ref type="bibr" target="#b41">[41]</ref> and some plain text from WIKIPEDIA, while RoBERTa is trained with additional corpus like CC-News. By default we use BERT-baseuncased (L = 12, H = 768, A = 12, 110M params), and choose BERT, RoBERTa <ref type="bibr" target="#b20">[21]</ref>, SpanBERT <ref type="bibr" target="#b12">[13]</ref> for comparison. In <ref type="figure" target="#fig_6">Figure 7</ref>, we select three different models with two sizes to do the experiments to test the influence the PLMs bring to our approach. Notably, SpanBERT shows worse performance than the two other models; we suppose it mainly attributes to its different pre-training tasks.  Case Study. We also conduct case studies to analyze the different reasoning results with or without querying the knowledge store. From <ref type="table" target="#tab_6">Table 5</ref>, we observe that NN-KGE w/o Knowledge Store can infer better entities given head/tail entities and relations. Note  Statistics of Knowledge Store. Note that our knowledge store is built from two different resources: the descriptions of entities (D) and the triples in the knowledge graph (T ). We count the percentage of both in the retrieved entity embeddings in <ref type="table" target="#tab_7">Table 6</ref>. We observe that the triples in knowledge graphs account for most parts and our model mainly retrieves triples for help. We further notice that descriptions of entities occupy a relatively high proportion (13/87) of the WN18RR. We think this is because that WN18RR is a lexical database, and text descriptions are important. Besides, we also find that our model has to build a memory-consuming knowledge store for large-scale knowledge graphs (e.g., 1671M for FB15k-237). More recent work <ref type="bibr" target="#b9">[10]</ref> explores improving efficiency along various dimensions, and we leave this for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Knowledge Graph Embedding Models. Mainstream methods of KGC are based on knowledge graph embeddings (KGE), which generally leverage an embedding vector in the continuous embedding space to represent each entity and relation and train the embeddings based on the observed fact. One line of KGE methods is translationbased, such as TransE <ref type="bibr" target="#b1">[2]</ref>, TotatE <ref type="bibr" target="#b27">[28]</ref>, ConE <ref type="bibr" target="#b40">[40]</ref>, which focus on learning distance-based scoring functions considering relations as the translational operation between entities, The objective of translation-based methods is that the translated head entity should be close to the tail entity in real space <ref type="bibr" target="#b1">[2]</ref>, complex space <ref type="bibr" target="#b27">[28]</ref>, or quaternion space <ref type="bibr" target="#b39">[39]</ref>, and have shown state-of-the-art performance on handling multiple relation patterns. Another line of work is semantic matching models, where they calculate the semantic similarity by multi-linear or bilinear product to learn the scoring functions. However, these KGE models require a large amount of labeled data to optimize the representation of the entity and relation, thus not applicable when facing rare or emerging entities.</p><p>Transductive KGC VS. Inductive KGC. Most of the models focus on transductive KGC that essentially constructs a lookup table of entity and relation embeddings. Thus these models are not applicable to the scenario where new entities with descriptions are added to the knowledge graph. Overall, the key limitation of transductive approaches based on graph embeddings is that the completion function learned for a given KG is not applicable to any other new emerging KG <ref type="bibr" target="#b19">[20]</ref>. To address this problem, recent works Daza et al. <ref type="bibr" target="#b6">[7]</ref>, Shi and Weninger <ref type="bibr" target="#b26">[27]</ref> begin to pay attention to inductive KGC by exploring leveraging the text descriptions of entities and relations to encode the information. Unlike those that utilize the transformer as an encoder, we fully exploit the mask language modeling ability of PLMs for inductive KGC.</p><p>Pre-trained Language Models for KGC. Since pre-trained language models, such as BERT <ref type="bibr" target="#b8">[9]</ref>, have shown significant improvement on several natural language processing tasks. Several works have emerged to leverage PLMs to handle the KGC tasks. KG-BERT <ref type="bibr" target="#b33">[34]</ref> takes the first step to utilize BERT for KGC by treating a triple as a sequence and turns KGC into a sequence classification problem with binary cross-entropy optimization. Despite the success of KG-BERT, they perform far behind the state-of-the-art performances in evaluating higher ranks, such as Hits@1 and Hits@3. Kim et al. <ref type="bibr" target="#b16">[17]</ref> proposes to adopt the multi-task learning framework with two additional tasks, i.e., relation prediction and relevance ranking, to improve high-rank performance based on KG-BERT. Wang et al. <ref type="bibr" target="#b31">[32]</ref> adopts language models to derive knowledge embeddings. However, it is still non-trivial for those models to handle inductive settings. Our approach is built upon pre-trained models with an explicit knowledge store, which is pluggable, flexible, and adaptive for both transductive and inductive reasoning.</p><p>Retrieval-augmented PLMs for downstream tasks. Recently,nearest neighbor classifier, which is a non-parametric algorithm that makes the prediction based on representation similarities, has been explored in pre-trained language models <ref type="bibr" target="#b8">[9]</ref>. He et al. <ref type="bibr" target="#b10">[11]</ref>, Khandelwal et al. <ref type="bibr" target="#b14">[15]</ref> regard nearest neighbors as the augmentation of the language model predictions by using neighbors of the predictions as targets for language model learning. Kassner and Sch?tze <ref type="bibr" target="#b13">[14]</ref> applies NN as additional predictions to enhance question answering. KNN-BERT <ref type="bibr" target="#b17">[18]</ref> extends the NN classifier on text classification tasks. GNN-LM <ref type="bibr" target="#b21">[22]</ref> and Borgeaud et al. <ref type="bibr" target="#b2">[3]</ref> extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. These methods use nearest neighbors to find augmented samples based on the pre-trained language models and have demonstrated that making predictions based on the NN can improve model robustness. To the best of our knowledge, we are the first NN-based approach for knowledge graph embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We have introduced NN-KGE, which can outperform previous knowledge graph embedding models in both transductive and inductive settings by directly querying entities at test time. Empirical results on two benchmark datasets demonstrate the effectiveness of our approach, especially in low-resource settings. The success of NN-KGE suggests that explicitly memorizing entities can be helpful for knowledge graph reasoning. Note that our approach is simple and effective that can be applied to any evolving KGs without further training. Future work should explore how to edit and delete entities from the knowledge store and apply our approach to other tasks, such as question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Settings</head><p>We detail the training procedures and hyperparameters for each of the datasets. We utilize Pytorch to conduct experiments with one NVIDIA RTX 3090 GPU. All optimization is performed with the AdamW optimizer with a linear warmup of the learning rate over the first 10% of steps, then linear decay over the remainder of the training. Gradients are clipped if their norm exceeds 1.0, and weight decay on all non-bias parameters is set to 1e-2. The grid search is used for hyperparameter tuning (maximum values bolded below), using five random restarts for each hyperparameter setting for all datasets. Early stopping is performed on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Transductive Setting</head><p>FB15k-237. The FB15k-237 dataset is available in https://github. com/yao8839836/kg-bert/tree/master/data. The hyper-parameter search space is:</p><p>? epoch: <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Inductive Setting</head><p>FB15k-237 inductive. The FB15k-237 inductive dataset process code is available in https://github.com/dfdazac/blp/tree/master/data. The hyper-parameter search space is:</p><p>? epoch: <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref> ?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Dataset Details</head><p>The dataset details in transductive and inductive settings can be seen in <ref type="table" target="#tab_2">Table 2</ref>. Moreover, we list some examples of the description of the entity and the relation in the FB15k-237 dataset in <ref type="table" target="#tab_11">Table 8</ref> and <ref type="table" target="#tab_10">Table 7</ref>. Entity ID Entity Description 2206 Software Engineer GB , Software engineers apply the principles of engineering to the design, development, maintenance, testing, and evaluation of the software and systems that make computers or anything containing software work.l formal definitions of software engineering are: "the application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software". </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The virtual entity token embedding ? E in the word embedding layer (head) is firstly optimized by Entity Vocabulary Expansion (Figure a). Then the model ? shares the weight ? E in Masked Entity Modeling (Figure b) for training. Finally, with the model trained above, the entities in triples and descriptions colored in blue will be encoded into contextualized entity representation and added to our knowledge store (Figure c). Examples are taken from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Varying the size of the training samples. In the low resource setting, knowledge store monotonically improves performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Long-tailed distribution in the FB15k-237 dataset. There are about 82% entities occur less than 50 times. NN-KGE and RotatE MRR result in FB15k-237 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Significant improvement in entities with frequency below 50 colored in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Effect of the number of neighbors from knowledge store.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Effect of interpolation parameter on FB15k-237 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>PLMs. PLMs are trained with a large amount of corpus, and thus, the stored knowledge in PLMs can be quite different. For Effect of different PLMs on FB15k-237 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>2D t-SNE visualisation of 20 nearest neighbor entities with all their embeddings in knowledge store retrieved by predicting the missing entity in triple (?, nationality, America). Anchor embedding is marked as ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2225</head><label></label><figDesc>Alberta , Alberta is a province of Canada. With a population of 3,645,257 in 2011 and an estimated population of 4,025,074 in 2013, it is Canada's fourth-most populous province and most populous of Canada's three prairie provinces. Alberta and its neighbour, Saskatchewan, were established as provinces on September 1, 1905.2248AlexandreDumas , Alexandre Dumas, also known as Alexandre Dumas, p?re, was a French writer. His works have been translated into nearly 100 languages, and he is one of the most widely read French authors. Many of his historical novels of high adventure, including The Count of Monte Cristo, The Three Musketeers, Twenty Years After, and The Vicomte de Bragelonne: Ten Years Later were originally published as serials. His novels have been adapted since the early twentieth century for nearly 200 films. Dumas' last novel, The Knight of Sainte-Hermine, unfinished at his death, was completed by a scholar and published in 2005, becoming a bestseller. It was published in English in 2008 as The Last Cavalier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>MEM Head (b) Masked Entity ModelingMEM Head (c) Knowledge Store Construction From Descriptions and Triples</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sharing Weight</cell></row><row><cell></cell><cell></cell><cell>Plato</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Steve Jobs</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Obama ...</cell><cell></cell></row><row><cell>[CLS]</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[SEP] Greece</cell><cell>[SEP]</cell></row><row><cell>description</cell><cell></cell><cell cols="2">relation &amp; entity</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Knowledge Store</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell></cell></row><row><cell>Plato was an Athenian</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>philosopher.</cell><cell></cell><cell></cell><cell></cell><cell>Plato</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Inference efficiency comparison. | | is the length of the entity description. |E |, |R| and |T | are the numbers of all unique entities, relations and triples in the graph respectively. Usually, |E | exceeds hundreds of thousands and is much greater than |R|.</figDesc><table><row><cell>Inference</cell><cell>Method</cell><cell>Complexity</cell><cell>Speed up</cell><cell>GPU time under RTX 3090</cell></row><row><cell>One Triple</cell><cell>StAR NN-KGE</cell><cell>| | 2 (1 + |E |) | | 2 + |E | + |T |</cell><cell>? |E |?</cell><cell>--</cell></row><row><cell>Entire Graph</cell><cell>StAR NN-KGE</cell><cell>| | 2 |E |(1 + |R|) (| | 2 + |T | + |E |)|T |</cell><cell>? 2? because |E ||R| ? |T |</cell><cell>28 min 15 min</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets used in the link prediction task under transductive and inductive settings.</figDesc><table><row><cell></cell><cell>WN18RR</cell><cell></cell><cell cols="2">FB15k-237</cell></row><row><cell></cell><cell cols="4">Transductive Inductive Transductive Inductive</cell></row><row><cell>Relations</cell><cell>11</cell><cell></cell><cell>237</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell></row><row><cell>Entities</cell><cell>40,943</cell><cell>32,755</cell><cell>14,541</cell><cell>11,633</cell></row><row><cell>Triples</cell><cell>86,835</cell><cell>69,585</cell><cell>272,115</cell><cell>215,082</cell></row><row><cell></cell><cell></cell><cell cols="2">Validation</cell><cell></cell></row><row><cell>Entities</cell><cell>-</cell><cell>4,094</cell><cell>-</cell><cell>1,454</cell></row><row><cell>Triples</cell><cell>3,034</cell><cell>11,381</cell><cell>17,535</cell><cell>42,164</cell></row><row><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>Entities</cell><cell>-</cell><cell>4,094</cell><cell>-</cell><cell>1,454</cell></row><row><cell>Triples</cell><cell>3,134</cell><cell>12,087</cell><cell>20,466</cell><cell>52,870</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>First five entities with their probability predicted by NN-KGE w/o Knowledge Store, and its reranking with NN-KGE, for two example queries.</figDesc><table><row><cell cols="2">Query:(?,ethnicity,Timothy Spall)</cell><cell></cell></row><row><cell>Rank</cell><cell>NN-KGE w/o Knowledge Store</cell><cell>NN-KGE</cell></row><row><cell>1</cell><cell>Jewish people (0.285)</cell><cell>English people (0.548)</cell></row><row><cell>2</cell><cell>English people (0.202)</cell><cell>Jewish people (0.028)</cell></row><row><cell>3</cell><cell>British people (0.132)</cell><cell>British people (0.026)</cell></row><row><cell>4</cell><cell>White British (0.080)</cell><cell>Irish people in ... (0.021)</cell></row><row><cell>5</cell><cell>Scottish people (0.037)</cell><cell>Irish people (0.018)</cell></row><row><cell cols="2">Query: (Tom Rosenberg,profession,?)</cell><cell></cell></row><row><cell>Rank</cell><cell>NN-KGE w/o Knowledge Store</cell><cell>NN-KGE</cell></row><row><cell>1</cell><cell>Television Director (0.240)</cell><cell>Film Producer-GB (0.319)</cell></row><row><cell>2</cell><cell>Television producer-GB (0.163)</cell><cell>Film Director (0.249)</cell></row><row><cell>3</cell><cell>Film Director (0.135)</cell><cell>Screenwriter (0.230)</cell></row><row><cell>4</cell><cell>Film Producer-GB (0.120)</cell><cell>Television Director (0.200)</cell></row><row><cell>5</cell><cell>Screenwriter (0.076)</cell><cell>Writer-GB (0.005)</cell></row><row><cell cols="3">that NN-KGE can explicitly memorize those entities in Knowledge</cell></row><row><cell cols="3">Store; thus, it can directly reason through memorization.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Statistics of knowledge store built on two datasets, radio of retrieved entity embeddings from two resources, and inference time per query triple.</figDesc><table><row><cell>KG</cell><cell cols="3">memory from D from T retrieved D/T (%)</cell><cell>time</cell></row><row><cell>WN18RR</cell><cell>509M</cell><cell>40,943 139,170</cell><cell>13/87</cell><cell>7.75ms</cell></row><row><cell cols="2">FB15k-237 1671M</cell><cell>14,541 544,230</cell><cell>4/96</cell><cell>13.6ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>] ? batch size: [64, 128, 256] ? accumulate: 1 ? bert learning rate: [1e-5, 2e-5, 5e-5, 1e-4] ? warmup rate: 0.1 ? in Knowledge Store: [8, 16, 32, 64, 128, 256, 512] ? : 0.2 WN18RR. The WN18RR dataset is available in https://github. com/yao8839836/kg-bert/tree/master/data. The hyper-parameter search space is: ? epoch: [8, 12, 20] ? batch size: [64, 128, 256] ? accumulate: 1 ? learning rate: [1e-5, 2e-5, 5e-5, 1e-4] ? warmup rate: 0.1 ? in Knowledge Store: [8, 16, 32, 64, 128, 256, 512] ? : 0.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>batch size: [64, 128, 256] ? accumulate: 1 ? bert learning rate: [1e-5, 2e-5, 5e-5, 1e-4] ? warmup rate: 0.1 ? in Knowledge Store: [8, 16, 32, 64, 128, 256, 512] ? : 0.5 WN18RR inductive. The WN18RR inductive dataset process code is available in https://github.com/dfdazac/blp/tree/master/data. The hyper-parameter search space is:</figDesc><table><row><cell>? epoch: [8, 12, 20]</cell></row><row><cell>? batch size: [64, 128, 256]</cell></row><row><cell>? accumulate: 1</cell></row><row><cell>? learning rate: [1e-5, 2e-5, 5e-5, 1e-4]</cell></row><row><cell>? warmup rate: 0.1</cell></row><row><cell>? in Knowledge Store: [8, 16, 32, 64, 128, 256, 512]</cell></row><row><cell>? : 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Examples of the description corresponding to the relation in FB15k-237 dataset.</figDesc><table><row><cell cols="2">Relation ID Relation Description</cell></row><row><cell>6</cell><cell>government this politician party. in</cell></row><row><cell></cell><cell>party political politicians tenure</cell></row><row><cell>25</cell><cell>this employment job people title. title</cell></row><row><cell></cell><cell>with business company tenure</cell></row><row><cell>37</cell><cell>location place hud county</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Examples of the description corresponding to the entity in FB15k-237 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">or head entity prediction denoted by (?, , )</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saffron</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04426</idno>
		<ptr target="https://arxiv.org/abs/2112.04426" />
		<editor>Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2021</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-Dimensional Hyperbolic Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2022/273</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2022/273" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</title>
		<editor>Luc De Raedt</editor>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonggang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3531757</idno>
		<ptr target="https://doi.org/10.1145/3477495.3531757" />
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>Enrique Amig?, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai</editor>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-07-11" />
			<biblScope unit="page" from="927" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive Entity Representations from Text via Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics, Online and Punta Cana</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.461</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.emnlp-main.461" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5703" to="5714" />
		</imprint>
	</monogr>
	<note>Efficient Nearest Neighbor Language Models</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient Nearest Neighbor Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00300" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization through Memorization: Nearest Neighbor Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.153</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.coling-main.153" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1737" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02523</idno>
		<title level="m">Xipeng Qiu, and Xuanjing Huang. 2021. KNN-BERT: Fine-Tuning Pre-Trained Models with KNN Classifier</title>
		<imprint/>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">INDIGO: GNN-based inductive knowledge graph completion using pair-wise encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Kostylev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">GNN-LM: Language Modeling based on Global Contexts via GNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08743</idno>
		<ptr target="https://arxiv.org/abs/2110.08743" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Knowledge Graph Completion and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.201</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.acl-long.201" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2814" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-4_38" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<editor>Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha?l Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam</editor>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Open-World Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language Models as Knowledge Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2022/318</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2022/318" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</title>
		<editor>Luc De Raedt (Ed.). ijcai.org</editor>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07" />
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="2291" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Document-level Relation Extraction as Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation Adversarial Network for Low resource Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianghuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaixiao</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nengwei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">AliCG: Fine-grained and Evolvable Conceptual Graph Construction for Semantic Search at Alibaba</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quaternion Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">1 Entity As Special Tokens For each entity that appears in the knowledge graph, we set the unique id to the entity. We use the format of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Appendices A</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>to distinguish different entities with their ids. For example, we treat Plato as a special token. and we feed the input &quot;The description of [ENTITY_0] is that Plato was an Athenian philosopher. &quot; to the pre-trained language model to obtain the embedding of Plato, and add it to the knowledge store</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
