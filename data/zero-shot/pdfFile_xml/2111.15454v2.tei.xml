<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BOOSTING DISCRIMINATIVE VISUAL REPRESENTA- TION LEARNING WITH SCENARIO-AGNOSTIC MIXUP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BOOSTING DISCRIMINATIVE VISUAL REPRESENTA- TION LEARNING WITH SCENARIO-AGNOSTIC MIXUP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint. Work in progress.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup is a popular data-dependent augmentation technique for deep neural networks, which contains two sub-tasks, mixup generation, and classification. The community typically confines mixup to supervised learning (SL) and the objective of the generation sub-task is fixed to selected sample pair instead of considering the whole data manifold. To overcome such limitations, we systematically study the mixup generation objective and propose Scenario-Agnostic Mixup for both SL and Self-supervised Learning (SSL) scenarios, named SAMix. Specifically, we hypothesize and verify the objective function of mixup generation as optimizing local smoothness between two mixed classes subject to global discrimination from other classes. Therefore, we propose ?-balanced mixup loss for complementary learning of the two sub-objectives. Meanwhile, we parameterize the generation sub-task as a learnable sub-network, Mixer, with mixing attention which avoids trivial solutions and improves transferable abilities. To eliminate the computational cost of online training, we introduce a pre-trained version, SAMix P , that achieves efficient performance in various tasks. Extensive experiments on SL and SSL benchmarks demonstrate that SAMix consistently outperforms leading methods.</p><p>Recent proposed optimizable approaches use labels to localize task-relevant targets (e.g., grad-CAM (Selvaraju et al., 2019)) and generate semantic mixed samples such as offline maximizing saliency information of related samples <ref type="bibr" target="#b50">Uddin et al., 2020)</ref>. These handcrafted mixing policies as shown in the red box of <ref type="figure">Figure 1</ref>. Additionally, (Zhu et al., 2020) learns a mixup generator by supervised adversarial training. * Equal contribution arXiv:2111.15454v2 [cs.CV] 29 Sep 2022 Preprint. Work in progress.</p><p>Although optimizable mixup methods have achieved significant gains in supervised learning (SL), it is not applicable in self-supervised learning (SSL). Recent AutoMix (Liu et al., 2022b) provides a new perspective to make mixup policy parameterized and can be trained online. However, the trivial solution is generated without the guidance of labels. Then there is the natural question of whether we can design a more general and trained mixup policy that can be used for both SL and SSL. Motivated by this intuition, there are two remaining open problems to be solved: (i) how to solve the problem of trivial solution; (ii) what is a proper mixup generation objective for both SL and SSL. Most current works combine mixup with contrastive learning, which directly transfers linear mixup methods into contrastive learning (Lee et al., 2021;<ref type="bibr" target="#b76">Kalantidis et al., 2020;</ref><ref type="bibr" target="#b76">Shen et al., 2021)</ref>. Although simple, these approaches do not exploit the underlying structure of the data manifold. In this paper, we propose SAMix <ref type="figure">(Figure 1)</ref>, which stands for Scenario-Agnostic Mixup, a framework that employs Mixer to generate mixed samples adaptively either at instance-level or cluster-level. To guarantee that task-relevant information can be captured by Mixer, we propose ?-balanced mixup loss for treating mixup generation and classification differently from a local and global perspective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Mixed samples with green boxes rely on labels, while blue boxes do not. The solid line indicates that the local relationship directly influences the mixed sample, and the dashed line indicates that the other class samples serve as global constraints to the current mixed sample. Compared to handcrafted mixup policies that focus only on local sample pairs, SAMix can generate semantic mixed samples by exploiting the local and global information and has no dependency on labels.</p><p>One of the fundamental problems in machine learning is to learn a proper low-dimensional representation efficiently that captures the intrinsic structures of data and facilitates downstream tasks <ref type="bibr" target="#b1">(Bengio et al., 2013;</ref><ref type="bibr" target="#b12">Devlin et al., 2018;</ref><ref type="bibr" target="#b27">Korbar et al., 2018;</ref>. Data mixing, as a means of generating symmetric mixed data and labels, largely improves the efficiency of deep neural networks (DNNs) learning discriminative representation in various scenarios <ref type="bibr">Lee et al., 2021)</ref>, especially for Vision Transformers (ViTs) <ref type="bibr">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b55">Touvron et al., 2021)</ref>. Despite its general application, the policy of the generation process in data mixing requires an explicit design. For example, mixed samples are constructed by linear interpolation or random local patch replacement of sample pairs .</p><p>Furthermore, in order to solve the additional computational overhead caused by the optimization process for mixup policy, we propose a pre-trained setting, SAMix P , which employs a pretrained Mixer to generate high-quality mixed samples balancing performance and speed. It is worth noticing that SAMix P can achieve competitive or slightly higher performances like SAMix in classification tasks. Extensive experiments on both SL and SSL demonstrate the effectiveness and transferring abilities of SAMix. Our contributions are summarized as follows:</p><p>? We propose mixing attention and non-linear content modeling to design a label-free mixup generator, Mixer, with generalizable and transferable abilities in both SL and SSL. ? We decompose learning objectives (local and global terms) and analyze properties (local smoothness and global discrimination) of mixup generation, then propose ?-balanced loss to boost mixup generation. ? Combining the above, a scenario-agnostic mixup training framework that supports online and pre-trained pipelines are designed for both SL and SSL tasks. ? Comprehensive experiments are conducted on nine benchmarks, and our SAMix and SAMix P achieve state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Given a finite set of i.i.d samples, X = [x i ] n i=1 ? R D?n , each data x i ? R D is drawn from a mixture of, say C, distributions D = {D c } C c=1 . Our basic assumption for discriminative representations is that the each component distribution D c has relatively low-dimensional intrinsic structures, i.e., the distribution D c is constrained on a sub-manifold, say M c with dimension d c D. The distribution D of X is consisted of sub-manifolds, M = ? C c=1 M c . We seek a low-dimensional representation z i ? M of x i by learning a continuous mapping by a network encoder, f ? (x) : x ?? z with the parameter ? ? ?, which captures intrinsic structures of M and facilitates the discriminative tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DISCRIMINATIVE REPRESENTATION LEARNING</head><p>Parametric training with class supervision. Some supervised class information is available to ease the discriminative tasks in practical scenarios. Here, we assume that a one-hot label y i ? R C of each sample x i can be somehow obtained, Y = [y 1 , y 2 , ..., y n ] ? R C?n . We denote the labels generated during training as pseudo labels (PL), while the fixed as ground truth labels (L). Notice that each component D c is considered separated according to Y in this scenario. Then, a parametric classifier, g ? (z) : z ?? p with the parameter ? ? ?, can be learned to map the representation z i of each sample to its class label y i by predicting the probability of p i being assigned to the c-th class using the softmax criterion,</p><formula xml:id="formula_0">p c|i = exp(w T c zi) C j=1 exp(w T j zi) ,</formula><p>where w c is a weight vector for the class c, and w T c z i measures how similar between z i and the class c. The learning objective is to minimize the cross-entropy loss (CE) between y i and p i ,</p><formula xml:id="formula_1">CE (y i , p i ) = ?y i log p i .<label>(1)</label></formula><p>Non-parametric training as instance discrimination. Complementary to the above parametric settings, non-parametric approaches are usually adopted in unsupervised scenarios. Due to the lack of class information, an instance discriminative task can be designed based on an assumption of local compactness. We mainly discuss contrastive learning (CL) and take MoCo  as an example. Consider a pair of augmented image (x ?q i , x ? k i ) from the same instance x i ? R C?H?W , the local compactness is introduced by alignment of the encoded representation pair (z ?q i , z ? k i ) from f ?,q and the momentum f ?,k , and constrained to the global uniformity by contrasting z ?q i to a dictionary of encoded keys from other images, {z ? k j } K j=1 , where K denotes the length of the dictionary. It can be achieved by the popular non-parametric CL loss with temperature t, called infoNCE (van den <ref type="bibr" target="#b51">Oord et al., 2019)</ref>:</p><formula xml:id="formula_2">N CE (z ?q i , z ? k i ) = ? log exp(z ?q i z ? k i /t) K j=1 exp(z ?q i z ? k j /t) .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MIXUP FOR DISCRIMINATIVE REPRESENTATION</head><p>We further consider mixup as a generation task into discriminative representation learning to form a closed-loop framework. Then we have two mutually benefited sub-tasks: (a) mixed data generation and (b) classification. As for the sub-task (a), we define two functions, h(?) and v(?), to generate mixed samples and labels with a mixing ratio ? ? Beta(?, ?). Given the mixed data, (b) defines a mixup training objective to optimize the representation space between instances or classes.</p><p>Mixup classification as the main task. Since we aim to seek a good representation to facilitate discriminative tasks, thus the mixed samples should be diverse and well-characterized. The mixed samples with semantic information can be easily obtained by parametric learning, while it becomes a challenge without supervision. Therefore, two types of the mixup classification objective L ?,? can be defined for class-level and instance-level mixup training. As for parametric training, given two randomly selected data pairs, (x i , y i ) and (x j , y j ), the mixed data is generated as x m = h(x i , x j , ?) and y m = v(y i , y j , ?). The objective of class-level mixup is similar to Eq. 1 as,</p><formula xml:id="formula_3">CE (y m , p m ) = ? CE (y m , p m ) + (1 ? ?) CE (y m , p m ).<label>(3)</label></formula><p>Notice that we fix v(?) as the linear interpolation in our discussions, i.e., v(y i , y j , ?) ?y i +(1??)y j . Symmetrically, we denote h(?) as a pixel-wise mixing policy with element-wise product for most input mixup methods , i.e., x m = s i x i + s j x j , where s i ? R H?W is a pixel-wise mask and s j = 1 ? s i . Notice that each coordinate s w,h ? [0, 1]. We can generate x m with a pair of randomly selected samples (x i , x j ) and formulate mixup infoNCE loss for instance-level mixup:</p><formula xml:id="formula_4">N CE (z m ) = ? N CE (z m , z i ) + (1 ? ?) N CE (z m , z j ),<label>(4)</label></formula><p>where z m , z i and z j denote the corresponding representations. The major difference with Eq. 3 is that the augmentation view that generates z m is not from the same view, i.e., z i and z j , as the objective function, which is effective in retaining task-relevant information, details in A.5.1.</p><p>Mixup generation as the auxiliary task. Unlike the learning object on the unmixed data X in Sec. 2.1, the performance of (b) mixup classification mainly depends on the quality of (a) mixup generation. We thus regard (a) as an auxiliary task to (b) and model h(?) as a sub-network M ? with the parameter ? ? ?, called Mixer. Specifically, (a) aims to generate a pixel-wise mask s ? R H?W for mixing sample pairs. The mixup mask s i should directly related to ? and the contents of (x i , x j ). Practically, our M ? takes l-th layer feature maps z l ? R C l ?H l ?W l and ? value as the input, M ? :</p><formula xml:id="formula_5">x i , x j , z l i , z l j , ? ?? x m .</formula><p>The generation process of M ? can be trained by a mixup generation loss as L gen ? , and a mask loss designed for generated mask s i denoted as L mask ? . Formally, we have the mixup generation loss as L ? = L cls ? + L mask ? , and the final learning objective is,</p><formula xml:id="formula_6">min ?,?,? L ?,? + L ? .<label>(5)</label></formula><p>Both L ?,? and L ? can be optimized alternatively in a unified framework using a momentum pipeline with the stop-gradient operation <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr" target="#b34">Liu et al., 2022b)</ref>, as shown in <ref type="figure" target="#fig_4">Figure 5</ref> (left). However, in SSL, this framework can easily make the generator fall into a trivial solution. To solve this problem, we propose a novel mixup loss function and a generator architecture, Mixer. Combining the two can fully exploit the ability of mixup in learning discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SAMIX FOR DISCRIMINATIVE REPRESENTATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LEARNING OBJECTIVE FOR MIXUP GENERATION</head><p>Typically the objective function L ? corresponding to the mixup generation is consistent with the classification in parametric training (e.g., CE ). In this paper, we argue that mixup generation is aimed to optimize the local term subject to the global term. The local term focuses on the classes of sample pairs to be mixed, while the global term introduces the constraints of other classes. For example, CE is the global term whose each class produces an equivalent effect on the final prediction without focusing on the relevant classes of the current sample pair. At the class level, to emphasize the local term, we introduce a parametric binary cross-entropy (pBCE) loss for the generation task. Formally, assuming y i and y j belong to the class a and class b, pBCE can be summarized as:</p><formula xml:id="formula_7">CE + (p m ) = ??y i,a log p m ? (1 ? ?)y j,b log p m ,<label>(6)</label></formula><p>where y i,a = 1 and y i,b = 1 denote the one-hot label for the class a and b. Notice that we use + and ? to represent the local and global terms, and ? for the parametric loss refers to CE . Symmetrically, we have non-parametric binary cross-entropy mixup loss (BCE) for CL:</p><formula xml:id="formula_8">N CE + (z m ) = ?? log p m,i ? (1 ? ?) log p m,j ,<label>(7)</label></formula><p>where p m,i = exp(zmzi/t) exp(zmzi/t)+exp(zmzj /t) and its ? refers to N CE . Balancing local and global terms. Since both the local and global terms contribute to mixup generation, we analyze the importance of each term in both the SL and SSL tasks to design a balanced learning objective. We first analyze the properties of both terms with two hypothesizes: (i) the local term + determines the generation performance, (ii) the global term ? improves global discrimination but is sensitive to class information. To verify these properties, we design an empirical experiment based on the proposed Mixer on Tiny (see A.5). The main difference between the mixup CE and infoNCE is whether to adopt parametric class centroids. Therefore, we compare the intensity of class information among unlabeled (UL), pseudo labels (PL), and ground truth labels (L). Notice that PL is generated by ODC <ref type="bibr" target="#b9">(Zhan et al., 2020)</ref> with the cluster number C. The class supervision can be imported to mixup infoNCE loss by filtering out negative samples with PL or L as <ref type="bibr">(Khosla et al., 2020)</ref> denoted as infoNCE (L) and infoNCE (PL). As shown in <ref type="figure">Figure 2</ref> (left), our hypothesizes are verified in the SL task (as the performance decreases from CE(L) to pBCE(L) and CE(PL) losses), but the opposite result appears in the CL task. The performance increases from InfoNCE(UL) to InfoNCE(L) as the false negative samples are removed <ref type="bibr" target="#b42">(Robinson et al., 2021;</ref><ref type="bibr">Khosla et al., 2020)</ref> while trivial solutions occur using BCE(UL) (in <ref type="figure" target="#fig_3">Figure 6</ref>). Therefore, we propose it is better to explicitly import class information as PL for instance-level mixup to generate "strong" inter-class mixed samples while preserving intra-class compactness.</p><p>SAMix with ?-balanced generation objectives. Practically, we provide two versions of the learning objective: the mixup CE loss with PL as the class-level version (SAMix-C), and the mixup infoNCE loss as the instance-level version (SAMix-I). Then, we hypothesize that the best performing mixed samples will be close to the sweet spot: achieving ? smoothness locally between two classes or neighborhood systems while globally discriminating from other classes or instances. We propose an ?-balanced mixup loss as the objective of mixup generation, We analyze the performance of using various ? in Eq. 8 on Tiny, as shown in <ref type="figure">Figure 2</ref> (right), and find that using ? = 0.5 performs best on both the SL and CL tasks. In the end, we provide the learning objective, L cls ? CE + + ? CE ? , with L for class-level mixup and with PL for SAMix-C,</p><formula xml:id="formula_9">? = + + ? ? , ? ? [0, 1].<label>(8)</label></formula><formula xml:id="formula_10">L cls ? N CE + + ? N CE ? for SAMix-I (details in A.2).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DE NOVO MIXER FOR MIXUP GENERATION</head><p>Although AutoMix proposes the MixBlock that learns adaptive mixup generation policies online, there are three drawbacks in practice: (a) fail to encode the mixing ratio ? on small datasets; (b) fall into trivial solutions when performing CL tasks; (c) the online training pipeline leads to double or more computational costs than MixUp. Our Mixer M ? solves these problems individually.</p><p>Adaptive ? encoding and mixing attention. Since a randomly sampled ? should directly guide mixup generation, the predicted mask s should be semantic proportional to ?. The previous typical design regards ? as the prior knowledge and concatenates ? to input feature maps, which might be unable to encode ? properly (the analysis in A.6.1). We propose an adaptive ? encoding as,</p><formula xml:id="formula_11">z l i,? = (1 + ??)z l i ,<label>(9)</label></formula><p>where ? is a learnable scalar that constrained to [0, 1]. Notice that ? is initialized to 0 during training. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, we computes the mixing relationship between two samples using a new mixing attention: we concatenate (z l i,? , z l j,1?? ) as the input,z l = concat(z l i,? , z l j,1?? ), and compute the attention matrix as, where ?(?) is the softmax function, N (z l ) denotes a normalization factor, and ? is matrix multiplication. Notice that the mixing attention provides both the cross-attention between z l i,? and z l j,? and the self-attention of each feature itself.</p><formula xml:id="formula_12">P i,j = ?( (W Pz l ) T ? W Pz l N (z l ) ),<label>(10)</label></formula><p>Non-linear content modeling. In classical selfattention mechanism, the content sub-module C is a linear projection, C i = W zz l , where W z denotes a 1 ? 1 convolution. However, we find the training process of this structure is unstable when performing CL tasks with the linear C in the early period and sometimes trapped in trivial solutions, such as all coordinates on s i predicted as a constant. As shown in <ref type="figure">Figure 3</ref>, we visualize C i and P i,j of trivial and non-trivial results, and find that the trivial s i is usually caused by a constant C i . We hypothesize that trivial solutions happen earlier in the linear C than in P i,j , because it might be unstable to project high-dimensional features to 1-dim linearly. Hence, we design a non-linear content modeling sub-module C N C that contains two 1 ? 1 convolution layers with a batch normalization layer and a ReLU layer in between, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. To increase the robustness and randomness  of mixup training, we add a Dropout layer with a dropout ratio of 0.1 in C N C . Formally, Mixer M ? can be written as,   <ref type="bibr" target="#b11">(Dabouei et al., 2021)</ref>. From this perspective, we propose the pretrained pipeline of SAMix, denoted as SAMix P , in <ref type="figure" target="#fig_4">Figure 5</ref>. We can conclude: (i) SAMix P pre-trained on large-scale datasets achieves similar or better performance as its online training version on current or relevant datasets with less computational cost. (ii) SAMix P with light or median CNN encoders (e.g., ResNet-18) yields better performance than heavy ones (e.g., ResNet-101). (iii) SAMix P has better transferring abilities than AutoMix P . (iv) Online training pipeline is still irreplaceable on small datasets (e.g., CIFAR and CUB).</p><formula xml:id="formula_13">s i = U ? ? (W Pz l ) T ? W Pz l N (z l ) ? C N C (z l i,? ) .<label>(11)</label></formula><p>Prior knowledge of mixup. Moreover, we summarize some commonly adopted prior knowledge <ref type="bibr" target="#b11">Dabouei et al., 2021)</ref> for mixup as two aspects: (a) adjusting the mean of s i correlated with ?, and (b) balancing the smoothness of local image patches while maintaining discrimination of x m . Based on them, we introduce ? adjusting and modifying the mask loss L mask ? (details in A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DISCUSSION AND VISUALIZATION OF SAMIX</head><p>To show the influence of local and global constraints on mixup generation, we visualize mixed samples generated from Mixer on various scenarios in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>Class-level. In the supervised task, global constraint localizes key features by discriminating to other classes, while the local term is prone to preserve more information related to the current two samples and classes. For example, comparing the mixed results with and without ?-balanced mixup loss, it was found that pixels of the foreground target were of interest to Mixer. When the global constraint is balanced (? = 0.5), the foreground target is retained more completely. Importantly, our designed Mixer remains invariant to the background for the more challenging fine-grained classification and preserves discriminative features.</p><p>Instance-level. Since no label supervision is available for SSL, the global and local terms are transformed from class to instance. Similar results are shown in the top row, and the only difference is that SAMix-C has a more precise target correspondence compared to SAMix-I via introducing class information by PL, which further indicates the importance of the information of classes. If we only focus on local relationships, Mixer can only generate mixed samples with fixed patterns (the last two results in the top row). These failure cases imply the importance of global constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We  <ref type="bibr" target="#b35">(Loshchilov &amp; Hutter, 2016)</ref>. A special case in <ref type="table" target="#tab_5">Table 4</ref>: RSB A3 (using a LAMB optimizer <ref type="bibr">(You et al., 2020)</ref> for R-50) in timm <ref type="bibr" target="#b55">(Wightman et al., 2021)</ref> and DeiT ((using an AdamW optimizer <ref type="bibr" target="#b36">(Loshchilov &amp; Hutter, 2019)</ref>      Comparison and discussion <ref type="table" target="#tab_4">Table 3</ref> shows results on small-scale and fine-grained classification tasks. SAMix consistently improves classification performances over the previous best algorithm, AutoMix, with the improved Mixer. Notice that SAMix significantly improved the performance of CUB-200 and Aircraft by 1.24% and 0.78% based on ResNet-18, and continued to expand its dominance on Tiny by bringing 1.23% and 1.40% improvement on ResNet-18 and ResNeXt-50.</p><p>As for the large-scale classification task, we benchmark popular mixup methods in <ref type="table" target="#tab_2">Table 1</ref>, 4 and 2, SAMix and SAMix P outperform all existing methods on IN-1k, iNat2017/2018 and Places205. Especially, SAMix P yields similar or better performances than SAMix with less computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EVALUATION ON SELF-SUPERVISED LEARNING</head><p>Then, we evaluate SAMix on SSL tasks pre-training on STL-10, Tiny, and IN-1k. All comparing methods are based on MoCo.V2 except SwAV . We adopt all hyper-parameter configurations from MoCo.V2 unless otherwise stated. We compared SAMix in two dimensions in CL: (i) compare with other mixup variants, based on our proposed cross-view pipeline, and the predefined cluster information is given (denotes by C) or not, as shown in  <ref type="bibr" target="#b9">(Chu et al., 2020)</ref>, as shown in <ref type="table" target="#tab_8">Table 6</ref>. Notice that denotes our modified methods (PuzzleMix * uses PL and Inter-Intra * combines inter-class CutMix with intra-class MixUp.</p><p>Linear Classification Following the linear classification protocol proposed in MoCo, we train a linear classifier on top of frozen backbone features with the supervised train set. We train 100 epochs using SGD with a batch size of 256. The initialized learning rate is set to 0.1 for Tiny and STL-10 while 30 for IN-1k, and decay by 0.1 at epochs 30 and 60.</p><p>Comparison and discussion As shown in <ref type="table" target="#tab_6">Table 5</ref>, SAMix-I outperforms all the linear mixup methods by a large margin, while SAMix-C surpasses the saliency-based PuzzleMix when PL is available. And SAMix-I has both global and local properties through infoNCE and BCE losses. Meanwhile, <ref type="table" target="#tab_8">Table 6</ref> demonstrates that both SAMix-I and SAMix-C surpass other CL methods combined with the predefined mixup. Overall, SAMix-C yields the best performance in CL takes which indicates it provides task-relevant information with the help of PL.  Downstream Tasks Following the protocol in MoCo, we evaluate transferable abilities of the learned representation of comparing methods to object detection task on PASCAL VOC <ref type="bibr" target="#b14">(Everingham et al., 2010)</ref> and COCO <ref type="bibr" target="#b32">(Lin et al., 2014)</ref> in Detectron2 <ref type="bibr" target="#b56">(Wu et al., 2019)</ref>. We fine-tune Faster R- <ref type="bibr">CNN (Ren et al., 2015)</ref> with pre-trained models on VOC trainval07+12 and evaluate on the VOC test2007 set. Similarly, Mask R-CNN  is fine-tuned (2? schedule) on the COCO train2017 and evaluated on the COCO val2017. SAMix still achieves comparable performance with state-of-the-art mixup methods. More results are detailed in A.5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDY</head><p>We conduct ablation studies in four aspects: (i) Mixer: <ref type="table" target="#tab_10">Table 8</ref> verifies the effectiveness of each proposed module in both SL and CL tasks on Tiny. The first three modules enable Mixer to model the non-linear mixup relationship, while the next two modules enhance Mixer, especially in CL tasks. (ii) Learning objectives: We analyze the effectiveness of proposed ? with other losses, as shown in <ref type="table" target="#tab_9">Table 7</ref>. Using ? for the mixup CE and infoNCE consistently improves the performance both for the CL task on STL-10 and Tiny.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Class-level Mixup There are four types sample mixing policies for class-level mixup: linear mixup of input space <ref type="bibr" target="#b21">Hendrycks et al., 2019;</ref><ref type="bibr" target="#b17">Harris et al., 2020;</ref><ref type="bibr" target="#b40">Qin et al., 2020)</ref> and latent space <ref type="bibr" target="#b15">Faramarzi et al., 2020</ref><ref type="bibr">), saliency-based (Uddin et al., 2020</ref>, generation-based <ref type="bibr" target="#b64">(Zhu et al., 2020;</ref><ref type="bibr">Venkataramanan et al., 2022)</ref>, and learning mixup generation and classification end-to-end <ref type="bibr" target="#b34">(Liu et al., 2022b;</ref><ref type="bibr" target="#b11">Dabouei et al., 2021)</ref>. SAMix belongs to the fourth type and learns both class-and instance-level mixup relationships, and its pre-trained SAMix P eliminates high time-consuming problems of this type of method. Additionally, some researchers <ref type="bibr" target="#b38">(Park et al., 2022;</ref><ref type="bibr" target="#b4">Chen et al., 2021;</ref><ref type="bibr" target="#b33">Liu et al., 2022a)</ref> improve class mixing policies upon linear mixup. See A.7 for details. Instance-level Mixup A complementary method to learn better instance-level representation is to apply mixup in SSL scenarios <ref type="bibr">(Lee et al., 2021)</ref>. However, most approaches are limited to using linear mixup variants, such as applying MixUp and CutMix in the input or latent space mixup <ref type="bibr" target="#b76">(Kalantidis et al., 2020;</ref><ref type="bibr">Lee et al., 2021;</ref><ref type="bibr" target="#b9">Chu et al., 2020)</ref> for SSL without ground-truth labels. SAMix improves SSL tasks by learning mixup policies online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND CONLUSIONS</head><p>We first study objectives for mixup generation as local-emphasized and global-constrained terms for learning adaptive mixup policy at both class-and instance-level. SAMix provides a unified framework with both online and pre-trained pipelines to boost discriminative representation learning based on improved Mixer. As a limitation, the Mixer only takes two samples as input and conflicts when the task-relevant information is overlapping. In the future, more than two samples or conflict-aware Mixer is another promising avenue for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>We first provide dataset settings in A.1 and implementation details for supervised (SL) and selfsupervised learning (SSL) tasks in A.2, A.3 and A.4. Then, we detailed experiment settings and analysis for Sec. 3 in A.5. Moreover, we visualize mixed samples in A.6, and provide detailed related work in A.7.</p><p>A.1 BASIC SETTINGS Reproduction details. We use OpenMixup <ref type="bibr">Li et al. (2022)</ref> implemented in PyTorch <ref type="bibr" target="#b39">(Paszke et al., 2019)</ref> as our code-base for both supervised image classification and contrastive learning (CL) tasks. Except results marked by ? and ?, we reproduce most experiment results of compared methods, including <ref type="bibr" target="#b75">Mixup (Zhang et al., 2018)</ref>, CutMix , ManifoldMix , SaliencyMix (Uddin et al., 2020), FMix <ref type="bibr" target="#b17">(Harris et al., 2020)</ref>, and ResizeMix <ref type="bibr" target="#b40">(Qin et al., 2020)</ref>. Dataset information. We briefly introduce image datasets used in Sec. 4: (1) CIFAR-100 <ref type="bibr" target="#b28">(Krizhevsky et al., 2009)</ref>   <ref type="bibr" target="#b32">(Lin et al., 2014)</ref> is an objection detection and segmentation benchmark containing 118k scenic images with many objects for 80 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 IMPLEMENTATION OF SAMIX</head><p>Online training pipeline. We provide the detailed implementation of SAMix in SL tasks. As shown in <ref type="figure" target="#fig_4">Figure 5</ref> (left), we adopt the momentum pipeline <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr" target="#b34">Liu et al., 2022b)</ref> to optimize L ?,? for mixup classification and L ? for mixup generation in Eq. 5 in an end-to-end manner:</p><formula xml:id="formula_14">? t q , ? t q ? argmin ?,? L ? t?1 q ,? t?1 q ,<label>(12)</label></formula><formula xml:id="formula_15">? t ? argmin ? L ? t k ,? t k + L ? t?1 ,<label>(13)</label></formula><p>where t is the iteration step, ? q , ? q and ? k , ? k denote the parameters of online and momentum networks, respectively. The parameters in the momentum networks are an exponential moving average of the online networks with a momentum decay coefficient m, taking ? k as an example,</p><formula xml:id="formula_16">? t k ? m? t?1 k + (1 ? m)? t q .<label>(14)</label></formula><p>The training process of SAMix is summarized as four steps: (1) using the momentum encoder to generate the feature maps Z l for Mixer M ? ; (2) generating X q mix and X k mix by Mixer for the online networks and Mixer; (3) training the online networks by Eq. 12 and the Mixer by Eq. 13 separately; (4) updating the momentum networks by Eq. 14. Prior knowledge of mixup. As we discussed in Sec. 3.2, we introduce some prior knowledge to the Mixer from two aspects: (a) To adjust the mean of s i correlated with ?, we introduce a mask loss that aligns the mean of s i to ?, ? = ? max(|? ? ? i | ? , 0), where ? i = 1 HW h,w s i,h,w is the mean and = 0.1 as a margin. Meanwhile, we propose a test time ? adjusting method. Assuming ? i &lt; ?, we adjust each coordinate on s i as? i = ?i ? s i , and? j = 1?? i . (b) To balance the smoothness of local image patches and the discrimination (e.g., variance) of x m , we adopt a bilinear upsampling as U (?) for smoother masks and propose a variance loss to encourage the sparsity of learned masks, ? = 1 W H w,h (? i ? s w,h ) 2 . We summarize the mask loss as, L mask ? = ?( ? + ? ), where ? is a balancing weight. ? is initialized to 0.1 and linearly decreases to 0 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same-view</head><p>Cross-view (a) (b) (c) <ref type="figure">Figure 8</ref>: (a) Graphical models and information diagrams of same-view and cross-view training pipeline for instance-level mixup. Taking the cross-view as an example, x m = h(x ?2 i , x ?2 j , ?), the ? region denotes the corresponding information partition for ?I(z . We hypothesize that the cross-view objective yields better CL performance than the same-view because the mutual information between two augmented views should be reduced while keeping task-relevant information <ref type="bibr" target="#b47">(Tian et al., 2020b;</ref><ref type="bibr">Tsai et al., 2021)</ref>. To verify this hypothesis, we design an experiment of various mixup methods with ? = 1 on STL-10 with ResNet-18. As shown in <ref type="figure">Figure 8 (b)</ref>, we compare using the same-view or cross-view pipelines combined with using N CE (z ?q i , z ? k i ) + N CE (z m ) or only using N CE (z m ). We can conclude: (i) Degenerated solutions occur when using the same-view pipeline while using the cross-view pipeline outperforms the CL baseline. It is mainly caused by degenerated mixed samples which contain parts of the same view of two source images. Therefore, we propose the cross-view pipeline for the instance-level mixup, where z i and z j in Eq. 4 are representations of x ? k i and x ? k j . (ii) Combining both the original and mixup infoNCE loss, N CE (z ?q i , z ? k i ) + N CE (z m ), surpasses only using one of them, which indicates that mixup enables f ? to learn relationship between local neighborhood systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.2 ANALYSIS OF INSTANCE-LEVEL MIXUP</head><p>As we discussed in Sec. A.5.1, we propose the cross-view training pipeline for instance-level mixup classification. We then discuss inter-and intra-class proprieties of instance-level mixup. As shown in <ref type="figure">Figure 8</ref> (c), we adopt inter-cluster and intra-cluster mixup from {Mixup, CutMix} with ? ? {0.2, 1, 2, 4} to verify that instance-level mixup should treat inter-and intra-class mixup differently. Empirically, mixed samples provided by Mixup preserve global information of both source samples (smoother) while samples generated by CutMix preserve local patches (more discriminative). And we introduce pseudo labels (PL) to indicate different clusters by clustering method ODC <ref type="bibr" target="#b9">(Zhan et al., 2020)</ref> with the class (cluster) number C. Based on experiment results, we can conclude that inter-class mixup requires discriminative mixed samples with strong intensity while the intra-class needs smooth samples with low intensity. Moreover, we provide two cluster-based instance-level mixup methods in <ref type="table" target="#tab_6">Table 5</ref> and 6 (denoting by * ): (a) Inter-Intra * . We use CutMix with ? ? 2 as inter-cluster mixup and Mixup with ? = 0.2 as intra-cluster mixup. (b) PuzzleMix * . We introduce saliency-based mixup methods to SSL tasks by introducing PL and a parametric cluster classifier g C ? after the encoder. This classifier g C ? and encoder f ? are optimized online like AutoMix and SAMix mentioned in A.2. Based on Grad-CAM (Selvaraju et al., 2019) calculated from the classifier, PuzzleMix can be adopted on SSL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.3 ANALYSIS OF MIXUP GENERATION OBJECTIVES</head><p>In Sec. 3.1, we design experiments to analyze various losses for mixup generation in <ref type="figure">Figure 2</ref> (left) and the proposed ?-balanced loss in <ref type="figure">Figure 2</ref> (right) for both SL and SSL tasks with ResNet-18 on STL-10 and Tiny. Basically, we assume both STL-10 and Tiny datasets have 200 classes on their 100k images. Since STL-10 does not provide ground truth labels (L) for 100k unlabeled data, we introduce PL generated by a supervised pertained classifier on Tiny as the "ground truth" for its 100k training set. Notice that L denotes ground truth labels and PL denotes pseudo labels generated by ODC <ref type="bibr" target="#b9">(Zhan et al., 2020)</ref> with C = 200. As for the SL task, we use the labeled training set for mixup classification (100k on Tiny v.s. 5k on STL-10). Notice that SL results are worse than using SSL settings on STL-10, since the SL task only trains a randomly initialized classifier on 5k labeled data. Because the infoNCE and BCE loss require cross-view augmentation (or they will produce trivial solutions), we adopt MoCo.V2 augmentation settings for these two losses when performing the SL task. Compared to CE (L), we corrupt the global term in CE as CE (PL) or directly remove them as pBCE (L) to show that pBCE is vital to optimizing mixed samples. Similarly, we show that the global term is used as the global constraint by comparing BCE (UL) with infoNCE (UL), infoNCE (PL), and infoNCE (L). As for the SSL task, we verify the conclusions drawn from the SL task and conclude that (a) the local term optimizes mixup generation directly, corresponding to the smoothness property, and (b) the global term serves as the global constraint corresponding to the discriminative property. Moreover, we verified that using the ?-balanced loss as L cls ? yields the best performance on SL and SSL tasks. Notice that we use ? = 0.5 on small-scale datasets and ? = 0.1 on large-scale datasets for SL tasks, and use ? = 0.5 for all SSL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.4 ANALYSIS OF MUTUAL INFORMATION FOR MIXUP</head><p>Since mutual information (MI) as usually adopted to analyze contrastive-based augmentations <ref type="bibr" target="#b46">(Tian et al., 2020a;</ref><ref type="bibr">b)</ref>, we estimate MI between x m of various methods and x i by MINE <ref type="bibr" target="#b0">(Belghazi et al., 2018)</ref> with 100k images in 64?64 resolutions on Tiny-ImageNet. We sample ? = from 0 to 1 with the step of 0.125 and plot results in <ref type="figure" target="#fig_7">Figure 7 (d)</ref>. Here we see that SAMix-C and SAMix-I with more MI when ? ? 0.5 perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.5 RESULTS OF DOWNSTREAM TASKS</head><p>In Sec. 4.2, we evaluate transferable abilities of the learned representation of self-supervised methods to object detection task on PASCAL VOC <ref type="bibr" target="#b14">(Everingham et al., 2010)</ref> and COCO <ref type="bibr" target="#b32">(Lin et al., 2014)</ref>. In <ref type="table" target="#tab_11">Table 9</ref>, the online SAMix-C and the pre-trained SAMix-I P achieve the best detection performances among the compared methods and significantly improves the baseline MoCo.V2 (e.g., SAMix-C gains 0.9% AP and +0.7% AP b over MoCo.V2). Notice that MoCHi, i-Mix, and UnMix introduce mixup augmentations in both the input and latent spaces, while our proposed SAMix only generates mixed samples in the input space. In Sec. 3.2, we discuss the trivial solutions of Mixer, which are usually occurred in SSL tasks. Given the sample pair (x i , x j ) and ? = 0.5, we visualize the content C i and P i,j to compare the trivial and non-trivial results in the SSL task on STL-10, as shown in <ref type="figure">Figure 3</ref>. As we can see, both C i Image A infoNCE BCE infoNCE( ) Image B <ref type="figure">Figure 9</ref>: Visualization of loss effect. Both infoNCE and BCE loss have different emphases: infoNCE shows a similar effect of supervised fine-grained classification, focusing on fragmented and essential features, while BCE focuses on object completeness. and P i,j from the trivial solution has extremely large or small scale values while C i generated by C N CL containing more balanced values. Since the attention weight P i,j is normalized by softmax, we hypothesize that C i more likely causes trivial solutions. To verify our hypothesis, we freeze W P in the original MB and compare the original linear content projection W z with the non-linear content modeling. The results confirm that the non-linear module can prevent large-scale values on C i and eliminate the trivial solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.2 EFFECTS OF MIXUP GENERATION LOSS</head><p>In addition to Sec.3.3, we further provide visualization of mixed samples using the infoNCE (Eq. 4), BCE (Eq. 7), and ?-balanced infoNCE loss (Eq. 8) for Mixer. As shown in <ref type="figure">Figure 9</ref>, we find that mixed samples using infoNCE mixup loss prefer instance-specific and fine-grained features. On the contrary, mixed samples of the BCE loss seem only to consider discrimination between two corresponding neighborhood systems. It is more inclined to maintain the continuity of the whole object relative to infoNCE. Thus, combining both the characteristics, the ?-balanced infoNCE loss yields mixed samples that retain both instance-specific features and global discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.3 VISUALIZATION OF MIXED SAMPLES IN SAMIX</head><p>SAMix in various scenarios. In addition to Sec. 3.3, we visualize the mixed samples of SAMix in various scenarios to show the relationship between mixed samples and class (cluster) information. Since IN-1k contains some samples in CUB and Aircraft, we choose the overlapped samples to visualize SAMix trained for the fine-grained SL task (CUB and Aircraft) and SSL tasks (SAMix-I and SAMix-C). As shown in <ref type="figure">Figure 10</ref>, mixed samples reflect the granularity of class information adopted in mixup training. Specifically, we find that mixed samples using infoNCE mixup loss (Eq.4) is more closely to the fine-grained SL because they both have many fine-grained centroids.</p><p>Comparison with PuzzleMix in SL tasks. To highlight the accurate mixup relationship modeling in SAMix compared to PuzzleMix (standing for saliency-based methods), we visualize the results </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>CE (L ) pB CE (L ) CE (P L) BC E( UL ) In fo (U L) In fo (L ) Analysis of the learning objective of Mixer on Tiny ImageNet with ResNet-18. The left figure shows results of various losses on the SL task (left y axis) and the CL task (right y axis). The right one shows the effect of using various negative weights ?. Visualization of trivial (using linear C and self-attention) and non-trivial solutions (using the proposed non-linear C N C and mixing attention) for SAMix-I on STL-10 and IN-1k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The network architecture of the proposed Mixer for mixup generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visualization and comparison of mixed samples from Mixer in various learning scenarios on IN-1k and iNat2017. Note that ? = 0.5 and ? = 0.5 if the balance coefficient ? is included. CL(C) and CL(I) denote using SAMix-C and SAMix-I separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Training framework comparison of the online pipeline (left) in AutoMix and the proposed pre-trained pipeline (right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>first evaluate SAMix for supervised learning (SL) in Sec. 4.1 and self-supervised learning (SSL) in Sec. 4.2, and then perform ablation studies in Sec. 4.3. Nine benchmarks are used for evaluation: CIFAR-100<ref type="bibr" target="#b28">(Krizhevsky et al., 2009</ref>), Tiny-ImageNet (Tiny)<ref type="bibr" target="#b8">(Chrabaszcz et al., 2017)</ref>, ImageNet-1k (IN-1k)<ref type="bibr" target="#b43">(Russakovsky et al., 2015)</ref>, STL-10<ref type="bibr" target="#b10">(Coates et al., 2011)</ref>,CUB-200 (Wah et al., 2011), FGVC-Aircraft (Aircraft)<ref type="bibr" target="#b37">(Maji et al., 2013)</ref>, iNaturalist2017/2018 (iNat2017/2018)<ref type="bibr" target="#b22">(Horn et al., 2018)</ref>, and Place205<ref type="bibr" target="#b63">(Zhou et al., 2014)</ref>. All experiments are conducted with PyTorch and reported the mean of 3 trials. SAMix uses ? = 2 and the feature layer l = 3 while SAMix P is pre-trained 100 epochs on IN-1k with ResNet-18. The median validation top-1 Acc of the last 10 epochs is recorded.4.1 EVALUATION ON SUPERVISED IMAGE CLASSIFICATIONCNNs and ViTs are used as backbone networks, including ResNet (R), Wide-ResNet (WRN)<ref type="bibr" target="#b60">(Zagoruyko &amp; Komodakis, 2016)</ref>, ResNeXt-32x4d (RX)<ref type="bibr" target="#b57">(Xie et al., 2017)</ref>, and DeiT<ref type="bibr" target="#b55">(Touvron et al., 2021)</ref>. We use PyTorch training procedures<ref type="bibr" target="#b39">(Paszke et al., 2019)</ref> by default: an SGD optimizer with cosine scheduler</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>for DeiT-S) training recipes are fully adopted on IN-1k. MCE and MBCE denote mixup cross-entropy and mixup binary cross-entropy in RSB A3. For a fair comparison, grid search is performed for hyper-parameters ? ? {0.1, 0.2, 0.5, 1.0, 2.0, 4.0} of all mixup variants. We follow hyper-parameters in original papers by default. * denotes unpublished work on arxiv. ? and ? denote reproduced results by official codes and originally reported results, the rest are reproduced (see A.3 and A.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>(a) Hyper-parameter ? for mixup. (b) The cluster number C for SAMix-C in CL tasks on Tiny. (c) Top-1 accuracy v.s. training time on IN-1k based on ResNet-50 with 100 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(iii) Time complexity analysis: Figure 7 (c) shows computational analysis conducted on the SL task on IN-1k using PyTorch 100-epoch settings. It shows that the overall accuracy v.s. time efficiency of SAMix and SAMix P are superior to other methods. (iv) Hyper-parameters: Figure 7 (a) and (b) show ablation results of the hyper-parameter ? and the clustering number C for SAMix-C. We empirically choose ?=2.0 and C = 200 as default.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>contains 50k training images and 10K test images of 100 classes. (2) ImageNet-1k (IN-1k) (Krizhevsky et al., 2012) contains 1.28 million training images and 50k validation images of 1000 classes. (3) Tiny-ImageNet (Tiny) (Chrabaszcz et al., 2017) is a rescaled version of ImageNet-1k, which has 100k training images and 10k validation images of 200 classes. (4) STL-10 (Coates et al., 2011) benchmark is designed for semi-or unsupervised learning, which consists of 5k labeled training images for 10 classes 100K unlabelled training images, and a test set of 8k images. (5) CUB-200-2011 (CUB) (Wah et al., 2011) contains over 11.8k images from 200 wild bird species for fine-grained classification. (6) FGVC-Aircraft (Aircraft) (Maji et al., 2013) contains 10k images of 100 classes of aircraft. (7) iNaturalist2017 (iNat2017) (Horn et al., 2018) is a large-scale fine-grained classification benchmark consisting of 579.2k images for training and 96k images for validation from over 5k different wild species. (8) PASCAL VOC (Everingham et al., 2010) is a classical objection detection and segmentation dataset containing 16.5k images for 20 classes. (9) COCO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>?q m , z ? k i ) and the 1 ? ? region for (1 ? ?)I(z ?q m , z ? k j ). (b) Linear evaluation (top-1 accuracy on STL-10) of whether to use the cross-view pipeline and to combine the original infoNCE loss with the mixup infoNCE loss. (c) A heat map of linear evaluation (top-1 accuracy on Tiny) represents the effects of using MixUp and CutMix as the inter-class (y-axis) and intra-class mixup (x-axis) using various ?. instance-level mixup defined in Eq. 4: the same-view, max ? k i ) + (1 ? ?)I(z ?q m , z ? k j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :Figure 12 :</head><label>1012</label><figDesc>Visualization of SAMix in various scenarios on CUB and Aircraft. Given images A and B, the middle three mixed samples are generated by SAMix with ? = 0.5 trained in the fine-grained SL task and the SSL tasks (SAMix-C and SAMix-I). Visualization of SAMix-I v.s. SAMix-C for SSL tasks on IN-1k. In every four rows, the upper and lower two rows represent mixed samples generated by SAMix-I and SAMix-C, respectively. ? value changes from left (? = 0) to right (? = 1) by an equal step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). X and Z denote the input sample and corresponding feature maps from Momentum Encoder. Blue modules are not updated by back-propagation. The online pipeline optimizes Mixer and Encoder alternatively, while the pre-trained pipeline adopts pre-trained Mixer on large datasets.Pre-trained pipeline v.s. online pipeline. Despite mixup methods optimized online<ref type="bibr" target="#b34">(Liu et al., 2022b)</ref> outperforming hand-crafted variants by a large margin, their extra computational cost is unbearable, especially on large-scale datasets. On large-scale benchmarks, we notice that the mixed samples generated by SAMix in the early and late training periods or using different CNN encoders vary little. So we hypothesize that we can replace the online training Mixer with a pre-trained one in SAMix, similar to knowledge distillation</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>73.85 76.83 78.18 78.71 71.83 75.29 77.35 78.91 MixUp 69.98 73.97 77.12 78.97 79.98 71.72 75.73 78.44 80.60 CutMix 68.95 73.58 77.17 78.96 80.42 71.01 75.16 78.69 80.59 ManifoldMix 69.98 73.98 77.01 79.02 79.93 71.73 75.44 78.21 80.64 SaliencyMix 69.16 73.56 77.14 79.32 80.27 70.21 75.01 78.46 80.45 FMix * 69.96 74.08 77.19 79.09 80.06 70.30 75.12 78.51 80.20 PuzzleMix 70.12 74.26 77.54 79.43 80.63 71.64 75.84 78.86 80.67 ResizeMix * 69.50 73.88 77.42 79.27 80.55 71.32 75.64 78.91 80.52 AutoMix * 70.50 74.52 77.91 79.87 80.89 72.05 76.10 79.25 80.98 SAMix P 70.83 74.95 78.06 80.05 80.98 72.27 76.28 79.39 81.10 SAMix 70.85 74.96 78.11 80.02 81.03 72.33 76.35 79.40 81.06</figDesc><table><row><cell></cell><cell>PyTorch 100ep</cell><cell>PyTorch 300ep</cell></row><row><cell>Methods</cell><cell cols="2">R-18 R-34 R-50 R-101 RX-101 R-18 R-34 R-50 R-101</cell></row><row><cell>Vanilla</cell><cell>70.04</cell></row></table><note>Top-1 Acc (%) of image classification on IN-1k training 100-epoch and 300-epoch using procedures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>66.27 62.69 67.56 59.33 63.01 CutMix 62.34 67.59 63.91 69.75 59.21 63.75 ManifoldMix 61.47 66.08 63.46 69.30 59.46 63.23 SaliencyMix 62.51 67.20 64.27 70.01 59.50 63.33 FMix * 61.90 66.64 63.71 69.46 59.51 63.63 PuzzleMix 62.66 67.72 64.36 70.12 59.62 63.91 ResizeMix * 62.29 66.82 64.12 69.30 59.66 63.88 AutoMix * 63.08 68.03 64.73 70.49 59.74 64.06 SAMix P 63.38 68.23 65.16 70.56 59.82 64.35 SAMix 63.32 68.26 64.84 70.54 59.86 64.27</figDesc><table><row><cell></cell><cell cols="3">: Top-1 Acc (%) of image classifica-</cell></row><row><cell cols="4">tion on iNat2017/2018 and Places205.</cell></row><row><cell></cell><cell>iNat2017</cell><cell>iNat2018</cell><cell>Places205</cell></row><row><cell>Method</cell><cell cols="3">R-50 RX-101 R-50 RX-101 R-18 R-50</cell></row><row><cell>Vanilla</cell><cell cols="3">60.23 63.70 62.53 66.94 59.63 63.10</cell></row><row><cell>MixUp</cell><cell>61.22</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Top-1 Acc (%) of supervised image classification on CIFAR-</cell></row><row><cell cols="4">100, Tiny, CUB-200 and Aircraft.</cell></row><row><cell></cell><cell cols="2">CIFAR-100</cell><cell>Tiny</cell><cell>CUB-200 FGVC-Aircraft</cell></row><row><cell>Method</cell><cell cols="4">R-18 RX-50 WRN-28-8 R-18 RX-50 R-18 RX-50 R-18 RX-50</cell></row><row><cell>Vanilla</cell><cell>78.04 81.09</cell><cell>81.63</cell><cell cols="2">61.68 65.04 77.68 83.01 80.23 85.10</cell></row><row><cell>MixUp</cell><cell>79.12 82.10</cell><cell>82.82</cell><cell cols="2">63.86 66.36 78.39 84.58 79.52 85.18</cell></row><row><cell>CutMix</cell><cell>78.17 81.67</cell><cell>84.45</cell><cell cols="2">65.53 66.47 78.40 85.68 78.84 84.55</cell></row><row><cell cols="2">ManifoldMix 80.35 82.88</cell><cell>83.24</cell><cell cols="2">64.15 67.30 79.76 86.38 80.68 86.60</cell></row><row><cell cols="2">SaliencyMix 79.12 81.53</cell><cell>84.35</cell><cell cols="2">64.60 66.55 77.95 83.29 80.02 84.31</cell></row><row><cell>FMix  *</cell><cell>79.69 81.90</cell><cell>84.21</cell><cell cols="2">63.47 65.08 77.28 84.06 79.36 84.85</cell></row><row><cell>PuzzleMix</cell><cell>80.43 82.57</cell><cell>85.02</cell><cell cols="2">65.81 66.92 78.63 84.51 80.76 86.23</cell></row><row><cell cols="2">ResizeMix  *  80.01 81.82</cell><cell>84.87</cell><cell cols="2">63.74 65.87 78.50 84.77 78.10 84.08</cell></row><row><cell>AutoMix  *</cell><cell>82.04 83.64</cell><cell>85.16</cell><cell cols="2">67.33 70.72 79.87 86.56 81.37 86.69</cell></row><row><cell>SAMix</cell><cell>82.30 84.42</cell><cell>85.50</cell><cell>68.89</cell></row></table><note>72.18 81.11 86.83 82.15 86.80</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Top-1 Acc (%) of im-</cell></row><row><cell cols="4">age classification on IN-1k.</cell></row><row><cell></cell><cell cols="3">R-50 (A3) DeiT-S</cell></row><row><cell>Methods</cell><cell cols="3">MCE MBCE MCE</cell></row><row><cell cols="4">MixUp+CutMix 76.49 78.08 79.80</cell></row><row><cell>MixUp</cell><cell cols="3">76.01 77.66 79.65</cell></row><row><cell>CutMix</cell><cell cols="3">76.47 77.62 79.78</cell></row><row><cell>SaliencyMix</cell><cell cols="3">76.85 77.93 79.32</cell></row><row><cell>FMix  *</cell><cell cols="3">76.09 77.76 79.41</cell></row><row><cell>PuzzleMix</cell><cell cols="3">77.27 78.02 79.84</cell></row><row><cell>ResizeMix  *</cell><cell cols="3">76.90 77.85 79.93</cell></row><row><cell>TransMix  ?</cell><cell>-</cell><cell>-</cell><cell>80.70</cell></row><row><cell>AutoMix  *</cell><cell cols="3">77.45 78.34 80.75</cell></row><row><cell>SAMix</cell><cell></cell><cell></cell><cell></cell></row></table><note>P 77.80 78.73 80.87 SAMix 78.33 78.45 80.94</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>. (ii) longitudinal comparison with CL methods that utilize input space and input+latent space mixup strategies, including MoCHi (Kalantidis et al., 2020), i-Mix (Lee et al., 2021), Un-Mix (Shen et al., 2021), and WBSIM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">: Top-1 Acc (%) of linear classification</cell></row><row><cell cols="2">pre-trained on STL-10.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>R18</cell><cell>R50</cell></row><row><cell>CL baseline</cell><cell>Methods</cell><cell cols="2">400ep 800ep 400ep 800ep</cell></row><row><cell>MoCo.V2</cell><cell>-</cell><cell cols="2">81.50 85.64 84.89 89.68</cell></row><row><cell></cell><cell>MixUp</cell><cell cols="2">84.51 87.93 88.24 92.20</cell></row><row><cell></cell><cell cols="3">ManifoldMix 84.17 87.70 88.06 91.65</cell></row><row><cell></cell><cell>CutMix</cell><cell cols="2">84.28 87.60 87.51 90.81</cell></row><row><cell>MoCo.V2</cell><cell cols="3">SaliencyMix 84.33 87.27 87.35 90.77</cell></row><row><cell></cell><cell>FMix  *</cell><cell cols="2">84.43 87.68 88.14 91.56</cell></row><row><cell></cell><cell cols="3">ResizeMix  *  83.88 87.25 86.88 90.83</cell></row><row><cell></cell><cell>SAMix-I</cell><cell cols="2">85.44 88.58 88.87 92.41</cell></row><row><cell>SwAV  ? (C)</cell><cell>-</cell><cell cols="2">81.10 85.56 84.35 88.79</cell></row><row><cell cols="2">MoCo.V2 (C) Inter-Intra</cell><cell cols="2">84.89 87.85 88.33 92.24</cell></row><row><cell cols="2">MoCo.V2 (C) PuzzleMix</cell><cell cols="2">84.98 88.07 88.40 91.98</cell></row><row><cell cols="2">MoCo.V2 (C) SAMix-C</cell><cell cols="2">85.60 88.63 88.91 92.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="6">: Top-1 Acc (%) of linear classification</cell></row><row><cell cols="3">pre-trained on Tiny and IN-1k.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tiny</cell><cell></cell><cell></cell><cell>IN-1k</cell></row><row><cell>CL method</cell><cell>Methodd</cell><cell cols="4">R18 R50 R18 R50</cell></row><row><cell>MoCo.V2</cell><cell>-</cell><cell cols="4">38.29 42.08 52.85 67.66</cell></row><row><cell>MoCo.V2</cell><cell>MixUp</cell><cell cols="4">41.24 46.61 53.03 68.07</cell></row><row><cell>MoCo.V2</cell><cell>CutMix</cell><cell cols="4">41.62 46.24 52.98 68.28</cell></row><row><cell>MoCo.V2</cell><cell cols="5">SaliencyMix 41.14 46.13 53.06 68.31</cell></row><row><cell cols="2">MoCo.V2 (C) PuzzleMix</cell><cell cols="4">41.86 46.72 53.46 68.48</cell></row><row><cell>MoCHi  ?</cell><cell cols="5">input+latent 41.78 46.55 53.12 68.01</cell></row><row><cell>i-Mix  ?</cell><cell cols="5">input+latent 41.61 46.57 53.09 68.10</cell></row><row><cell>UnMix  ?</cell><cell>input+latent</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.60</cell></row><row><cell>WBSIM  ?</cell><cell>input</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.40</cell></row><row><cell>MoCo.V2</cell><cell>SAMix-I</cell><cell cols="4">41.97 47.23 53.75 68.76</cell></row><row><cell>MoCo.V2</cell><cell>SAMix-I P</cell><cell cols="4">43.57 48.10 53.72 68.82</cell></row><row><cell cols="2">MoCo.V2 (C) SAMix-C</cell><cell cols="4">43.68 47.51 53.93 68.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">: Ablation of the proposed learning</cell></row><row><cell cols="3">objectives in the SSL task on STL-10 and Tiny.</cell></row><row><cell>method</cell><cell cols="2">STL-10 Tiny</cell></row><row><cell>BCE</cell><cell>85.25</cell><cell>41.28</cell></row><row><cell>infoNCE</cell><cell>85.36</cell><cell>41.85</cell></row><row><cell>infoNCE (? = 0.5)</cell><cell>85.44</cell><cell>41.97</cell></row><row><cell>CE (PL)</cell><cell>85.56</cell><cell>42.36</cell></row><row><cell>CE (PL)+infoNCE</cell><cell>85.41</cell><cell>42.12</cell></row><row><cell>CE (PL, ? = 0.5)</cell><cell>85.60</cell><cell>42.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Ablation of proposed modules in Mixer and the ?-balanced loss on Tiny.</figDesc><table><row><cell>module</cell><cell>SL</cell><cell>CL</cell></row><row><cell>Mixing attention</cell><cell cols="2">67.17 40.58</cell></row><row><cell>+Adaptive ?</cell><cell cols="2">67.95 41.82</cell></row><row><cell cols="3">+Non-linear content 68.46 42.45</cell></row><row><cell>+L mask</cell><cell cols="2">68.57 42.68</cell></row><row><cell>+? adjusting</cell><cell cols="2">68.61 43.14</cell></row><row><cell>+ ? (? = 0.5)</cell><cell cols="2">68.82 43.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Transferring to object detection with Faster R-CNN on VOC and Mask R-CNN on COCO. AP 75 AP b AP b</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Faster R-CNN</cell><cell></cell><cell cols="2">Mask R-CNN</cell></row><row><cell cols="2">CL Method Methods</cell><cell cols="4">AP AP 50 50</cell><cell>AP b 75</cell></row><row><cell>MoCo.V2</cell><cell>-</cell><cell>56.9 82.2</cell><cell cols="3">63.4 40.6 60.1</cell><cell>44.0</cell></row><row><cell>MoCo.V2</cell><cell>Mixup</cell><cell>57.4 82.5</cell><cell cols="3">64.0 41.0 60.8</cell><cell>44.3</cell></row><row><cell>MoCo.V2</cell><cell>CutMix</cell><cell>57.3 82.7</cell><cell cols="3">64.1 41.1 60.8</cell><cell>44.4</cell></row><row><cell>MoCo.V2</cell><cell>Inter-Intra</cell><cell>57.5 82.8</cell><cell cols="3">64.2 41.2 60.9</cell><cell>44.4</cell></row><row><cell>MoCHi  ?</cell><cell cols="2">input+latent 57.1 82.7</cell><cell cols="3">64.1 41.0 60.8</cell><cell>44.5</cell></row><row><cell>i-Mix  ?</cell><cell cols="2">input+latent 57.5 82.7</cell><cell>64.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UnMix  ?</cell><cell cols="2">input+latent 57.7 83.0</cell><cell cols="3">64.3 41.2 60.9</cell><cell>44.7</cell></row><row><cell>WBSIM  ?</cell><cell>input</cell><cell>57.4 82.8</cell><cell cols="3">64.2 40.7 60.8</cell><cell>44.2</cell></row><row><cell>MoCo.V2</cell><cell>SAMix-I</cell><cell>57.5 83.1</cell><cell cols="3">64.2 41.2 61.0</cell><cell>44.5</cell></row><row><cell>MoCo.V2</cell><cell>SAMix-I P</cell><cell>57.8 83.2</cell><cell cols="3">64.3 41.3 61.1</cell><cell>44.6</cell></row><row><cell>MoCo.V2</cell><cell>SAMix-C</cell><cell>57.7 83.1</cell><cell cols="3">64.4 41.3 61.1</cell><cell>44.7</cell></row><row><cell cols="2">A.6 VISUALIZATION OF SAMIX</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>A.6.1 MIXING ATTENTION AND CONTENT IN MIXER</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A.4 CONTRASTIVE LEARNING Implementation of SAMix-C and SAMix-I. As for SSL tasks, we adopt the cross-view objective,</p><p>for instance-level mixup classification in all methods (except for ? and ? marked methods). We provide two variants, SAMix-C and SAMix-I, which use different learning objectives of mixup classification. The basic network structures (an encoder f ? and a projector g ? ) are adopted as MoCo.V2 . Similar to SAMix in SL tasks, SAMix-C employ a parametric cluster classification head g C ? for online clustering <ref type="bibr" target="#b2">(Caron et al., 2018;</ref><ref type="bibr" target="#b9">Zhan et al., 2020)</ref> to provide pseudo labels (PL) to calculate L cls ? . It takes feature vectors from the momentum encoder as the input (optimized by Eq. 13) and has no impact on the mixup classification objective for the online networks. Meanwhile, SAMix-I employs the instance-level classification loss for both L ?,? and L cls ? . Moreover, we use the proposed ?-balanced mixup loss L cls ? for both SAMix-C and SAMix-I with ? = 0.5 and the objective L ? for Mixer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mine: Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transmix: Attend to mix for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Neng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Beyond single instance multi-view unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13356</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supermix: Supervising the mixing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobhan</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13794" to="13803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akilesh</forename><surname>Badrinaaraayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07794</idno>
		<title level="m">Patchup: A regularization technique for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Pr?gel-Bennett Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Fmix: Enhancing mixed sample data augmentation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03065</idno>
		<title level="m">Saliency guided joint mixup with supermodular diversity</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00230</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">I-mix: A domain-agnostic strategy for contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Openmixup: Open mixup toolbox and benchmark for visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/Westlake-AI/openmixup" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Decoupled mixup for data-efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automix: Unveiling the power of mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency grafting: Innocuous attribution-guided mixup with calibrated label mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonhyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><forename type="middle">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11101</idno>
		<title level="m">Resizemix: Mixing data with preserved object information and true labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">David</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02391</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Un-mix: Rethinking image mixtures for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-supervised learning from a multi-view perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Saliencymix: A saliency guided data augmentation strategy for better regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mst</forename><surname>Afm Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wheemyung</forename><surname>Monira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taechoong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Ho</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01791</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Alignmixup: Improving representations by interpolating aligned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><surname>Kijak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Yew Soon Ong, and Chen Change Loy. Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Automix: Mixup networks for sample interpolation via cooperative barycenter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">SUPERVISED IMAGE CLASSIFICATION</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">As for hyper-parameters of SAMix, we follow the basic setting in AutoMix for both SL and SSL tasks: SAMix adopts ? = 2, the feature layer l = 3, the bilinear upsampling, and the weight ? = 0.1 which linearly decays to 0. We use ? = 0.5 for small-scale datasets (CIFAR-100, Tiny, CUB and Aircraft) and ? = 0.1 for large-scale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">-1k and iNat2017). As for other methods</title>
		<imprint>
			<publisher>Co-Mixup</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Hyper-parameter settings</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">As for mixup methods reproduced by us, we provide dataset-specific hyper-parameter settings as follows. For CIFAR-100, Mixup and ResizeMix use ? = 1, and CutMix, FMix and SaliencyMix use ? = 0.2, and ManifoldMix uses ? = 2. For Tiny</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Augmix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">-1k, and iNat2017 datasets, ManifoldMix uses ? = 0.2, and the rest methods adopt ? = 1 for median and large backbones</title>
		<imprint/>
	</monogr>
	<note>ResNet-50). Specially, all these methods use ? = 0.2 (only) for ResNet-18. For small-scale fine-grained datasets. SaliencyMix and FMix use ? = 0.2, and ManifoldMix uses ? = 0.5, while the rest use ? = 1</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">2016) as the encoder f ? with two-layer MLP projector g ? and is optimized by SGD optimizer and Cosine scheduler with the initial learning rate of 0.03 and the batch size of 256. The length of the momentum dictionary is 65536 for IN-1k and 16384 for STL-10 and Tiny datasets. The data augmentation strategy is based on IN-1k in MoCo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caron</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">all compared CL methods use MoCo.V2 pre-training settings, which adopts ResNet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Hyper-parameter settings. Except for SwAV. v2 as following: Geometric augmentation is RandomResizedCrop with the scale in [0.2, 1.0] and RandomHorizontalFlip. Color augmentation is ColorJitter with {brightness, contrast, saturation, hue} strength of {0.4, 0.4, 0.4, 0.1} with a probability of 0.8, and RandomGrayscale with a probability of 0.2. Blurring augmentation is using a square Gaussian kernel of size 23 ? 23 with a std uniformly sampled in [0.1, 2.0]. We use 224?224 resolutions for IN-1k and 96?96 resolutions for STL-10 and Tiny datasets</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The linear classifier is trained 100 epochs by an SGD optimizer with the SGD momentum of 0.9 and the weight decay of 0. We set the initial learning rate of 30 for IN-1k as MoCo, and 0.1 for STL-10 and Tiny. The learning rate decays by 0.1 at epochs 60 and 80. Moreover, we adopt object detection task to evaluate transfer learning abilities following MoCo, which uses the 4-th layer feature maps of ResNet (ResNet-C4) to fine-tune Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">which trains a linear classifier on top of the frozen representation on the training set</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>with 24k iterations on the trainval07+12 set and Mask R-CNN. He et al., 2017) with 2? training schedule (24-epoch) on the train2017 set</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A.5 EMPIRICAL EXPERIMENTS A.5.1 CROSS-VIEW TRAINING PIPELINE FOR INSTANCE-LEVEL MIXUP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">We first analyze the learning objective of instance-level mixup classification for contrastive learning (CL) as discussed in Sec. 2.2. As shown in Figure 8 (a), there are two possible objectives for of mixed samples from these two methods in the supervised case in Figure 11. There is three main difference: (a) bilinear upsampling strategy in SAMix makes the mixed samples more smooth in local patches. (b) adaptive ? encoding and mixing attention enhances the correspondence between mixed samples and ? value. (c) ?-balanced mixup loss enables SAMix to balance global discriminative and fine-grained features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">As shown in Figure 12, we provide more mixed samples of SAMix-I and SAMix-C in the SSL tasks to show that introducing class information by PL can help Mixer generate mixed samples that retain both the fine-grained features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Comparison Of Samix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-C</forename><surname>Samix</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>instance discrimination) and whole targets</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">DETAILED RELATED WORK</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">which maximizes similarities of positive pairs while minimizing similarities of negative pairs. To provide a global view of CL, MoCo (He et al., 2020) proposes a memory-based framework with a large number of negative samples and model differentiation using the exponential moving average. SimCLR (Chen et al., 2020a) demonstrates a simple memory-free approach with large batch size and strong data augmentations that is also competitive in performance to memory-based methods. Unlike other CL approaches, BYOL (Grill et al., 2020) does not require negative pairs or a large batch size for the proposed pretext task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contrastive Learning. CL amplifies the potential of SSL by achieving significant improvements on classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>which tries to estimate latent representations from the same instance</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">2020) inserts a whole image into a local rectangular area of another image after scaling down. FMix (Harris et al., 2020) converts the image to Fourier space (spectrum domain) to create binary masks. To generate more semantic virtual samples, offline optimization algorithms are introduced for the saliency regions. SaliencyMix (Uddin et al., 2020) obtains the saliency using a universal saliency detector. With optimization transportation, PuzzleMix (Kim et al., 2020) and Co-Mixup (Kim et al., 2021) present more precise methods for finding appropriate mixup masks based on saliency statistics. SuperMix (Dabouei et al., 2021) combines mixup with knowledge distillation, which learns a pixel-wise sample mixing policy via a teacher-student framework to distill class knowledge. Differing from previous methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">convex interpolations of any two samples and their unique one-hot labels, was presented as the first mixing-based data augmentation approach for regularising the training of networks. ManifoldMix</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>AutoMix. 2022b) can learn the mixup generation by a sub-network end-to-end. which generates mixed samples via feature maps and the mixing ratio</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">2021) attempts to use MixUp in the input space for self-supervised learning, whereas the developers of MoChi (Kalantidis et al., 2020) propose mixing the negative sample in the embedding space to increase the number of hard negatives but at the expense of classification accuracy. i-Mix (Lee et al., 2021) and BSIM (Chu et al., 2020) demonstrated how to regularize contrastive learning by mixing instances in the input or latent spaces. We introduce an automatic mixup for SSL tasks, which adaptively learns the instance relationship based on inter-and intra-cluster properties online. 11: Visualization of PuzzleMix v.s. SAMix for SL tasks on IN-1k. In every four rows, the upper and lower two rows represent mixed samples generated by PuzzleMix and SAMix, respectively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mixup for contrastive learning. A complementary method for better instance-level representation learning is to use mixup on CL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>When used in collaboration with CE loss, Mixup and its several variants provide highly efficient data augmentation for SL by establishing a relationship between samples. Without a ground-truth label, most of the approaches are limited to linear mixup methods. For example, Un-mix. ? value changes from left (? = 0) to right (? = 1) by an equal step</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
