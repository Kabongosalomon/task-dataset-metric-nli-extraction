<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-semantic segmentation</term>
					<term>fine-resolution remote sensing images</term>
					<term>transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Fully Convolutional Network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multi-level feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavours are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we introduce the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme. Code is available at https://github.com/WangLibo1995/GeoSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As an effective method to extract features automatically and hierarchically from images, the convolutional neural network (CNN) has become the common framework for tasks related to computer vision (CV) <ref type="bibr" target="#b3">[4]</ref>. For semantic segmentation, the Fully Convolutional Network (FCN) <ref type="bibr" target="#b6">[7]</ref> is the first proven and effective end-to-end CNN structure. Specifically, there are two symmetric paths in the FCN and its variants: a contracting path, i.e., the encoder, for extracting features, and an expanding path, i.e., the decoder, for exacting positions <ref type="bibr" target="#b9">[10]</ref>. The contracting path, by definition, gradually downsamples the resolution of feature maps to reduce the computational consumption, while the expanding path can learn more semantic meaning via a progressively increasing receptive field. Benefit from its translation equivariance and locality, the FCN enhances the segmentation performance significantly and influences the entire field. Specifically, the translation equivariance underpins the generalization capability of the model to unseen data, while the locality reduces the complexity of the model by sharing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>parameters.</head><p>The outcome of FCN, although encouraging, appears to be coarse due to the over-simplified design of the decoder. Subsequently, more elaborate encoder-decoder structures were proposed <ref type="bibr" target="#b16">[17]</ref>, thus increasing the accuracy further. However, the long-range dependency is limited by the locality property of FCN-based methods, which is critical for segmentation in unconstrained scene images. There are two types of methods to address the issue, either modifying the convolution operation or utilizing the attention mechanism. The former aiming to enlarge the receptive fields using large kernel sizes <ref type="bibr" target="#b17">[18]</ref>, dilated convolutions <ref type="bibr" target="#b18">[19]</ref>, or feature pyramids <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>, whereas the latter focuses on integrating attention mechanisms with the FCN architecture to capture long-range dependencies of the feature maps <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Nevertheless, both methods fail to liberate the network from the dependence of the FCN structure. More recently, several inspiring advances <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> attempt to avoid convolution operations completely by employing attentionalone models, thereby achieving feature maps with long-range dependencies effectively.</p><p>For natural language processing (NLP), the dominant architecture is the Transformer <ref type="bibr" target="#b23">[24]</ref>, which adopts the multihead attention to model long-range dependencies for sequence modelling and transduction tasks. The tremendous breakthrough in the natural language domain inspires researchers to explore the potential and feasibility of Transformer in the computer vision field. Obviously, the successful application of Transformer will become the first and foremost step to integrate computer vision and NLP, thereby providing a universal and uniform artificial intelligence (AI) scheme.</p><p>The pioneering work of Swin Transformer <ref type="bibr" target="#b21">[22]</ref> presents a hierarchical feature representation scheme that demonstrates impressive performances with linear computational complexity. In this Letter, we first introduce the Swin Transformer for semantic segmentation of fine-resolution remote sensing images. Most importantly, we propose a densely connected </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>The overall architecture of our DC-Swin is constructed based on the encoder-decoder structure, where the Swin Transformer is introduced as the encoder while the proposed DCFAM is selected as the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Swin Transformer</head><p>As shown in <ref type="figure" target="#fig_2">Fig.1 (a)</ref>, the Swin Transformer backbone <ref type="bibr" target="#b21">[22]</ref> first utilizes a patch partition module to split the input RGB image into non-overlapping patches as "tokens". The feature of each patch is set as a concatenation of the raw pixel RGB values. Subsequently, this raw-valued feature is fed into the multistage feature transformation. In stage 1, a linear embedding layer is deployed to project features to an arbitrary dimension C. Thereafter, pairs of Swin Transformer blocks ( <ref type="figure" target="#fig_2">Fig.1 (b)</ref>), which can maintain the number of tokens (e.g., HW/16), are adopted to extract semantic features. In the remaining stages, the number of tokens is gradually reduced by patch merging layers along with the increasing depth of the network to produce a hierarchical representation. The outputs of the four stages are processed by a standard 1?1 convolution to generate four hierarchical Swin Transformer features (ST1, ST2, ST3, and ST4).</p><p>By choosing diverse hyper-parameters, i.e., the dimensions C and the number of Swin Transformer blocks in each stage, four Swin Transformer backbones with different complexities can be obtained: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Densely Connected Feature Aggregation Module</head><p>Multi-scale and confusing geospatial objects appear frequently in fine-resolution remote sensing images, which seriously affects the quality of segmentation. To handle this issue, we propose a novel DCFAM method for feature representation. To be specific, we design a Shared Spatial Attention (SSA) and a Shared Channel Attention (SCA) to enhance the spatial-wise and channel-wise relationship of the semantic features based on our previous work of linear attention mechanism <ref type="bibr" target="#b24">[25]</ref>. Besides, multi-level features are further integrated using the Downsample Connection and the Largefield Upsample Connection for improving multi-scale representation. As shown in <ref type="figure" target="#fig_2">Fig.1</ref>, the DCFAM connects the four hierarchical transformer features with cross-scale connections (i.e., Downsample Connection and Large Field Upsample Connection) and attention blocks (i.e., Shared Spatial Attention and Shared Channel Attention), generating four aggregation features (i.e., AF1, AF2, AF3, and AF4). Capitalising on the benefits provided by the DCFAM, the final segmentation feature AF1 is abundant in multi-scale information and relation-enhanced context.</p><p>Downsample Connection: The Downsample connection aims to connect the low-level and high-level transformer features for fusion, which can be defined as follow: </p><p>where is the input vector. is a ReLU activation function. and are a 3?3 convolution layer with a stride of 2, is a 3?3 convolution layer with a stride of 1, and each convolution layer involves a batch normalization operation. and denote the number of the input channels and output channels, respectively.</p><p>Large field Upsample Connection: To capture multi-scale context effectively, we embedded the dilated convolution into the Large filed Upsample Connection formulated as:</p><formula xml:id="formula_1">( ) = 12 ( ( 6 ( )))<label>(2)</label></formula><p>where 12 is a composite function that contains a standard 1?1 convolution, a dilated convolution with a dilated rate of 12, and a standard transpose convolution. Similarly, 6 has a dilated rate of 6. and represent the number of the input channel and output channel, respectively.</p><p>Shared Spatial Attention: Based on the linear attention mechanism <ref type="bibr" target="#b24">[25]</ref>, we utilize the Shared Spatial Attention to model the long-range dependencies in the spatial dimension defined as:</p><formula xml:id="formula_2">( ) = ? ( ) , + ? ( ) ? ( )? 2 ? ?? ( ) ? ( )? 2 ? ( )? + ? ( ) ? ( )? 2 ? ? ? ( ) ? ( )? 2 ? ,<label>(3)</label></formula><p>where Q( ) , K( ), and V( ) represent the convolutional operation to generate the query matrix ? ? ? , key matrix ? ? ? , and value matrix ? ? ? . N is the number of pixels in the input feature maps. c and n indicate the channel dimension and the flattened spatial dimension.</p><p>Shared Channel Attention: Similarly, the Shared Channel Attention is designed to extract the long-range dependencies among the channel dimension:</p><formula xml:id="formula_3">( ) = ? ( ) , + ? ( ) , ? ( ) ? ( )? 2 ? ? ( ) ? ( )? 2 + ? ( ) ? ( )? 2 ? ? ? ( ) ? ( )? 2 ? ,<label>(4)</label></formula><p>where ( ) indicate the reshape operation to flatten the spatial dimension. The detailed information about our previous work on the linear attention mechanism can be referred to <ref type="bibr" target="#b24">[25]</ref>.</p><p>Feature aggregation: The four aggregation features (AF1, AF2, AF3, and AF4) can eventually be computed by the following equations: </p><formula xml:id="formula_4">= + 384 768 ( ( 192 384 ( ))<label>(5)</label></formula><p>Here, is a bilinear interpolation upsample operation with a scale factor of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We test the effectiveness of the proposed scheme on the wellknown ISPRS Vaihingen and Potsdam semantic labelling datasets. There are 33 tiles extracted from true orthophoto and the co-registered normalized DSMs in the Vaihingen dataset with an average size of 2494?2064 pixels. The Potsdam dataset contains 38 tiles and the size of each tile is 6000?6000. Following previous pieces of literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>, in the Vaihingen dataset, we use the benchmark organizer defined 16 images for training and 17 for testing, while the setting in the Potsdam dataset is 24 tiles for training and 14 tiles for testing. The image tiles are cropped into 1024?1024 px patches as the input. We do not employ DSMs in our experiments to reduce computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setting</head><p>All of the experiments are implemented with PyTorch on a single RTX 3090, and the optimizer is set as AdamW with a 0.0003 learning rate. The soft cross-entropy is used as the loss function. For each method, the overall accuracy (OA), mean Intersection over Union (mIoU), and F1-score (F1) are chosen as evaluation indices:</p><formula xml:id="formula_6">= ? =1 ? + + + =1 ,<label>(9)</label></formula><formula xml:id="formula_7">= 1 ? + + =1 ,<label>(10)</label></formula><formula xml:id="formula_8">= 1 ? + =1 ,<label>(11)</label></formula><formula xml:id="formula_9">= 1 ? + =1 ,<label>(12)</label></formula><formula xml:id="formula_10">1 = 2 ? ? + ,<label>(13)</label></formula><p>where , , , and indicate the true positive, false positive, true negative, and false negatives, respectively, for the specific object indexed as class k. OA is computed for all categories including the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Segmentation Results and Analysis 1) Performance Comparison:</head><p>The experimental results on the Vaihingen and Potsdam datasets among state-of-the-art methods are listed in <ref type="table" target="#tab_2">Table ? and Table ?</ref>. The quantitive indices demonstrate the effectiveness of the proposed segmentation scheme. To be specific, our proposed DC-Swin achieves 90.71% in mean F1-score, 91.63% in OA, and 83.22% in mIoU for the Vaihingen dataset, with 93.25%, 92.00%, and 87.56% for the Potsdam dataset, outperforming the majority of ResNet-based methods with highly competitive accuracy. Benefit from the global context information modelled by the Swin-S and the DCFAM, the performance of our scheme not only outperforms recent contextual information aggregation methods designed initially for natural images, such as DeepLabV3+ and PSPNet, but also prevails over the lastest multi-scale feature aggregation models proposed for remote sensing images, such as EaNet and DDCM-Net, as well as the transformer networks BoTNet and ResT.</p><p>2) Ablation Study: As we not only propose a novel feature aggregation model but also introduce a brand-new backbone for segmentation, it is valuable to conduct the ablation study and investigate the contribution of each part upon accuracy. For the ablation study, we select ResNet-101 and Swin-S with the direct upsample operation as the baseline. ResNet101+DC and Swin-S+DC, which remove the SCA and SSA from DCFAM, are developed for the ablation study of dense connections. DCFAM-NS denotes the modified DCFAM that adopts the noshared form structure. As shown in <ref type="table" target="#tab_3">Table ?</ref>, the substitution of the backbone from ResNet-101 to Swin-S yields a 3% increase in the Vaihingen dataset and a 4.05% increase in the Potsdam dataset for the mIoU index, showing the superiority of Swin-S. ResNet101+DC and Swin-S+DC improve the performance of the corresponding baseline method dramatically, indicating the effectiveness of dense connections. Meanwhile, deploying the shared attention modules in DCFAM further increases the accuracy, demonstrating the effectiveness of the SCA and SSA. Besides, the employment of DCFAM-NS obtains lower scores compared to the utilization of DCFAM, which demonstrates the advantage of our shared form structure. Benefiting from the long-range dependencies and shared multi-scale structure, Swin-S+DCFAM obtains the highest accuracy on the two datasets, whose performance can also be observed in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this Letter, for the first time, we introduce Transformer into semantic segmentation of fine-resolution remote sensing images. We develop a densely connected feature aggregation module to capture multi-scale relation-enhanced semantic features, thereby increasing the segmentation accuracy. Numerical experiments conducted on the ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness of our scheme in segmentation accuracy. We envisage this pioneering Letter could inspire researchers and practitioners in this field to explore the potential and feasibility of the Transformer more widely in the remote sensing and Earth observation domain. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was funded by National Natural Science Foundation of China (NSFC) under grant number 41971352. (Corresponding author: Shenghui Fang.) L. Wang, R. Li, X. Meng and S. Fang are with School of Remote Sensing and Information Engineering, Wuhan University, Wuhan 430079, China (email: wanglibo@whu.edu.cn; lironui@whu.edu.cn; xmeng@whu.edu.cn; shfang@whu.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>Swin-T: C = 96, block numbers = {2, 2, 6, 2} ? Swin-S: C = 96, block numbers = {2, 2, 18, 2} ? Swin-B: C = 128, block numbers = {2, 2, 18, 2} ? Swin-L: C = 192, block numbers = {2, 2, 18, 2} In this letter, to balance the efficiency and effectiveness, we choose Swin-S pre-trained on the ImageNet as the backbone of the encoder, with the number of parameters (50M) comparable to ResNet-101 (45M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>(a) The overall architecture of DC-Swin, (b) Pair of Swin Transformer Blocks, (c) Downsample Connection, (d) Large Field Upsample Connection, (e) Shared Spatial Attention, and (f) Shared Channel Attention. The values of H and W are both 1024. Please enlarge the PDF to &gt;=200% to get a better view. ( ) = ( ( ) + ( ( )))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>Enlarged visualization of results on the Vaihingen dataset (Top) and Potsdam dataset (Bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE ? THE</head><label>?</label><figDesc>EXPERIMENTAL RESULTS ON THE VAIHINGEN DATASET.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Imp. surf.</cell><cell>Building</cell><cell>Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell>Mean F1</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>DeepLabV3+ [1]</cell><cell>ResNet101</cell><cell>92.38</cell><cell>95.17</cell><cell>84.29</cell><cell>89.52</cell><cell>86.47</cell><cell>89.57</cell><cell>90.56</cell><cell>81.47</cell></row><row><cell>PSPNet [2]</cell><cell>ResNet101</cell><cell>92.79</cell><cell>95.46</cell><cell>84.51</cell><cell>89.94</cell><cell>88.61</cell><cell>90.26</cell><cell>90.85</cell><cell>82.58</cell></row><row><cell>DANet [5]</cell><cell>ResNet101</cell><cell>91.63</cell><cell>95.02</cell><cell>83.25</cell><cell>88.87</cell><cell>87.16</cell><cell>89.19</cell><cell>90.44</cell><cell>81.32</cell></row><row><cell>EaNet [8]</cell><cell>ResNet101</cell><cell>93.40</cell><cell>96.20</cell><cell>85.60</cell><cell>90.50</cell><cell>88.30</cell><cell>90.80</cell><cell>91.20</cell><cell>-</cell></row><row><cell>DDCM-Net [3]</cell><cell>ResNet50</cell><cell>92.70</cell><cell>95.30</cell><cell>83.30</cell><cell>89.40</cell><cell>88.30</cell><cell>89.80</cell><cell>90.40</cell><cell>-</cell></row><row><cell>CASIA2 [11]</cell><cell>ResNet101</cell><cell>93.20</cell><cell>96.00</cell><cell>84.70</cell><cell>89.90</cell><cell>86.70</cell><cell>90.10</cell><cell>91.10</cell><cell>-</cell></row><row><cell>V-FuseNet [9]</cell><cell>FuseNet</cell><cell>91.00</cell><cell>94.40</cell><cell>84.50</cell><cell>89.90</cell><cell>86.30</cell><cell>89.20</cell><cell>90.00</cell><cell>-</cell></row><row><cell>DLR_9 [15]</cell><cell>-</cell><cell>92.40</cell><cell>95.20</cell><cell>83.90</cell><cell>89.90</cell><cell>81.20</cell><cell>88.50</cell><cell>90.30</cell><cell>-</cell></row><row><cell>BoTNet [14]</cell><cell>ResNet50</cell><cell>92.24</cell><cell>95.28</cell><cell>83.88</cell><cell>89.99</cell><cell>85.47</cell><cell>89.37</cell><cell>90.51</cell><cell>81.05</cell></row><row><cell>ResT [16]</cell><cell>ResT-Base</cell><cell>92.15</cell><cell>94.88</cell><cell>84.17</cell><cell>90.02</cell><cell>84.97</cell><cell>89.24</cell><cell>90.43</cell><cell>80.82</cell></row><row><cell>Ours</cell><cell>Swin-S</cell><cell>93.60</cell><cell>96.18</cell><cell>85.75</cell><cell>90.36</cell><cell>87.64</cell><cell>90.71</cell><cell>91.63</cell><cell>83.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">THE EXPERIMENTAL RESULTS ON THE POTSDAM DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>Imp. surf.</cell><cell>Building</cell><cell>Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell>Mean F1</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>DeepLabV3+ [1]</cell><cell>ResNet101</cell><cell>92.95</cell><cell>95.88</cell><cell>87.62</cell><cell>88.15</cell><cell>96.02</cell><cell>92.12</cell><cell>90.88</cell><cell>84.32</cell></row><row><cell>PSPNet [2]</cell><cell>ResNet101</cell><cell>93.36</cell><cell>96.97</cell><cell>87.75</cell><cell>88.50</cell><cell>95.42</cell><cell>92.40</cell><cell>91.08</cell><cell>84.88</cell></row><row><cell>DDCM-Net [3]</cell><cell>ResNet50</cell><cell>92.90</cell><cell>96.90</cell><cell>87.70</cell><cell>89.40</cell><cell>94.90</cell><cell>92.30</cell><cell>90.80</cell><cell>-</cell></row><row><cell>CCNet [6]</cell><cell>ResNet101</cell><cell>93.58</cell><cell>96.77</cell><cell>86.87</cell><cell>88.59</cell><cell>96.24</cell><cell>92.41</cell><cell>91.47</cell><cell>85.65</cell></row><row><cell>AMA_1</cell><cell>-</cell><cell>93.40</cell><cell>96.80</cell><cell>87.70</cell><cell>88.80</cell><cell>96.00</cell><cell>92.54</cell><cell>91.20</cell><cell>-</cell></row><row><cell>SWJ_2</cell><cell>ResNet101</cell><cell>94.40</cell><cell>97.40</cell><cell>87.80</cell><cell>87.60</cell><cell>94.70</cell><cell>92.38</cell><cell>91.70</cell><cell>-</cell></row><row><cell>V-FuseNet [9]</cell><cell>FuseNet</cell><cell>92.70</cell><cell>96.30</cell><cell>87.30</cell><cell>88.50</cell><cell>95.40</cell><cell>92.04</cell><cell>90.60</cell><cell>-</cell></row><row><cell>DST_5 [12]</cell><cell>FCN</cell><cell>92.50</cell><cell>96.40</cell><cell>86.70</cell><cell>88.00</cell><cell>94.70</cell><cell>91.66</cell><cell>90.30</cell><cell>-</cell></row><row><cell>BoTNet [14]</cell><cell>ResNet50</cell><cell>93.13</cell><cell>96.37</cell><cell>87.31</cell><cell>88.01</cell><cell>95.79</cell><cell>92.12</cell><cell>90.76</cell><cell>85.62</cell></row><row><cell>ResT [16]</cell><cell>ResT-Base</cell><cell>92.74</cell><cell>96.08</cell><cell>87.48</cell><cell>88.55</cell><cell>94.76</cell><cell>91.92</cell><cell>90.57</cell><cell>85.23</cell></row><row><cell>Ours</cell><cell>Swin-S</cell><cell>94.19</cell><cell>97.57</cell><cell>88.57</cell><cell>89.62</cell><cell>96.31</cell><cell>93.25</cell><cell>92.00</cell><cell>87.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE ? ABLATION</head><label>?</label><figDesc>STUDY ON THE VAIHINGEN AND POTSDAM DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Mean F1</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell></cell><cell>ResNet101</cell><cell>85.31</cell><cell cols="2">89.59 75.48</cell></row><row><cell></cell><cell>ResNet101+DC</cell><cell>88.96</cell><cell cols="2">90.73 80.48</cell></row><row><cell></cell><cell>ResNet101+DCFAM-NS</cell><cell>89.48</cell><cell cols="2">90.87 81.26</cell></row><row><cell>Vaihingen</cell><cell>ResNet101+DCFAM Swin-S</cell><cell>90.22 87.54</cell><cell cols="2">91.04 82.43 90.50 78.48</cell></row><row><cell></cell><cell>Swin-S+DC</cell><cell>89.91</cell><cell cols="2">91.11 81.94</cell></row><row><cell></cell><cell>Swin-S+DCFAM-NS</cell><cell>89.96</cell><cell cols="2">91.26 82.02</cell></row><row><cell></cell><cell>Swin-S+DCFAM</cell><cell>90.71</cell><cell cols="2">91.63 83.22</cell></row><row><cell></cell><cell>ResNet101</cell><cell>88.66</cell><cell cols="2">89.24 79.97</cell></row><row><cell></cell><cell>ResNet101+DC</cell><cell>91.75</cell><cell cols="2">90.45 84.95</cell></row><row><cell></cell><cell>ResNet101+DCFAM-NS</cell><cell>91.81</cell><cell cols="2">90.49 85.05</cell></row><row><cell>Potsdam</cell><cell>ResNet101+DCFAM Swin-S</cell><cell>92.28 91.20</cell><cell cols="2">90.81 85.87 90.54 84.02</cell></row><row><cell></cell><cell>Swin-S+DC</cell><cell>92.55</cell><cell cols="2">91.33 86.32</cell></row><row><cell></cell><cell>Swin-S+DCFAM-NS</cell><cell>92.82</cell><cell cols="2">91.47 86.80</cell></row><row><cell></cell><cell>Swin-S+DCFAM</cell><cell>93.25</cell><cell cols="2">92.00 87.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense Dilated Convolutions&apos; Merging Network for Land Cover Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Salberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2020.2976658</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6309" to="6320" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Land cover classification from remote sensing images based on multi-scale fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geo-spatial Information Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CCNet: Criss-Cross Attention for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3007032</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing very high resolution urban scene images by learning deep ConvNets with edge-aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiattention network for semantic segmentation of fine-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="78" to="95" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MACU-Net for semantic segmentation of fine-resolution remotely sensed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2017.11.009</idno>
		<idno>2018/01/01/ 2018</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2017.11.009" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ResT: An Efficient Transformer for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large kernel matters--improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-Aware Neural Network for Semantic Segmentation of Multi-Resolution Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">5015</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ABCNet: Attentive bilateral contextual network for efficient semantic segmentation of Fine-Resolution remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2021.09.005</idno>
		<idno>2021/11/01/ 2021</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2021.09.005" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="84" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformer Meets Convolution: A Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">3065</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2021.3063381</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
