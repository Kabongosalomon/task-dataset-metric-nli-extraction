<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><surname>Heo</surname></persName>
							<email>yukheo@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong</forename><forename type="middle">Jun</forename><surname>Koh</surname></persName>
							<email>yjkoh@cnu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Chungnam National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
							<email>changsukim@korea.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel guided interactive segmentation (GIS) algorithm for video objects to improve the segmentation accuracy and reduce the interaction time. First, we design the reliability-based attention module to analyze the reliability of multiple annotated frames. Second, we develop the intersection-aware propagation module to propagate segmentation results to neighboring frames. Third, we introduce the GIS mechanism for a user to select unsatisfactory frames quickly with less effort. Experimental results demonstrate that the proposed algorithm provides more accurate segmentation results at a faster speed than conventional algorithms. Codes are available at https://github.com/yuk6heo/GIS-RAmap.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) is a task to cut out objects of interest in a video. It is useful in various applications such as video editing, video summarization, video inpainting, and self-driving cars. VOS is challenging since it should deal with multiple objects, object deformation, and object occlusion. Because of this difficulty, semisupervised VOS, which uses a fully annotated segmentation mask in the first frame, has been widely researched. This approach can improve the segmentation performance but requires a lot of time and effort for annotations (e.g. around 79 seconds per instance <ref type="bibr" target="#b3">[4]</ref>). Also, it does not have a fallback mechanism when unsatisfactory results are obtained.</p><p>Interactive VOS adopts user-friendly annotations, e.g. scribbles, which are simple enough to provide repeatedly. <ref type="figure" target="#fig_0">Figure 1</ref> shows the round-based interactive VOS process. First, a user selects a target frame and draws scribble annotations on it. After extracting query object information from the scribbles, the algorithm obtains segmentation results for all frames. Second, the user finds a frame with unsatisfactory results and then provides additional scribbles. The algorithm then exploits both sets of scribbles to refine the VOS results. This is repeated until the user is satisfied. Recently, an automatic simulation scheme of the roundbased interactive VOS was designed in <ref type="bibr" target="#b3">[4]</ref>. However, the simulation is significantly different from real applications in that it immediately determines segmentation results to correct, by comparing them with the ground-truth. In contrast, a real user should spend considerable time to inspect the results and select poorly segmented regions. Since conventional interactive VOS algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> have been developed based on the simulation in <ref type="bibr" target="#b3">[4]</ref>, they do not consider the time for finding unsatisfactory results in practice. In contrast, we propose a guided interactive segmentation (GIS) algorithm for video objects, which guides users to find poorly segmented regions quickly and effectively.</p><p>Moreover, although interactive VOS can use the information in N annotated frames in the N th round, the conventional algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref> do not exploit those multiple annotated frames thoroughly. Heo et al. <ref type="bibr" target="#b6">[7]</ref> simply average the features from multiple annotated frames. Miao et al. <ref type="bibr" target="#b19">[20]</ref> use only the best matching result between a target frame and multiple annotated frames. On the contrary, we analyze the reliability of each annotated frame to refine segmentation results in a target frame more accurately.</p><p>In this paper, we propose the GIS algorithm using reliability-based attention (R-attention) maps. First, we transfer query object information from annotated frames to a target frame using R-attention maps, which represent pixel-wise reliability of the annotated frames. Next, we perform intersection-aware propagation to propagate segmentation results to neighboring frames sequentially. Third, we compute a guidance score, called R-score, to reduce or remove the processing time for selecting the frame to be annotated in each round. Experimental results demonstrate that the proposed GIS algorithm outperforms recent stateof-the-arts in both the interactive VOS simulation in <ref type="bibr" target="#b3">[4]</ref> and real-world applications.</p><p>This paper has three main contributions:</p><p>1. Two novel operators to exploit multiple annotations and neighboring results are developed for VOS: Rattention and intersection-aware propagation modules. 2. We propose the notion of guidance in interactive VOS. 3. The proposed GIS algorithm outperforms the state-ofthe-arts significantly in both speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised VOS: It is a task to find primary objects <ref type="bibr" target="#b13">[14]</ref> without any user annotations in video sequences. Traditional approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> use motion, object proposals, or saliency to solve this problem. Recently, with the availability of big VOS datasets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref>, many deeplearning-based unsupervised methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref> have been proposed.</p><p>Semi-supervised VOS: In semi-supervised VOS, a user provides fully annotated masks for target objects at the first frame. Many algorithms have been developed to extract significant features for target objects using user annotations. Early deep learning methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref> focused on fine-tuning networks using annotation masks at the first frames. Instead of the computationally demanding fine-tuning, some algorithms employ optical flow for initial segment propagation <ref type="bibr" target="#b11">[12]</ref> or motion feature extraction <ref type="bibr" target="#b8">[9]</ref>. Also, networks to refine segmentation results in previous frames without using motion have been developed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. Instead of the first frame, optimal frames to be annotated in semi-supervised VOS were determined in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Matching-based algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref> perform pixelwise feature matching between annotated and target frames to segment out target objects. For example, key-value memory operations based on non-local networks <ref type="bibr" target="#b35">[36]</ref> are performed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> to perform the matching between a target frame and already segmented frames. In <ref type="bibr" target="#b36">[37]</ref>, the selection network, which predicts scores of previously segmented frames, is used to choose the frames for segmentation propagation.</p><p>Interactive VOS: It aims at achieving satisfactory VOS re-sults through an iterative process of drawing simple annotations, such as scribbles, point clicks, or bounding boxes. Early interactive VOS algorithms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> constructed graph models, by connecting pixels with edges and then assigning edge weights using hand-craft features. Segmentation results for query objects were then obtained by graph optimization techniques.</p><p>Recently, deep-learning methods have been developed for interactive VOS. Benard and Gygli <ref type="bibr" target="#b1">[2]</ref> used point clicks to extract an object mask in a single frame and apply a semisupervised VOS algorithm to propagate the mask. Chen et al. <ref type="bibr" target="#b4">[5]</ref> employed pixel-wise metric learning to cut out a query object using a few point clicks. Caelles et al. <ref type="bibr" target="#b3">[4]</ref> introduced a round-based interactive VOS process and the automatic simulation algorithm to mimic human interactions in real applications. Many recent interactive algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> follow this round-based process.</p><p>Oh et al. <ref type="bibr" target="#b21">[22]</ref> developed two segmentation networks for interactive VOS: the first one estimates target object regions from user interactions and the second one propagates the segmentation results to neighboring frames. Miao et al. <ref type="bibr" target="#b19">[20]</ref> proposed networks to obtain segmentation results through both interaction and propagation. They employed global and local distance maps in <ref type="bibr" target="#b30">[31]</ref> to match a target frame to an annotated frame and the previous frame, respectively. Heo et al. <ref type="bibr" target="#b6">[7]</ref> designed global and local transfer modules to effectively transfer features in annotated and previous frames to a target frame. Oh et al. <ref type="bibr" target="#b23">[24]</ref> encoded annotation regions into keys and values in a non-local manner. These interactive VOS algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, however, have limitations. First, they do not consider the processing time to select the poorest segmentation results, on which additional annotations are provided. Second, they do not fully exploit the property that scribble data in multiple annotated frames have different reliability and different relevance to a target frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>The proposed algorithm cuts query objects off in a video I = {I 1 , . . . , I T } with T frames interactively using sparse annotations (scribbles or points) in each segmentation round. Let I ai at time instance a i be the annotated frame in the ith round. First, the segmentation is performed bidirectionally starting from I a1 . Subsequently, in the N -th round, it is also done bidirectionally, but using all previously annotated frames I a = {I a1 , . . . , I a N }. <ref type="figure" target="#fig_1">Figure 2</ref> shows how the proposed algorithm segments a target frame I t . First, we encode I t into the frame feature F t . Second, we obtain the interfused object feature G t by combining query object information in all annotated frames in I a using the R-attention module. Third, using the neighbor frame I n ? {I t?1 , I t+1 } and its segmentation result Y n , we perform the intersection-aware propagation to yield the  overlapped object feature H t . Last, the segmentation head decodes these three features F t , G t , and H t to generate the segmentation result Y t of the target frame I t .</p><p>Moreover, we propose a novel guidance mechanism for interactive VOS. From the R-attention module, we extract the reliability map R t to represent pixel-wise reliability of the segmentation result Y t . Also, by averaging these pixelwise scores, we obtain the R-score r t . These guidance data R t and r t enable a user to select a less reliable frame and provide annotations on it more quickly and more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Interfused Object Feature</head><p>As in <ref type="bibr" target="#b6">[7]</ref>, we transfer segmentation information in each annotated frame into a target frame. However, whereas <ref type="bibr" target="#b6">[7]</ref> combines the information from multiple annotated frames simply through averaging, we fuse transferred object features based on their reliability levels. To this end, we develop the R-attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transition matrix computation:</head><p>We encode each annotated frame I ai ? I a into F ai to obtain the annotated frame feature set F a = {F a1 , . . . , F a N }. Each frame feature is an HW ? C 1 matrix, in which each row contains the C 1dimensional feature vector for a pixel. Here, H ? W is the spatial resolution of the feature. Then, using the ith annotated feature F ai and the target feature F t , we obtain the transition matrix</p><formula xml:id="formula_0">A ai?t = softmax ? A (F t ) ? ? A (F ai ) T<label>(1)</label></formula><p>of size HW ? HW . Here, ? A is a feature transform, implemented by a learnable 1 ? 1 convolution, to reduce the dimension of each row vector from C 1 to C 2 . Note in <ref type="figure">Fig-ure</ref> 2 that a frame feature is used by different modules. To adapt the same feature for different purposes, we employ multiple feature transforms, including ? A .</p><p>In <ref type="formula" target="#formula_0">(1)</ref>, the softmax operation is applied to each column. Thus, A ai?t is a positive matrix with each column adding to 1. It is hence the transition matrix <ref type="bibr" target="#b28">[29]</ref>, whose entry in row r and column c represents the probability that the cth pixel in I ai is mapped to the rth pixel in I t . We compute the transition matrices from all annotated frames to I t to yield the transition matrix set A a?t = {A a1?t , . . . , A a N ?t }.</p><p>Object feature transfer: We use annotations on I ai to generate an object saliency map via a sparse-to-dense network in <ref type="figure">Figure 3</ref>. We adopt A-Net <ref type="bibr" target="#b6">[7]</ref>, the encoder of which is based on SE-ResNet50 <ref type="bibr" target="#b7">[8]</ref>, as the sparse-to-dense network. To form the feature of the query object, we combine three intermediate features: R3 and R5 features from the encoder and the context feature of the penultimate layer of the decoder. After making their spatial resolutions identical, we convolve and then concatenate them. Then, the concatenated feature passes through another convolution layer to form the object feature E ai of size HW ? C 3 . Consequently, the object feature set E a = {E a1 , . . . , E a N } is obtained from the N annotated frames.</p><p>We transfer all object features in E a to the target frame  <ref type="figure">Figure 3</ref>: A diagram of the object feature extraction.</p><formula xml:id="formula_1">I t by E t|ai = A ai?t ? E ai<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reliability Estimation</head><p>Aggregation Softmax Reliability Map for the query object segmentation. To address this issue, we propose the R-attention mechanism.</p><formula xml:id="formula_2">| 1 | | 1 | ( 1 ) 1 ? ( ) ? | 1 , ? , |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-Attention Maps</head><p>R-attention: <ref type="figure" target="#fig_2">Figure 4</ref> shows how to generate R-attention maps. Similar to (2), let F t2|t1 = A t1?t2 ?? R (F t1 ) denote the transferred frame feature from I t1 to I t2 , where ? R is a feature transform. We obtain the feature difference matrix</p><formula xml:id="formula_3">D t|ai = [F t|ai ? F t|t ] ?2<label>(3)</label></formula><p>where ?2 is the entry-wise power operator. Note that its entry D t|ai (p, c) equals the squared distance between the cth feature components of pixel p in F t|ai and F t|t . Ideally, all entries in D t|ai should be near zero, because both F t|ai and F t|t represent the same frame I t . However, they are not in practice, since the transition matrix A ai?t in <ref type="formula" target="#formula_1">(2)</ref> is imperfect. For the same reason, the transferred object feature E t|ai in (2) may be unreliable. Hence, we define the reliability map R t|ai for E t|ai as</p><formula xml:id="formula_4">R t|ai (p) = 1 max c D t|ai (p, c) + for each p,<label>(4)</label></formula><p>where is a small positive number to prevent division by zero. A large value of R t|ai (p) indicates that the pth row vector (i.e. feature vector for pixel p) in E t|ai is reliable. By applying the softmax function over the N reliability maps, we generate the R-attention map M t|ai for the transferred object feature E t|ai , which is given by</p><formula xml:id="formula_5">M t|ai (p) = exp R t|ai (p) N k=1 exp R t|a k (p)</formula><p>for each p. Next, we obtain the interfused object feature G t by fusing all transferred object features using the R-attention maps,</p><formula xml:id="formula_6">G t = N i=1 M t|ai ? E t|ai<label>(6)</label></formula><p>where ? means that M t|ai is multiplied entry-wise to each column in E t|ai . Through this R-attention mechanism, the interfused feature G t contains more reliable information about the query object than each individual E t|ai does. Furthermore, by aggregating R t|a1 , . . . , R t|a N , we generate the overall reliability map R t by</p><formula xml:id="formula_7">R t (p) = max i exp R t|ai (p) ? 1 .<label>(7)</label></formula><p>Note from <ref type="formula" target="#formula_4">(4)</ref> and <ref type="formula" target="#formula_7">(7)</ref> that each R t (p) is in the range [0, 1], with 1 indicating the maximum reliability level. A high R t (p) means that the interfused feature vector for pixel p in G t is reliable. Because G t plays an essential role in segmenting the target frame I t , R t (p) also represents the reliability of the segmentation result. Section ?? describes how to use R t to guide the interactive VOS process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overlapped Object Feature</head><p>When segmenting the target frame I t , the segmentation result Y n of the neighbor frame I n ? {I t?1 , I t+1 } is available according to the segmentation direction. We exploit this neighbor information to delineate the query object in I t more accurately. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we first obtain the neighbor similarity</p><formula xml:id="formula_8">S t = exp ? [? S (F t ) ? ? S (F n )] ?2<label>(8)</label></formula><p>using a feature transform ? S . S t represents how similar the features of I t and I n are to each other. A row vector in S t tends to have entries near one, when the corresponding pixel belongs to the intersection of the same object between the adjacent frames, as illustrated in <ref type="figure">Figure 6</ref>. Next, we convert F n via another feature transform ? Y and concatenate it with Y n . We convolve this concatenated signal to yield Y n that represents the query object in I n . We then concatenate S t and Y n and use another convolution layer to obtain the overlapped object feature H t . Note that S t indicates the overlapped region (or intersection) of the query object in I n and I t . Therefore, we combine S t and  <ref type="figure">Figure 6</ref>: An example of the neighbor similarity S t . The green pixels belong to the intersection, whereas the blue ones do not.</p><p>Y n to recognize the query object feature in the overlapped region, and hand over this information to the target frame I t via H t . This intersection-aware propagation of object information enables the segment head to exploit the neighbor information selectively and reliably.</p><p>Last, we use the frame feature F t , the interfused object feature G t , and the overlapped object feature H t to obtain the segmentation mask Y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Guided Interactive VOS Process</head><p>In the first round, to segment the target frame I t = I a1 , we set I n = I a1 and substitute Y n with the saliency map of I t generated by the sparse-to-dense network. This is because there is no neighbor frame already segmented. After performing the segmentation of I a1 , we propagate the segmentation mask Y a1 bidirectionally to segment the other frames. In the second round, sparse annotations are given on I a2 to refine its segmentation result Y a2 , which is also propagated bidirectionally until another annotated frame is met. This is repeated until the user is satisfied with the VOS result. <ref type="figure" target="#fig_6">Figure 7</ref> illustrates this process.</p><p>Suppose that there are K query objects with their annotations. Then, for each query object in each target frame I t , the segmentation head uses the three features F t , G t , H t to estimate the object probability map. Consequently, we have? t,1 , . . . ,? t,K , where? t,k denotes the probability map for the kth query object. By applying the soft aggregation scheme <ref type="bibr" target="#b22">[23]</ref> to these maps and then allocating each pixel to the background or the query object with the highest probability, we yield the binary segmentation masks Y t,1 , . . . , Y t,K at the target frame.</p><p>In interactive VOS, it is important to enable the user to provide annotations quickly with less effort. Therefore, after performing the segmentation in each round, we compute the R-score r t of each frame I t ,</p><formula xml:id="formula_9">r t = ? U t p?U R t (p) + 1 ? ? O t p?O R t (p)<label>(9)</label></formula><p>where U t is the set of pixels in the entire frame and O t is the union set of pixels in the segmented object regions. When ? = 0, the pixel-wise reliability in <ref type="formula" target="#formula_7">(7)</ref> is averaged over the foreground segments only. When ? = 1, it is averaged over  the entire frame. In this work, ? is set to 0.5 to consider all pixels but also to emphasize the foreground segments.</p><p>Guided selection of annotated frames: In practice, it takes considerable time to find the most poorly segmented frame and provide annotations. To alleviate this problem, from the second round, the R-scores {r 1 , ..., r T } are used to guide the user to provide additional annotations for the next round. Instead of a time-consuming search over the entire video, the user can select the frames for annotations in two ways.</p><p>? RS1: The single frame with the lowest R-score is chosen for next annotations.</p><p>? RS4: The four frames with the lowest R-scores are determined subject to the constraint that their time distances are at least T /10. In the interactive VOS simulation, the most poorly segmented frame is selected among the four frames, by comparing the segmentation results with the ground-truth. In real applications, users are provided with the segmentation results of these four guided frames only. Then, the user chooses a frame among them and provides annotations. The interactive process terminates, when the user is satisfied with the guided frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Network details: To encode each frame I t to the frame feature F t , we employ SE-ResNet50 <ref type="bibr" target="#b7">[8]</ref> from the first layer to R4 with an output stride 8. Thus, H and W in Section 3.1 are 1 8 of the height and width of an input frame, respectively. The dimension C 1 of each frame feature vector is 1,024, while the dimensions C 2 of output vectors of the four feature transforms ? A , ? R , ? S , and ? Y are equally set to 128. Also, C 3 for E t , G t , and H t is set to 256. For the segmentation head, we adopt the decoder architecture in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Training: We use the training sets of DAVIS2017 <ref type="bibr" target="#b25">[26]</ref> and YouTube-VOS <ref type="bibr" target="#b37">[38]</ref>. To emulate the first round, we randomly form a mini-sequence by taking five consecutive frames (one annotated frame and four target frames) from a video sequence. To emulate the second round, we pick one additional frame as the second annotated frame. In the training, we proceed up to the second round due to limited GPU memories. To imitate sparse annotations, we use two types: 1) random points and 2) scribble generation in <ref type="bibr" target="#b3">[4]</ref>. More implementation details are in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>First, we compare the proposed GIS algorithm with the state-of-the-art interactive VOS algorithms. Second, we analyze the proposed algorithm through various ablation studies and visualization of feature maps. Third, we perform a user study to demonstrate the effectiveness of the proposed algorithm in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparative Assessment</head><p>Interactive VOS simulation is conducted on two datasets: DAVIS2017 <ref type="bibr" target="#b25">[26]</ref> and YouTube-IVOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS2017:</head><p>In the DAVIS interactive VOS simulation <ref type="bibr" target="#b3">[4]</ref>, human interactions are emulated up to 8 rounds by an algorithm. In each round, after VOS is performed, the algorithm determines the frame with the poorest performance, by comparing the segmentation results with the groundtruth, and provides additional annotations on it. We follow this procedure for the comparison with conventional algorithms. We also follow the two guided procedures RS1 and RS4 in Section 3.3 to confirm the effectiveness of R-scores. The validation set of 30 video sequences in DAVIS2017 is used for the assessment. For each video sequence, three distinct initial scribbles are provided, which means that the performance is averaged over 90 interactive VOS trials.</p><p>We quantify the segmentation performance using the region similarity (J) and the contour accuracy (F). We measure the area under the curve (AUC) of a performanceversus-time graph from 0 to 488 seconds for J score (AUC-J) or for joint J and F scores (AUC-J&amp;F). Also, we measure the performance at 60 seconds for J score (J@60s) or for joint J and F scores (J&amp;F@60s) to assess how accurately the segmentation is carried out within 60 seconds. <ref type="table" target="#tab_2">Table 1</ref> compares the proposed GIS algorithm with the recent state-of-the-art algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, in which Oh et al. <ref type="bibr" target="#b21">[22]</ref> Maio et al. <ref type="bibr" target="#b19">[20]</ref> Heo et al. <ref type="bibr" target="#b6">[7]</ref> Oh et al. <ref type="bibr" target="#b23">[24]</ref> Proposed-GT Number of rounds J&amp;F <ref type="figure">Figure 8</ref>: Comparison of J&amp;F scores on the DAVIS2017 validation set according to the rounds.</p><p>Proposed-GT, Proposed-RS1, and Proposed-RS4 denote the settings when the ground-truth, RS1, and RS4 are used to choose frames to be annotated in next rounds, respectively. Note that Proposed-GT has the same experimental conditions as the conventional algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. In Proposed-RS4, the DAVIS algorithm selects the poorest frame among the four guided frames using the groundtruth. In all settings, the DAVIS algorithm provides annotations. The scores of the conventional algorithms are provided by the respective authors. It can be observed from <ref type="table" target="#tab_2">Table 1</ref> that the proposed algorithm outperforms the stateof-the-arts by significant margins in all metrics. In other words, the proposed algorithm performs the best in both accuracy and speed. Also, Proposed-RS1 and Proposed-RS4 perform better than Proposed-GT, by employing R-scores and selecting annotated frames effectively. <ref type="figure">Figure 8</ref> shows the J&amp;F scores according to the rounds. The proposed algorithm yields the best score in every round with no exception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube-IVOS:</head><p>For extensive experiments, we construct the YouTube-IVOS dataset from YouTube-VOS <ref type="bibr" target="#b37">[38]</ref>, which is the largest VOS dataset. For the DAVIS algorithm to emulate user interactions, ground-truth segmentation masks are needed. Since the validation set in YouTube-VOS does not provide the ground-truth, we sample 200 videos from its training set to compose YouTube-IVOS. For each video, we generate four different initial annotations by varying the number of point clicks. Specifically, we randomly pick 5, 10, 20, and 50 point clicks from the ground-truth mask for each query object and then use those clicks as annotations in the first round. The interactive VOS is performed up to 4 rounds, since the performance is saturated in early rounds due to the short lengths of the YouTube-VOS videos. <ref type="table" target="#tab_3">Table 2</ref> compares the average J&amp;F scores of the proposed algorithm with those of Miao et al. <ref type="bibr" target="#b19">[20]</ref> and Heo et al. <ref type="bibr" target="#b6">[7]</ref> according to the rounds. Notice that <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b23">[24]</ref> are not compared in this test, because their full source codes are unavailable. In this test, the proposed GIS network and the Heo et al.'s network are trained without the 200 videos in YouTube-IVOS. We see that the proposed algorithm outperforms the other algorithms meaningfully in all rounds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis</head><p>Ablation study: We analyze the effectiveness of three components in the proposed algorithm:</p><p>(1) R-attention (RA)</p><p>(2) Intersection-aware propagation (IAP) (3) R-score guidance with four candidate frames (RS4) <ref type="figure" target="#fig_8">Figure 9</ref> plots the J&amp;F scores of four settings A, B, C, and D, combining these three components, on the DAVIS2017 validation set. First, using R-attention maps, the overall performance in every round increases significantly (gaps between A and B). Also, the intersection-aware propagation improves the performance especially in early rounds (gaps between B and C). The R-score guidance affects the performance only slightly (gaps between C and D), because in this test the computer searches the frames to be annotated using the ground-truth. In real applications, the R-score guidance enables human users to search the frames efficiently and thus reduces the overall segmentation time, as will be verified in later experiments.</p><p>Intersection-aware propagation: We compare the proposed IAP module with two existing propagation methods: the local distance map (LDM) in <ref type="bibr" target="#b19">[20]</ref> and the local transfer module (LTM) in <ref type="bibr" target="#b6">[7]</ref>. LDM matches each pixel in a target frame to a local region in a neighbor frame at the feature level to estimate a local distance map, while LTM transfers the segmentation result of a neighbor frame based on the local affinity. In <ref type="table" target="#tab_4">Table 3</ref>, the baseline means the proposed network without IAP. In other words, the baseline uses only the frame feature F t and the interfused object feature G t for the segmentation. We plug LDM or LTM into the baseline. <ref type="table" target="#tab_4">Table 3</ref> compares the J&amp;F scores in 1st, 3rd, and 5th rounds and the segmentation speeds (frames per second, FPS). Compared with LDM and LTM, the proposed IAP helps the baseline network to achieve higher segmentation accuracies and a faster speed.</p><p>R-attention maps: <ref type="figure" target="#fig_0">Figure 10</ref> illustrates R-attention maps M t|ai in <ref type="bibr" target="#b4">(5)</ref>. Specifically, the first and second rows in <ref type="figure" target="#fig_0">Figure 10</ref> show the R-attention maps of the first annotation at I a1 and the second annotation at I a2 , respectively. We sample time instances t 1 ? t 5 uniformly between a 1 and a 2 .</p><p>Note that R-attention values depend on the pixel-wise feature similarities between annotated and target frames. For instance, M t4|a1 has low R-attention values on the query objects (cart and two people), which have significantly different appearance and sizes between I a1 and I t4 . Also, M t3|a2 has low R-attention values around the plywood ramp that almost disappears in I a2 . This example indicates that R-attention module faithfully provides the reliability information of the transferred object feature from the annotated frame to the target frame.</p><p>Reliability maps and R-scores: Examples of reliability maps and R-scores are in <ref type="figure" target="#fig_0">Figure 11</ref>. In the first round, a poor segmentation result for a woman marked in yellow is obtained, because she is partly occluded by a man riding a bike. Thus, the proposed algorithm yields the reliability map that has low values within the black box containing the woman. As the segmentation result is refined in subsequent rounds, the reliability map has higher values and the R-score gets larger. This means that both the reliability map R t in (4) and the R-score r t in (9) are good indicators of how accurately the target frame I t is segmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">User Study</head><p>We conducted a user study to assess the proposed GIS algorithm in real applications. We recruited 12 volunteers, who provided scribbles iteratively until they were satisfied. We measured the average time in seconds per video (SPV), including the time for providing scribbles, the running time of the algorithm, and the time for finding unsatisfactory frames. Also, we measured the average rounds per video (RPV) and the average J&amp;F score over all video sequences.   <ref type="figure" target="#fig_0">Figure 11</ref>: The reliability maps and R-scores for the 33rd frame of the "India" video in the first, third, and eighth rounds. The number in the upper right corner of each reliability map is the R-score. <ref type="table" target="#tab_6">Table 4</ref> compares these user study results on the validation set in DAVIS2017 <ref type="bibr" target="#b25">[26]</ref>. 'Proposed w/o RS' denotes the proposed algorithm without using R-scores. All three settings of the proposed algorithm outperform the state-of-theart algorithm <ref type="bibr" target="#b6">[7]</ref> in all metrics. This indicates that the proposed algorithm requires less running time and less interaction, while providing better segmentation results. Proposed-RS1 and Proposed-RS4 require less time to complete the process than 'Proposed w/o RS' with only negligible performance degradation, by removing or reducing the time for selecting unsatisfactory frames as shown in <ref type="figure" target="#fig_0">Figure 12</ref>. Especially, the total time for segmenting a video is significantly reduced in Proposed-RS1, which needs no time for inspection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed the novel GIS algorithm for video objects based on R-attention and intersection-aware propagation. First, the interfused object feature is extracted by transferring query object information from annotated frames to a target frame using the R-attention module. Second, the overlapped object feature is obtained via the intersectionaware propagation using a neighbor frame. Then, the segmentation is performed using the frame feature, interfused  object feature, and overlapped object feature. Moreover, we developed the GIS mechanism that enables users to determine next annotated frames quickly. Experimental results showed that the proposed algorithm outperforms the conventional algorithms significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the round-based interactive VOS. The proposed algorithm greatly reduces the inspection time, by guiding users to select a frame for annotations efficiently and effectively. It is recommended to watch the supplementary video with a real-time demo of the proposed guided interactive system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed GIS algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>A diagram of the R-attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>A diagram of the intersection-aware propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of the interactive VOS process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Ablation study results on the DAVIS2017 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of R-attention maps and segmentation results on the "soapbox" sequence in the second round.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>The total time and the inspection time for selecting unsatisfactory frames per video. Each user is represented by each mark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>using the transition matrix A ai?t in (1). Thus, we have the transferred object feature set E t|a = {E t|a1 , . . . , E t|a N }, which encodes the object information in I t approximately. Since the reliability of the transition matrix A ai?t is different for each i, it is unreasonable to exploit E t|ai equally</figDesc><table><row><cell></cell><cell cols="2">Sparse-to-Dense</cell></row><row><cell></cell><cell cols="2">Network</cell></row><row><cell>Annotated frame a</cell><cell></cell><cell></cell><cell>Saliancy map</cell></row><row><cell></cell><cell cols="2">Context feature</cell></row><row><cell></cell><cell></cell><cell>Conv</cell></row><row><cell></cell><cell>R5</cell><cell>Conv</cell><cell>Conv</cell></row><row><cell></cell><cell>R3</cell><cell>Conv</cell><cell>Object</cell></row><row><cell>Sparse annotations</cell><cell cols="3">Object feature extractor</cell><cell>feature a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparative assessment of the proposed algorithm with the state-of-the-art interactive VOS algorithms on the DAVIS2017 validation set. The best results are boldfaced.</figDesc><table><row><cell></cell><cell>AUC-J</cell><cell>J@60s</cell><cell>AUC-J&amp;F</cell><cell>J&amp;F@60s</cell></row><row><cell>Oh et al. [22]</cell><cell>0.691</cell><cell>0.734</cell><cell>0.778</cell><cell>0.787</cell></row><row><cell>Miao et al. [20]</cell><cell>0.749</cell><cell>0.761</cell><cell>0.787</cell><cell>0.795</cell></row><row><cell>Heo et al. [7]</cell><cell>0.771</cell><cell>0.790</cell><cell>0.809</cell><cell>0.827</cell></row><row><cell>Oh et al. [24]</cell><cell>-</cell><cell>-</cell><cell>0.839</cell><cell>0.848</cell></row><row><cell>Proposed-GT</cell><cell>0.817</cell><cell>0.826</cell><cell>0.853</cell><cell>0.863</cell></row><row><cell>Proposed-RS1</cell><cell>0.818</cell><cell>0.827</cell><cell>0.855</cell><cell>0.864</cell></row><row><cell>Proposed-RS4</cell><cell>0.820</cell><cell>0.829</cell><cell>0.856</cell><cell>0.866</cell></row><row><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparative assessment of the proposed algorithm with the state-of-the-art interactive VOS algorithms on the Youtube-IVOS dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">J&amp;F-1st</cell><cell cols="2">J&amp;F-2nd</cell><cell cols="3">J&amp;F-3rd</cell><cell cols="2">J&amp;F-4th</cell></row><row><cell cols="4">Miao et al. [20]</cell><cell></cell><cell></cell><cell cols="2">0.525</cell><cell cols="2">0.620</cell><cell></cell><cell>0.674</cell><cell></cell><cell cols="2">0.706</cell></row><row><cell cols="4">Heo et al. [7]</cell><cell></cell><cell></cell><cell cols="2">0.643</cell><cell cols="2">0.721</cell><cell></cell><cell>0.768</cell><cell></cell><cell cols="2">0.797</cell></row><row><cell cols="4">Proposed-GT</cell><cell></cell><cell></cell><cell cols="2">0.672</cell><cell cols="2">0.754</cell><cell></cell><cell>0.806</cell><cell></cell><cell cols="2">0.830</cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>J&amp;F</cell><cell>0.80</cell><cell></cell><cell cols="5">w/o RA, IAP, and RSG RA IAP RS4</cell><cell>J&amp;F</cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell cols="2">RA IAP RS4</cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell cols="4">w/o IAP and RSG Setting A Setting B ? w/o RSG Setting C ? ?</cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell cols="4">Setting A Setting B ? Setting C ? ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Proposed Setting D ? ? ?</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Setting D ? ? ?</cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Number of rounds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Time (seconds)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the proposed intersection-aware propagation (IAP) module with conventional local propagation methods on the DAVIS2017 validation dataset.</figDesc><table><row><cell></cell><cell>J&amp;F-1st</cell><cell>J&amp;F-3rd</cell><cell>J&amp;F-5th</cell><cell>FPS</cell></row><row><cell>Baseline</cell><cell>0.717</cell><cell>0.821</cell><cell>0.839</cell><cell>9.26</cell></row><row><cell>Baseline+LDM [20]</cell><cell>0.727</cell><cell>0.824</cell><cell>0.844</cell><cell>8.23</cell></row><row><cell>Baseline+LTM [7]</cell><cell>0.730</cell><cell>0.828</cell><cell>0.846</cell><cell>8.69</cell></row><row><cell>Baseline+IAP</cell><cell>0.737</cell><cell>0.832</cell><cell>0.850</cell><cell>8.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>User study results.</figDesc><table><row><cell></cell><cell>SPV(s)</cell><cell>RPV</cell><cell>J&amp;F</cell></row><row><cell>Heo et al. [7]</cell><cell>66.83</cell><cell>2.42</cell><cell>0.769</cell></row><row><cell>Proposed w/o RS</cell><cell>46.01</cell><cell>1.89</cell><cell>0.794</cell></row><row><cell>Proposed-RS1</cell><cell>34.59</cell><cell>1.82</cell><cell>0.789</cell></row><row><cell>Proposed-RS4</cell><cell>37.10</cell><cell>1.69</cell><cell>0.794</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CNN in MRF: Video object segmentation via inference in a CNN-based higher-order spatiotemporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The 2018 DAVIS challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BubbleNets: Learning to select the guidance frame in video object segmentation by deep sorting frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using global and local transfer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motionguided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">POD: Discovering primary objects in videos based on evolutionary refinement of object recurrence, background, and primary object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequential clique optimization for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AGSS-VOS: Attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention Siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1515" to="1530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memory aggregation networks for efficient interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Space-time memory networks for video object segmentation with user guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The 2017 DAVIS Challenge on Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LIVEcut: Learningbased interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Linear transformations. In Introduction to Linear Algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>5th Ed</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FEELVOS: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memory selection network for video propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">YouTube-VOS: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anchor diffusion for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning discriminative feature with crf for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MATNet: Motion-attentive transition network for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8326" to="8338" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
