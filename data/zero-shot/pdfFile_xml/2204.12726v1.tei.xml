<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRE-NAS: Predictor-assisted Evolutionary Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yameng</forename><surname>Peng</surname></persName>
							<email>yameng.peng@student.rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Song</surname></persName>
							<email>andy.song@rmit.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vic</forename><surname>Ciesielski</surname></persName>
							<email>vic.ciesielski@rmit.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haytham</forename><forename type="middle">M</forename><surname>Fayek</surname></persName>
							<email>haytham.fayek@ieee.org</email>
							<affiliation key="aff3">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
							<email>xiaojun.chang@uts.edu.au</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Sydney Sydney</settlement>
									<region>New South Wales</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PRE-NAS: Predictor-assisted Evolutionary Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Artificial intelligence</term>
					<term>Search methodologies</term>
					<term>Discrete space search</term>
					<term>? Networks ? Network performance modeling KEYWORDS Evolutionary algorithm, architecture search, performance predictor</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) aims to automate architecture engineering in neural networks. This often requires a high computational overhead to evaluate a number of candidate networks from the set of all possible networks in the search space during the search. Prediction of the networks' performance can alleviate this high computational overhead by mitigating the need for evaluating every candidate network. Developing such a predictor typically requires a large number of evaluated architectures which may be difficult to obtain. We address this challenge by proposing a novel evolutionary-based NAS strategy, Predictor-assisted E-NAS (PRE-NAS), which can perform well even with an extremely small number of evaluated architectures. PRE-NAS leverages new evolutionary search strategies and integrates high-fidelity weight inheritance over generations. Unlike one-shot strategies, which may suffer from bias in the evaluation due to weight sharing, offspring candidates in PRE-NAS are topologically homogeneous, which circumvents bias and leads to more accurate predictions. Extensive experiments on NAS-Bench-201 and DARTS search spaces show that PRE-NAS can outperform state-of-the-art NAS methods. With only a single GPU searching for 0.6 days, competitive architecture can be found by PRE-NAS which achieves 2.40% and 24% test error rates on CIFAR-10 and ImageNet respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite the overwhelming success of deep learning, the high computational cost associated with model development remains a challenge in the field, especially in real-world applications which often require carefully constructing complicated structures such as ResNet <ref type="bibr" target="#b14">[15]</ref>, DenseNet <ref type="bibr" target="#b15">[16]</ref> and manually tuning the hyperparameters <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. One approach to address this challenge is NAS (neural architecture search). The aim is to automatically create a competitive neural network for a given task. A fully-machinedesigned neural network <ref type="bibr" target="#b42">[43]</ref> can achieve a test accuracy of 96.35% on the CIFAR-10 dataset, in comparison with 96.54% from DenseNet <ref type="bibr" target="#b15">[16]</ref>, a hand-designed neural network. However, in this task, NAS requires 800 Tesla K40 GPUs parallel running for nearly one month. This high computational cost compromises the saving on manual architecture design. Hence, reducing of the computational costs is one of the key targets in NAS.</p><p>One way to reduce the cost in NAS is to cut down evaluation, for example, using fewer epochs or a small portion of data to train the networks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. However, this approach may lead to inadequate training, and therefore inaccurate results. Another approach, learning curve extrapolation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, predicts the tendency of network optimisation based on early epochs in training. Similarly, a surrogate model can be built to predict the performances of candidate networks <ref type="bibr" target="#b20">[21]</ref>. Predictor-based evaluation methods require their own training, hence there is a need to sample networks from the search space with associated high computational cost. Weights sharing provides a good alternative, in particular, the one-shot model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>, which treats all candidate networks as sub-networks of an over-parameterised super-network. Subnetworks within the same search space can share weights, hence computational costs can be reduced. However sharing weights between heterogeneous architectures is problematic, and can easily lead to incorrect ranking of the candidate networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Hence, this work addresses the balance between reducing computational cost and improving network ranking by proposing a predictor-assisted evolutionary architecture search algorithm (PRE-NAS). It is driven by an evolutionary algorithm that is based on a population of candidate networks rather than one network. We introduce several new strategies to train predictors more effectively, especially with extremely limited training samples. Firstly, elitist evolution is introduced to maintain a good pool of candidates. Secondly, multi-mutation and a representative selection strategy are introduced. These strategies can improve the training set for the predictor by heuristically increasing the number of mutations and sampling representative candidates from each generation. So the predictor can evaluate multiple candidate networks with no significant cost increase. In addition, a high-fidelity weight inheritance is incorporated to reduce the computational cost further.</p><p>The contributions of this work are summarised as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>NAS is generally studied from three perspectives: search algorithm, search space, and network evaluation strategy. Typical search algorithms include reinforcement learning (RL), evolutionary algorithm (EA), gradient-based and Bayesian optimisation (BO). Search space is continuous in gradient-based methods like DARTS <ref type="bibr" target="#b21">[22]</ref>. Thus, the search space and evaluation strategy are tightly coupled. In contrast, search space in evolutionary-based <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> and reinforcement learning-based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref> methods is usually discrete, offering better flexibility and compatibility for network evaluation strategies. RL is more costly in comparison. Hence we leverage the advantages of EA as the search algorithm in this NAS study. There are many successful evolutionary or predictor-assisted NAS work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. AmoebaNet <ref type="bibr" target="#b25">[26]</ref> adopts a regularised evolutionary algorithm called Aging Evolution. It uses one mutation operation to generate a new architecture and discards the oldest architecture from the population in each search cycle. However, it is not efficient. Duplicate offspring are not checked so it is possible to see identical architectures in two different search cycles. The oldest individual could be the best-performing one in the population, so adding more unnecessary cost onto the search. PNAS <ref type="bibr" target="#b20">[21]</ref> adopts a sequential model-based optimisation (SMBO) search algorithm and a multi-layer perceptron (MLP) ensemble predictor. PNAS search starts from shallow cell networks and progresses to complex ones. During the architecture search, a predictor is trained to predict the performance of candidate networks without needing to train all of the candidate networks. After evaluation, top-candidates are selected for further training <ref type="bibr" target="#b20">[21]</ref>. This strategy needs to define a suitable which may change for different tasks.</p><p>Besides, in order to calibrate the performance predictor during architecture search, we adopt weight inheritance training for a few representative candidate networks. Unlike other work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref> inheriting weights between variable-size of generation, our work adopts fixed-size weight inheritance. The topology of parent and offspring networks are similar, hence helpful for function preservation during weight inheritance. In PRE-NAS, we propose a percentile representative selection strategy that is more generalizable than the top-strategy. PRE-NAS incorporates the flexibility of the EA, efficiency of performance prediction and weight inheritance to address the aforementioned issues in NAS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PREDICTOR-ASSISTED EVOLUTIONARY NAS</head><p>The details of PRE-NAS are described here, including the search space, search algorithm, multi-mutation, representative selection, performance predictor, and weight inheritance training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Space of Cell-based Networks</head><p>The search space is defined by the specific task and the chosen representation. Two search spaces, NAS-Bench-201 <ref type="bibr" target="#b10">[11]</ref> and DARTS <ref type="bibr" target="#b21">[22]</ref> are studied here. They are both cell-based search spaces, as many well-performing networks have been manually designed based on this representation. <ref type="figure" target="#fig_0">Fig. 1</ref> is an illustration of the construction of a cell-based network by stacking repeated modules together <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. The shallow network in the oval callout on the right is called a Cell Network. Each cell network consists of several pre-defined operations. A complete network can be constructed by stacking these cells ( <ref type="figure" target="#fig_0">Fig. 1 left)</ref>. The number in a stack, e.g. the depth of the network, depends on the difficulty of the target task. Thus, the search algorithm only needs to focus on the microstructure of a cell network. The search space of NAS-Bench-201 <ref type="bibr" target="#b10">[11]</ref> is relatively simple as it aims to provide a fair environment for comparison between NAS algorithms. The elements are four nodes (one input node, two intermediate nodes and one output node) and five pre-defined network operations (none, skip-connection, 1x1 convolution, 3x3 convolution, 3x3 average pooling) connecting these nodes. So the search space consists of 5 6 = 15625 architectures. Thus, it is possible to exhaustively train all 15625 architectures for 200 epochs on three different image datasets and record their individual performance. Then the accuracy of a candidate network can be obtained by querying the records instead of training from scratch. DARTS space appears much more recently <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. It has a much larger and more complex search space than NAS-Bench-201. There are eight different operations available in the search space of DARTS: 3 ? 3 and 5 ? 5 separable convolutions, 3 ? 3 and 5 ? 5 dilated convolutions, 3 ? 3 max pooling, 3 ? 3 average pooling, skip connection and none. Each cell network has 2 input nodes, 4 intermediate nodes and 1 output node, leading to 4 =1 ( +1) 2 ? 7 2 ? 10 9 possible architectures. Thus, more complex and powerful neural architectures can be found in this search space. This study adopts a similar setup as DARTS, except that DARTS searches for a Reduction Cell with a different structure from the Normal Cell. Our Reduction Cell and Normal Cell have the same structure, similar to the strategy in PNAS <ref type="bibr" target="#b20">[21]</ref> and BONAS <ref type="bibr" target="#b27">[28]</ref>.</p><p>The encoding of the above two search spaces both use upper triangular adjacency matrices, as demonstrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. A cell network is represented as an upper triangular matrix where the number in a cell indicates the network operation connecting the two nodes represented in the row and in the column, similar to the encoding schemes in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. This matrix is also the input to our performance predictor. Note that the output (No. 9) in the operation list is to demonstrate the encoding scheme, but it is not involved in the actual architecture search, e.g, mutations or predictor training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evolutionary Search Algorithm</head><p>As the evolutionary search algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref> have shown competitiveness in flexibility and performance perspectives over RL, BO, and gradient-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43]</ref>, it is the basis of our PRE-NAS. But unlike other works such as Amoe-baNet <ref type="bibr" target="#b25">[26]</ref> which uses aging evolution, PRE-NAS adopts an elitism strategy. Note that, EA algorithms in NAS usually manipulate and update only a few networks in a population as the cost of operating on all networks can be prohibitive <ref type="bibr" target="#b6">[7]</ref>.</p><p>The details are shown in Algorithm 1. Steps 4-8 comprise the initialisation phase which prepares the population by randomly generating architectures (Step 5) and using conventional training to obtain validation accuracy on the target task (Step 6). A</p><formula xml:id="formula_1">Algorithm 1 PRE-NAS Require: Population size P, Search cycle C, Sample size S 1: population ? ? 2: history ? ? 3: Children ? ? 4: while &lt; do 5: model.arch ? ? () 6: model.valid_accuracy ? ( . ?) 7:</formula><p>Add to the <ref type="bibr">8:</ref> Add to ? 9: end while 10: while not fulfilled do <ref type="bibr">11:</ref> Predictor ? training the predictor <ref type="bibr">12:</ref> candidates ? randomly sample architectures from the <ref type="bibr">13:</ref> parent ? best-performing one in the <ref type="bibr">14:</ref> while _ not fulfilled do <ref type="bibr">15:</ref> child.arch ? ( . ?) <ref type="bibr">16:</ref> child.valid_accuracy ? ( ? . ?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>Add ? to ? 18: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>Representatives ? select few representatives from ? 20:</p><p>for all ? ? do <ref type="bibr">21:</ref> child.valid_accuracy ? ? _ ( ? . ?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>Add ? to ? <ref type="bibr">23:</ref> end for <ref type="bibr">24:</ref> Calculate Spearman coefficient between predicted and trained accuracy of Representatives;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>Enlarge _ if Spearman coefficient higher than previous <ref type="bibr">26:</ref> Add the best-performing ? to the <ref type="bibr">27:</ref> Remove the worst ? from the 28: end while 29: end contains architecture and its accuracy will be added into the and ? (Steps 7&amp;8). The training samples for the performance predictor are stored in the ? . After the initialisation, the search begins. The key differences compared to other EAs are: (1) multi-mutation is used to generate a group of child networks (Step 14); (2) the performance predictor is used to predict the accuracy of child networks (Step 16) which allows us to evaluate many architectures with a much lower cost; (3) a representative selection strategy is adopted to pick distinctive architectures (Step 19); (4) weight inheritance is used in offspring training (Step 21). The Spearman coefficient is calculated between predicted and trained accuracy (Step 24). A high Spearman coefficient indicates the predictor can better predict the tendency of the child networks.  <ref type="figure" target="#fig_2">Fig. 3</ref> shows two mutation operators in PRE-NAS. Each graph is a cell network and a grey circle is a network node. Coloured lines represent different network operations, e.g. convolution or pooling. An arrow represents the direction of the tensor flow. The operation mutation ( <ref type="figure" target="#fig_2">Fig. 3a)</ref> generates an offspring network from a parent network by randomly choosing a network operation and replacing it with another randomly picked operation. The connection mutation <ref type="figure" target="#fig_2">(Fig. 3b</ref>) is similar but changes the destination node of an operation. Theoretically, it is possible to explore the entire space by alternating these two mutation operators. The AmoebaNet <ref type="bibr" target="#b25">[26]</ref> suffers from high computational overhead, as it only generates one offspring by mutating the parent in each search cycle. In PRE-NAS, a multimutation strategy is used to take advantage of the predictor so all possible offspring networks of a given parent can be generated and evaluated in each search cycle to better cover the search space. The total number of possible offsprings from a parent network can be calculated by 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-mutation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Representative Selection</head><p>After mutation, the performances of offspring networks will be predicted by our performance predictor. Good candidate networks can be picked by the top-strategy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. However, this strategy is not adequate to cover the entire distribution of different scenarios, thus, it is hard to train a predictor with good generalisation. Besides, it needs a way to determine the optimal . To better maintain diversity and coverage, we propose a representative selection strategy that selects representative architectures from the ? , not just the best ones. More specifically, the architectures are selected based on the statistical percentile of predicted validation accuracy: the architectures at the maximum percentile, 75% point, 50% point, 25% point, and the minimum percentile. After the selection, the ground-truth accuracy of these representative candidates can be obtained by training them on the target dataset. The Spearman Ranking Correlation Coefficients (a monotonic relationship between two groups) are calculated between their predicted accuracy and ground-truth accuracy of the representative architectures. The higher the Spearman value, the better ranking of the predictor. Further, the representative architectures and their accuracy values will be added to the ? to be the samples for further training of the performance predictor. More exhibitions and discussions on the advantages of our representative selection strategy will be introduced in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Performance Predictor</head><p>The desired characteristics of a PRE-NAS predictor are: (1) it can learn from limited training samples, as the cost of obtaining the target value (validation accuracy) is expensive; (2) it contains only a few model parameters, as training a big model will slow down the overall search process. The encoding scheme described in the Section 3.1, architectures are encoded as upper triangular adjacency matrices. Multi-mutations are applied to this encoding, which will also be the input to the performance predictor. Note that architectures from different search spaces would have matrices of different sizes. For instance, architectures from the search space of NAS-Bench-201 will be encoded as a 4 ? 4 matrix, architectures from DARTS will be encoded as an 11 ? 11 matrix <ref type="figure" target="#fig_1">(Fig. 2)</ref>.</p><p>The performance predictors we considered are Random Forest Regressor, Support Vector Regressor, Bayesian Ridge Regressor, Kernel Ridge, Linear Regressor, and Multi-Layer Perceptron, due to their computational efficiency. These predictors are tested on the NAS-Bench-201. To simulate the evaluation process during the architecture search, we randomly sample 100 architectures and their accuracy as the training data (less than 1% of the total amounts of the search space) and randomly sample another 100 pairs from the rest of 15525 architectures as the test data. We record the Spearman coefficient between predicted and ground-truth accuracy. 500 independent experiments show that Random Forrest is the best performing and most consistent predictor when facing extremely limited training samples. The results are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictor</head><p>Spearman Coefficient Random Forest 0.65?0.08 Support Vector 0.39?0.11 Bayesian Ridge 0.04?0.11 Kernel Ridge 0.06?0.10 Linear Regressor 0.04?0.11 Multi-Layer Perceptron 0.06?0.10 <ref type="table">Table 1</ref>: Results of 6 regressors trained with 100 samples. Each regressor is trained with the same data in each experiment. We record the Spearman coefficient between predicted and ground-truth accuracy, and report the mean and standard deviation based on 500 independent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">High-Fidelity Weight Inheritance</head><p>Similar to the well known one-shot model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, weight inheritance is also based on the weight sharing technique. One-shot allows sharing the weights across the entire search space, even for heterogeneous architectures. Weight inheritance however only shares weights between topologically homogeneous networks, e.g., parent and offspring networks. Thus, evolutionary search is naturally suitable for the weight inheritance training, because offspring networks are highly similar in terms of topology as they are generated from parent networks by mutating particular operations or connections <ref type="figure" target="#fig_3">(Fig. 4)</ref>. Unlike previous work on the variable size of networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>, we adopt a fixed size weight inheritance strategy to best preserve the network function. Moreover, we use Kaiming Norm <ref type="bibr" target="#b13">[14]</ref> to initialise the mutated connection or operation weights. Solid lines indicate that these operations will inherit the weights from the parent network. Dash lines indicate that the weights of these operations will reset.</p><p>To investigate the effectiveness of our weight inheritance, we generated 350 candidate networks by implementing multi-mutation on randomly generated parent networks from the search space of DARTS. Both conventional (from scratch) and weight inheritance training were employed on the CIFAR-10 dataset. Both training runs were optimised with momentum SGD, and a batch size of 128. The hyper-parameters for the former strategy were set as follows: learning rate was 0.025 (annealed via cosine strategy), momentum was 0.9, weight decay was 0.0001, and number of training epochs was 100. For weight inheritance training, as offsprings will partially inherit trained weights from parent networks, both learning rate and training epochs were reduced as follow: the learning rate was set to 0.01 and the number of training epochs was set to 50. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the CIFAR-10 validation accuracy of these 350 networks obtained by (1) weight inheritance and (2) conventional training. The Spearman Coefficient between these two groups is 0.83, which indicates the high fidelity of inherited weights. This is significantly better than the one-shot model that shows poor correlation with the ground-truth accuracy <ref type="bibr" target="#b38">[39]</ref>. The weight inheritance strategy can obtain both training efficiency and high ranking correlation, as it required much fewer training epochs to reach a similar performance when compared with conventional training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>We have implemented our experiments on two search spaces which are NAS-Bench-201 <ref type="bibr" target="#b10">[11]</ref> and DARTS <ref type="bibr" target="#b21">[22]</ref>. We use NAS-Bench-201 space to implement a large scale of experiments, it aims to prove the stability of search performance of PRE-NAS. We use DARTS space to implement real-world architecture search, which can generate high-performance neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search on NAS-Bench-201</head><p>As the search space of NAS-Bench-201 has been exhaustively evaluated and the performance of every network within the search space is known, there is no need for training the initial population or offspring candidates, but query the record. On this benchmark, PRE-NAS was repeated 500 times in order to reduce the variance between  <ref type="table">Table 2</ref>: Search results of our proposed PRE-NAS and other search algorithms on the NAS-Bench-201 search space. Each algorithm is searching under a similar computational budget and repeated 500 times. We recorded the CIFAR-10 validation accuracy of the best-performing architecture which found in each experiment and reported the mean and standard deviation over 500 independent experiments. *Optimal indicates the accuracy of best-performing architecture recorded in the benchmark.</p><p>runs. The computational budget was set according to the experiments in <ref type="bibr" target="#b10">[11]</ref>. We set the search to 20, _ to 20, _ to 10, and initial _ equal to the _ . This means that the algorithm runs mutation operations 10 times to generate 10 offspring networks. Once the child networks are evaluated by the performance predictor, then the percentile selection will apply. There is one additional hyper-parameter called _ , which is used to increase _ . For instance, if the predictor performed well in the current search cycle, _ will be increased by _ . Thus, the predictor could gradually evaluate more architectures if it performs well. This parameter was set to 1.2. This setup ensures a similar computational budget which is the total number of architectures that have been queried by using the benchmark. Note that the total number of training samples for the performance predictor is around 100 under this setup (approx. 10 ?4 of the search space), which is similar to our simulation experiment shown in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">NAS-Bench-201</head><p>Results. <ref type="table">Table 2</ref> shows the results from PRE-NAS and respective SOTA algorithms over 500 independent experiments. The columns under CIFAR-10, CIFAR-100 and ImageNet-16-120 indicate the average validation and test accuracy with the standard deviation of the best architecture by each algorithm. The last row shows the best accuracy that the architecture in this search space could possibly reach. The large scale of independent architecture search experiments clearly show that the overall performance of PRE-NAS dominates all other algorithms, given a similar computational budget.</p><p>We also recorded the Spearman coefficient between predicted and ground-truth CIFAR-10 validation accuracy at each search cycle. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the average value of the Spearman coefficient over 500 independent experiments. Even with extremely limited training samples, the performance predictor was still continuously improving, taking advantage of multi-mutation and representatives selection. Moreover, we have investigated two extra candidate selection strategies, top-(e.g. top-5) and random. The experimental setups are the same as for our percentile strategy. <ref type="figure" target="#fig_6">Fig. 7</ref> demonstrate that our percentile-based representative selection strategy is superior on both predictor training and architecture search. Based on these results, we think that percentile or even random-based representative selection strategies are more likely to cover the distribution of the search space than the top-strategy. Thus, it constructed richer training samples for the performance predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search on DARTS</head><p>DARTS search space contains approximately 10 9 possible architectures, much larger than NAS-Bench-201. We follow a similar procedure to the experiments on NAS-Bench-201 that use the CIFAR-10 as the proxy task dataset, except that it requires training the networks (shown in Algorithm 1). Specifically, we randomly generate architectures to form the initial population and use conventional training to obtain their accuracy. Then the offspring networks can inherit weights from them. Therefore we can use weight inheritance training to train the following offspring networks with a much less computational cost. We set the search to 100, _ to 64, _ to 32, initial _ equals the _ , and the _ to 1.2. For the initial population, we use conventional training and set the epoch number to 100, momentum to 0.9, learning rate to 0.025 (anneal cosine strategy) and weight decay to 0.0001. We use weight inheritance training for the offspring candidate networks and set the learning rate to 0.01, epoch number to 50 which is the same as the setup in Section 3.6. Both conventional and weight inheritance training are optimised by momentum SGD and batch size to 128. We set the network layer to 1 (single cell) and the initial channel to 16 during the architecture search, as training the shallow networks would significantly reduce the computational cost. Under this setup, there are 64 networks (initial population) trained by the conventional way and approximately 500 networks (offspring networks) trained by weight inheritance. Thus, the theoretical number of training samples for the performance predictor is 564 (approx. 10 ?8 of the search space). PRE-NAS would evaluate around 7000 architectures by implementing the multi-mutation. Each network would take approximately 10 minutes on a single Tesla V100 GPU if training from scratch. This leads to approximate 48 days to complete the search. By utilising the performance predictor and weight inheritance training, PRE-NAS reduces the computational cost to 0.6 GPU days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>Result on CIFAR-10. After PRE-NAS found the promising architecture, we use it to construct a full-size network as discussed in Section 3.1. For the training of our full-size network, we apply a similar setup with DARTS <ref type="bibr" target="#b21">[22]</ref>, i.e., stacked the cell network with 20 layers. <ref type="table">Table 3</ref> shows the results of the comparison between our full-size network and the networks from other methods. As the CIFAR-10 results have high variance between different runs, we trained our full-size model 10 times and reported the mean and standard deviation. From the results, the architecture found by PRE-NAS is significantly better than most state-of-the-art which also searched on DARTS space. PRE-NAS only took 0.6 GPU days for the search. From the perspective of search cost, our method is still not inferior to the mainstream one-shot NAS. In evolution-based search algorithms, PRE-NAS also shows a huge advantage in both search efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Result on ImageNet.</head><p>As the ImageNet dataset contains approximate 13k large size images with 1000 classes, directly searching on ImageNet will cause gigantic computational overhead. The performance transferability of cell networks have been proved in most of the previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. Our full-size network for ImageNet is also constructed from the architecture found on CIFAR-10, we apply a similar setup with DARTS <ref type="bibr" target="#b21">[22]</ref> to build a slightly larger model. <ref type="table">Table 4</ref> shows the results of comparison between our model and the networks from other work, which is consistent with the results shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a predictor-assisted evolutionary search algorithm called PRE-NAS. We demonstrated that the predictorbased evolutionary NAS is highly competitive with mainstream methods. By adopting the multi-mutation and representative selection strategies, we can train a good performance predictor with extremely limited data (e.g. approx. 10 ?8 to 10 ?4 of the total size of the search space). The predicted accuracy shows a strong correlation with the ground-truth accuracy. By utilising the high-fidelity weight inheritance strategy, we can further reduce the cost of candidate network training. On DARTS search space, the search cost is reduced from 48 GPU days to 0.6 GPU days. Extensive experiments show that the network found by PRE-NAS can outperform several state-of-the-art search algorithms on the search spaces of NAS-Bench-201 and DARTS.</p><p>In this work, we only used network accuracy as the search objective. Introducing multi-objectives could be a promising direction  <ref type="table">Table 3</ref>: Performance comparison between the networks found by PRE-NAS and other search algorithms on CIFAR-10, the lower test error rate is better. ? means the method has also been used on DARTS search space. ? indicates the original paper only provided their best performance. Dash means the original paper has not provided the information. 'w/i' is short for the weight inheritance training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Test Error(%)  <ref type="table">Table 4</ref>: Performance comparison between the networks found by PRE-NAS and other search algorithms on the ImageNet (mobile setting), the lower test error rate is better. ? means the method has also been used on DARTS search space. Dash means the original paper has not provided the information. 'w/i' is short for the weight inheritance training.</p><p>for future work as multiple requirements, e.g. accuracy and model size, then can be considered simultaneously during architecture search. Furthermore, it can be observed that the performance of evolutionary algorithms depends on the quality of the initial population. A high-quality initial population is more likely to generate good offspring networks. In near future, we will investigate ways to better initialize the initial populations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of a cell-based network. Left-hand side demonstrates the complete stacking-constructed network. The micro-structure in the orange dash line represents a cell network. An inside circle represents an intermediate node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of upper triangular adjacency matrix representation for architecture encoding. The corresponding architecture is visualised on the top right demonstrating a cell network (b). The list (c) on the bottom right shows the available network operations and their indices, which are the cell values of the matrix (a) on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the two mutation strategies of PRE-NAS. Grey circles represent nodes of the cell network and coloured lines represent different network operations. (a) An offspring network generated by operation mutation, the red dashed circle indicates that the network operation at this position has been mutated. (b) An offspring network generated by connection mutation, the red dash line circle indicates the connection between two nodes has been mutated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of weight inheritance, grey circles represent nodes of the cell network, and colour lines represent different network operations. (a) and (b) demonstrate two offspring networks generated by operation and connection mutations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results of weight inheritance and conventional training on 350 candidate networks. The network order is sorted by the CIFAR-10 validation accuracy. The Spearman Correlation Coefficient between these two groups is 0.83.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Result of Spearman coefficient between predicted and ground-truth accuracy. The line indicates the average value of the Spearman coefficient at each search cycle based on 500 independent experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>(a) Spearman coefficient between predicted and ground-truth accuracy under three candidates selection strategies. (b) Ground-truth accuracy under three candidates selection strategies. Each line indicates the average value at each search cycle based on 500 independent experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>algorithms on benchmark and real-world search spaces in terms of efficiency and performance.(2) We introduce a representative selection strategy that can train a good performance predictor using an extremely limited number of training samples in the NAS context.</figDesc><table><row><cell>NAS More-</cell></row><row><cell>over, we apply a multi-mutation strategy to utilise the per-</cell></row><row><cell>formance predictor maximally in the evolutionary NAS.</cell></row><row><cell>(3) We report a 2.40% test error rate on CIFAR-10 and a 24%</cell></row><row><cell>top-1 test error rate on ImageNet (mobile setting), with only</cell></row><row><cell>0.6 GPU days to search.</cell></row></table><note>We propose a predictor-assisted evolutionary search algo- rithm (PRE-NAS) which outperforms several mainstream arXiv:2204.12726v1 [cs.CV] 27 Apr 2022</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? 1) + 2 !, where N represents the number of intermediate nodes and O represents the number of network operations. As discussed in Section 3.1, the DARTS space consists of 4 intermediate nodes and 8 operations, which means a parent network can generate up to 76 different offsprings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>16?1.69 87.16?1.69 59.00?4.60 58.33?4.34 31.56?3.28 31.14?3.88 DARTS-V1 [22] 39.77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 DARTS-V2 [22] 39.77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 GDAS [10] 90.00?0.21 93.51?0.13 71.14?0.27 70.61?0.26 41.70?1.26 41.84?0.90 SETN [9] 82.25?5.17 86.19?4.63 56.86?7.59 56.87?7.77 32.54?3.63 31.90?4.07 ENAS [25] 39.77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 AmoebaNet [26] 91.19?0.31 93.92?0.30 71.81?1.12 71.84?0.99 45.15?0.89 45.54?1.03 Random Search [3] 90.93?0.36 93.70?0.36 70.93?1.09 71.04?1.07 44.45?1.10 44.57?1.25 REINFORCE [43] 91.09?0.37 93.85?0.37 71.61?1.12 71.71?1.09 45.05?1.02 45.24?1.18 BOHB [13] 90.82?0.53 93.61?0.52 70.74?1.29 70.85?1.25 44.26?1.36 44.42?1.49 PRE-NAS (ours) 91.37?0.28 94.04?0.34 71.95?1.21 72.02?1.22 45.16?1.00 45.34?1.03</figDesc><table><row><cell>Algorithm</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell cols="2">ImageNet-16-120</cell></row><row><cell></cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell cols="2">RSPS [20] 84.*Optimal 91.61</cell><cell>94.37</cell><cell>73.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Practical Neural Network Performance Prediction for Early Stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1705.10823</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Ramesh Raskar, and Nikhil Naik</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random Search for Hyper-Parameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="465" to="480" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: NSGA-II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-Shot Neural Architecture Search via Self-Evaluated Template Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="3680" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for a Robust Neural Architecture in Four GPU Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BOHB: Robust and Efficient Hyperparameter Optimization at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">sharpDARTS: Faster and More Accurate Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno>abs/1903.09900</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning curve prediction with Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random Search and Reproducibility for Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 35th Uncertainty in Artificial Intelligence Conference (Proceedings of Machine Learning Research</title>
		<meeting>The 35th Uncertainty in Artificial Intelligence Conference ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NSGANetV2: Evolutionary Multi-objective Surrogate-Assisted Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NSGA-Net: neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><forename type="middle">D</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<editor>Anne Auger and Thomas St?tzle</editor>
		<meeting>the Genetic and Evolutionary Computation Conference<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-07-13" />
			<biblScope unit="page" from="419" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameters Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized Evolution for Image Classifier Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-Scale Evolution of Image Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<meeting>the 34th International Conference on Machine Learning ( Machine Learning Research<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovita</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/2008.09777</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evolving neural architecture using one shot model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilotpal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO &apos;21: Genetic and Evolutionary Computation Conference</title>
		<editor>Francisco Chicano and Krzysztof Krawiec</editor>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-07-10" />
			<biblScope unit="page" from="910" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Genetic Programming Approach to Designing Convolutional Neural Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5369" to="5373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Surrogate-Assisted Evolutionary Deep Learning Using an End-to-End Random Forest-Based Performance Predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaochu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="350" to="364" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network Morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>Maria-Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="564" to="572" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural Predictor for Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Helen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-01" />
			<biblScope unit="volume">12374</biblScope>
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIX</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CARS: Continuous Evolution for Efficient Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1826" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating The Search Phase of Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2018 AutoML Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">EcoNAS: Finding Proxies for Economical Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11393" to="11401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EENA: Efficient Evolution of Neural Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhulin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanguang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="1891" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
