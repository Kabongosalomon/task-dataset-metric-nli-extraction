<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled High Quality Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouhong</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mofei</forename><surname>Song</surname></persName>
							<email>songmf@seu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">The School of Computer Science and Engineering</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">The Key Lab of Computer Network and Information Integration (Ministry of Education)</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled High Quality Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding author and equal contribution to first author. This work was supported by National Natural Science Foundation of China 61906036 and the Fundamental Research Funds for the Central Univer-sities (2242021k30056).</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aiming at discovering and locating most distinctive objects from visual scenes, salient object detection (SOD) plays an essential role in various computer vision systems.</p><p>Coming to the era of high resolution, SOD methods are facing new challenges. The major limitation of previous methods is that they try to identify the salient regions and estimate the accurate objects boundaries simultaneously with a single regression task at low-resolution. This practice ignores the inherent difference between the two difficult problems, resulting in poor detection quality. In this paper, we propose a novel deep learning framework for high-resolution SOD task, which disentangles the task into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). As a pixel-wise classification task, LRSCN is designed to capture sufficient semantics at low-resolution to identify the definite salient, background and uncertain image regions. HRRN is a regression task, which aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. It is worth noting that by introducing uncertainty into the training process, our HRRN can well address the high-resolution refinement task without using any high-resolution training data. Extensive experiments on high-resolution saliency datasets as well as some widely used saliency benchmarks show that the proposed method achieves superior performance compared to the state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection (SOD) is derived with the goal of accurately detecting and segmenting the most distinctive objects from visual scenes. As a preliminary step, it plays an essential role in various visual systems, such as video object segmentation <ref type="bibr" target="#b48">[49]</ref>, light field image segmentation <ref type="bibr" target="#b44">[45]</ref>, image-sentence matching <ref type="bibr" target="#b20">[21]</ref>, person re-identification <ref type="bibr" target="#b26">[27]</ref> and instance segmentation <ref type="bibr" target="#b69">[70]</ref>.</p><p>Recently, the rapid development of the commodity imaging and display device, has resulted in higher requirements for the producing and editing of high-resolution (e.g., 720p, 1080p and 4K) images. Salient object detection as well as many state-of-the-art computer vision tasks are facing various challenges when encountering high-resolution scenarios. A good high-resolution salient object detection method should not only accurately detect the whole salient object but also predict the precise boundaries of salient objects. Despite the conventional Deep Neural Networks (DNNS) based SOD models have achieved remarkable performance at low-resolution (e.g., typical size 224 ? 224, 384 ? 384), they often fail to generate high quality detection results for high-resolution images. The major reason for this drawback is that the most previous methods try to identify the salient regions and estimate the accurate objects boundaries simultaneously in one step, which are two difficult and inherently different problems for high-resolution salient object detection. To address the first problem, a network is required to capture sufficient semantics by maintaining a larger receptive field. However, since the memory usage increases dramatically along with the image resolution, it is impractical for these models to directly learn sufficient semantics for high-resolution images. One plausible way is introducing downsample operations, but the structure details are inevitably lost during the downsampling, which however is precisely the key to solving the second problem.</p><p>Unfortunately, most of the existing low-resolution SOD Img&amp;GT Ours LDF GateNet <ref type="figure">Figure 1</ref>. Comparison with state-of-the-art method in highresolution SOD. Best viewed by zooming in. methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b52">53]</ref> try to address the aforementioned two problems with a single regression framework, which ignore the inherent difference between the two problems and result in blurry boundaries. As shown in <ref type="figure">Fig.1</ref>, if we take a deeper look at the saliency map generated by the representative existing methods LDF <ref type="bibr" target="#b52">[53]</ref> and GateNet <ref type="bibr" target="#b67">[68]</ref>, we can observe that pixels can be divided into three different sets: (1) most pixels inside the salient object have the highest saliency value, and we call these pixels as definite salient pixels;</p><p>(2) most pixels in the background regions have the lowest salient value, which belong to definite background pixels; (3) saliency values of the pixels at blurry object boundaries fluctuate between 0 and 1, so we call these pixels as uncertain pixels. An ideal SOD method should effectively identify the definite salient and background regions in the image and accurately calculate the saliency value of pixels in the uncertain region to preserve a clear object boundary. From this perspective, there are essentially two tasks in SOD which demand quite different abilities to address the aforementioned two problems. The former task can be viewed as a classic classification task, while the later one is a typical regression task.</p><p>Despite the demand for effective high-resolution SOD methods, this line of work is rarely studied. In this paper, motivated by the new observation that SOD should be disentangled into two tasks, we propose a novel deep learning framework for high-resolution salient object detection. Specifically, we decouple the high-resolution salient object detection into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). LRSCN is designed to capture sufficient semantics at low-resolution and classify the pixels into three different sets for later process. HRRN aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. As discussed above, HRRN requires structure details in high-resolution image. However, widely used low-resolution saliency datasets generally have some problems in annotation quality <ref type="bibr" target="#b57">[58]</ref>, making it almost impossible to directly obtain enough object boundary details from these defective datasets to train the high-resolution network. In the very recent work, Zeng et al. <ref type="bibr" target="#b57">[58]</ref> proposed to train their SOD network by using high-resolution images with accurate annotation. However, such high-quality image annotation requires heavy labor costs. In our paper, we argue that it is unnecessary to use such accurately annotated high-resolution images in network training. By introducing uncertainty <ref type="bibr" target="#b21">[22]</ref> in the training process, our HRRN can well address the high-resolution refinement task only using the low-resolution training datasets with poor annotation.</p><p>Our major contributions can be summarized as:</p><p>? We provide a new perspective that high-resolution salient object detection should be disentangled into two tasks, and demonstrate that the disentanglement of the two tasks is essential for improving the performance of DNN based SOD models.</p><p>? Motivated by the principle of disentanglement, we propose a novel framework for high-resolution salient object detection, which uses LRSCN to capture sufficient semantics at low-resolution and HRRN for accurate boundary refinement at high-resolution.</p><p>? We make the earliest efforts to introduce the uncertainty into SOD network training, which empowers HRRN to well address the high-resolution refinement task without any high-resolution training datasets.</p><p>? We perform extensive experiments to demonstrate the proposed method refreshes the SOTA performance on high-resolution saliency datasets as well as some widely used saliency benchmarks by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the past decades, a large amount of SOD algorithms have been developed. Traditional models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b22">23]</ref>detect salient objects by utilizing various heuristic saliency priors with hand-crafted features. More details about the traditional methods can be found in the survey <ref type="bibr" target="#b0">[1]</ref>. Recently, with the development of deep learning, the performance of saliency detection has archived great improvment <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b64">65]</ref>. Here we mainly focus on deep learning based saliency detection models.</p><p>Recently, some DNN-based models use various feature enhancement strategies to improve the ability of localization and awareness of salient objects <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b12">13]</ref>, or take advantage of edge features to restore the structural details of salient objects <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">69]</ref>. For example, Pang et al. <ref type="bibr" target="#b31">[32]</ref> applied the transformationinteraction-fusion strategy on multi-level and multi-scale features to learn discriminant feature representation. Zhao et al. <ref type="bibr" target="#b67">[68]</ref> designed a gated dual branch structure to build the cooperation among different levels of features and improve the discriminability of the whole network. In <ref type="bibr" target="#b65">[66]</ref>, En-Conv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-Res1</head><p>En-Res2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-Res3</head><p>En-Res4  edge features from edge detection branch was fused with salient features as complementary information to enhance the structural details for accurate saliency detection. Zhou et al. <ref type="bibr" target="#b68">[69]</ref> used two individual branches for representing saliency and contour stream respectively, and a novel feature fusion module for their correlation combination.</p><p>Different from the above methods, some methods consider leveraging predict-refine architecture to generate fine salient objects. For example, Wang et al. <ref type="bibr" target="#b46">[47]</ref> proposed to localize salient objects globally and then refine them by a local boundary refinement module. Qin et al. <ref type="bibr" target="#b34">[35]</ref> was composed of an Encoder-Decoder network and a residual refinement module, which were respectively in charge of saliency prediction and saliency map refinement.</p><p>However, all these methods cannot handle highresolution salient object detection problem well since such simple regression framework cannot identify the salient regions and estimate the accurate objects boundaries simultaneously and their architectures are not optimized for highresolution SOD. Zeng et al. <ref type="bibr" target="#b57">[58]</ref> tried to alleviate this problem by leveraging both global semantic information and local high-resolution details to accurately detect salient objects in high-resolution images. However, Zeng et al. <ref type="bibr" target="#b57">[58]</ref> relies on high-resolution training images with accurate annotation, which requires heavy labor costs. Different from the above methods, we disentangle high-resolution SOD into two tasks at different resolutions: identifying the salient regions at low-resolution and estimating the accurate objects boundaries at high-resolution. Moreover, unlike Zeng et al. <ref type="bibr" target="#b57">[58]</ref>, we introduce novel uncertainty loss, which empowers our HRRN to well address the high-resolution refinement task without using any high-resolution training datasets. Recently, Wei et al. <ref type="bibr" target="#b52">[53]</ref> and Zhang et al. <ref type="bibr" target="#b60">[61]</ref> also leverage disentanglement in their SOD methods. However, they still try to address the SOD task under a single regression framework but with decoupled supervisions. Unlike our proposed methods, their disentanglement frameworks barely touch the very nature of the SOD, which essentially contains two different tasks. For more information about the DNN-based methods, please refer to survey <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first describe the overall architecture of the proposed disentangled high quality salient object detection network, then elaborate our main contributions, which are corresponding to LRSCN and HRRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Overview</head><p>The architecture of the proposed approach is illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>. As can be seen, the disentanglement includes two decoupled tasks at two different resolutions. LRSCN aims at capturing sufficient semantics at low-resolution and classifying the pixels into three different sets, which also can save the memory usage. While estimating the accurate objects boundaries needs more local details at high-resolution. So, we design HRRN to regress the saliency value of pixels and preserve a clear object boundary at high-resolution.</p><p>LRSCN has a simple U-Net like Encoder-Decoder architecture <ref type="bibr" target="#b35">[36]</ref>. VGG-16 <ref type="bibr" target="#b37">[38]</ref> is used as backbone. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b65">66]</ref>, we connect another side path to the last pooling layer in VGG-16. Hence, we obtain six side features Conv1-2, Conv2-2, Conv3-3, Conv4-3, Conv5-3 and Conv6-3 from backbone network. Because Conv1-2 and Conv2-2 are too close to the input and their receptive fields are too small, following <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b53">54]</ref>, we only use the last four levels features for the following process. Conv6-3 is denoted as {F h |h = 6}, the other three levels features are denoted as {F l |l = 3, 4, 5}. Multi-scale feature extraction and Cross-level feature fusion (MECF) module is added between encoder and decoder to help improve the discriminability of feature representations. Decoder fuses the output features from MECF and the upsampled features from the previous stage in a bottom-up manner. The output of each decoder is defined as {D i |i = 3, 4, 5, 6}. Finally, SGA module is built upon D 3 for accurate trimap T generation.</p><p>As described, LRSCN is classification task and aims at capturing sufficient semantics at low-resolution. To regress a clear object boundary value, the input of HRRN is a highresolution image under guidance of the trimap provided by LRSCN. HRRN has a basic Encoder-Decoder architecture and with the help of uncertainty loss, the network can be more robust to noisy data and predict a high-resolution saliency map with clear boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture of LRSCN</head><p>To capture sufficient semantics at low-resolution, learning discriminant feature representations is essential. The network should not only consider scale and location variations of different salient objects, but also distinguish the appearance difference between the salient object and the non-salient regions. To achieve the first goal, we develop a multi-scale feature extraction module (ME) based on Global Convolutional Network (GCN) <ref type="bibr" target="#b32">[33]</ref> to enlarge the feature receptive field and obtain multi-scale information. To achieve the second goal, we utilize cross-level feature fusion module (CF) to leverage the advantages of features at different levels.Moreover, in designing the network architecture, inspired by <ref type="bibr" target="#b55">[56]</ref>, we use split-transformmerge strategy to further enlarge feature receptive fields and hence results in more discriminative feature representations. Specifically, we uniformly split the input F into two portions {F 1 , F 2 } by channel dimension, then F 1 is sent into multi-scale feature extraction pathway and F 2 is sent into cross-level feature fusion pathway. The outputs of these two pathways are concatenated together as the final output. we call this bridge module as MECF module, which is shown in <ref type="figure" target="#fig_2">Fig.3</ref>. More details about MECF module can be found in section 6 of supplementary materials.</p><p>SGA Module. As illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>, each decoder fuses features from MECF module and previous decoder stage, then uses 3 ? 3 convolutional layer for final prediction. To maintain consistency between trimap and saliency map and ensure the uncertain regions of the trimap can accurately cover the boundary of saliency map, we design a saliency guide attention module (SGA) on D 3 . Specifically, we first use a 3 ? 3 convolution and sigmoid function to compute a saliency map. Then, the saliency map is treated as spatial weight map which can help refine feature and generate an  accurate trimap. Finally, the output trimap T is 3-channel classification logits. The whole SGA module guarantees the alignment of trimap and saliecny map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture of HRRN</head><p>Following the principle of disentanglement, HRRN aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution under the guidance of the trimap provided by LRSCN. The architecture of HRRN is shown in <ref type="figure" target="#fig_1">Fig.2</ref>. HRRN has a simple U-NET like architecture. For better prediction at high-resolution, we then do some nontrivial modifications. First, lower-level features contain rich spatial and detail information which play a crucial role in restoring a clear object boundary, so decoder combines encoder features before each upsampling block instead of after each upsampling block. Moreover, we use a two layers short cut block to align channels of encoder features for feature fusion. Second, to let network pay more attention to detail information, we directly feed the original input to the last convolutional layer through a short cut block to generate better results. Finally, learning from image generation tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b58">59]</ref>, we use the spectral normalization <ref type="bibr" target="#b30">[31]</ref> to each convolutional layer to add a constraint on Lipschitz constant of the network and stable the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function of LRSCN</head><p>To supervise LRSCN, inspired by some typical matting mehtods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>, we generate trimap groundtruth T gt , which can represent the definite salient, definite background and uncertain regions. As described, uncertain regions exist mainly at the boundaries of the objects. So we erase and dilate binary groundtruth maps at the object boundaries with a random pixel number <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13)</ref> to generate the GT uncertain regions. The remaining foreground and background regions represent definite salient and background regions. T gt is defined as:</p><formula xml:id="formula_0">T gt (x, y) = ? ? ? ? ? 2, T gt (x, y) ? def inite salient 0, T gt (x, y) ? def inite background 1, T gt (x, y) ? uncertain region<label>(1)</label></formula><p>where (x, y) stands for each pixel location on the image. Some examples can be seen in <ref type="figure" target="#fig_4">Fig.4</ref>. For trimap supervision, we use Softmax cross-entropy loss, which is defined as:</p><formula xml:id="formula_1">L trimap = 1 N i ?log( e Ti j e Tj ).<label>(2)</label></formula><p>To guarante the arruracy of trimap, we add extra saliency supervision L saliency as the supplement of trimap supervision. Similar to BASNet <ref type="bibr" target="#b34">[35]</ref>, we use pixel-level, regionlevel and object-level supervision strategy on multi-levels to better keep the uniformity and wholeness of the salient objects. Specifically, binary cross-entropy (BCE) <ref type="bibr" target="#b7">[8]</ref>, SSIM <ref type="bibr" target="#b50">[51]</ref> and F-measure loss <ref type="bibr" target="#b66">[67]</ref> are denoted as pixellevel, region-level and object-level loss. Note that all parts of LRSCN are trained jointly, so the overall loss function is given as:</p><formula xml:id="formula_2">L LRSCN = L saliency + L trimap .<label>(3)</label></formula><p>We do not use uncertainty loss because the main goal of LRSCN is to capture sufficient semantics, not accurate boundary. More details about L saliency can be found in section 5 of supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function of HRRN</head><p>We perform a L 1 loss and novel uncertainty loss to restore the fine structures and boundaries of salient objects. For an input high-resolution image I, let G H denote its groundtruth, and predicted saliency map is S H .</p><p>We leverage the L 1 loss to compare an absolute difference between predicted saliency map and groundtruth over the definite salient and background regions:</p><formula xml:id="formula_3">L 1 = 1 E i?E |S H i ? G H i |,<label>(4)</label></formula><p>where E indicates the number of pixels which are labeled as definite salient or background in the trimap, S H i and G H i denote the predicted and groundtruth value at position i. We cannot directly compute L 1 loss between predicted saliency map and groundtruth over the uncertain regions because widely used saliency training datasets have some problems in annotation quality <ref type="bibr" target="#b57">[58]</ref>. We show these low quality annotations in section 4 of supplementary materials. It is almost impossible to directly obtain enough object boundary details from these defective datasets to train the high-resolution network. To address this problem, we design uncertainty loss, which empowers our HRRN to well address the high-resolution refinement task only using these defective low-resolution training datasets. It is worth noting that there are some previous works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b63">64]</ref> involving "uncertainty" in their titles, which seem relevant to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Img</head><p>GT !" Loss Uncertainty Loss + !" Loss Uncertainty Value <ref type="figure">Figure 5</ref>. The impact of the losses. Best viewed by zooming in.</p><p>However, in <ref type="bibr" target="#b59">[60]</ref>, "uncertainty" means the human perceptual uncertainty modeled by CVAE. While in <ref type="bibr" target="#b63">[64]</ref>, "uncertainty" indicates the saliency prediction system uncertainty modeled by R-dropout. Obviously, their usages of uncertainty are different from ours. Inspired by <ref type="bibr" target="#b21">[22]</ref>, Gaussian likelihood is used to model the uncertainty. Let x and f (x) be the input and output of HRRN, and Gaussian likelihood is defined as:</p><formula xml:id="formula_4">p(y|f (x)) = N (f (x), ? 2 ),<label>(5)</label></formula><p>where ? measures uncertainty of the estimation, y is the label of output. In maximum likelihood inference, we maximize the log likelihood of the model, which is written as:</p><formula xml:id="formula_5">logp(y|f (x)) ? ? ||y ? f (x)|| 2 2? 2 ? 1 2 log? 2 ,<label>(6)</label></formula><p>so the proposed uncertainty loss is defined as:</p><formula xml:id="formula_6">L uncertainty = ||y ? f (x)|| 2 2? 2 + 1 2 log? 2 .<label>(7)</label></formula><p>We only care about the pixels in uncertain regions, so L uncertainty is written as:</p><formula xml:id="formula_7">L uncertainty = 1 U i?U ||S H i ? G H i || 2 2? 2 i + 1 2 log? 2 i , (8)</formula><p>where U is the total number of pixels in uncertain region, ? i is the uncertainty of each pixel and is generated from HRRN. Different from directly learning from noisy data, uncertainty loss can allow the network to learn how to attenuate the effect from erroneous labels. Specifically, pixels for which the network learned to predict high uncertainty will have a smaller value of the first term of Eq.8, so have little effect on the loss. Meanwhile, large uncertainty increases the contribution of the second term of Eq.8, and in turn penalizes the model and lets the model make a better prediction that has low uncertainty. Note that all parts of HRRN are trained jointly, so the over all loss function is given as:</p><formula xml:id="formula_8">L HRRN = L uncertainty + L 1 .<label>(9)</label></formula><p>To show how proposed uncertainty loss makes the network attenuate the effect from erroneous labels during training, we visualize the impact of L 1 loss and uncertainty loss at the same training iteration in <ref type="figure">Fig.5</ref>. We show the impact of both these losses in the same image. The images shown in <ref type="figure">Fig.5</ref> are these which have problems in annotation quality. Compared with column.4 and column.5, if we only use L 1 loss, the weight of the loss in the uncertainty region will be large, which leads the network hard to converge. While uncertainty loss will make the weight of the loss in the uncertainty region be small and let the network ignore effects from noisy data as much as possible. Column.6 shows the uncertainty value of pixels in uncertain regions. It can be seen that pixels in uncertain regions usually have higher uncertainty value. In general, compared to L 1 loss, uncertainty loss reduces the weight of the loss in the uncertainty region, thus mitigating the impact of noisy data on the network. But due to the uncertainty value, it will allow the network to learn how to predict a better prediction with a low certainty value, instead of ignoring the learning of the uncertainty region completely. These visual comparisons show how uncertainty loss makes network more robust to noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Implementation Details. Following the works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53]</ref>, we train our proposed network on DUTS-TR. We use Pytorch 1 to implement our model. A GTX 1080Ti GPU is used for acceleration. VGG-16 <ref type="bibr" target="#b37">[38]</ref> is used as the backbone network of LRSCN, and the whole network is trained end-to-end by stochastic gradient descent (SGD). For a more comprehensive demonstration, we also trained our network with ResNet-50 <ref type="bibr" target="#b16">[17]</ref> backbone. Maximum learning rate is set to 0.001 for backbone and 0.01 for other parts. Warm-up and linear decay strategies are used to adjust the learning rate. Momentum and weight decay are set to 0.9 and 0.0005 respectively. Batchsize is set to 32 and maximum epoch is set to 100. Horizontal flip and multiscale input images are utilized for data augmentation as done in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b51">52]</ref>. During testing, the input of LRSCN is about 352 ? 352 resolution.</p><p>The learning rate of HRRN is initialized to 0.0005. Warmup and cosine decay are applied to the learning rate. The network HRRN is trained for 10000 iterations with a batch size of 20. During training, the resolution of input image and trimap is 512 ? 512. During testing, we first resize the image and trimap to 1024 ? 1024, then we split the image and trimap into four sub-images and sub-trimaps with 512 ? 512 resolution, as shown in <ref type="figure" target="#fig_1">Fig.2</ref>. Finally, we send each sub-image and sub-trimap together to HRRN to generate sub-prediction result, and use 4 sub-predictions stitched together to make one high-resolution saliency result. 1 https://pytorch.org/ Evaluation Datasets. Following work <ref type="bibr" target="#b57">[58]</ref>, we evaluate our method on two high-resolution saliency detection datasets, including HRSOD-TE and DAVIS-S, which contain 400 and 92 images. DAVIS-S dataset is collected from DAVIS <ref type="bibr" target="#b33">[34]</ref>. Images in these two datasets are precisely annotated and have very high resolutions (i.e.,1920 ? 1080). We also evaluate our method on three low-resolution datasets, including DUT-OMRON <ref type="bibr" target="#b56">[57]</ref>, DUTS-TE <ref type="bibr" target="#b42">[43]</ref> and HKU-IS <ref type="bibr" target="#b24">[25]</ref>, which contain 5168, 5019 and 4447 images. Our results are available at https://github.com/ luckybird1994/HQSOD. Evaluation Metrics. Six metrics are used to evaluate the performance of our method. The first is Mean Absolute Error (MAE), which characterize the average 1-norm distance between ground truth maps and predictions. The second is F-measure (F ? and F max ? ), a weighted mean of average precision and average recall, calculated by F ? = (1+? 2 )?P recision?Recall ? 2 ?P recision+Recall . We set ? 2 to be 0.3 as suggested in <ref type="bibr" target="#b1">[2]</ref>. The third is Structure Measure (S m ), a metric to evaluate the spatial structure similarities of saliency maps based on both region-aware structural similarity S r and objectaware structural similarity S o , defined as S ? = ? * S r + (1 ? ?) * S o , where ? = 0.5 <ref type="bibr" target="#b10">[11]</ref>. In addition, precisionrecall (PR) curve is used to show the whole performance. To further evaluate the boundary quality, Following <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b61">[62]</ref>, we use Boundary Displacement Error (BDE) <ref type="bibr" target="#b11">[12]</ref> and B ? metrics. More details about BDE and B ? can be found in section 7 of supplementary materials. The last two metrics are only used in two high-resolution datasets, because their boundaries annotation is accurate and evaluating results are reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with the State-of-the-Arts</head><p>We compare our approach with 16 SOTA methods, including Amulet <ref type="bibr" target="#b62">[63]</ref>, R3Net <ref type="bibr" target="#b8">[9]</ref>, DGRL <ref type="bibr" target="#b46">[47]</ref>, DSS <ref type="bibr" target="#b17">[18]</ref>, BASNet <ref type="bibr" target="#b34">[35]</ref>, CPD <ref type="bibr" target="#b53">[54]</ref>, EGNet <ref type="bibr" target="#b65">[66]</ref>, PFPN <ref type="bibr" target="#b40">[41]</ref>, GCPA <ref type="bibr" target="#b5">[6]</ref>, F3N <ref type="bibr" target="#b51">[52]</ref>, MINet <ref type="bibr" target="#b31">[32]</ref>, ITSD <ref type="bibr" target="#b68">[69]</ref>, LDF <ref type="bibr" target="#b52">[53]</ref>, GateNet <ref type="bibr" target="#b67">[68]</ref>, CSF <ref type="bibr" target="#b12">[13]</ref> and HRNet <ref type="bibr" target="#b57">[58]</ref>. For a fair comparison, we use either the implementations with recommended parameter settings or the saliency maps provided by the authors. The evaluation toolbox used in this paper is same as F3N <ref type="bibr" target="#b51">[52]</ref>.</p><p>Quantitative Evaluation. From <ref type="table">Table.</ref>1, when we train our network only using DUTS (Ours), our method can already improve the F max ? ,F ? , S m and MAE achieved by the best-performing existing algorithms, especially two highresolution test datasets. It is worth noting that for boundary accuracy, our method is far better than other methods on two high-resolution. These results demonstrate the efficiency of the proposed disentangled SOD framework in both identifying the salient regions and estimating the accurate objects boundaries. Other than numerical results, we also show the PR curves on two high-resolution datasets <ref type="table">Table 1</ref>. Quantitative comparison with SOTA on two high-resolution and three low-resolution datasets. The best three results are in red , green and blue fonts. " ?" means the results are post-processed by dense conditional random field(CRF) <ref type="bibr" target="#b23">[24]</ref>. "*" means using ResNeXt-101 <ref type="bibr" target="#b55">[56]</ref> backbone. " " means using ResNet-101 backbone. " ?" means using Res2Net50 <ref type="bibr" target="#b13">[14]</ref> backbone. MK: MSRA10K <ref type="bibr" target="#b6">[7]</ref>, DUTS: DUTS-TR <ref type="bibr" target="#b42">[43]</ref>, MB: MSRA-B <ref type="bibr" target="#b28">[29]</ref>, HR: HRSOD-Training <ref type="bibr" target="#b57">[58]</ref>, HR-L: HRSOD-Training resized in low-resolution. Smaller MAE, BDE and B?, larger F max ? , F ? and Sm correspond to better performance. <ref type="bibr">HRSOD</ref>  It can be clearly observed that our method achieves impressive performance in all these cases.</p><p>and three low-resolution datasets in <ref type="figure">Fig.6</ref>. As can be seen, the PR curves by our method (red ones) are especially outstanding compared to all other previous methods. Besides, shorter PR curves imply that our saliency maps are usually more assertive with sharper boundaries than the results of other methods. An interesting observation is that when we add HRSOD-training datasets (resized to low-resolution like 352 ? 352) in LRSCN, the performance in two highresolution datasets HRSOD-TE and DAVIS-S can be further improved. However, this practice seems to be of little help in improving the performance in other three low-resolution datasets. A similar phenomenon can also be found in the performance of HRNet <ref type="bibr" target="#b57">[58]</ref>. We think there could be some image selection or data annotation biases between the highresolution datasets and the low-resolution datasets, which cause this phenomenon.</p><p>Qualitative Evaluation. To exhibit the superiority of the proposed approach, <ref type="figure">Fig.7</ref> shows representative examples of saliency maps generated by our approach and other state-of-the-art algorithms. As can be seen, with the help of LRSCN, our method can not only keep the wholeness of the salient object (row 3), but also accurately locate salient objects and suppress non-salient regions (row 5), compared to other methods. HRRN can help the model to restore accurate and complete boundaries of salient objects, which are more consistent with the GT boundaries. It can be clearly   observed that our method achieves impressive performance in all these cases, which indicates the effectiveness of disentangled framework and uncertainty loss. More comparison experiments can be found in section 2 of supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To validate the effectiveness of the proposed components of our method, we conduct a series of experiments on two high-resolution datasets with different settings under VGG-16 backbone. Specifically, we first verify the effectiveness of MECF and SGA in LRSCN. Then we validate the effectiveness of uncertainty loss in HRRN and the superiority of the proposed disentangled architecture.</p><p>Ablation Studies of LRSCN. To prove the effectiveness of MECF and SGA module, we report the quantitative comparison results of LRSCN with different architectures in Table.2. Baseline denotes that we conduct a experiment over on L LRSCN with a pure U-Net architecture. We can see that only using ME or CF can already heavily improve the performance. A better performance has been achieved through the combination of these two architectures. Finally, performace can be further improved by SGA module, especially BDE and B ? , which means that SGA can help generate accurate trimap. While L saliency is not our core innovation, more ablation studies about L saliency can be found in section 5 of supplementary materials.</p><p>Ablation Studies of HRRN. In HRRN, uncertainty loss play a key role to estimate the accurate objects boundaries, so we first investigate the effectiveness of our proposed uncertainty loss. From <ref type="table">Table.</ref>3, we can see that without uncertainty loss (Ours(L 1 )), the performance decreased a lot. Besides, when we add high-resolution HRSOD-Training datasets in HRRN (Ours-DH(L 1 + L uncertainty )), the performance has no obvious improvement, which demonstrates that our network is not reliant on accurately annotated high-resolution images during training. To further demonstrate the effectiveness of our disentangled framework, we compare our HRRN with CRF <ref type="bibr" target="#b23">[24]</ref>, a widely used postprocessing for saliency detection. Results in <ref type="table">Table.</ref>3 show that our proposed method (Ours <ref type="figure">(LRSCN + HRRN)</ref>) outperforms CRF <ref type="figure">(Ours (LRSCN + CRF)</ref>) by a large margin. The same phenomenon can be found in the refinement of EGNet, CPD and BASNet (Their trimaps are generated with the corresponding saliency maps following Eq.1). Moreover, compared to RRM module proposed in BASNet, our HRRN can better improve performance. This ablation study demonstrates the superiority of HRRN within our novel disentangled framework. More analyses of the proposed disentangled framework can be found in section 3 of supplementary materials.</p><p>Analysis of Uncertainty Loss To further demonstrate that uncertainty loss can make network more robust to noisy data, we erase and dilate binary groundtruth maps of DUT-TR at the object boundaries with a random pixel number <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13)</ref> to generate noisy training data. Then we train the network on these noisy data, BDE and B ? results on HRSOD-TE are reported in <ref type="figure" target="#fig_6">Fig.8(a)</ref> and <ref type="figure" target="#fig_6">Fig.8(b)</ref>. When the erosion or dilation kernel ranges from 3 to 7, the network trained with uncertainty loss has a rather stable performance. With the increase in erosion or dilation kernels, even though the performance is dropped, training with uncertainty loss still yields a better model than training without uncertainty loss. This experiment further validates the effectiveness of uncertainty loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we argue that there are two difficult and inherently different problems in high-resolution SOD. From this perspective, we propose a novel deep learning framework to disentangle the high-resolution SOD into two tasks: LRSCN and HRRN. LRSCN can identify the definite salient, background and uncertain regions at low-resolution with sufficient semantics. While HRRN can accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. We also make the earliest efforts to introduce the uncertainty into SOD network training, which empower HRRN to learn rich details without using any high-resolution training datasets. Extensive evaluations on high-resolution datasets and popular benchmark datasets not only verify the superiority of our method but also demonstrate the importance of disentanglement for SOD. We believe our novel disentanglement view in this work can contribute to other high-resolution computer vision tasks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Quantitative and Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Quantitative Comparison on more datasets</head><p>We compare our method with other SOTA methods on another two conventional low-resolution datasets EC-SSD <ref type="bibr" target="#b36">[37]</ref> and PASCAL-S <ref type="bibr" target="#b25">[26]</ref>, which have 1000 and 850 images respectively. The results are reported in <ref type="table">Table.</ref>4. It can be seen that our method consistently outperforms other methods across these two conventional datasets. We also show their PR curves in <ref type="figure" target="#fig_7">Fig.9</ref>. It should be noted that F max represents F max ? . We apologize for this writing error of Table.2 in the main text.</p><p>F-measure curves of different methods are displayed in <ref type="figure">Fig.10</ref>, for overall comparisons. One can observe that our approach noticeably outperforms all the other state-of-theart methods. These observations demonstrate the efficiency and robustness of our proposed method across various challenging datasets.</p><p>SOC <ref type="bibr" target="#b9">[10]</ref> is a new challenging dataset with nine attributes. In <ref type="table">Table.</ref>5, we evaluate the mean F-measure score of our method as well as 11 state-of-the-art methods. We can see the proposed model achieves the competitive results among most of attributes and the overall score is best.</p><p>Model size and running time comparisons among different methods are also reported in <ref type="table">Table.</ref>6. It can be seen that with the high-resolution input, our method is more efficient than HRNet. For fair, the running time analysis of our method is also conducted with the low-resolution input (352?352), and our method runs at a competitive efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Quantitative Comparison with different settings</head><p>Although the effectiveness of our method has been confirmed by existing quantitative comparison experiments, to further illustrate the superiority of our method in handling high-resolution SOD task, we modify the setting of existing methods to allow for a more comprehensive comparison.</p><p>First, we change the input for the current SOTA methods from low-resolution (e.g., typical size 320?320, 352?352) to high-resolution (1024 ? 1024). The results are reported in <ref type="table">Table.</ref>7. It can be found that all the compared SOTA methods perform better at low-resolution on most evaluation metrics. Therefore, we only compare our methods to these SOTA methods' low-resolution results in our main paper. In particular, it is worth pointing out that due to GPU memory limitations, we cannot run BASNet, PFPN and ITSD at high-resolution. So we don't report their results in <ref type="table">Table.</ref>7.</p><p>Then, we fine-tune 11 SOTA methods on high-resolution datasets (HRSOD-Training) which have high quality annotations, the results are reported on <ref type="table">Table.</ref>8. As can be seen, high annotation quality can improve their original performance. However, even fine-tuned on HRSOD-Training datasets, our method (only trained on DUTS) still outperforms all of them by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Qualitative Comparison</head><p>As shown in <ref type="figure">Fig.11</ref>, we provide a comprehensive qualitative comparison of our method with other 12 methods on challenging cases. These visual examples can further demonstrate that our method is able to restore accurate and complete boundaries of salient objects.</p><p>B. More analyses of the proposed disentangled framework</p><p>As described, high-resolution salient object detection task should be disentangled into two tasks. One can be viewed as a classic classification task, while the other one is a typical regression task. To further illustrate the validity of our theory, we conduct additional experiments. Specifically, we consider these two tasks as regression or classification tasks simultaneously. The results are reported in <ref type="table">Table.</ref>9. Compared with our proposed method, if we take the disentangled framework as the combination of the two regression or classification tasks, the performance will be degraded. Because the purpose of the proposed disentangled framework is to capture sufficient semantics at lowresolution (LRSCN Stage) and refine accurate boundary at high-resolution (HRRN Stage), which should be viewed as a classic classification task and a typical regression task. <ref type="figure" target="#fig_1">Fig.12</ref> shows some examples that our proposed HRRN can further refine accurate boundary, guided by trimaps. Specifically, column.3 and column.4 show the saliency maps and trimaps generated by LRSCN, and column.5 shows the results refined by HRRN. <ref type="figure" target="#fig_1">From Fig.12</ref>, guided by trimaps, our proposed HRRN can further refine the pixels value in uncertain regions to get more clear saliency results.</p><p>Aforementioned work LDF <ref type="bibr" target="#b52">[53]</ref> has also introduced concepts related to decoupling. However, they still try to address the SOD task under a single regression framework. Their approach is essentially an expansion of additional boundary supervision, which barely touches the very nature of the SOD. As illustrated in our experiments, it is more natural to disentangle the SOD into two different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Annotation Problems</head><p>As described in <ref type="bibr" target="#b57">[58]</ref>, widely used saliency datasets have some problems in annotation quality. So, to quantify the annotation quality problem, we randomly select 100 images from DUT-TR, and 10 of them have easily spotted annotation errors. We manually relabel the 10 images. The B ? between the two different annotations is 0.49 and 42% of the boundary pixel annotations are inaccurate. <ref type="figure" target="#fig_2">Fig.13</ref> shows some examples which have annotation problems, including wrong semantic annotation (row 1 and row 2), boundary an-   <ref type="figure">Figure 10</ref>. Comparison of the F-measure curves across on two high-resolution and five low-resolution datasets.  <ref type="table">Table 7</ref>. Quantitative comparison with SOTA methods where the inputs are resized to high-resolution.   <ref type="figure">Figure 11</ref>. Visual comparison between our method and other SOTA methods. Each sample occupies two rows. Best viewed by zooming in. It can be clearly observed that our method achieves impressive performance in all these cases. <ref type="figure" target="#fig_1">Figure 12</ref>. Examples of coarse saliency maps, trimaps and refined saliency map. </p><formula xml:id="formula_9">HRSOD-TE DAVIS-S Models F max ? F ? S m MAE BDE B ? F max ? F ? S m MAE BDE B ? CPD(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image&amp;GT Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Original Annotation Relabeled Annotation <ref type="figure" target="#fig_2">Figure 13</ref>. Examples that have annotation quality problem. Best viewed by zooming in. notation shifting (row 3) and low contour accuracy (row4, row5 and row 6). In conclusion, the DUTS-TR training dataset does have annotation problems <ref type="bibr" target="#b57">[58]</ref>, and we relabeled some examples to demonstrate these problems in the supplemental material. Since correcting annotations for the whole DUT-TR is a time-consuming task, we will provide an accurate GT of DUT-TR in the future for statistical analysis D. Details of L saliency As described, to guarantee the arruracy of trimap, we add extra saliency supervision L saliency as the supplement of trimap supervision. Here we give more details about L saliency .</p><p>After LRSCN, the prediction saliency map is S, and the binary groundtruth is G. In SOD, binary cross entropy (BCE) is the most widely used loss function, and it is a pixel-wise loss which is defined as:</p><formula xml:id="formula_10">L P ixel = ?(Glog(S) + (1 ? G)log(1 ? S)).<label>(10)</label></formula><p>To learn the structural information of the salient objects, following the setting of <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b10">11]</ref>, we use the sliding window fashion to model region-level similarity between groundtruth and saliency map. The corresponding regions are denoted as S i = {S i : i = 1, ...M } and G i = {G i : i = 1, ...M }, where M is the total number of region. Then we use SSIM to evaluate the similarity between S i and G i , which is defined as:</p><formula xml:id="formula_11">SSD i = (2? s ? g + C 1 )(2? sg + C 2 ) (? 2 s + ? 2 g + C 1 )(? 2 s + ? 2 g + C 2 )<label>(11)</label></formula><p>where local statistics ? s , ? s is mean and std vector of S i , ? g , ? g is mean and std vector of G i . The overall loss function is defined as:</p><formula xml:id="formula_12">L Region = 1 ? 1 M M i=1 SSD i .<label>(12)</label></formula><p>Finally, inspired by <ref type="bibr" target="#b66">[67]</ref>, we directly optimize the Fmeasure to learn the global information from groundtruth. For easy remembering, we denote F-measure as F ? in the following. F ? is defined as:</p><formula xml:id="formula_13">precision = S ? G S + , recall = S ? G G + ,<label>(13)</label></formula><formula xml:id="formula_14">F ? = (1 + ? 2 ) ? precision ? recall ? 2 ? precision + recall ,<label>(14)</label></formula><p>where ? means pixel-wise multiplication, = 1e ?7 is a regularization constant to avoid division of zero. L Object loss function is defined as:</p><formula xml:id="formula_15">L Object = 1 ? F ? .<label>(15)</label></formula><p>The whole loss is defined as:</p><formula xml:id="formula_16">L = L Object + L Region + L P ixel .<label>(16)</label></formula><p>Besides, following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b51">52]</ref>, we used multi-levels saliency supervision to facilitate sufficient training, so the whole saliency loss is defined as:</p><formula xml:id="formula_17">L saliency = 4 i=1 1 2 i?1 L i ,<label>(17)</label></formula><p>where i means the i-th level.</p><p>To further validate the role of L saliency , we train the LRSCN with different loss functions and the results are reported on <ref type="table">Table.</ref>10. As can be can, without L saliency , the performance is dropped lot. Because the trimap groundtruth is randomly generated from binary groundtruth, so only using L trimap cannot maintain consistency between trimap and saliency map. When we only add L P on multi-levels, the model can already achieve the largest performance boost. A better performance has been achieved through the combination of L P , L R and L O .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Details of MECF Module</head><p>As described, we develop a multi-scale feature extraction module (ME) and cross-level feature fusion module (CF) to help LRSCN capture sufficient semantics at low-resolution. Here we give more details about MECF module. The architecture of MECF Module is shown in <ref type="figure" target="#fig_4">Fig.14.</ref> Multi-scale feature extraction module can allow each spatial location to view the local context at small scale spaces and capture multi-scale contextual information, which can enlarge the feature F 1 l receptive field. Specifically, we first use an average pooling and a 3 ? 3 convolutional layer to downsample F 1 l . Then upsampled feature from small scale is added with F 1 l . Finally, Global Convolutional Network (GCN) <ref type="bibr" target="#b32">[33]</ref> is used to further enlarge the feature receptive field. Because F 1 3 and F 1 4 are close to the input and receptive field is relatively small, we use GCNs with k = 7, 11, 15 to fully enlarge receptive field. Receptive fields of F 1 5 and F 1 6 are relatively bigger, we only use GCNs with k = 7, 11 and k = 7.</p><p>Low-level features have rich details but full of background noises, so we design cross-level feature fusion module, which can leverage the rich semantics of high-level feature F 2 h and help restrain the non-salient regions in lowlevel features. Specifically, we first use a 1?1 convolutional layer to compress the channels of F 2 l , then use two 3 ? 3 convolutional layer to transfer the feature for SOD task. Finally, the transferred feature is fused with high-level feature F 2 h as the output of this module. Each of these convolution layers is followed by a batch normalization <ref type="bibr" target="#b18">[19]</ref> and a ReLU activation <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Formulas of Evaluation Metrics</head><p>Following <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b61">[62]</ref>, we use Boundary Displacement Error(BDE) <ref type="bibr" target="#b11">[12]</ref> and B ? metrics to evaluate the boundary quality.</p><p>BDE measures the average displacement error of boundary pixels between two predictions, which can be formulated as:</p><formula xml:id="formula_18">BDE(X, Y ) = x inf y?Y d(x, y) 2N X + y inf x?X d(x, y) 2N Y ,<label>(18)</label></formula><p>where X and Y are two boundary pixel sets which represent saliency prediction and their corresponding groundtruth, and x, y are pixels in them. N x and N y denote the number pixels in X and Y . inf represents for the infimum and d(?) denotes Euclidean distance.</p><p>B ? evaluates the structure alignment between saliency map and their groundtruth, it can be expressed as:</p><formula xml:id="formula_19">B ? = 1 ? 2 (g s g y ) (g 2 s + g 2 y ) ,<label>(19)</label></formula><p>where g s and g y represent the binarized edge maps of predicted saliency map and groundtruth. Following <ref type="bibr" target="#b61">[62]</ref>, we use Canny edge detector to compute edge maps. B ? reflect the sharpness of predictions which is consistent with human perception.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The framework of the proposed disentangled high quality salient object detection method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of MECF and SGA Modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Examples of trimap. Column 3 shows the trimaps generated from GT. Column 4 shows the trimaps predicted by LRSCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>0.717 0.829 0.075 139.889 0.947 0.802 0.755 0.848 0.042 64.827 0.856 0.743 0.647 0.781 0.098 0.778 0.678 0.804 0.085 0.897 0.841 0.886 0.051 DGRL(CVPR2018) DUTS 0.821 0.789 0.847 0.055 95.034 0.889 0.803 0.772 0.859 0.038 50.323 0.826 0.774 0.709 0.810 0.063 0.828 0.794 0.842 0.050 0.910 0.881 0.896 0.037 DSS ?(TPAMI2019) MB 0.826 0.756 0.840 0.060 145.403 0.952 0.830 0.728 0.865 0.041 94.069 0.890 0.781 0.740 0.790 0.062 0.825 0.808 0.820 0.057 0.916 0.902 0.878 0.040 CPD(CVPR2019) DUTS 0.876 0.829 0.887 0.039 72.686 0.824 0.878 0.822 0.903 0.025 36.649 0.703 0.794 0.745 0.818 0.057 0.864 0.813 0.867 0.043 0.924 0.896 0.904 0.033 EGNet(ICCV2019) DUTS 0.883 0.814 0.888 0.044 73.500 0.896 0.886 0.794 0.897 0.030 37.369 0.799 0.803 0.744 0.813 0.057 0.877 0.800 0.866 0.044 0.927 0.893 0.910 0.035 MINet(CVPR2020) DUTS 0.902 0.851 0.903 0.032 76.291 0.849 0.915 0.864 0.926 0.019 32.304 0.742 0.794 0.741 0.822 0.057 0.877 0.823 0.875 0.039 0.930 0.904 0.912 0.031 ITSD(CVPR2020) DUTS 0.824 0.715 0.834 0.071 139.943 0.924 0.806 0.687 0.843 0.055 92.864 0.861 0.802 0.745 0.828 0.063 0.876 0.798 0.877 0.042 0.927 0.890 0.906 0.035 GateNet(ECCV2020) DUTS 0.905 0.825 0.906 0.035 79.468 0.886 0.914 0.825 0.923 0.023 44.827 0.778 0.794 0.723 0.821 0.061 0.870 0.783 0.870 0.045 0.929 0.889 0.910 0.036 HRNet(ICCV2019) DUTS+HR 0.905 0.888 0.897 0.030 88.017 0.888 0.899 0.888 0.876 0.026 44.359 0.801 0.743 0.690 0.762 0.065 0.835 0.788 0.824 0.050 0.910 0.886 0.877 0.042 Ours DUTS 0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536 0.804 0.769 0.829 0.053 0.882 0.855 0.879 0.036 0.935 0.918 0.913 0.029 Ours-DH DUTS+HR-L 0.921 0.907 0.917 0.024 45.462 0.706 0.938 0.926 0.936 0.014 14.412 0.531 0.795 0.764 0.820 0.052 0.894 0.865 0.879 0.035 0.933 0.914 0.904 0.031 ResNet-50/ResNet-101/ResNeXt-101/Res2Net50 backbone R3Net*(IJCAI2018) MK 0.798 0.744 0.812 0.081 108.910 0.931 0.806 0.753 0.835 0.041 47.373 0.868 0.785 0.690 0.819 0.073 0.778 0.716 0.837 0.067 0.915 0.853 0.894 0.047 BASNet(CVPR2019) DUTS 0.878 0.831 0.890 0.038 67.643 0.823 0.857 0.806 0.881 0.039 46.283 0.705 0.805 0.766 0.838 0.056 0.859 0.791 0.866 0.048 0.928 0.895 0.909 0.032 PFPN (AAAI2020) DUTS 0.889 0.825 0.897 0.042 65.048 0.896 0.886 0.822 0.912 0.025 30.488 0.848 0.818 0.748 0.841 0.057 0.885 0.805 0.887 0.041 0.937 0.896 0.919 0.033 GCPA(AAAI2020) DUTS 0.889 0.827 0.894 0.039 70.320 0.873 0.912 0.833 0.924 0.021 24.132 0.759 0.812 0.748 0.838 0.056 0.888 0.817 0.891 0.038 0.938 0.898 0.920 0.031 F3N(AAAI2020) DUTS 0.900 0.853 0.897 0.035 65.901 0.817 0.915 0.845 0.913 0.020 45.106 0.719 0.813 0.766 0.838 0.053 0.891 0.840 0.888 0.035 0.937 0.910 0.917 0.028 LDF(CVPR2020) DUTS 0.905 0.866 0.905 0.032 58.655 0.812 0.911 0.864 0.922 0.019 35.496 0.713 0.817 0.773 0.839 0.052 0.894 0.855 0.890 0.034 0.939 0.914 0.919 0.028 CSF ?(ECCV2020) DUTS 0.894 0.832 0.900 0.038 71.293 0.922 0.899 0.822 0.912 0.025 30.488 0.848 0.815 0.750 0.838 0.055 0.894 0.823 0.890 0.038 0.935 0.902 0.921 0.030 Ours DUTS 0.915 0.902 0.919 0.024 47.804 0.750 0.935 0.923 0.937 0.013 14.396 0.576 0.818 0.785 0.842 0.051 0.895 0.870 0.892 0.033 0.943 0.928 0.923 0.025 Ours-DH DUTS 0.922 0.909 0.922 0.022 46.495 0.746 0.938 0.926 0.939 0.012 14.266 0.571 0.820 0.791 0.843 0.048 0.900 0.876 0.892 0.031 0.944 0.929 0.922 0Comparison of PR curves across two high-resolution and three low-resolution datasets. Visual comparison between our method and other SOTA methods. Each sample occupies two rows. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Training the network with noisy data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of PR curves across another two conventional low-resolution datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>High-Resolution) 0.868 0.735 0.809 0.073 181.770 0.819 0.720 0.679 0.799 0.062 126.281 0.748 CPD(Low-Resolution) 0.876 0.829 0.887 0.039 72.686 0.824 0.878 0.822 0.903 0.025 36.649 0.703 EGNet(High-Resolution) 0.745 0.693 0.791 0.082 213.333 0.867 0.692 0.644 0.801 0.069 149.537 0.821 EGNet(Low-Resolution) 0.883 0.814 0.888 0.044 73.500 0.896 0.886 0.794 0.897 0.030 37.369 0.799 F3N(High-Resolution) 0.834 0.757 0.825 0.066 187.942 0.798 0.698 0.712 0.826 0.054 130.603 0.716 F3N(Low-Resolution) 0.900 0.853 0.897 0.035 65.901 0.817 0.915 0.845 0.913 0.020 45.106 0.719 GCPA(High-Resolution) 0.810 0.771 0.830 0.066 164.142 0.793 0.750 0.714 0.829 0.057 122.068 0.708 GCPA(Low-Resolution) 0.889 0.827 0.894 0.039 70.320 0.873 0.912 0.833 0.924 0.021 24.132 0.759 MINet(High-Resolution) 0.687 0.629 0.742 0.111 250.149 0.913 0.580 0.508 0.681 0.129 176.671 0.888 MINet(Low-Resolution) 0.902 0.851 0.903 0.032 76.291 0.849 0.915 0.864 0.926 0.019 32.304 0.742 LDF(High-Resolution) 0.650 0.586 0.673 0.133 208.545 0.898 0.590 0.553 0.696 0.101 150.540 0.844 LDF(Low-Resolution) 0.905 0.866 0.905 0.032 58.655 0.812 0.911 0.864 0.922 0.019 35.496 0.713 CSF(High-Resolution) 0.802 0.756 0.843 0.063 181.705 0.873 0.700 0.685 0.824 0.058 137.592 0.816 CSF(Low-Resolution) 0.894 0.832 0.900 0.038 71.293 0.922 0.899 0.822 0.912 0.025 30.488 0.848 Ours 0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Architecture of MECF Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation Studies of LRSCN. .880 0.896 0.034 65.732 0.842 0.899 0.887 0.919 0.021 31.201 0.678 Baseline+ME+HRRN 0.910 0.894 0.902 0.031 60.040 0.801 0.920 0.904 0.924 0.019 24.022 0.612 Baseline+CF+HRRN 0.909 0.892 0.900 0.030 59.028 0.804 0.918 0.905 0.923 0.018 23.988 0.605 Baseline+MECF+HRRN 0.913 0.898 0.909 0.029 54.377 0.766 0.928 0.915 0.929 0.016 20.010 0.578 Baseline+MECF+SGA+HRRN 0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536</figDesc><table><row><cell>Configurations</cell><cell>F max ?</cell><cell>F?</cell><cell>Sm</cell><cell>HRSOD-TE MAE BDE</cell><cell>B?</cell><cell>F max ?</cell><cell>F?</cell><cell>Sm</cell><cell>DAVIS-S MAE</cell><cell>BDE</cell><cell>B?</cell></row><row><cell>Baseline+HRRN</cell><cell>0.900 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation Studies of HRRN.</figDesc><table><row><cell>Ablation</cell><cell>Configurations</cell><cell>F max ?</cell><cell>F?</cell><cell>HRSOD-TE Sm MAE</cell><cell>BDE</cell><cell>B?</cell><cell>F max ?</cell><cell>F?</cell><cell>DAVIS-S Sm MAE</cell><cell>BDE</cell><cell>B?</cell></row><row><cell>Loss</cell><cell cols="11">Ours(L1 + Luncertainy) Ours(L1) Ours-DH(L1 + Luncertainy) 0.918 0.901 0.911 0.027 48.878 0.712 0.933 0.920 0.934 0.015 17.670 0.540 0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536 0.907 0.896 0.908 0.029 53.891 0.780 0.921 0.909 0.927 0.016 18.014 0.622</cell></row><row><cell></cell><cell>Ours(LRSCN)</cell><cell cols="10">0.898 0.885 0.899 0.034 64.805 0.822 0.909 0.898 0.920 0.022 28.798 0.684</cell></row><row><cell></cell><cell>Ours(LRSCN+HRRN)</cell><cell cols="10">0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536</cell></row><row><cell></cell><cell>Ours(LRSCN+CRF)</cell><cell cols="10">0.905 0.896 0.897 0.029 60.521 0.797 0.920 0.907 0.918 0.018 24.455 0.665</cell></row><row><cell></cell><cell>EGNet</cell><cell cols="10">0.883 0.814 0.888 0.044 73.500 0.896 0.886 0.794 0.897 0.030 37.369 0.799</cell></row><row><cell>Architecture</cell><cell>EGNet(+HRRN) EGNet(+CRF) CPD CPD(+HRRN) CPD(+CRF)</cell><cell cols="10">0.900 0.862 0.889 0.039 72.982 0.753 0.904 0.858 0.898 0.024 34.860 0.603 0.895 0.858 0.882 0.039 73.348 0.796 0.902 0.846 0.892 0.025 34.721 0.679 0.876 0.829 0.887 0.039 72.686 0.824 0.878 0.822 0.903 0.025 36.649 0.703 0.891 0.855 0.888 0.036 72.796 0.734 0.893 0.859 0.907 0.022 35.294 0.564 0.885 0.851 0.884 0.037 76.440 0.772 0.884 0.852 0.903 0.023 36.915 0.633</cell></row><row><cell></cell><cell>BASNet(En-De)</cell><cell cols="10">0.873 0.827 0.888 0.039 70.944 0.824 0.852 0.802 0.880 0.039 48.309 0.703</cell></row><row><cell></cell><cell>BASNet(En-De+HRRN)</cell><cell cols="10">0.895 0.858 0.892 0.036 67.191 0.719 0.875 0.832 0.882 0.036 45.922 0.566</cell></row><row><cell></cell><cell>BASNet(En-De+RRM)</cell><cell cols="10">0.878 0.831 0.890 0.038 67.643 0.823 0.857 0.806 0.881 0.039 46.283 0.705</cell></row><row><cell></cell><cell>BASNet(En-De+CRF)</cell><cell cols="10">0.888 0.850 0.886 0.036 77.235 0.758 0.867 0.825 0.878 0.037 46.423 0.629</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HRSOD-TE</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>!</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>" Erosion or Dilation Kernel</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparison with SOTA methods on another two conventional datasets.</figDesc><table><row><cell>Models</cell><cell>Training datasets</cell><cell>F max ?</cell><cell>F ?</cell><cell>ECSSD S m</cell><cell>MAE F max ?</cell><cell>PASCAL-S F ? S m</cell><cell>MAE</cell></row><row><cell></cell><cell></cell><cell cols="3">VGG-16 backbone</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Amulet(ICCV2017)</cell><cell>MK</cell><cell cols="6">0.915 0.868 0.894 0.059 0.828 0.757 0.818 0.100</cell></row><row><cell>DGRL(CVPR2018)</cell><cell>DUTS</cell><cell cols="6">0.922 0.903 0.906 0.043 0.849 0.807 0.834 0.074</cell></row><row><cell>DSS(TPAMI2019)</cell><cell>MB</cell><cell cols="6">0.921 0.904 0.882 0.052 0.831 0.802 0.798 0.094</cell></row><row><cell>CPD(CVPR2019)</cell><cell>DUTS</cell><cell cols="6">0.936 0.917 0.917 0.037 0.861 0.824 0.842 0.072</cell></row><row><cell>EGNET(ICCV2019)</cell><cell>DUTS</cell><cell cols="6">0.943 0.913 0.913 0.041 0.858 0.809 0.848 0.077</cell></row><row><cell>MINet(CVPR2020)</cell><cell>DUTS</cell><cell cols="6">0.943 0.922 0.917 0.036 0.865 0.829 0.854 0.064</cell></row><row><cell>ITSD(CVPR2020)</cell><cell>DUTS</cell><cell cols="6">0.939 0.875 0.914 0.040 0.869 0.773 0.853 0.068</cell></row><row><cell>GateNet(ECCV2020)</cell><cell>DUTS</cell><cell cols="6">0.941 0.896 0.917 0.041 0.870 0.797 0.853 0.068</cell></row><row><cell>HRNet(ICCV2019)</cell><cell>DUTS+HR</cell><cell cols="6">0.925 0.905 0.888 0.052 0.846 0.804 0.817 0.079</cell></row><row><cell>Ours</cell><cell>DUTS</cell><cell cols="6">0.948 0.931 0.918 0.034 0.874 0.845 0.854 0.063</cell></row><row><cell>Ours-DH</cell><cell cols="7">DUTS+HR-L 0.938 0.918 0.904 0.040 0.871 0.845 0.851 0.061</cell></row><row><cell></cell><cell cols="5">ResNet-50/ResNet-101/ResNeXt-101/Res2Net50 backbone</cell><cell></cell><cell></cell></row><row><cell>R3Net(IJCAI2018)</cell><cell>MK</cell><cell cols="6">0.934 0.883 0.910 0.051 0.834 0.775 0.809 0.101</cell></row><row><cell>BasNet(CVPR2019)</cell><cell>DUTS</cell><cell cols="6">0.942 0.880 0.916 0.037 0.854 0.775 0.832 0.076</cell></row><row><cell>PFPN(AAAI2020)</cell><cell>DUTS</cell><cell cols="6">0.947 0.917 0.927 0.035 0.870 0.824 0.851 0.065</cell></row><row><cell>GCPA(AAAI2020)</cell><cell>DUTS</cell><cell cols="6">0.948 0.919 0.927 0.035 0.869 0.827 0.860 0.062</cell></row><row><cell>F3N(AAAI2020)</cell><cell>DUTS</cell><cell cols="6">0.945 0.925 0.924 0.036 0.872 0.840 0.855 0.062</cell></row><row><cell>LDF(CVPR2020)</cell><cell>DUTS</cell><cell cols="6">0.950 0.930 0.924 0.034 0.874 0.843 0.859 0.061</cell></row><row><cell>CSF(ECCV2020)</cell><cell>DUTS</cell><cell cols="6">0.950 0.925 0.927 0.033 0.874 0.823 0.858 0.069</cell></row><row><cell>Ours</cell><cell>DUTS</cell><cell cols="6">0.952 0.941 0.928 0.029 0.880 0.852 0.861 0.059</cell></row><row><cell>Ours-DH</cell><cell cols="7">DUTS+HR-L 0.953 0.941 0.926 0.030 0.878 0.852 0.859 0.060</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Performance on SOC of different attributes. The last row shows the whole performance on the SOC dataset. Model size and running time comparisons between our approach and SOTA methods.</figDesc><table><row><cell cols="11">Attr BASNet CPD EGNet F3N GCPA PFPN ITSD LDF MINet CSF GateNet Ours Ours-DH</cell></row><row><cell>AC</cell><cell>0.723</cell><cell cols="7">0.750 0.756 0.784 0.780 0.772 0.611 0.796 0.790 0.730</cell><cell>0.748</cell><cell>0.793</cell><cell>0.788</cell></row><row><cell>BO</cell><cell>0.511</cell><cell cols="7">0.794 0.702 0.791 0.882 0.837 0.499 0.807 0.814 0.825</cell><cell>0.737</cell><cell>0.858</cell><cell>0.848</cell></row><row><cell>CL</cell><cell>0.682</cell><cell cols="7">0.771 0.726 0.757 0.765 0.765 0.610 0.763 0.770 0.751</cell><cell>0.754</cell><cell>0.789</cell><cell>0.789</cell></row><row><cell>HO</cell><cell>0.772</cell><cell cols="7">0.777 0.756 0.790 0.780 0.777 0.685 0.797 0.792 0.779</cell><cell>0.788</cell><cell>0.817</cell><cell>0.817</cell></row><row><cell>MB</cell><cell>0.687</cell><cell cols="7">0.715 0.687 0.761 0.691 0.705 0.589 0.758 0.708 0.702</cell><cell>0.725</cell><cell>0.764</cell><cell>0.768</cell></row><row><cell>OC</cell><cell>0.686</cell><cell cols="7">0.719 0.702 0.724 0.720 0.729 0.629 0.739 0.729 0.703</cell><cell>0.728</cell><cell>0.771</cell><cell>0.771</cell></row><row><cell>OV</cell><cell>0.720</cell><cell cols="7">0.764 0.764 0.793 0.802 0.806 0.639 0.805 0.788 0.772</cell><cell>0.787</cell><cell>0.798</cell><cell>0.802</cell></row><row><cell>SC</cell><cell>0.708</cell><cell cols="7">0.723 0.683 0.747 0.707 0.697 0.592 0.746 0.726 0.690</cell><cell>0.715</cell><cell>0.785</cell><cell>0.782</cell></row><row><cell>SO</cell><cell>0.632</cell><cell cols="7">0.643 0.614 0.668 0.640 0.636 0.523 0.691 0.652 0.621</cell><cell>0.641</cell><cell>0.713</cell><cell>0.713</cell></row><row><cell>Avg</cell><cell>0.680</cell><cell cols="7">0.740 0.710 0.757 0.752 0.747 0.597 0.767 0.753 0.730</cell><cell>0.736</cell><cell>0.788</cell><cell>0.787</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>Ours</cell><cell>DGRL</cell><cell>DSS</cell><cell>BASNet</cell><cell>EGNet</cell><cell cols="2">GCPA</cell><cell>PFPN</cell><cell>R3Net</cell></row><row><cell cols="2">Model Size(MB)</cell><cell>309.6</cell><cell>309.6</cell><cell>648</cell><cell>447.3</cell><cell>412.2</cell><cell>332.1</cell><cell cols="2">255.8</cell><cell>243.0</cell><cell>214.2</cell></row><row><cell></cell><cell>Time(s)</cell><cell>0.21</cell><cell>0.05</cell><cell>0.52</cell><cell>5.12</cell><cell>0.04</cell><cell>0.15</cell><cell></cell><cell>0.02</cell><cell>0.05</cell><cell>0.27</cell></row><row><cell></cell><cell>Size</cell><cell cols="9">1024 ? 1024 352 ? 352 384 ? 384 224 ? 224 256 ? 256 400 ? 300 320 ? 320 256 ? 256 256 ? 256</cell></row><row><cell></cell><cell></cell><cell>HRNet</cell><cell>MINet</cell><cell>CSF</cell><cell>Amulet</cell><cell>CPD</cell><cell>F3N</cell><cell></cell><cell>LDF</cell><cell>ITSD</cell><cell>GateNet</cell></row><row><cell cols="2">Model Size(MB)</cell><cell>129.6</cell><cell>181.4</cell><cell>139.3</cell><cell>132.6</cell><cell>111.5</cell><cell>97.4</cell><cell></cell><cell>95.9</cell><cell>63.7</cell><cell>-</cell></row><row><cell></cell><cell>Time(s)</cell><cell>0.39</cell><cell>0.01</cell><cell>0.01</cell><cell>0.05</cell><cell>0.02</cell><cell>0.03</cell><cell></cell><cell>0.02</cell><cell>0.02</cell><cell>0.03</cell></row><row><cell></cell><cell>Size</cell><cell cols="9">1024 ? 1024 320 ? 320 224 ? 224 256 ? 256 352 ? 352 352 ? 352 352 ? 352 288 ? 288 384 ? 384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Quantitative comparison with SOTA methods which are finetuned on HRSOD-Training dataset.</figDesc><table><row><cell>Models</cell><cell>F max ?</cell><cell>F ?</cell><cell>HRSOD-TE S m MAE BDE</cell><cell>B ?</cell><cell>F max ?</cell><cell>F ?</cell><cell>DAVIS-S S m MAE BDE</cell><cell>B ?</cell></row><row><cell cols="4">BASNet(finetune) 0.885 0.836 0.904 0.035 64.475</cell><cell cols="5">0.813 0.866 0.838 0.911 0.023 25.924 0.659</cell></row><row><cell cols="4">BASNet(original) 0.878 0.831 0.890 0.038 67.643</cell><cell cols="5">0.823 0.857 0.806 0.881 0.039 46.283 0.705</cell></row><row><cell>CPD(finetune)</cell><cell cols="3">0.890 0.846 0.899 0.035 80.857</cell><cell cols="5">0.783 0.890 0.871 0.925 0.020 29.376 0.671</cell></row><row><cell>CPD(original)</cell><cell cols="3">0.876 0.829 0.887 0.039 72.686</cell><cell cols="5">0.824 0.878 0.822 0.903 0.025 36.649 0.703</cell></row><row><cell>EGNet(finetune)</cell><cell cols="3">0.890 0.857 0.911 0.031 69.084</cell><cell cols="5">0.797 0.899 0.881 0.926 0.021 30.674 0.686</cell></row><row><cell>EGNet(original)</cell><cell cols="3">0.883 0.814 0.888 0.044 73.500</cell><cell cols="5">0.896 0.886 0.794 0.897 0.030 37.369 0.799</cell></row><row><cell>GCPA(finetune)</cell><cell cols="3">0.895 0.837 0.912 0.032 64.656</cell><cell cols="5">0.846 0.918 0.857 0.927 0.019 22.312 0.746</cell></row><row><cell>GCPA(original)</cell><cell cols="3">0.889 0.827 0.894 0.039 70.320</cell><cell cols="5">0.873 0.912 0.833 0.924 0.021 24.132 0.759</cell></row><row><cell>F3N(finetune)</cell><cell cols="3">0.905 0.865 0.909 0.033 60.803</cell><cell cols="5">0.787 0.920 0.860 0.921 0.019 29.106 0.661</cell></row><row><cell>F3N(original)</cell><cell cols="3">0.900 0.853 0.897 0.035 65.901</cell><cell cols="5">0.817 0.915 0.845 0.913 0.020 45.106 0.719</cell></row><row><cell>PFPN(finetune)</cell><cell cols="3">0.896 0.840 0.904 0.038 55.027</cell><cell cols="5">0.786 0.901 0.845 0.920 0.022 21.388 0.728</cell></row><row><cell>PFPN(original)</cell><cell cols="3">0.889 0.825 0.897 0.042 65.048</cell><cell cols="5">0.897 0.886 0.822 0.912 0.025 30.488 0.848</cell></row><row><cell>ITSD(finetune)</cell><cell cols="8">0.834 0.774 0.863 0.052 117.554 0.906 0.820 0.754 0.873 0.041 75.461 0.830</cell></row><row><cell>ITSD(original)</cell><cell cols="8">0.824 0.715 0.834 0.071 139.943 0.924 0.806 0.687 0.843 0.055 92.864 0.861</cell></row><row><cell>MINet(finetune)</cell><cell cols="3">0.908 0.871 0.908 0.029 66.089</cell><cell cols="5">0.749 0.923 0.879 0.928 0.017 25.408 0.692</cell></row><row><cell>MINet(original)</cell><cell cols="3">0.902 0.851 0.903 0.032 76.291</cell><cell cols="5">0.849 0.915 0.864 0.926 0.019 32.304 0.742</cell></row><row><cell>LDF(finetune)</cell><cell cols="3">0.910 0.862 0.910 0.031 77.098</cell><cell cols="5">0.812 0.920 0.867 0.922 0.018 42.226 0.727</cell></row><row><cell>LDF(original)</cell><cell cols="3">0.905 0.866 0.905 0.032 58.655</cell><cell cols="5">0.812 0.911 0.864 0.922 0.019 35.496 0.713</cell></row><row><cell cols="4">GateNet(finetune) 0.910 0.856 0.909 0.029 76.434</cell><cell cols="5">0.821 0.923 0.872 0.930 0.019 36.984 0.706</cell></row><row><cell cols="4">GateNet(original) 0.905 0.825 0.906 0.035 79.468</cell><cell cols="5">0.886 0.914 0.825 0.923 0.023 44.827 0.778</cell></row><row><cell>CSF(finetune)</cell><cell cols="3">0.902 0.859 0.909 0.029 56.425</cell><cell cols="5">0.884 0.910 0.870 0.931 0.017 24.669 0.791</cell></row><row><cell>CSF(original)</cell><cell cols="3">0.894 0.832 0.900 0.038 71.293</cell><cell cols="5">0.922 0.899 0.822 0.912 0.025 30.488 0.848</cell></row><row><cell>Ours</cell><cell cols="3">0.918 0.902 0.912 0.027 48.468</cell><cell cols="5">0.711 0.933 0.919 0.933 0.015 15.676 0.536</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Ablation Studies of disentangled framework. .894 0.899 0.031 56.251 0.814 0.923 0.909 0.918 0.019 22.737 0.649 Classification-Classification 0.913 0.895 0.898 0.030 54.143 0.809 0.921 0.907 0.921 0.020 23.892 0.662 Ours 0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536</figDesc><table><row><cell>Configurations</cell><cell>F max ?</cell><cell>F?</cell><cell>HRSOD-TE Sm MAE</cell><cell>BDE</cell><cell>B?</cell><cell>F max ?</cell><cell>F?</cell><cell>DAIVS-S Sm</cell><cell>MAE</cell><cell>BDE</cell><cell>B?</cell></row><row><cell>Regression-Regression</cell><cell>0.912 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Ablation Studies of L saliency . LR + LO + Ltrimap)+HRRN 0.918 0.902 0.912 0.027 48.468 0.711 0.933 0.919 0.933 0.015 15.676 0.536</figDesc><table><row><cell>Configurations</cell><cell>F max ?</cell><cell>F?</cell><cell>HRSOD-TE Sm MAE</cell><cell>BDE</cell><cell>B?</cell><cell>F max ?</cell><cell>F?</cell><cell>DAIVS-S Sm</cell><cell>MAE</cell><cell>BDE</cell><cell>B?</cell></row><row><cell>LRSCN(Ltrimap)+HRRN</cell><cell cols="11">0.895 0.870 0.883 0.035 75.732 0.879 0.900 0.880 0.890 0.026 41.221 0.733</cell></row><row><cell>LRSCN(LP + Ltrimap)+HRRN</cell><cell cols="11">0.912 0.898 0.908 0.029 53.040 0.764 0.925 0.910 0.926 0.018 19.022 0.569</cell></row><row><cell>LRSCN(LP + LR + Ltrimap)+HRRN</cell><cell cols="11">0.917 0.900 0.910 0.029 52.048 0.743 0.932 0.914 0.930 0.017 17.688 0.552</cell></row><row><cell>LRSCN(LP +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="252" />
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">R 3 net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (15)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Yet another survey on image segmentation: Region and boundary information integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Mart?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Cuf?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">2352</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Highly efficient salient object detection with 100k parameters. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Qiang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Permitted and forbidden sets in symmetric threshold-linear networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Sebastian</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Advanced deep-learning techniques for salient and category-specific object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency-guided attention network for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dominik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2214" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-saliency spatio-temporal interaction network for person re-identification in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xierong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Jiang</surname></persName>
		</author>
		<idno>ij- cai.org, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1012" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Employing deep part-object relationships for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1232" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>J?gersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI (3)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended CSSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-spectral salient object detection by adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjiang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12023" to="12030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Cengiz?ztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tun? Ozan</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aydin</surname></persName>
		</author>
		<idno>2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="3055" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Progressive feature polishing network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="268" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning for light field saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernelized subspace ranking for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="450" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">F3net: Fusion, feedback and focus for salient object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Label decoupling framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7263" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>PMLR, 2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Uc-net: Uncertainty inspired RGB-D saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8579" to="8588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning noise-aware encoder-decoder from noisy labels by alternating back-propagation for saliency detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weakly-supervised salient object detection via scribble annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peipei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Optimizing the f-measure for threshold-free salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Suppress and balance: A simple gated network for salient object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Interactive two-stream decoder for accurate and fast saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning saliency propagation for semisupervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10304" to="10313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multi-type self-attention guided degraded saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meijun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13082" to="13089" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
