<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AAAI 2021 Context-aware Attentional Pooling (CAP) for Fine-grained Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardhendu</forename><surname>Behera</surname></persName>
							<email>beheraa@edgehill.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Lancashire United Kingdom</orgName>
								<address>
									<addrLine>Edge Hill University St Helen Road</addrLine>
									<postCode>L39 4QP</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Wharton</surname></persName>
							<email>zachary.wharton@go.edgehill.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Lancashire United Kingdom</orgName>
								<address>
									<addrLine>Edge Hill University St Helen Road</addrLine>
									<postCode>L39 4QP</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Hewage</surname></persName>
							<email>pradeep.hewage@edgehill.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Lancashire United Kingdom</orgName>
								<address>
									<addrLine>Edge Hill University St Helen Road</addrLine>
									<postCode>L39 4QP</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asish</forename><surname>Bera</surname></persName>
							<email>beraa@edgehill.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Lancashire United Kingdom</orgName>
								<address>
									<addrLine>Edge Hill University St Helen Road</addrLine>
									<postCode>L39 4QP</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AAAI 2021 Context-aware Attentional Pooling (CAP) for Fine-grained Visual Classification</title>
					</analytic>
					<monogr>
						<title level="m">35 th AAAI Conference on Artificial Intelligence</title>
						<imprint>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<note>Extended version of the accepted paper in</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNNs) have shown a strong ability in mining discriminative object pose and parts information for image recognition. For fine-grained recognition, context-aware rich feature representation of object/scene plays a key role since it exhibits a significant variance in the same subcategory and subtle variance among different subcategories. Finding the subtle variance that fully characterizes the object/scene is not straightforward. To address this, we propose a novel context-aware attentional pooling (CAP) that effectively captures subtle changes via sub-pixel gradients, and learns to attend informative integral regions and their importance in discriminating different subcategories without requiring the bounding-box and/or distinguishable part annotations. We also introduce a novel feature encoding by considering the intrinsic consistency between the informativeness of the integral regions and their spatial structures to capture the semantic correlation among them. Our approach is simple yet extremely effective and can be easily applied on top of a standard classification backbone network. We evaluate our approach using six state-of-the-art (SotA) backbone networks and eight benchmark datasets. Our method significantly outperforms the SotA approaches on six datasets and is very competitive with the remaining two.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Over recent years, there has been significant progress in the landscape of computer vision due to the adaptation and enhancement of a fast, scalable and end-to-end learning framework, the <ref type="bibr">CNN (LeCun et al. 1998</ref>). This is not a recent invention, but we now see a profusion of CNN-based models achieving SotA results in visual recognition <ref type="bibr" target="#b17">(He et al. 2016;</ref><ref type="bibr" target="#b21">Huang et al. 2017;</ref><ref type="bibr" target="#b65">Zoph et al. 2018;</ref><ref type="bibr" target="#b46">Sandler et al. 2018)</ref>. The performance gain primarily comes from the model's ability to reason about image content by disentangling discriminative object pose and part information from texture and shape. Most discriminative features are often based on changes in global shape and appearance. They are often ill-suited to distinguish subordinate categories, involving subtle visual differences within various natural objects such as bird species <ref type="bibr" target="#b54">(Wah et al. 2011;</ref><ref type="bibr" target="#b53">Van Horn et al. 2015)</ref>, flower categories <ref type="bibr" target="#b41">(Nilsback and Zisserman 2008)</ref>, dog breeds <ref type="bibr">(Khosla et al. Copyright ? 2021</ref>, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 2011), pets <ref type="bibr" target="#b42">(Parkhi et al. 2012</ref>) and man-made objects like aircraft types <ref type="bibr" target="#b39">(Maji et al. 2013)</ref>, car models <ref type="bibr" target="#b30">(Krause et al. 2013)</ref>, etc. To address this, a global descriptor is essential which ensembles features from multiple local parts and their hierarchy so that the subtle changes can be discriminated as a misalignment of local parts or pattern. The descriptor should also be able to emphasize the importance of a part.</p><p>There have been some excellent works on fine-grained visual recognition (FGVC) using weakly-supervised complementary parts <ref type="bibr" target="#b14">(Ge, Lin, and Yu 2019)</ref>, part attention <ref type="bibr" target="#b37">(Liu et al. 2016)</ref>, object-part attention <ref type="bibr" target="#b44">(Peng, He, and Zhao 2018)</ref>, multi-agent cooperative learning <ref type="bibr" target="#b57">(Yang et al. 2018</ref>), recurrent attention <ref type="bibr" target="#b13">(Fu, Zheng, and Mei 2017)</ref>, and destruction and construction learning . All these approaches avoid part-level annotations and automatically discriminate local parts in an unsupervised/weakly-supervised manner. Many of them use a pre-trained object/parts detector and lack rich representation of regions/parts to capture the object-parts relationships better. To truly describe an image, we need to consider the image generation process from pixels to object to the scene in a more fine-grained way, not only to regulate the object/parts and their spatial arrangements but also defining their appearances using multiple partial descriptions as well as their importance in discriminating subtle changes. These partial descriptions should be rich and complementary to each other to provide a complete description of the object/image. In this work, we propose a simple yet compelling approach that embraces the above properties systematically to address the challenges associated with the FGVC. Thus, it can benefit to a wide variety of applications such as image captioning <ref type="bibr" target="#b18">(Herdade et al. 2019;</ref><ref type="bibr" target="#b22">Huang et al. 2019a;</ref><ref type="bibr" target="#b33">Li et al. 2019)</ref>, expert-level image recognition <ref type="bibr" target="#b51">(Valan et al. 2019;</ref><ref type="bibr" target="#b29">Krause et al. 2016)</ref>, and so on. Our work: To describe objects in a conventional way as in CNNs as well as maintaining their visual appearance, we design a context-aware attentional pooling (CAP) to encode spatial arrangements and visual appearance of the parts effectively. The module takes the input as a convolutional feature from a base CNN and then learns to emphasize the latent representation of multiple integral regions (varying coarseness) to describe hierarchies within objects and parts. Each region has an anchor in the feature map, and thus many regions have the same anchor due to the integral characteristics. These integral regions are then fed into a recurrent net-work (e.g. LSTM) to capture their spatial arrangements, and is inspired by the visual recognition literature, which suggests that humans do not focus their attention on an entire scene at once. Instead, they focus sequentially by attending different parts to extract relevant information <ref type="bibr" target="#b66">(Zoran et al. 2020)</ref>. A vital characteristic of our CAP is that it generates a new feature map by focusing on a given region conditioned on all other regions and itself. Moreover, it efficiently captures subtle variations in each region by the sub-pixel gradients via bilinear pooling. The recurrent networks are mainly designed for sequence analysis/recognition. We aim to capture the subtle changes between integral regions and their spatial arrangements. Thus, we introduce a learnable pooling to emphasize the most-informative hidden states of the recurrent network, automatically. It learns to encode the spatial arrangement of the latent representation of integral regions and uses it to infer the fine-grained subcategories. Our contributions: Our main contributions can be summarized as: 1) an easy-to-use extension to SotA base CNNs by incorporating context-aware attention to achieve a considerable improvement in FGVC; 2) to discriminate the subtle changes in an object/scene, context-aware attentionguided rich representation of integral regions is proposed; 3) a learnable pooling is also introduced to automatically select the hidden states of a recurrent network to encode spatial arrangement and appearance features; 4) extensive analysis of the proposed model on eight FGVC datasets, obtaining SotA results; and 5) analysis of various base networks for the wider applicability of our CAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Unsupervised/weakly-supervised parts/regions based approaches: Such methods learn a diverse collection of discriminative parts/regions to represent the complete description of an image. In , the global structure of an image is substantially changed by a random patch-shuffling mechanism to select informative regions. An adversarial loss is used to learn essential patches. In <ref type="bibr" target="#b14">(Ge, Lin, and Yu 2019)</ref>, Mask R-CNN and conditional random field are used for object detection and segmentation. A bidirectional LSTM is used to encode rich complementary information from selected part proposals for classification. A hierarchical bilinear pooling framework is presented in <ref type="bibr" target="#b58">(Yu et al. 2018a)</ref> to learn the inter-layer part feature interaction from intermediate convolution layers. This pooling scheme enables inter-layer feature interaction and discriminative part feature learning in a mutually reinforced manner. In <ref type="bibr" target="#b3">(Cai, Zuo, and Zhang 2017)</ref>, a higher-order integration of hierarchical convolutional features is described for representing parts semantic at different scales. A polynomial kernel-based predictor is defined for modelling part interaction using higher-order statistics of convolutional activations. A general pooling scheme is demonstrated in <ref type="bibr" target="#b9">(Cui et al. 2017)</ref> to represent higher-order and nonlinear feature interactions via compact and explicit feature mapping using kernels. Our approach is complementary to these approaches by exploring integral regions and learns to attend these regions using a bilinear pooling that encodes partial information from multiple integral regions to a comprehensive feature vector for subordinate classification. Object and/or part-level attention-based approaches:</p><p>Recently, there has been significant progress to include attention mechanisms <ref type="bibr" target="#b61">(Zhao, Jia, and Koltun 2020;</ref><ref type="bibr" target="#b32">Leng, Liu, and Chen 2019;</ref><ref type="bibr" target="#b1">Bello et al. 2019;</ref><ref type="bibr" target="#b43">Parmar et al. 2019)</ref> to boost image recognition accuracy. It is also explored in FGVC <ref type="bibr" target="#b63">(Zheng et al. 2019;</ref><ref type="bibr" target="#b25">Ji et al. 2018;</ref><ref type="bibr" target="#b48">Sun et al. 2018)</ref>. In <ref type="bibr" target="#b64">(Zheng et al. 2020</ref>), a part proposal network produces several local attention maps, and a part rectification network learns rich part hierarchies. Recurrent attention in <ref type="bibr" target="#b13">(Fu, Zheng, and Mei 2017)</ref> learns crucial regions at multiple scales. The attended regions are cropped and scaled up with a higher resolution to compute rich features. Object-part attention model (OPAM) in <ref type="bibr" target="#b44">(Peng, He, and Zhao 2018)</ref> incorporates object-level attention for object localization, and part-level attention for the vital parts selection. Both jointly learn multi-view and multi-scale features to improve performance. In ), a bidirectional attentionrecognition model (BARM) is proposed to optimize the region proposals via a feedback path from the recognition module to the part localization module. Similarly, in attention pyramid hierarchy , top-down and bottom-up attentions are integrated to learn both high-level semantic and low-level detailed feature representations. In <ref type="bibr" target="#b45">(Rodr?guez et al. 2020</ref>), a modular feed-forward attention mechanism consisting of attention modules and attention gates is applied to learn low-level feature activations. Our novel paradigm is a step forward and takes inspiration from these approaches. It is advantageous over the existing methods as it uses a single network and the proposed attention mechanism learns to attend both appearance and shape information from a single-scale image in a hierarchical fashion by exploring integral regions. We further extend it by innovating the classification layer, where the subtle changes in integral regions are learned by focusing on the most informative hidden states of an LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach</head><p>The overall pipeline of our model is shown in <ref type="figure" target="#fig_1">Fig. 1a</ref>. It takes an input image and provides output as a subordinate class label. To solve this, we are given N images I = {I n |n = 1, . . . , N } and their respective fine-grained labels. The aim is to find a mapping function F that predicts? n = F(I n ), which matches the true label y n . The ultimate goal is to learn F by minimizing a loss L(y n ,? n ) between the true and the predicted label. Our model consists of three elements ( <ref type="figure" target="#fig_1">Fig.  1a</ref>): 1) a base CNN F b (.; ? b ), and our novel 2) CAP F c (.; ? c ) and 3) classification F d (.; ? d ) modules. We aim to learn the model's parameters ? = {? b , ? c , ? d } via end-to-end training. We use the SotA CNN architecture as a base CNN F b (.; ? b ) and thus, we emphasize on the design and implementation of the rest two modules F c (.; ? c ) and F d (.; ? d ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-aware attentional pooling (CAP)</head><p>It takes the output of a base CNN as an input. Let us consider x = F b (I n ; ? b ) to be the convolutional feature map as the output of the base network F b for input image I n .</p><p>The proposed CAP considers contextual information from  pixel-level to small patches to large patches to image-level. The pixel refers to a spatial location in the convolutional feature map x of width W , height H and channels C. The aim is to capture contextual information hierarchically to better model the subtle changes observed in FGVC tasks. Our attention mechanism learns to emphasize pixels, as well as regions of different sizes located in various parts of the image I n . At pixel-level, we explicitly learn the relationships between pixels, i.e. p(x i |x j ; ? p ), ?i i = j and 1 ? i, j ? W ? H, even they are located far apart in x. It signifies how much the model should attend the i th location when synthesizing the j th position in x ( <ref type="figure">Fig. 2a</ref>). To achieve this, we compute the attention map ? p by revisiting the self-attention concept  where key k(</p><formula xml:id="formula_0">x) = W k x, query q(x) = W q x and value v(x) = W v x in x are computed using separate 1 ? 1 convolutions. The attentional output feature map o is a dot-product of attention map ? p and x. ? p = {W k , W q , W v } ? ? c is learned.</formula><p>Proposing integral regions: To learn contextual information efficiently, we propose multiple integral regions with varying level of coarseness on the feature map o. The level of coarseness is captured by different size of a rectangular region. Let us consider the smallest region r(i, j, ? x , ? y ) of width ? x , height ? y and is located (top-left corner) at the i th column and j th row of o. Using r(i, j, ? x , ? y ), we derive a set of regions by varying their widths and heights i.e. R = {r(i, j, m? x , n? y )}; m, n = 1, 2, 3, . . . and <ref type="figure" target="#fig_1">Fig. 1b</ref> (left) for the given spatial location of (i, j). The goal is to generate the similar set of regions R at various spatial locations (0 &lt; i &lt; W , 0 &lt; j &lt; H) in o. In this way, we generate a final set of regions R = {R} located at different places with different sizes and aspect ratios, as shown in <ref type="figure" target="#fig_1">Fig. 1b</ref>. The approach is a comprehensive contextaware representation to capture the rich contextual information characterizing subtle changes in images hierarchically. Bilinear pooling: There are |R| regions with size varies from a minimum of ? x ? ? y ? C to a maximum of W ? H ? C. The goal is to represent these variable size</p><formula xml:id="formula_1">i &lt; i + m? x ? W , j &lt; j + n? y ? H. This is illus- trated in</formula><formula xml:id="formula_2">regions (X ? Y ? C) ? (w ? h ? C)</formula><p>with a fixed size feature vector. Thus, we use bilinear pooling, typically bilinear interpolation to implement differentiable image transformations, which requires indexing operation. Let T ? (y) be the coordinate transformation with parameters ? and y = (i, j) ? R 2 denotes a region coordinates at which the feature value is R(y) ? R C . The transformed imageR at the target coordinate? is:</p><formula xml:id="formula_3">R(?) = y R(T ? (y)) K(?, T ? (y)),<label>(1)</label></formula><p>where R(T ? (y)) is the image indexing operation and is nondifferentiable; thus, the way gradients propagate through the network depends on the kernel K(., .). In bilinear interpolation, the kernel K(y 1 , y 2 ) = 0 when y 1 and y 2 are not direct neighbors. Therefore, the sub-pixel gradients (i.e. the feature value difference between neighboring locations in the original region) only flow through during propagation <ref type="bibr" target="#b27">(Jiang et al. 2019)</ref>. This is an inherent flaw in bilinear interpolation since the sub-pixel gradients will not associate to the large-scale changes which cannot be captured by the immediate neighborhood of a point. To overcome this, several variants <ref type="bibr" target="#b27">(Jiang et al. 2019;</ref><ref type="bibr" target="#b35">Lin and Lucey 2017)</ref> have been proposed. However, for our work, we exploit this flaw to capture the subtle changes in all regions via sub-pixel gradients. Note that the bilinear interpolation, although is not differentiable at all points due to the floor and ceiling functions, can backpropagate the error and is differentiable in most inputs as mentioned in the seminal work of Spatial Transform Networks <ref type="bibr" target="#b24">(Jaderberg et al. 2015)</ref>. We use bilinear kernel K(., .) in (1) to pool fixed size featuresf r (w?h?C) from all r ? R.</p><p>Context-aware attention: In this step, we capture the contextual information using our novel attention mechanism, which transformsf r to a weighted version of itself and conditioned on the rest of the feature mapsf r (r, r ? R). It enables our model to selectively focus on more relevant integral regions to generate holistic context information. The proposed context-aware attention takes a query q(f r ) and maps against a set of keys k(f r ) associated with the integral regions r in a given image, and then returns the output as a context vector c r and is computed as:</p><formula xml:id="formula_4">c r = |R| r =1 ? r,r f r , ? r,r = softmax (W ? ? r,r + b ? ) ? r,r = tanh q(f r ) + k(f r ) + b ? q(f r ) = W ?fr and k(f r ) = W ? f r ,<label>(2)</label></formula><p>where weight matrices W ? and W ? are for estimating the query and key from the respective feature mapsf r andf r ; W ? is their nonlinear combination; b ? and b ? are the biases. These matrices and biases</p><formula xml:id="formula_5">({W ? , W ? , W ? , b ? , b ? } ? ? c )</formula><p>are learnable parameters. The context-aware attention element ? r,r captures the similarity between the feature maps f r andf r of regions r and r , respectively. The attentionfocused context vector c r determines the strength off r in focus conditioned on itself and its neighborhood context. This applies to all integral regions r (refer <ref type="figure">Fig. 2b)</ref>. Spatial structure encoding: The context vectors c = {c r |r = 1 . . . |R|} characterize the attention and saliency.</p><p>To include the structural information involving the spatial arrangements of regions (see <ref type="figure" target="#fig_1">Fig. 1b and 2b</ref>), we represent c as a sequence of regions ( <ref type="figure">Fig. 2c</ref>) and adapt a recurrent network to capture the structural knowledge using its internal states, which is modeled via hidden units h r ? R n . Thus, the internal state representing the region r is updated as:</p><formula xml:id="formula_6">h r = F h (h r?1 , f r ; ? h ),</formula><p>where F h is a nonlinear function with learnable parameter ? h . We use a fully-gated LSTM as F h <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber 1997)</ref> which is capable of learning long-term dependencies. The parameter ? h ? ? c consists of weight matrices and biases linking input, forget and output gates, and cell states of F h . For simplicity, we omitted equations to compute these parameters and refer interested readers to <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber 1997)</ref> for further details. To improve the generalizability and lower the computational complexity of our CAP, the context feature f r is extracted from the context vector c r via global average pooling (GAP). This results in the reduction of feature map size from (w ? h ? C) to (1 ? C). The sequence of hidden states h = (h 1 , h 2 , . . . , h r , . . . , h |R| ) corresponding to the input sequence of context feature f = (f 1 , f 2 , . . . , f r , . . . , f |R| ) (see <ref type="figure" target="#fig_1">Fig. 1b</ref>) is used by our classification module F d <ref type="figure">(.; ? d</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head><p>To further guide our model to discriminate the subtle changes, we propose a learnable pooling approach <ref type="figure">(Fig. 2c)</ref>, which aggregates information by grouping similar responses from the hidden states h r . It is inspired by the existing feature encoding approach, such as NetVLAD <ref type="bibr" target="#b0">(Arandjelovic et al. 2016)</ref>. We adapt this differentiable clustering approach for the soft assignment of the responses from hidden states h r to cluster ? and their contribution to the VLAD encoding.</p><formula xml:id="formula_7">? ? (h r ) = e W T ? hr+b? K i=1 e W T i hr+bi N v (o, ?) = |R| r=1 ? ? (h r )h r (o),? = softmax(W N N v )<label>(3)</label></formula><p>where W i and b i are learnable clusters' weights and biases.</p><p>T signifies transpose. The term ? ? (h r ) refers to the soft assignment of h r to cluster ?, and N v is the encoded responses of hidden states from all the regions r ? R. In the original implementation of VLAD, the weighted sum of the residuals is used i.e. |R| r=1 ? ? (h r ) (h r (o) ?? ? (o)) in which? ? is the ? th cluster center and o ? h r is one of the elements in the hidden state response. We adapt the simplified version that averages the actual responses instead of residuals <ref type="bibr" target="#b40">(Miech, Laptev, and Sivic 2017)</ref>, which requires fewer parameters and computing operations. The encoded response is mapped into prediction probability? by using a learnable weight W N and softmax. The learnable parameter for the</p><formula xml:id="formula_8">classification module F d is ? d = {W i , b i , W N }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Discussion</head><p>We comprehensively evaluate our model on widely used eight benchmark FGVC datasets: Aircraft <ref type="bibr" target="#b39">(Maji et al. 2013</ref>  of 224?224 from 256?256. The last Conv layer of the base CNN (e.g. 7 ? 7 pixels) is increased to 42 ? 42 by using an upsampling layer (as in GAN) and then fed into our CAP <ref type="figure" target="#fig_1">(Fig. 1a</ref>) to pool features from multiple integral regions R. We fix bilinear pooling size of w = h = 7 for each region with minimum width ? x = 7 and height ? y = 7. We use spatial location gap of 7 pixels between consecutive anchors to generate |R| = 27 integral regions. This is decided experimentally by considering the trade-off between accuracy and computational complexity. We set the cluster size to 32 in our learnable pooling approach. We apply Stochastic Gradient Descent (SGD) optimizer to optimize the categorical cross-entropy loss function. The SGD is initialized with a momentum of 0.99, and initial learning rate 1e-4, which is multiplied by 0.1 after every 50 epochs. The model is trained for 150 epochs using an NVIDIA Titan V GPU (12 GB). We use Keras+Tensorflow to implement our algorithm.</p><p>Quantitative results and comparison to the SotA approaches: Overall, our model outperforms the SotA approaches by a clear margin on all datasets except the Stanford Dogs <ref type="bibr" target="#b28">(Khosla et al. 2011)</ref> and Oxford Flowers (Nilsback and Zisserman 2008) <ref type="table">(Table 1)</ref>. In <ref type="table">Table 1</ref>, we compare our performances with the two previous best (last two columns). One uses only the target dataset (primary) for training and evaluation (past best) and is the case in our model. The other (last column) uses primary and additional secondary (e.g. ImageNet, COCO, iNat, etc.) datasets for joint/transfer learning of objects/patches/regions during training. It is worth mentioning that we use only the primary datasets and our performance in most datasets is significantly better than those uses additional datasets. This demonstrates the benefit of the proposed approach for discriminating fine-grained changes in recognizing subordinate categories. Moreover, we use only one network for end-toend training, and our novel CAP and classification layers are added on top of a base CNN. Therefore, the major computations are associated with the base CNNs.</p><p>Using our model, the two highest gains are 5.6% and 3.1% in the respective Food-101 <ref type="bibr" target="#b2">(Bossard, Guillaumin, and Gool 2014)</ref> and NABirds <ref type="bibr" target="#b53">(Van Horn et al. 2015)</ref> datasets. In Dogs, our method (96.1%) is significantly better than the best SotA approach (93.9%) <ref type="bibr" target="#b14">(Ge, Lin, and Yu 2019)</ref> using only primary data. However, their accuracy increases to 97.1% when joint fine-tuning with selected ImageNet images are used. Similarly, in Flowers, our accuracy (97.7%) is the same as in ) which uses both primary and secondary datasets, and we achieve an improvement of 1.3% compared to the best SotA approach in <ref type="bibr" target="#b56">(Xie et al. 2016)</ref>    <ref type="bibr" target="#b23">(Huang et al. 2019b</ref>). The result of the Birds dataset is included in the supplementary document in the end.</p><p>using only primary data. We also compare our model's accuracy with the top-five SotA approaches on each dataset in <ref type="table" target="#tab_1">Table 2</ref>. Our accuracy is significantly higher than SotA methods using primary data in all six datasets in <ref type="table" target="#tab_1">Table 2</ref> and two in supplementary (provided in the end). Furthermore, it is also considerably higher than SotA methods, which use both primary and secondary data in six datasets <ref type="bibr">(Aircraft,</ref><ref type="bibr">Cars,</ref><ref type="bibr">Pets and NABirds)</ref>. This clearly proves our model's powerful ability to discriminate subtle changes in recognizing subordinate categories without requiring additional datasets and/or subnetworks and thus, has an advantage of easy implementation and a little computational overhead in solving FGVC.</p><p>Ablation study: We compare the performance of our approach using the benchmarked base CNN architectures such as ResNet-50 <ref type="bibr" target="#b17">(He et al. 2016)</ref>, Inception-V3 <ref type="bibr" target="#b49">(Szegedy et al. 2016)</ref>, Xception <ref type="bibr" target="#b6">(Chollet 2017)</ref> and DenseNet121 <ref type="bibr" target="#b21">(Huang et al. 2017)</ref>, as well as SotA lightweight architectures such as NASNetMobile <ref type="bibr" target="#b65">(Zoph et al. 2018</ref>) and MobileNetV2 <ref type="bibr" target="#b46">(Sandler et al. 2018)</ref>. The performance is shown in <ref type="table" target="#tab_3">Table  3</ref>. In all datasets, both standard and lightweight architectures have performed exceptionally well when our proposed CAP and classification modules are incorporated. Even our model outperforms the previous best (primary data) for both standard and lightweight base CNNs except in Cars and CUB-200 datasets in which our model with standard base CNNs exceed the previous best. Our results in <ref type="table">Table 1</ref> &amp; 2 are the best accuracy among these backbones. Nevertheless, the accuracy of our model using any standard backbones (ResNet50 / Inception V3 / Xception; <ref type="table" target="#tab_3">Table 3</ref>) is better than the SotA. In Flowers and Pets datasets, the lightweight NAS-NetMobile is the best performer, and the MobileNetV2 is not far behind <ref type="table" target="#tab_3">(Table 3</ref>). This could be linked to the dataset size since these two are of smallest in comparison to the rest (Table 1). However, in other datasets (e.g. Aircraft, Cars and Dogs), there is a little gap in performance between standard and lightweight CNNs. These lightweight CNNs involve significantly less computational costs, and by adding our modules, the performance can be as competitive as the standard CNNs. This proves the importance of our modules in enhancing performance and its broader applicability.</p><p>We have also evaluated the above base CNNs (B), and the influence of our novel CAP (+C) and the classification module (+E) in the recognition accuracy on Aircraft, Cars and Pets datasets (more in the supplementary in the end). The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. It is evident that the accuracy improves as we add our modules to the base networks, i.e., (B+C+E) &gt; (B+C) &gt; (B+E) &gt; B, resulting in the largest gain contributed by our novel CAP (B+C). This signifies the impact of our CAP. In B+C, the minimum gain is 7.2%, 5.7% and 5.1% on the respective Aircraft, Cars and Pets datasets for the Inception-V3 as a base CNN. Similarly, the highest gain is 12.5% and 11.3% in Aircraft and Cars, respectively. These two datasets are relatively larger than the Pets <ref type="table">(Table 1)</ref> in which the highest gain (7.9%) is achieved by using ResNet-50 as a base CNN. We also observe that there is a significant gap in baseline accuracy between lightweight and standard base CNNs in larger (Aircraft and Cars) datasets. These gaps are considerably reduced when our CAP is added. There is a further increase in accuracy when we add the classification module (B+C+E). This justifies the inclusion of our novel encoding by grouping hidden responses using residual-less NetVLAD and then infer class probability using learnable pooling from these encoded responses. For base CNNs, we use the standard transfer learning by fine-tuning it on the target dataset using the same data augmentation and hyper-parameters. For our models, we use pre-trained weights for faster convergence. We experimentally found that the random initialization takes nearly double iterations to converge (similar accuracy) than the pre-trained weights. A similar observation is shown in <ref type="bibr" target="#b16">(He, Girshick, and Doll?r 2019)</ref>.</p><p>Our model's accuracy is also compared using different numbers of regions |R|. It is a hyper-parameter and is computed from ? x and ? y . The results are shown in <ref type="table">Table 5</ref> (best |R| = 27). We have also provided results for top-N accuracy in the supplementary document provided in the end. The top-2 accuracy is around 99% and is independent of the CNN types. Model complexity: It is represented as a number of trainable parameters in millions and per-image inference time in millisecond <ref type="table" target="#tab_5">(Table 4</ref>). It also depends on the base CNNs types (e.g. standard vs lightweight). Given the number of trainable parameters (9.7M) and inference time (3.5ms), the performance of the lightweight NASNetMobile is very competitive in comparison to the rest. The role of secondary data has improved accuracy in <ref type="bibr" target="#b7">Cubuk et al. 2019;</ref><ref type="bibr" target="#b14">Ge, Lin, and Yu 2019;</ref><ref type="bibr" target="#b15">Ge and Yu 2017)</ref>. However, such models involve multiple steps and resource-intensive, resulting in difficulty in implementing. For example, 3 steps in <ref type="bibr" target="#b14">(Ge, Lin, and Yu 2019)</ref>: 1) object detection and instance segmentation (Mask R-CNN and CRF), 2) complementary part mining (512 ROIs) and 3) classification using context    . Class-specific c r in (2) for 9 classes (3 ? 3) from region 1 (g) and 20 (h). Blue to red represents class-specific less to more attention towards that region. Class-specific individual feature response within c r of the region 1 and class 4 (i). t-SNE plot of c r representing images from the above 9 classes (j).  <ref type="table">Table 5</ref>: Accuracy (%) of our model with a varying number of integral regions. More results in the supplementary in the end.</p><p>It is ?5.7 hrs for Cars and ?8.5 hrs for Dogs. Qualitative analysis: To understand the discriminability of our model, we use t-SNE (Van Der Maaten 2014) to visualize the class separability and compactness in the features extracted from a base CNN, and our novel CAP and classification modules. We also analyze the impact of our CAP in enhancing the discriminability of a base CNN. We use test images in Aircraft and Xception as a base CNN. In <ref type="figure" target="#fig_3">Fig. 3(ad)</ref>, it is evident that when we include our CAP + encoding modules, the clusters are farther apart and compact, resulting in a clear distinction of various clusters representing dif-ferent subcategories. Moreover, the discriminability of the base CNN is significantly improved <ref type="figure" target="#fig_3">(Fig. 3b</ref>) in comparison to without our modules shown in <ref type="figure" target="#fig_3">Fig. 3a</ref>. More results are shown in the supplementary material, added in the end. We have also looked the inside of our CAP by visualizing its class-specific attention-aware response using ? r,r and context vector c r in (2). Aircraft images (randomly selected 9 classes) are used in <ref type="figure" target="#fig_3">Fig. 3(e-j)</ref>. Such results clearly show our model's power in capturing the context information for discriminating subtle changes in FGVC problems. We have also included some examples, which are incorrectly classified by our model with an explanation in the supplementary information in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have proposed a novel approach for recognizing subcategories by introducing a simple formulation of context-aware attention via learning where to look when pooling features across an image. Our attention allows for explicit integration of bottom-up saliency by taking advantages of integral regions and their importance, without requiring the bounding box/part annotations. We have also proposed a feature encoding by considering the semantic correlation among the regions and their spatial layouts to encode complementary partial information. Finally, our model's SotA results on eight benchmarked datasets, quantitative/qualitative results and ablation study justify the efficiency of our approach. Code is available at https://ardhendubehera.github.io/cap/. Remaining results of <ref type="table" target="#tab_3">Table 3</ref>: The accuracy of the proposed method is evaluated on the NABirds dataset using six different SotA base CNNs for <ref type="table" target="#tab_3">Table 3</ref> in the main paper. It is presented in <ref type="table" target="#tab_8">Table 7</ref> below.  Remaining results of <ref type="table">Table 5</ref>: The performance is evaluated using a different number of integral regions on the Aircraft and Stanford Cars datasets <ref type="table">(Table 5</ref>). The same experiment is also carried out on the Stanford Dogs dataset, and the results are given in <ref type="table" target="#tab_10">Table 9</ref> below. Top-N Accuracy (%): We have also evaluated the proposed approach using top-N accuracy metric on Oxford-IIIT Pets, Stanford Cars and Aircraft datasets. The performance of our modules on top of various base architectures is presented in <ref type="table" target="#tab_11">Table  10</ref> below. On all three datasets, the top-2 accuracy is around 99% and is independent of the type of base CNN architecture used. Moreover, the top-5 accuracy is nearly 100%. This justifies the significance of our novel attentional pooling and encoding modules in enhancing performance and their wider applicability. Additional Qualitative Analysis:</p><p>We have provided the additional qualitative analysis of our model's performance by selecting a few example images, which are wrongly classified against the label they are mistaken for (selected from the mistaken subcategories). This is presented in <ref type="figure" target="#fig_4">Figure 4</ref>. It is evident that the mistaken labels come from classes with extremely similar features, often being from the same manufacturer (Boeing 747, Audi, etc.). We have also noticed that subcategories can have very specific defining features that are not clearly visible in every image due to poor angles or lighting conditions (e.g. The chin of a Ragdoll and legs of a Birman cat shown in <ref type="figure" target="#fig_4">Fig. 4g</ref>). It can be seen that the mistaken labelling comes from classes with extremely similar appearance features and/or perspective changes, often being from the same manufacturer (Boeing 747, Audi, etc.). We have also noticed that subcategories can have very specific defining features that are not clearly visible in every image due to poor angles or lighting conditions (e.g. The chin of a Ragdoll and legs of a Birman cat).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :?</head><label>1</label><figDesc>a) High-level illustration of our model (left). b) The detailed architecture of our novel CAP (right).Learning pixel-level (convolutional feature map) relationships i.e. | where ? ? and 1 level (convolutional feature map) relationships i.e. | where ? ? and 1 ? , ? ? self self (b) Attention-focused contextual information from integral regions (surrounding context) Capturing structural information exploring spatial arrangement ? = ? ? (? ?1 , ; ? ) ? (d) Aggregated hidden state responses to class prediction Figure 2: a) Learning pixel-level relationships from the convolutional feature map of size W ? H ? C. b) CAP using integral regions to capture both self and neighborhood contextual information. c) Encapsulating spatial structure of the integral regions using an LSTM. d) Classification by learnable aggregation of hidden states of the LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>? r,r for class 1 (f) ? r,r for class 2 (g) cr of region 1 (h) cr of region 20 (i) cr of class 1 (j) t-SNE plot of cr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Discriminability using t-SNE to visualize class separability and compactness (a-d). Aircraft test images using Xception: a) base CNN's output, b) our CAP's impact on the base CNN's output, c) our CAP's output, and d) our model's final output. Our CAP's class-specific attention-aware response for class 1 (e) and class 2 (f) to capture the similarity between 27 integral regions (27?27)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Some of the example images, which are incorrectly classified by our model (left) against the label they are mistaken for (right -selected from the mistaken subcategories): Aircraft (a-c), Stanford Cars (d-f), Oxford-IIIT Pets (g-i), Caltech-UCSD Birds -CUB-200 (j-l), and Oxford Flowers (m-o).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b30">Krause et al. 2013)</ref>, Stanford Dogs<ref type="bibr" target="#b28">(Khosla et al. 2011)</ref>, Caltech Birds (CUB-200)(Wah et al. 2011), Oxford  Flower (Nilsback and<ref type="bibr" target="#b41">Zisserman 2008)</ref>, Oxford-IIIT Pets<ref type="bibr" target="#b42">(Parkhi et al. 2012), and</ref> NABirds (Van Horn et al. 2015). We do not use any bounding box/part annotation. Thus, we do not compare with methods which rely on these. Statistics of datasets and their train/test splits are shown inTable 1. We use the top-1 accuracy (%) for evaluation. Experimental settings: In all our experiments, we resize images to size 256 ? 256, apply data augmentation techniques of random rotation (?15 degrees), random scaling (1 ? 0.15) and then random cropping to select the final size<ref type="bibr" target="#b14">Ge, Lin, and Yu 2019)</ref> 97.1 (Ge, Lin, and Yu 2019) CUB-200 5,994 / 5,794 200 91.8 90.3 (Ge, Lin, and Yu 2019) 90.4<ref type="bibr" target="#b14">(Ge, Lin, and Yu 2019)</ref> Dataset statistics and performance evaluation. FGVC accuracy (%) of our model and the previous best using only the primary dataset. The last column involves the transfer/joint learning strategy consisting of more than one dataset.</figDesc><table><row><cell>), Food-101 (Bossard, Guillaumin, and Gool 2014), Stanford #Classes Our Past Best (primary) Past Best (primary + secondary) 100 94.9 93.0 (Chen et al. 2019) 92.9 (Yu et al. 2018b) 101 98.6 93.0 (Huang et al. 2019b) 90.4 (Cui et al. 2018) 196 95.7 94.6 (Huang et al. 2019b) 94.8 (Cubuk et al. 2019) 120 96.1 93.9 (Oxford Flower #Train / #Test Aircraft 6,667 / 3,333 Food-101 75,750 / 25,250 Stanford Cars 8,144 / 8,041 Stanford Dogs 12,000 / 8,580 2,040 / 6,149 102 97.7 96.4 (Xie et al. 2016) 97.7 (Chang et al. 2020) Oxford Pets 3,680 / 3,669 37 97.3 95.9 (Huang et al. 2019b) 93.8 (Peng, He, and Zhao 2018) NABirds 23,929 / 24,633 555 91.0 86.4 (Luo et al. 2019) 87.9 (Cui et al. 2018) Table 1: Aircraft Food-101 Stanford Cars Method ACC Method ACC Method ACC DFL (Wang et al. 2018) 92.0 WISeR (Martinel et al., 2018) 90.3 BARM (Liu et al. 2019) 94.3 BARM (Liu et al. 2019) 92.5 DSTL  *  (Cui et al. 2018) 90.4 MC  *  Loss (Chang et al. 2020) 94.4 GPipe (Huang et al. 2019b) 92.7 MSMVFA (Jiang et al. 2020) 90.6 DCL(Chen et al. 2019) 94.5 MC  *  Loss (Chang et al. 2020) 92.9 JDNet  *  (Zhao et al. 2020) 91.2 GPipe (Huang et al. 2019b) 94.6 DCL (Chen et al. 2019) 93.0 GPipe (Huang et al. 2019b) 93.0 AutoAug  *  (Cubuk et al. 2019) 94.8 Proposed 94.9 Proposed 98.6 Proposed 95.7 CUB-200 Oxford-IIIT Pets NABirds iSQRT (Li et al. 2018) 88.7 NAC (Simon and Rodner 2015) 91.6 T-Loss (Taha et al. 2020) 79.6 DSTL  *  (Cui et al. 2018) 89.3 TL-Attn  *  (Xiao et al. 2015) 92.5 PC-CNN (Dubey et al. 2018a) 82.8 DAN (Hu et al. 2019) 89.4 InterAct (Xie et al. 2016) 93.5 MaxEnt  *  (Dubey et al. 2018b) 83.0 BARM (Liu et al. 2019) 89.5 OPAM  2019) 86.4 CPM  *  (Ge, Lin, and Yu 2019) 90.4 GPipe (Huang et al. 2019b) 95.9 DSTL  *  (Cui et al. 2018) 87.9 Cars (Dataset Proposed 91.8 Proposed 97.3 Proposed 91.0</cell></row></table><note>* (Peng, He, and Zhao 2018) 93.8 Cross-X (Luo et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Accuracy (%) comparison with the recent top-five SotA approaches. Methods marked with * involve transfer/joint learning strategy for objects/patches/regions consisting more than one dataset (primary and secondary). Please refer to the supplementary page in the end for the results of Stanford Dogs and Oxford Flowers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Our model's accuracy (%) with different SotA base</cell></row><row><cell>CNN architectures. Previous best accuracies for these results</cell></row><row><cell>are; Aircraft: 93.0 (Chen et al. 2019), Cars: 94.6 (Huang</cell></row><row><cell>et al. 2019b), Dogs: 93.9 (Ge, Lin, and Yu 2019), CUB:</cell></row><row><cell>90.3 (Ge, Lin, and Yu 2019), Flowers: 96.4 (Xie et al. 2016),</cell></row><row><cell>and Pets: 95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance (accuracy in %) of our model with the addition of our novel CAP (+C) and classification (+E) module to various SotA base (B) CNNs. The observed accuracy trend is (B+C+E) &gt; (B+C) &gt; (B+E) &gt; B for all base CNNs. Final model's (B+C+E) trainable parameters (Param) are given in million (M) and the respective per-frame inference time in millisecond (ms).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison with the recent top-five SotA approaches on each dataset. Methods marked with * involve transfer/joint learning strategy for objects/patches/regions consisting more than one dataset (primary and secondary)</figDesc><table><row><cell>Stanford Dogs</cell><cell></cell><cell>Oxford Flowers</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Accuracy (%) Method</cell><cell>Accuracy (%)</cell></row><row><cell>FCANs (Liu et al. 2016)</cell><cell>89.0</cell><cell>InterAct (Xie et al. 2016)</cell><cell>96.4</cell></row><row><cell>SJFT  *  (Ge and Yu 2017)</cell><cell>90.3</cell><cell>SJFT  *  (Ge and Yu 2017)</cell><cell>97.0</cell></row><row><cell>DAN (Hu et al. 2019)</cell><cell>92.2</cell><cell>OPAM  *  (Peng, He, and Zhao 2018)</cell><cell>97.1</cell></row><row><cell>WARN (Rodr?guez et al. 2020)</cell><cell>92.9</cell><cell>DSTL  *  (Cui et al. 2018)</cell><cell>97.6</cell></row><row><cell>CPM  *  (Ge, Lin, and Yu 2019)</cell><cell>97.1</cell><cell>MC Loss  *  (Chang et al. 2020)</cell><cell>97.7</cell></row><row><cell>Proposed</cell><cell>96.1</cell><cell>Proposed</cell><cell>97.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Our model's accuracy (%) on the NABirds dataset with different SotA base CNN architectures. Previous best accuracy is 86.4% for primary only and 87.9%<ref type="bibr" target="#b8">(Cui et al. 2018)</ref> for combined primary and secondary datasets.Remaining results ofTable 4: In ablation study (Table 4of the main paper), we have presented the performance of the proposed model (with the addition of our novel context-aware attentional pooling (+C) and classification (+E) module) on the Aircraft, Stanford Cars and Oxford-IIIT Pets datasets. The same evaluation procedure is performed on the Stanford Dogs, Oxford Flowers and Caltech Birds (CUB-200) datasets and the recognition accuracy (%) is presented inTable 8. Like inTable 4, a similar trend is observed in the improvement of accuracy when our context-aware attentional pooling (+C) and classification (+E) modules are added to various SotA base CNN architectures (B).</figDesc><table><row><cell>Base CNN</cell><cell>Accuracy(%)</cell></row><row><cell>ResNet-50</cell><cell>88.8</cell></row><row><cell>Inception V3</cell><cell>89.1</cell></row><row><cell>Xception</cell><cell>91.0</cell></row><row><cell>DenseNet-121</cell><cell>88.3</cell></row><row><cell>NASNet-Mobile</cell><cell>88.7</cell></row><row><cell>MobileNet V2</cell><cell>89.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (%) of the proposed model with the addition of our novel context-aware attentional pooling (+C) and classification (+E) module to various SotA base (B) CNN architectures. It presents the remaining evaluation ofTable 4.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Stanford Dogs</cell><cell></cell><cell cols="2">Oxford Flowers</cell><cell cols="3">Caltech Birds: CUB-200</cell></row><row><cell>Base CNN</cell><cell>B</cell><cell>B+C</cell><cell>B+C+E</cell><cell>B</cell><cell>B+C</cell><cell>B+C+E</cell><cell>B</cell><cell>B+C</cell><cell>B+C+E</cell></row><row><cell>Inception-V3</cell><cell>78.7</cell><cell>94.2</cell><cell>95.7</cell><cell>92.3</cell><cell>94.9</cell><cell>97.6</cell><cell>76.0</cell><cell>87.1</cell><cell>91.4</cell></row><row><cell>Xception</cell><cell>82.7</cell><cell>94.8</cell><cell>96.1</cell><cell>91.9</cell><cell>94.9</cell><cell>97.7</cell><cell>75.6</cell><cell>87.4</cell><cell>91.8</cell></row><row><cell>DenseNet-121</cell><cell>79.5</cell><cell>94.5</cell><cell>95.5</cell><cell>94.4</cell><cell>95.1</cell><cell>97.6</cell><cell>79.1</cell><cell>87.2</cell><cell>91.6</cell></row><row><cell cols="2">NASNet-Mobile 79.5</cell><cell>94.7</cell><cell>96.0</cell><cell>90.7</cell><cell>95.0</cell><cell>97.7</cell><cell>73.0</cell><cell>86.8</cell><cell>89.7</cell></row><row><cell>MobileNetV2</cell><cell>76.5</cell><cell>94.3</cell><cell>95.9</cell><cell>92.3</cell><cell>95.0</cell><cell>97.4</cell><cell>74.5</cell><cell>87.0</cell><cell>89.2</cell></row><row><cell>Previous Best</cell><cell cols="2">( Ge et al. 2019)</cell><cell>93.9</cell><cell cols="2">( Xie et al. 2016)</cell><cell>96.4</cell><cell cols="2">( Ge et al. 2019)</cell><cell>90.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Accuracy (%) of our model with numbers of 9, 27, and 36 integral regions on Stanford Dogs dataset.</figDesc><table><row><cell>Base CNN</cell><cell>#9</cell><cell>#27 #36</cell></row><row><cell>ResNet-50</cell><cell cols="2">90.5 95.8 92.1</cell></row><row><cell>Xception</cell><cell cols="2">95.3 96.1 95.2</cell></row><row><cell cols="3">NASNet-M 91.7 96.0 93.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Top-N accuracy (in %) of the proposed model using different base architectures on Oxford-IIIT Pets, Stanford Cars and Aircraft datasets. The top-2 accuracy is around 99% and is independent of the type of base CNN architecture used. The top-5 accuracy is nearly 100%. This shows the significance of the proposed attentional pooling and encoding modules.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Base CNN architecture Top 1 Top 2 Top 3 Top 5</cell></row><row><cell cols="2">Oxford-IIIT Pets Inception-V3</cell><cell>96.2</cell><cell>99.0</cell><cell>99.5</cell><cell>99.9</cell></row><row><cell></cell><cell>Xception</cell><cell>97.0</cell><cell>99.7</cell><cell>99.9</cell><cell>99.9</cell></row><row><cell></cell><cell>DenseNet121</cell><cell>96.9</cell><cell>99.2</cell><cell>99.6</cell><cell>99.7</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>97.3</cell><cell>99.4</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>96.4</cell><cell>98.9</cell><cell>99.5</cell><cell>99.6</cell></row><row><cell>Stanford Cars</cell><cell>Inception-V3</cell><cell>94.8</cell><cell>99.4</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell></cell><cell>Xception</cell><cell>95.7</cell><cell>99.3</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell></cell><cell>DenseNet121</cell><cell>93.6</cell><cell>98.7</cell><cell>99.5</cell><cell>99.9</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>93.7</cell><cell>99.1</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>94.0</cell><cell>99.3</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell>Aircraft</cell><cell>Inception-V3</cell><cell>94.8</cell><cell>99.1</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell></cell><cell>Xception</cell><cell>94.1</cell><cell>98.9</cell><cell>99.2</cell><cell>99.5</cell></row><row><cell></cell><cell>DenseNet121</cell><cell>94.6</cell><cell>98.8</cell><cell>99.3</cell><cell>99.4</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>93.8</cell><cell>99.4</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>94.4</cell><cell>99.1</cell><cell>99.7</cell><cell>99.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">gating. The model is trained using 4 GPUs. In contrast, our model can be trained on a single GPU (12 GB). The perimage inference time is 4.1ms. In<ref type="bibr" target="#b14">(Ge, Lin, and Yu 2019)</ref>, it is 27ms for step 3 and additional 227ms in step 2. FCANs<ref type="bibr" target="#b37">(Liu et al. 2016</ref>) reported its inference time as 150ms. Using 27 integral regions and ResNet50 as a base, the training time for the Aircraft is ?4.75 hrs for 150 epochs (12 batch size).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">paper.Remaining results ofTable 2: The performance comparison (accuracy in %) using the remaining two datasets (Stanford Dogs and Oxford Flowers) forTable 2in the main paper. It is presented inTable 6below.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the UKIERI-DST grant CHARM (DST UKIERI-2018-19-10). The GPU used in this research is generously donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Document</head><p>In this document, we have included the remaining quantitative and qualitative results, which we could not include in the main   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101 -Mining Discriminative Components with Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Eur. Conf., Part VI</title>
		<meeting>13th Eur. Conf., Part VI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domainspecific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Weakly Supervised Attention Pyramid Convolutional Neural Network for Fine-Grained Visual Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1086" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE conf. Comp. Vis. Patt. Recog. (CVPR)</title>
		<meeting>IEEE conf. Comp. Vis. Patt. Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image Captioning: Transforming Objects into Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Info. Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11135" to="11145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09891</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked semantics-guided attention model for fine-grained zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5995" to="6004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Scale Multi-View Deep Feature Aggregation for Food Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="265" to="276" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linearized Multi-Sampling for Differentiable Image Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conf. on computer vision workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context-aware attention network for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9295" to="9305" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Entangled Transformer for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8928" to="8937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inverse compositional spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2568" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional Attention-Recognition Model for Finegrained Object Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fully convolutional attention networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-X Learning for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conf. on Comp. Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pay attention to the activations: a modular attention mechanism for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Velazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonz?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="502" to="514" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Comp. Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Boosting Standard Classification Architectures Through a Ranking Regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automated taxonomic identification of insects with expert-level accuracy using effective feature transfer from convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Makonyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vondr??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ronquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic biology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="876" to="895" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using treebased algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Interactive: Inter-layer activeness propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploring selfattention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">JDNet: A Joint-learning Distilled Network for Mobile Visual Food Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning rich part hierarchies with progressive attention networks for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="476" to="488" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards robust image classification using sequential attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9483" to="9492" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
