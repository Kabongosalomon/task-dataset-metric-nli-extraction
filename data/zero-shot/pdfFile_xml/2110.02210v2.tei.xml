<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mix3D: Out-of-Context Data Augmentation for 3D Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Nekrasov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">ETH AI Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mix3D: Out-of-Context Data Augmentation for 3D Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Mix3D, a data augmentation technique for segmenting large-scale 3D scenes. Since scene context helps reasoning about object semantics, current works focus on models with large capacity and receptive fields that can fully capture the global context of an input 3D scene. However, strong contextual priors can have detrimental implications like mistaking a pedestrian crossing the street for a car. In this work, we focus on the importance of balancing global scene context and local geometry, with the goal of generalizing beyond the contextual priors in the training set. In particular, we propose a "mixing" technique which creates new training samples by combining two augmented scenes. By doing so, object instances are implicitly placed into novel out-of-context environments, therefore making it harder for models to rely on scene context alone, and instead infer semantics from local structure as well. We perform detailed analysis to understand the importance of global context, local structures and the effect of mixing scenes. In experiments, we show that models trained with Mix3D profit from a significant performance boost on indoor (ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially used with any existing method, e.g., trained with Mix3D, MinkowskiNet outperforms all prior state-of-the-art methods by a significant margin on the ScanNet test benchmark (78.1% mIoU). Code is available at: https://nekrasov.dev/mix3d/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This supplementary material provides more details on the network architectures, including training and inference details (Sec. A). In Sec. 4.2, we provide additional analysis, experiments and results. Finally, in Sec. C we report per-class semantic segmentation scores on all three datasets (ScanNet, S3DIS, SemanticKITTI).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we address the task of 3D semantic segmentation which assigns a semantic label to every point in a given 3D point cloud. Popularized by PointNet <ref type="bibr" target="#b37">[38]</ref>, the broader field of deep learning for 3D scene understanding has experienced tremendous growth. While these early approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> can classify individual objects or segment larger scenes by splitting them into cubical (or spherical) chunks, recent high-capacity models such as MinkowskiNets <ref type="bibr" target="#b8">[9]</ref> and SparseConvNets <ref type="bibr" target="#b21">[22]</ref> have large receptive fields and can directly process full rooms or even * Equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outdoor Scene</head><p>Indoor Scene with Mix3D without Mix3D outdoor scenes without splitting them up. By doing so, they can capture the global context of a scene. Context refers to the strong configuration rules present in man-made environments <ref type="bibr" target="#b35">[36]</ref>. These rules manifest in re-occurrences of object arrangements. For example, based on context, we can expect to see chairs around a table situated in a conference room. Importantly, context supports reasoning about object semantics, as confirmed by research in visual cognition, computer vision, and cognitive neuroscience <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>However, while contextual information has proven useful, relying too heavily on global scene priors can be harmful when reasoning about object semantics. Models may overfit to contextual cues of the training set, resulting in poor generalization to unseen data, and in particular can lead to failure on long tail examples ( <ref type="figure" target="#fig_0">Fig. 1, right)</ref>. Outof-context generalization is even more important in safety critical settings, such as autonomous driving, where rare but dangerous situations are often not well represented in training sets, e.g., pedestrians walking in the middle of the road or people appearing in the front of a vehicle <ref type="figure" target="#fig_0">(Fig. 1, left)</ref>.</p><p>3D datasets are usually significantly smaller and exhibit little in-data variance compared to 2D image datasets that are often collected "in the wild" <ref type="bibr" target="#b33">[34]</ref>. High-capacity models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49]</ref> are particularly vulnerable to learn strong context-based priors when trained on small 3D datasets. As a result, modern network architectures for 3D processing often struggle with overfitting and typically expose large generalization gaps <ref type="figure">(Fig. 6</ref>).</p><p>To this end, we propose a new data augmentation technique, called Mix3D, that "mixes" pairs of training scenes. Specifically, mixing consists in randomly transforming pairs of point clouds such that they loosely align, followed by taking the union over the two point sets (see <ref type="figure">Fig. 2</ref>). This is implemented as concatenation of two point clouds as well as their corresponding training labels. Mixing two scenes essentially generates new training samples, that increase the training set, and reduces overfitting as demonstrated in our analysis. In contrast to standard augmentations that apply transformations to single training samples without changing the scene context, Mix3D exposes the objects of each original scene to the novel context of the mixed scenes. The result is an effective data augmentation strategy that can easily be incorporated into existing training pipelines, works well even without labeled data, and effectively reduces overfitting while increasing segmentation performance.</p><p>We evaluate Mix3D on three large-scale 3D semantic segmentation benchmarks: ScanNet <ref type="bibr" target="#b9">[10]</ref>, S3DIS <ref type="bibr" target="#b2">[3]</ref> and SemanticKITTI <ref type="bibr" target="#b3">[4]</ref>. We compare current state-ofthe-art voxel-based MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> and point-based KP-Conv <ref type="bibr" target="#b48">[49]</ref> trained with and without Mix3D. On all datasets and models, Mix3D consistently improves segmentation performance on both indoor and outdoor data. Our analyses show that mixing scenes is key to consistent improvements and verify that Mix3D effectively reduces overfitting to the contextual priors of the training scenes.</p><p>Our contributions are summarized as:</p><p>? We propose Mix3D, a data augmentation technique for large-scale 3D scenes and high-capacity models that can balance global context and local geometry. As a result, the trained models can generalize beyond the contextual priors of the training set.</p><p>? Applying Mix3D to recent point-based and voxelbased 3D models results in consistently improved segmentation performance on three large-scale indoor and outdoor datasets.</p><p>? New state-of-the-art semantic segmentation performance of 78.1% mean intersection over union (mIoU) on the ScanNet test benchmark challenge.</p><p>? In-depth analyses of Mix3D show the importance of global context, local geometry and mixing scenes, in terms of model generalization and segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mixed Sample Data Augmentation. In contrast to standard augmentations applied to single samples, MixUp pioneered by Zhang et al. <ref type="bibr" target="#b61">[62]</ref> enforces smooth label transitions by linearly interpolating between pairs of input samples and their corresponding class labels <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. Training on these virtual training samples encourages linear behavior outside the empirical train distribution, leading the classifier to show fewer label oscillations and, thus, improved generalization <ref type="bibr" target="#b61">[62]</ref>. Numerous works extend MixUp: AdaMixUp <ref type="bibr" target="#b22">[23]</ref> takes special provisions to prevent manifold intrusions of virtual training examples. Saliency guided approaches <ref type="bibr">[30,</ref><ref type="bibr" target="#b50">51]</ref> and (attentive) CutMix <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b59">60]</ref> paste image crops into the image and thus increase localization accuracy. In contrast to mixing train samples in the input domain, Verma et al. <ref type="bibr" target="#b51">[52]</ref> show that interpolating in the latent feature space leads to compressed representations which are theoretically linked to increased generalization capabilities. However, these methods are inherently limited to dense representations such as images, waveforms and spectrograms <ref type="bibr" target="#b61">[62]</ref>, as they rely on one-toone correspondences between parts of the input signals, e.g., pixel coordinates in an image. Sparse representations such as point clouds do not provide readily accessible optimal bijective correspondences. Therefore, PointMixUp <ref type="bibr" target="#b7">[8]</ref> leverages the Earth Mover's Distance <ref type="bibr" target="#b42">[43]</ref> to interpolate between optimal pairwise assignments that lie on the shortest path between two point clouds. Inspired by (Attentive) CutMix <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b59">60]</ref> and saliency guided approaches <ref type="bibr">[30,</ref><ref type="bibr" target="#b50">51]</ref>, PointCutMix <ref type="bibr" target="#b62">[63]</ref> improves upon this work by replacing subsets with corresponding points of other point clouds.</p><p>Circumventing the problem of establishing bijective correspondences, Achituve et al. <ref type="bibr" target="#b0">[1]</ref> explore combining point clouds for domain adaptation for single object semantic segmentation. Analogously to Achituve et al. <ref type="bibr" target="#b0">[1]</ref>, we treat augmented point clouds as their own entity, i.e., point features and associated labels do not change in the presence of a second point cloud. Our approach is thus not limited by establishing optimal one-to-one point correspondences of equally sized sets. Moreover, we do not rely on heuristic preprocessing such as aligning the main symmetry axes of point clouds <ref type="bibr" target="#b7">[8]</ref>. Thus, our approach is applicable to realistic tasks such as semantic segmentation of full scenes and is not limited to single objects of fixed point set sizes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Context Augmentations. Rosenfeld et al. <ref type="bibr" target="#b40">[41]</ref> show that object detection models are vulnerable towards placing outof-context instances in an image and therefore over-rely on context information. Shetty et al. <ref type="bibr" target="#b44">[45]</ref> thus seek to alleviate the dependency to contextual variations by introducing a removal-based data augmentation where context or objects are cropped out of the image. Alternative methods propose to place novel instances into an image: On the one hand, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b63">64]</ref> constitute a line of work which places objects in the scene at visually plausible locations. Dvornik et al. <ref type="bibr" target="#b11">[12]</ref> show that merely placing additional instances in images in fact leads to decreased performances for 2D object detection. However, by placing instances at semantically plausible positions in an image, they were able to report improved performances. Similar to <ref type="bibr" target="#b11">[12]</ref>, Tripathi et al. <ref type="bibr" target="#b49">[50]</ref> propose an adversarial learning strategy that estimates plausible positions for additional instances. Zhang et al. <ref type="bibr" target="#b63">[64]</ref> improve upon <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">50]</ref> by conditioning the placement of instances not only on context information but also on the instance itself. In the task of 3D semantic segmentation of outdoor scenes, Yan et al. <ref type="bibr" target="#b57">[58]</ref> carefully populate scenes with car instances taking special care to avoid overlapping objects. A number of other methods ignores the requirement for realistic data and randomly place object instances into the scene <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref> providing realistic context information only on a local patch-level. This is comparable to Mix3D as mixing two 3D scenes can also introduce unrealistic merging of objects. In contrast to approaches adding instances into the input image, other approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b64">65]</ref> explore augmentation methods for removing information from the input image. The key idea is that rectangular regions of the input image are removed which reduces the risk to overfit to small train set specific context features <ref type="bibr" target="#b10">[11]</ref>. Unlike works aiming to retain a visually plausible context, we argue that maintaining the context of augmented images amplifies the context bias. We therefore seek to regularize its effects by adding instances together with their respective contexts and voluntarily create context overlaps. Moreover, predicting plausible placements of objects demands complex augmentation systems, whereas our approach is fast, and can be added to existing approaches with a few lines of code (c.f . <ref type="figure" target="#fig_2">Fig. 3</ref>, right).</p><p>Context Aggregation of Deep Learning Models. Pointbased approaches such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> perform semantic segmentation by splitting scenes into smaller chunks, effectively restricting the model's ability to learn from global context. Subsequent works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b58">59]</ref> report improvements by increasing the spatial context. By leveraging data efficient sparse convolutions, recent voxelbased methods e.g., MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> and SparseCon-vNet <ref type="bibr" target="#b21">[22]</ref> are capable of processing full scenes at once, thus, capturing the global context of the scene. 3D-MPA <ref type="bibr" target="#b13">[14]</ref> and DOPS <ref type="bibr" target="#b41">[42]</ref> extend on this idea for instance segmentation and object detection. They learn higher-level relationships among object proposals with the goal to learn global scene configuration rules <ref type="bibr" target="#b35">[36]</ref>. Context is equally important in recent methods for 3D reconstruction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>. These developments allow to fully capture and rely on global scene context. However, our experiments show that this can have detrimental implications. By applying Mix3D, we seek to generalize beyond the contextual priors of the training set. <ref type="figure">Figure 2</ref>: Illustration of two mixed 3D scenes. The Mix3D data augmentation consists in taking the union over two 3D scenes after random transformations while ensuring sufficient overlap. This results in the effect of objects appearing now in the novel context of both mixed scenes, e.g., the classroom table directly faces the apartment door. The goal is to train a model that learns when to rely on context and when to focus on local geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We present a data augmentation technique for 3D deep learning on realistic large-scale scenes. The goal is to train 3D models that are less biased by misleading context priors, and learn to balance between local structures and global scene context without overfitting to training scenes.</p><p>Mix3D creates novel training examples by mixing two original scenes. The mixing of 3D scenes is illustrated in <ref type="figure">Fig. 2</ref>. In particular, we expose objects from a single input scene to the combined context of both mixed scenes. By doing so, the network does not only need to learn how to implicitly disentangle two mixed scene contexts, but also sees every object in a large variety of object arrangements that one would not normally encounter. Most notable, from initially N different scene contexts, with Mix3D we obtain O(N 2 ) novel contexts via pairwise combination of the existing scenes. In the following, we explain the details of the Mix3D training pipeline ( <ref type="figure" target="#fig_2">Fig. 3</ref>, left).</p><p>Data Augmentation <ref type="figure" target="#fig_2">(Fig. 3, )</ref>. When augmenting input scenes, we first translate them to the origin by subtracting the centroid from all point positions. By doing so, we make sure that the two scenes are overlapping in the following mixing stage. We randomly flip the point cloud in both horizontal directions, randomly rotate the scene along the up-right axis and along the other axis by Uniform[? ? 64 , + ? 64 ]. We also apply random sub-sampling, elastic distortion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref> and randomly scale the scene by Uniform[0.9, 1.1]. When the input features include color, we additionally apply random brightness and contrast augmentation as well as color-jitter.  Mix3D is easy to incorporate into existing code bases: instead of feeding a single scene S1 into the 3D model, a second input scene S2 is augmented in parallel and mixed with the first one. The resulting mixed scene S1,2 is input to the 3D model, which remains unchanged. For semantic segmentation, we compute the standard cross entropy loss LCE on the predicted labels and the concatenated ground truth labels Y1,2 of both scenes. (Right) Exemplary implementation of Mix3D. The individual scenes are augmented by centering at the origin, followed by random rotation and translation while ensuring overlap between both scenes. Additionally, we perform augmentations such as color-jitter, elastic distortion etc. (Details in Sec. 3). Finally, since the order of the points does not change during augmentation, the ground truth semantic labels are obtained using concatenation as well.</p><p>Mixing <ref type="figure" target="#fig_2">(Fig. 3, )</ref>. After augmenting, both scenes are mixed together, which is simply the union of both augmented point clouds. Technically, we implemented this by concatenating batch entries in pairs. Moreover, since the order of the points is not modified during augmentation, the ground truth labels of the mixed point cloud are also obtained by concatenation ( <ref type="figure" target="#fig_2">Fig. 3</ref>, right).</p><p>3D Model <ref type="figure" target="#fig_2">(Fig. 3, )</ref>. Mix3D is independent of the underlying 3D deep learning model. In experiments, we use MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> and KPConv <ref type="bibr" target="#b48">[49]</ref> as state-of-the-art representatives of both voxel-based and point-based models. The exact experimental setup is described in more detail in Sec. 4 and the supplementary.</p><p>Loss function. We supervise the model for semantic segmentation using the standard cross-entropy loss L CE which we apply to the predictions of the mixed scene S 1,2 and the concatenated ground truth labels Y 1,2 of both scenes.</p><p>Discussion. Crucially, Mix3D differs from existing point cloud mixing-based techniques in two main aspects. Firstly, we are the first to explore mixed sample augmentations beyond single object classification for tackling large-scale scene segmentation. We see this as an important step towards improving real-world 3D scene understanding. Second, inspired by MixUp <ref type="bibr" target="#b61">[62]</ref>, recent techniques propose to interpolate labels <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b62">63]</ref> and even input samples <ref type="bibr" target="#b7">[8]</ref>.</p><p>While appropriate for a holistic classification task, here we keep the mixed samples and labels in their original form. We motivate this in the following manner: consider the task of predicting the convex combination of two inputs</p><formula xml:id="formula_0">(aS 1 , (1 ? a)S 2 ) as aY 1 + (1 ? a)Y 2 . This implies that the prediction of Y 1 changes in the presence of Y 2 .</formula><p>Yet, this is the opposite of our aim here, where we wish to keep the prediction of Y 1 agnostic to the existence of Y 2 . In particular, we preserve the complete context information of each mixed sample, whereas PointMixUp <ref type="bibr" target="#b7">[8]</ref> distorts point clouds by interpolation and RSMix <ref type="bibr" target="#b31">[32]</ref> only preserves the spatial structure of locally restricted chunks. We provide pseudo code in <ref type="figure" target="#fig_2">Figure 3</ref> (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first compare a variety of state-of-theart models trained with and without Mix3D on three largescale indoor and outdoor 3D semantic segmentation benchmarks (Sec. 4.1). We provide detailed analysis experiments to motivate and understand the importance of scene context, local geometry and mixing scenes (Sec. 4.2). Finally, we show additional qualitative results. More analysis and results are found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-art Methods</head><p>Datasets. S3DIS <ref type="bibr" target="#b2">[3]</ref> consists of dense 3D point clouds from 6 large-scale areas including 271 rooms from 3 different buildings. The annotations cover 13 semantic classes. We follow the common evaluation protocol <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref> and train on all areas except Area 5, which serves as testset. ScanNet V2 <ref type="bibr" target="#b9">[10]</ref> contains 3D scenes of a wide variety of indoor rooms annotated with 20 semantic classes. We follow the public training, validation, and test split of 1201, 312 and 100 scans, respectively. SemanticKITTI <ref type="bibr" target="#b3">[4]</ref> ScanNet <ref type="bibr" target="#b9">[10]</ref> S3DIS <ref type="bibr" target="#b2">[3]</ref> SemanticKITTI  <ref type="table">Table 2</ref>: Semantic segmentation scores on ScanNet validation <ref type="bibr" target="#b9">[10]</ref>. We report the mean per-class scores over three trained models.</p><p>shows large-scale outdoor traffic scenes recorded with a Velodyne-64 laser scanner. It consists of 19130 training scenes, recorded over 10 different driving sequences, and a validation set of 4071 samples. There are 28 annotated semantic classes from which 19 are evaluated. We report scores in the single scan setting, i.e., individually per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models in comparison.</head><p>We apply Mix3D to Minkowski-Net <ref type="bibr" target="#b8">[9]</ref> and KPConv <ref type="bibr" target="#b48">[49]</ref>, as the recent best-performing point-based and voxel-based methods. MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> is a voxel-based model that implements the idea of sparse convolutional networks originally presented by Graham et al. <ref type="bibr" target="#b21">[22]</ref>. KPConv <ref type="bibr" target="#b48">[49]</ref> represent the currently best performing point-based approach. Its kernel weights are located in Euclidean space which is more flexible than fixed grid positions. CVPR'20 3D 66.6 DCM-Net <ref type="bibr" target="#b43">[44]</ref> CVPR'20 3D + Mesh 65.8  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Motivation and Analysis Experiments</head><p>In this section, we experimentally motivate and analyze Mix3D. Unless stated otherwise, the model is Minkowski-Net (5 cm voxels) trained on the ScanNet train split and evaluated on the validation split for semantic segmentation.</p><p>What is the effect of context? Recent developments focus on models with ever larger receptive fields, such that they can capture larger scene context. The intuition is that global context provides additional information that is helpful for making local decisions. To verify this hypothesis, we artificially reduce the scene context. In particular, we train different models on smaller regions of a scene, i.e., on crops of smaller sizes. The key idea is that, during training, the model sees only a fraction of the original scene, and therefore it cannot rely on global scene context (only on reduced context within the crop). In this experiment, we train MinkowskiNet [9] models on increasing chunk sizes, as well as KPConv <ref type="bibr" target="#b48">[49]</ref> models on spheric crops of increasing radius. Indeed, the results in <ref type="figure" target="#fig_3">Fig. 4</ref> show that increasing the crop size (and thus the context) is in general helpful. However, we also observe that MinkowskiNet models trained on 1 /4-fractions of a scene perform on-par to models trained on full scenes. This suggests that relying too much on global context can reduce performance. A similar effect is observed for KPConv where the performance does not significantly increase after 2 m radius. In conclusion, global context does indeed improve performance, however only up to a certain point at which overfitting to context limits the performance.</p><p>Does Mix3D help to focus more on local geometry? In the previous experiment, we have seen that increased global context is not necessarily beneficial as overfitting to scene priors can occur. The goal of the next experiment is to demonstrate that models trained with Mix3D are less reliant on context information and are able to make better use of local geometry and structures. For this experiment, the setup during training remains unchanged, i.e., we train the model with full scenes from the ScanNet training split. At test time, however, we simulate missing context by showing only isolated objects to the trained network. The individual objects are cropped out using the object mask annotations available in the ScanNet validation split. This experiment shows how much the model depends on context, and how much it can make use of local geometry. In Tab. 4, we show semantic segmentation scores for individual instances (mean IoU) for two types of models, one trained with, and one trained without Mix3D. mIoU Method (on single instances) MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> 24.5 ? 0.88 +11.5 MinkowskiNet + Mix3D (ours) 36.0 ? 0.57 <ref type="table">Table 4</ref>: Evaluation on single validation instances. Using the ground truth instance masks, we evaluate on cropped out single instances. Both models are trained on full scenes. The Mix3D model improves over the model trained without Mix3D by +11.5 mIoU. We report mean IoU scores and standard deviations of three trained models each.</p><p>While the overall scores are significantly lower than in full-scene validation experiments -showing the importance of global context -we also see that the Mix3D models perform significantly better (+11.5 mIoU) on single instances than models trained without Mix3D. In particular, this means that Mix3D models are notably less dependent on global context to make more accurate predictions given only local geometry. As such, we argue that they are less prone to overfitting on strong context priors and more importantly, Mix3D models can extract more discriminative information from local geometry alone. We conclude: although exposed to larger crops, models trained without Mix3D are limited in their ability to leverage context information fully (c.f . <ref type="figure" target="#fig_3">Fig. 4</ref>). As seen in this experiment, Mix3D trained models, however, balance better between global context and local geometry and are thus less prone to overfit to context cues, resulting in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do alternative out-of-context augmentations work?</head><p>We have already seen that Mix3D influences the scene context during training so that the model overfits less on global context and relies more on local geometry by exposing objects of one scene to the context of another scene, and vice versa. Next, we want to see if it is possible to find a simpler approach that achieves a similar effect as Mix3D. First, we look at random noise as a means of obscuring scene context as suggested in <ref type="bibr" target="#b5">[6]</ref>. We compare two different noise patterns: first, we add random points near the surface of 20% of original points, i.e., within a radius of 0.5 m around the original point. The intuition is that this noise pattern only affects instances locally by obscuring their context. The second approach consists in adding random, uniformly distributed points within the bounding volume of the scene, which can enable non-local information flow. For each 60cm 3 cube we add a point with a 0.1 m random offset. Tab. 5, 1 shows the resulting scores: adding these random noise patterns drastically reduces the performance by more than 6.5% mIoU compared to the baseline. Next, we adapt CutOut <ref type="bibr" target="#b10">[11]</ref> from the image domain and remove points that fall into randomly sampled cubical chunks. This resembles a thinning-out effect <ref type="bibr" target="#b46">[47]</ref> on the context level which reduces the risk of overfitting to small training set-specific context features <ref type="bibr" target="#b10">[11]</ref>. We evaluate varying cuboid sizes and cutting frequencies (see Tab. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2 )</ref>. The experiments show that CutOut is sensitive to the parameter choice and that Mix3D still outperforms the best performing CutOut model. Last, we mix a second scene without ground truth labels, i.e., the semantic segmentation loss is computed only for points from one scene. While this model (Tab. 5, 3 ) outperforms the baseline significantly, it does not reach the performance of the fully supervised model. Nevertheless, this reveals an additional strength of Mix3D: the ability to use unlabeled scenes for mixing demands minimal acquisition effort since no human effort for labelling is required.  How important is the scene overlap in Mix3D? The previous experiment established that mixing scenes is key to the improved performance of Mix3D. In the next experiment, we take a closer look at the mixing itself, i.e., the combination of two scenes with increasing overlap. The baseline is a MinkowskiNet trained without Mix3D, i.e., a single scene constitutes a training sample (c.f . Tab. 6, 1 ). As Mix3D technically concatenates batch entries in pairs, we conduct an experiment (c.f . Tab. 6, 2 ) to see the influence of the altered batching, i.e., the concatenation of two scenes that are placed very far away, such that the receptive field of the model does not enable any information flow between the two scenes. The scores are reported in Tab. 6. Perhaps not surprisingly, this setup shows no significant difference to the baseline, as contexts of the scenes do not mutually influence each other. In the next setup (c.f . Tab. 6, 3 ), the scenes are placed nearby, such that information can flow via the receptive field, however without physical overlap of the scenes. We observe an improvement of at least +1.2 mIoU compared to the baseline, as now each scene is influenced indirectly by the context of the neighboring scene via the receptive field of the model. In the last experiment (c.f . Tab. 6, 4 ), we also include physical overlap of the scenes (this corresponds to Mix3D) and we observe the biggest improvement (+2.4 mIoU) compared to the baseline. We conclude that the context overlap caused by mixing scenes is the decisive aspect which explains the performance of Mix3D. In particular, objects of one scene are additionally exposed to the context of the other scene, and vice versa. <ref type="bibr" target="#b0">1</ref> Single scene <ref type="bibr" target="#b1">2</ref> No overlap (and far away) <ref type="bibr" target="#b2">3</ref>    Does Mix3D improve generalization? Poor generalization can be caused by overfitting to the training set. Overfitting and large generalization gaps are a considerable problem for 3D scene understanding tasks where training samples are few, and in particular, when combined with high-capacity models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49]</ref> that can overfit by memorizing the global context of the training scenes. Learning curves are a useful tool to diagnose overfitting, specifically the generalization error on previously unseen data. <ref type="figure">Fig. 6</ref> shows the training loss L train , validation loss L val and the mean IoU per training epoch for models trained with and without Mix3D. Mix3D reduces the generalization gap between L train and L val , while L val of the Mix3D models increases much later than for models trained without Mix3D. This shows less overfitting to the training set and better generalization on the validation set in terms of semantic segmentation perfor-mance (mIoU). Increasing the size of the training set is also a simple approach to reduce the generalization error <ref type="bibr" target="#b20">[21]</ref>. Mix3D can be seen as a technique to create additional training examples by pairwise combination of existing samples.</p><p>Qualitative Results. We show qualitative comparisons of models trained with and without Mix3D. Specifically, in <ref type="figure" target="#fig_0">Fig. 1</ref> we show out-of-context scenarios, and <ref type="figure">Fig. 5</ref> shows predictions on indoor and outdoor datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>In this work, we have introduced Mix3D, a simple yet effective data augmentation technique for large-scale 3D scene segmentation. Models trained with Mix3D learn to better balance global context priors and local structure, as indicated by our analysis. In experiments, we have shown that Mix3D significantly boosts the semantic segmentation performance of multiple state-of-the-art 3D deep learning models on both indoor and outdoor datasets. Importantly, Mix3D is easy to implement and can directly be incorporated into existing training pipelines. It remains an open research question how Mix3D can be applied to approaches combining 3D scans and corresponding 2D images. Further, increased realism could improve the performance, since Mix3D generates unrealistic scenes. Similarly, other tasks, such as object detection or instance segmentation, can be investigated. In future research, we intend to investigate how to adapt the strong generalization abilities of Mix3D for weakly-and self-supervised learning, and we expect to see further developments along this line of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on Architectures and Training</head><p>MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> In Tab. 7, we provide the architectural details of the MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> model used for the reported scores on the datasets ScanNet <ref type="bibr" target="#b9">[10]</ref>, S3DIS <ref type="bibr" target="#b2">[3]</ref> and SemanticKITTI <ref type="bibr" target="#b3">[4]</ref>. We use the same model definitions as provided in the official code release of MinkowskiNet 1 . For ScanNet, we use 2 cm voxels for comparing to state-of-theart methods and for the benchmark results (see Tab. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. All other analysis experiments are performed on 5 cm voxels for faster training.</p><p>KPConv <ref type="bibr" target="#b48">[49]</ref> We use the official TensorFlow code release 2 for rigid and deformable KPConv on ScanNet and S3DIS. For rigid KPConv on SemanticKITTI, we utilize the PyTorch reimplementation 3 by the original authors. Code for deformable KPConv on SemanticKITTI is not directly available. In particular, we follow the same evaluation procedure as KPConv. That is, we perform 100 evaluation runs for ScanNet and S3DIS (20 runs for SemanticKITTI) on augmented validation samples and conduct majority voting on the resulting predictions. Note that we do not apply Mix3D during test time, i.e., during inference the trained networks operate one single scenes.</p><p>Training details. As Mix3D combines scenes, the total number of points in a single training sample consists of the sum over the number of all points of each mixed scene. To ensure a fair comparison between MinkowskiNets, we appropriately reduce the batch size of Mix3D for keeping the total number of points in a training iteration equal to the one of the baseline method. For instance, we train the 2 cm baseline MinkowskiNet on a batch size of 6. When training with Mix3D, we reduce the batch size to 3 as each training sample consists of 2 mixed scenes. By doing so, we ensure that each model is trained on the same number of labeled points and therefore the results are comparable. For KPConv, we resort to approximately keeping the total num-ber of points in a train iteration equal. Here, the original codebase operates on a variable batch size with a variable number of points in each sample. We therefore occasionally need to reject samples for Mix3D in order to mix the specified number of spheres. To ensure comparability to the baseline method, we follow the same training protocol proposed by Thomas et al. <ref type="bibr" target="#b48">[49]</ref> and evaluate after the same number of epochs.</p><p>Inference settings. For the ScanNet benchmark submission, we train an ensemble of four MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> models with Mix3D. In particular, the four models are Res16UNet34C and Res16UNet34A, each with the first convolution kernel size set to 3 and 5. We train each model for 12?10 4 iterations on both the official training and validation splits of ScanNet <ref type="bibr" target="#b9">[10]</ref>. Similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>, we perform test-time-augmentation (translation, rotation, scaling, color-jitter, random brightness and contrast) and average the predictions of 40 runs for each model. To obtain the final predictions, we employ a graph-based over-segmentation scheme <ref type="bibr" target="#b17">[18]</ref> (similar to OccuSeg <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b34">[35]</ref>) with the goal to further smooth the predicted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Analysis</head><p>Influence of the number of mixed scenes. In this experiment, we vary the number of mixed scenes to evaluate if mixing more than two scenes further improves the segmentation performance. Tab. 8 shows that the validation performance peaks at two mixed scenes and noticeably decreases with more scenes. This is in line with other mixed sample data augmentation works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Voxel Size</head><p>Model Input Features ScanNet <ref type="bibr" target="#b9">[10]</ref> 2 cm Res16UNet34A RGB 5 cm Res16UNet34C RGB S3DIS <ref type="bibr" target="#b2">[3]</ref> 5 cm Res16UNet34C RGB Sem.KITTI <ref type="bibr" target="#b3">[4]</ref> 15 cm Res16UNet14A Reflect., Dist.    Influence of non-mixed spheres in a batch. On Scan-Net, Thomas et al. <ref type="bibr" target="#b48">[49]</ref> propose to train KPConv on spheres with 2 m radii cropped from the original point cloud. As Mix3D is implemented as concatenating subsequent pairs of batch entries, we will always obtain fully overlapping spheres in each novel training sample. However, when training MinkowskiNets, we mix full scenes. This results in overlapping areas, but also in unmixed areas (c.f . <ref type="figure">Fig. 2)</ref>.</p><p>We therefore perform an ablation study with KPConv to measure the performance influence of adding non-mixed spheres to a batch, effectively imitating un-mixed regions. In Tab. 9, we observe that exposing the network also to nonmixed training samples improves the performance upon the initial Mix3D implementation by up to 0.6 mIoU. Comparison to single-instance mixing. In Tab. 11, we investigate an alternative formulation of Mix3D. Instead of mixing complete scenes, we consider mixing single instances randomly sampled from an instance database. This approach explicitly aligns with the key motivation of Mix3D -reducing the context bias by placing instances in novel out-of-context environments. Instance augmentation as proposed in 2D methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref> or 3D object detection <ref type="bibr" target="#b57">[58]</ref> has not yet been investigated for the task of 3D semantic segmentation. As such, we suggest that mixing single instances constitutes an interesting avenue worth investigating. To this end, we create a database consisting of instances from the training set. Next, we place randomly sampled instances from this database in the scene. In particular, we evaluate two strategies for instance placing: 1 creating overlapping instances by placing randomly sampled instances exactly at the center position of arbitrary instances of the scene and 2 randomly placing instances within the bounding scene volume. We evaluate two sampling ratios. For a sampling ratio of +1.0x, we sample exactly one other random instance from the instance database for each original instance in the scene. For a sampling ratio of +0.5x, we sample half the number of original instances from the instance set. We observe that single-instance mixing's performance is in the interval of three Mix3D runs and is therefore comparable to Mix3D. Perhaps unsurprisingly, single-instance mixing also improves significantly  <ref type="table" target="#tab_0">Table 11</ref>: Mixing with single instances. For every instance originally occurring in the scene we add 1x or 0.5x as many instances. We place these newly sampled instances either at positions of already existing instances, i.e. overlapping, or freely in the volume of the scene, i.e., free space, where we might create overlaps with existing instances. Above, we show an augmented scene with instances freely placed in the bounding volume with a sampling ratio of +1.0x. We observe that single-instance mixing's performance is in the interval of three Mix3D runs. over the baseline, as the key idea of instance mixing aligns with the motivation of Mix3D. However, Mix3D is easier to implement into existing pipelines, and avoids the following fundamental disadvantages of single-instance mixing:</p><p>(1) In contrast to Mix3D, single-instance mixing requires a database of object instances available during training to augment the scenes. <ref type="formula">(2)</ref> The construction of such a database requires instance level annotations which are not necessarily available in semantic segmentation datasets. As an alternative, one could rely on synthetic CAD models (e.g. from ShapeNet). Mix3D does not require additional annotations or other datasets. (3) Single-Instance mixing requires a dedicated instance placing procedure introducing potentially sensitive hyper-parameters, while Mix3D simply concatenates overlapping scenes. (4) Finally, by mixing the scene with instances, we effectively train on more data points, i.e., we introduce a compounding factor of exposing the model to instances of the training set more often as we do for Mix3D. This makes a fair comparison harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Per-Class Quantitative Evaluation</head><p>Tab. 12-14 show per-class semantic segmentation results on all three datasets and different models trained with and without Mix3D. Moreover, in Tab. 15, we extend Tab. 4 and provide the per-class segmentation scores on isolated instances of the ScanNet validation set. When no scene context is available, a model trained with Mix3D outperforms the same model trained without Mix3D on 18 out of 20 semantic classes, indicating that Mix3D enables models to extract more meaningful information from local geometry and structures alone.  <ref type="table" target="#tab_0">Table 15</ref>: Isolated instances. Per-class semantic segmentation IoU scores on the ScanNet validation set <ref type="bibr" target="#b9">[10]</ref>. We extend <ref type="table">Table 2</ref> of the main paper by providing per-class semantic segmentation scores. We report performance improvements on 18 out of 20 semantic classes.</p><p>The study is performed with MinkowskiNet on 5 cm voxels and we train 3 models to obtain the means and standard deviations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Semantic segmentation improvements using Mix3D. Mix3D is a data augmentation technique that increases generalization beyond the contextual priors of the training scenes and improves predictions for rare events not well captured by the training data. Models trained with Mix3D (bottom) can handle critical situations where pedestrians walk in the middle of the road (left), and can correctly identify objects in unusual settings (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(Left) Training pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Influence of context during training. Increasing context during training improves semantic segmentation performance on ScanNet validation. However only up to a certain point at which overfitting to scene context limits performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Qualitative results. We show qualitative results of models trained with and without Mix3D on ScanNet (left), SemanticKITTI (center) and S3DIS (right). Compared to the original model, in (a) Mix3D helps to tell apart two stacked washing machines labeled as G "other furniture" from G "fridge". In (b), the MinkowskiNet trained without Mix3D wrongly classifies the G "bicycle" on the sidewalk next to the vegetation as G "other-object". For S3DIS, models trained with Mix3D particularly profit in the G "window" class, as in (c). Learning curves with and without Mix3D. We show the mean and standard deviation over three training runs of a MinkowskiNet trained with and without Mix3D evaluated on the ScanNet validation set. The MinkowskiNet is trained on 5 cm voxels including the standard data augmentations: rotation, translation, color-jitter and elastic distortion. The training loss is notably lower without Mix3D (left). The validation loss increases much later for models trained with Mix3D (center) which indicates less overfitting, and results in overall improved performance (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of Mix3D on models for 3D semantic segmentation. We compare MinkowskiNet (voxel-based) and KPConv (pointbased), with and without Mix3D on large-scale indoor scenes (ScanNet, S3DIS) and outdoor scenes (SemanticKITTI). Mix3D mIoU wall floor cabinet bed chair sofa table door wndw books. pic counter desk curtain fridge shower toilet sink bathtub otherf. 82.1 91.0 84.4 74.5 65.0 62.8 79.5 32.4 64.4 63.7 75.5 51.6 69.0 93.0 67.6 87.6 56.3 (2cm) 73.6 86.0 96.6 66.3 82.1 91.9 86.1 75.7 64.7 64.7 79.6 36.3 67.7 67.0 74.7 59.3 68.8 93.9 68.6 87.9 55.1 KPConv [49] 69.3 82.4 94.4 64.5 79.2 88.5 77.2 73.0 60.5 59.1 79.8 28.4 59.9 63.7 71.6 53.1 54.1 91.5 63.3 86.0 56.4 (Deformable) 70.3 82.6 94.4 65.8 79.5 90.2 79.9 72.5 61.7 58.2 79.6 31.5 62.1 63.2 72.0 58.1 56.8 91.8 64.6 85.6 55.4</figDesc><table><row><cell>[4]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>1/16</cell><cell>1/4</cell><cell>full</cell></row></table><note>Semantic segmentation results on ScanNet test. We include methods that additionally rely on 2D images and polygon meshes as input. Benchmark accessed on 5 th October 2021.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Alternative Context Changes. We compare random noise patterns</figDesc><table /><note>1 , cutting out random chunks 2 and mixing scenes with and without annotated labels 3 . Mix3D with labels performs best, while mixing without labels is still a viable approach when large amounts of unlabeled data are available or too costly to label.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of scene overlap. Overlap of scene-context is the decisive aspect of Mix3D. Placing scenes next to each other without overlaps or too far away for the model's receptive field to enable mutual information exchange between both scenes.</figDesc><table><row><cell>Baseline Augmentation</cell><cell></cell><cell></cell></row><row><cell>Baseline Aug. + Mix3D</cell><cell></cell><cell></cell></row><row><cell>(a) ScanNet [10]</cell><cell>(b) SemanticKITTI [4]</cell><cell>(c) S3DIS [3]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>MinkowskiNet<ref type="bibr" target="#b8">[9]</ref> architectures. We provide architectural details about the MinkowskiNetworks used on Scan-Net, S3DIS and SemanticKITTI. In particular, we use standard MinkowskiNets provided in the official repository and did not change the model definitions. Our ablation study provided in the main paper is performed on 5 cm voxels.</figDesc><table><row><cell cols="2"># non-mixed spheres mIoU (? ?)</cell></row><row><cell>0</cell><cell>69.7 ? 0.11</cell></row><row><cell>1</cell><cell>70.1 ? 0.35</cell></row><row><cell>2</cell><cell>70.3 ? 0.06</cell></row><row><cell>3</cell><cell>69.9 ? 0.27</cell></row><row><cell>4</cell><cell>70.3 ? 0.11</cell></row><row><cell>baseline</cell><cell>69.3 ? 0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Influence of non-mixed spheres in a batch.</figDesc><table><row><cell>We ob-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Consistent improvements wrt. dataset size. We report consistent absolute performance gains in the range of +[2.0, 2.8] mIoU for all subsets of the ScanNet train set.</figDesc><table><row><cell></cell><cell cols="2">(a) One Scene</cell><cell></cell><cell cols="2">(b) Two Scenes</cell><cell></cell></row><row><cell></cell><cell cols="2">(c) Four Scenes</cell><cell></cell><cell cols="2">(d) Eight Scenes</cell><cell></cell></row><row><cell># Scenes</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>7</cell><cell>8</cell></row><row><cell>mIoU</cell><cell cols="6">66.6 69.0 68.6 68.7 68.3 65.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Varying number of mixed point clouds. While individual objects on two mixed point clouds (b) can still be easily distinguished, mixing eight point clouds (d) results in scenes where individual objects are hardly visually distinguishable.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Consistent improvements wrt. dataset size. In Tab. 10, we simulate a scarce data setting by training MinkowskiNet on randomly sampled subsets of the ScanNet train set and evaluate it on the full validation set. We observe that Mix3D achieves consistent absolute improvements (abs.?) over all dataset sizes in the range of [2.0, 2.8] mIoU, while the relative improvement (rel.?) becomes increasingly larger for smaller datasets. This shows the increased benefit of Mix3D when only few training samples are available.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>SemanticKITTI [4] test set. Per-class semantic segmentation IoU scores. Mix3D mIoU wall floor cabinet bed chair sofa table door wndw books. pic counter desk curtain fridge shower toilet sink bathtub otherf. 24.58 57.95 85.76 6.29 34.64 65.90 66.28 13.91 3.26 20.61 52.65 0.78 0.26 8.63 19.72 3.89 0.11 10.65 7.51 16.28 16.47 ?0.89 ?0.97 ?0.43 ?0.56 ?4.46 ?5.36 ?3.70 ?6.40 ?2.11 ?3.46 ?2.96 ?0.74 ?0.34 ?3.92 ?1.23 ?5.50 ?0.10 ?6.59 ?5.66 ?4.30 ?2.38 36.00 65.09 87.40 10.73 43.13 80.91 79.49 33.89 6.49 27.94 59.35 6.35 0.51 19.28 40.71 3.08 0.00 52.91 17.55 60.42 24.76 ?0.58 ?0.33 ?1.91 ?0.81 ?1.74 ?0.92 ?1.42 ?4.98 ?0.50 ?4.46 ?0.98 ?4.36 ?0.49 ?6.51 ?1.27 ?1.30 ?0.00 ?8.65 ?8.90 ?6.27 ?3.23</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.github.com/chrischoy/SpatioTemporalSegmentation-ScanNet 2 www.github.com/HuguesTHOMAS/KPConv 3 www.github.com/HuguesTHOMAS/KPConv-PyTorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work is supported by the ERC Consolidator Grant DeeViSe (ERC-2017-CoG-773161) and compute resources from RWTH Aachen University (rwth0629, thes0775). Contributions from the first author are part of his master thesis.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for Domain Adaptation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Semantic Parsing of Large-Scale Indoor Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perceiving Real-World Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vicinal Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04086,2020.3</idno>
		<title level="m">GridMask Data Augmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PointMixup: Data Augmentation for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved Regularization of Convolutional Neural Networks with Cutout</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling Visual Context is Key to Augmenting Object Detection Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dilated Point Convolutions: On the Receptive Field Size of Point Convolutions on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From Points to Multi-Object 3D Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Graphbased Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="www.deeplearningbook.org.8" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MixUp as Locally Linear Out-of-Manifold Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">OccuSeg: Occupancyaware 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Contextual Relations: The Influence of Familiarity, Physical Plausibility, and Belongingness. Perception and Psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitehurst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bidirectional Projection Network for Cross Dimension Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent Slice Networks for 3D Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual Multi-view Fusion for 3D Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularization Strategy for Point Cloud via Rigidly Mixed Sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution On X-Transformed Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One Thing One Click: A Self-Training Approach for Weakly Supervised 3D Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<idno>2021. 12</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Role of Context in Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Cognitive Sciences</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bauszat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03305</idno>
		<title level="m">The Elephant in the Room</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Rathod</surname></persName>
		</author>
		<title level="m">DOPS: Learning to Detect 3D Objects and Predict Their 3D Shapes. In Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance as a Metric for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DualConvMesh-Net: Joint Geodesic and Euclidean Convolutions on 3D Meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Not Using the Car to See the Sidewalk Quantifying and Controlling the Effects of Context in Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised Localization and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02545</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks From Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to Generate Synthetic Data via Compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F M S</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Monira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold Mixup: Better Representations by Interpolating Hidden States. In International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13048</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep Parametric Continuous Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">PointConv: Deep Convolutional Networks on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PointASNL: Robust Point Clouds Processing Using Nonlocal Neural Networks with Adaptive Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Sensors</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep Fusionnet for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01461</idno>
		<title level="m">Regularization Strategy for Point Cloud Classification</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning Object Placement by Inpainting for Compositional Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Random Erasing Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Method Mix3D mIoU wall floor cabinet bed chair sofa table door wndw books. pic counter desk curtain fridge shower toilet sink bathtub otherf</title>
		<imprint/>
	</monogr>
	<note>MinkNet [9] 66.7 81.0 93.8 61.0 79.7 88.6 79.6 71.9 55.6 57.54 75.4 25.4 55.0 61.3 61.3 44.2 61.6 85.9 61.8 79.3 53.8 (5cm) 69.1 82.1 93.9 63.9 79.2 89.2 81.1 70.8 59.3 59.8 78.1 25.6 62.2 62.1 67.0 54.1 64.0 89.8 64.1 81.1 54.3</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Table 12: ScanNet [10] validation set. Per-class semantic segmentation IoU scores</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Method Mix3D mIoU ceiling floor wall beam column window door chair table bookshelf sofa board clutter</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Per-class semantic segmentation IoU scores. We report the mean over 3 trained models</title>
		<idno>Table 13: S3DIS</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Method Mix3D mIoU car bike mbike truck vehicle person cyclist mcyclist road parking sidewalk other-gr building fence veget trunk terrain pole sign</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
