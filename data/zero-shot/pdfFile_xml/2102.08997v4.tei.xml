<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-shot action recognition in challenging therapy scenarios</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISR-Lisboa</orgName>
								<orgName type="institution" key="instit2">Instituto Superior T?cnico</orgName>
								<orgName type="institution" key="instit3">Universidade de Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DIIS-I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISR-Lisboa</orgName>
								<orgName type="institution" key="instit2">Instituto Superior T?cnico</orgName>
								<orgName type="institution" key="instit3">Universidade de Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Santos-Victor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISR-Lisboa</orgName>
								<orgName type="institution" key="instit2">Instituto Superior T?cnico</orgName>
								<orgName type="institution" key="instit3">Universidade de Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bernardino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISR-Lisboa</orgName>
								<orgName type="institution" key="instit2">Instituto Superior T?cnico</orgName>
								<orgName type="institution" key="instit3">Universidade de Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DIIS-I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Bitbrain Technologies</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DIIS-I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-shot action recognition in challenging therapy scenarios</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot action recognition aims to recognize new action categories from a single reference example, typically referred to as the anchor example. This work presents a novel approach for one-shot action recognition in the wild that computes motion representations robust to variable kinematic conditions. One-shot action recognition is then performed by evaluating anchor and target motion representations. We also develop a set of complementary steps that boost the action recognition performance in the most challenging scenarios. Our approach is evaluated on the public NTU-120 one-shot action recognition benchmark, outperforming previous action recognition models. Besides, we evaluate our framework on a real use-case of therapy with autistic people. These recordings are particularly challenging due to high-level artifacts from the patient motion. Our results provide not only quantitative but also online qualitative measures, essential for the patient evaluation and monitoring during the actual therapy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition is a challenging problem with high relevance for many application fields such as humancomputer interaction, virtual reality or medical therapy analysis. Certain works use raw appearance information for the action recognition tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. Most recent works use skeleton-based representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>. These representations encode the human pose independently of appearance, surroundings and being more robust to occlusions.</p><p>Real world scenarios often require the capability to recognize new action categories that cannot be learned, due to their creation on the fly or data limitations. Action classifiers that handle this problem <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11]</ref> of learning from limited data are based on encoding motion sequences into meaningful descriptors. Then, to recognize a new target sequence, they evaluate the similarity of the target descrip- <ref type="figure">Figure 1</ref>: Overview of the proposed action recognition method. An anchor action is processed and compared against a target motion sequence. Output results show when the anchor action has been performed within the target motion sequence.</p><p>tor with the descriptors from one (one-shot) or few (fewshot) anchor labeled actions. Learning discriminative action encodings and their application in real scenarios are still open challenges. This is partially due to variable motion recording set-ups and unconstrained action execution rules (unsegmented action sequences, multiple consecutive executions, heterogeneous action executions, etc.).</p><p>This work tackles the problem of one-shot action recognition in unconstrained motion sequences. Our novel skeleton-based solution 1 is summarized in <ref type="figure">Fig. 1</ref>. In the proposed workflow, a stream of skeleton poses is encoded in an online manner, generating a descriptor at each time-step. Comparing the descriptors from a given reference anchor action with descriptors from a target video, we can detect when the anchor action has been performed within the target sequence.</p><p>The main components of our work are two fold: 1) a motion encoder, based on geometric information, which is robust to heterogeneous movement kinematics; 2) the actual one-shot action recognition step, based on evaluating the similarity between an anchor action and target motion encodings. We propose a set of improvements to this final action recognition step designed to achieve a more accurate action recognition in the wild. These improvements include an extended anchor action representation and a dynamic threshold that discriminates challenging action sequences. Besides, the proposed action recognition approach can be easily extended to a few-shot problem, if multiple anchor actions are available.</p><p>The presented approach is validated on a generic and public one-shot action recognition benchmark, the NTU RGB+D 120 dataset <ref type="bibr" target="#b9">[10]</ref>, where it outperforms the existing baseline results. Besides, we exhaustively analyze the performance of our system on data from a real application, automatic analysis of therapies with autistic people (based on gesture and action imitation games). Evaluation shows our proposed improvements to work in the wild, managing to overcome challenging motion artifacts that are particular for this real environment. Final outcome provides online and real-time quantitative and qualitative results, essential to evaluate the patient attention and coordination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section summarizes the most relevant contributions for skeleton-based action recognition, making special emphasis on N-shot action recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Skeleton representations for action recognition</head><p>Although many skeleton-based action recognition approaches use the raw Cartesian pose coordinates with no special pre-processing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>, other works research on skeleton data transformations to achieve view-invariant coordinates or compute new geometric features. The approach in <ref type="bibr" target="#b22">[23]</ref> trains a Variational Autoencoder to estimate the most suitable observation view-points, and transforms the skeletons to those view-points. Other approach applies viewinvariant transformations to the skeleton coordinates <ref type="bibr" target="#b19">[20]</ref>. Regarding the computation of new skeleton features, earlier work <ref type="bibr" target="#b1">[2]</ref> computes multiple geometric features including joint distances and orientation, distances between joints, lines and planes, velocity and acceleration. More recent approaches describe a set of geometric features based on distances between joints and lines <ref type="bibr" target="#b23">[24]</ref> or propose to use pair-wise euclidean joint distances and two-scale motion speeds <ref type="bibr" target="#b21">[22]</ref>.</p><p>Our approach uses a skeleton representation that combines a view-invariant transformation of the skeleton Cartesian coordinates with the computation of a set of additional geometric features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Skeleton-based neural networks for action recognition</head><p>Recurrent neural networks have been widely applied to learn temporal dependencies for action recognition approaches. <ref type="bibr" target="#b19">[20]</ref> uses a Variational Autoencoder with bidirectional GRU layers for unsupervised training, and work in <ref type="bibr" target="#b11">[12]</ref> presents a recurrent attention mechanism that improves iteratively. Other approaches rely on 1D Convolutional Neural Networks (CNN), such as <ref type="bibr" target="#b21">[22]</ref>, that uses a ResNet backbone, and <ref type="bibr" target="#b22">[23]</ref> in which a CNN is combined with LSTM networks. Another common architecture within action recognition approaches is Graph Convolutional Networks, such as the models proposed in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b2">[3]</ref>, that reduce the computational complexity with flexible receptive fields.</p><p>In our method, a Temporal Convolutional Network (TCN) ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>) is chosen to encode temporal action segments into fixed-length representations. TCNs have already shown a good action recognition performance, easing the interpretability of their results <ref type="bibr" target="#b7">[8]</ref>. TCNs use onedimensional dilated convolutions to learn long-term dependencies from variable length input sequences. Convolutions allow parallelizing computations allowing fast inference and performing equally or even better than RNNs in sequence modeling tasks by exhibiting longer memory [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">N-shot action recognition</head><p>N-shot action recognition is still an active area of research. We find earlier methods like <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b8">[9]</ref> that use HOF and HOG features for one-shot action recognition in RGB+D videos. More recently, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref> use a 2D Spatio-Temporal LSTM Network, the latter with a bidirectional pass. Other applications join visual and semantic information to perform zero-shot action recognition, such as <ref type="bibr" target="#b5">[6]</ref>, that uses hierarchical LSTMs and word2vec <ref type="bibr" target="#b13">[14]</ref>, and <ref type="bibr" target="#b6">[7]</ref>, that uses a Relation Network, a Spatial Temporal Graph Convolution Network (ST-GCN) and sent2vec. <ref type="bibr" target="#b15">[16]</ref>.</p><p>Different from these methods, we propose the simple yet effective TCN described above as a feature extractor for one-shot action recognition. Additionally, we show how a robust variation of the approach can boost the final recognition performance in challenging real world scenarios. <ref type="figure" target="#fig_0">Figure 2</ref> summarizes our motion description approach. It first normalizes input streams of skeleton data (computed using standard techniques <ref type="bibr" target="#b25">[26]</ref>), it generates sets of pose features that are encoded into motion descriptors by a TCN. One-shot action recognition is performed evaluating the similarity between motion descriptors from anchor and target sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose normalization</head><p>A human movement is defined by a set of N poses</p><formula xml:id="formula_0">X = {X 1 , .., X N }. Each pose X n is defined as a set of J 3D body keypoint coordinates, X n = {x n 1 , ..., x n J }, x n j R 3 , composing what we name a skeleton.</formula><p>Human actions are frequently recorded in dynamic scenarios that involve different view-points and people moving and interacting freely. To achieve a better action recognition generalization, we normalize skeleton data by applying a per-frame coordinate transformation from the original coordinate system W to a new one H, obtaining new view and location-invariant coordinate sets. As represented in the <ref type="figure" target="#fig_1">Fig. 3</ref>, H is set to have its origin at the middle point of the vector composed by the two hip keypoints. This hip vector is aligned to the new X axis and oriented to always have left and right hip X coordinates negative and positive values respectively. Similarly, the vector composed by the spine keypoint and the origin becomes the Y axis, leaving the Z axis to describe the depth information by being orthogonal to X-Y. The corresponding transformation T HW is applied to each 3D point in a pose X n to obtain the new set of 3D keypoint coordinatesX n as:</p><formula xml:id="formula_1">X n = T HW n X n<label>(1)</label></formula><p>Regardless of the camera configuration, action sequences can be performed by different people with heterogeneous heights. To get scale-invariant coordinatesX n , each skeletonX n is scaled to a predefined size. In particular, since the joints defining the torso usually present little noise, we scale each skeleton to have a fixed torso lengthL:</p><formula xml:id="formula_2">X n =X n * L L n (2)</formula><p>where L n is the length of the corresponding torso andL is set as the average ratio between the torso length and the height (calculated from the NTU RBG+D 120 Dataset). <ref type="figure" target="#fig_2">Figure 4</ref> shows original and normalized skeleton coordinates after applying the two proposed normalization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose Features</head><p>The presented approach includes as final pose features the normalized coordinatesX n , described above, and the following additional geometric features:</p><p>? Pair-wise keypoint distances P n , calculated as the Euclidean distance between each possible pair of skeleton joints, encode the pose relative to the J skeleton joints. This set of features has the size of J 2 .</p><p>? Bone angles B n from the original coordinates, calculated as the elevation ? and azimuth ? (see <ref type="figure" target="#fig_1">Fig. 3</ref>) of each vector composed by two connected joints. These angles encode the orientation of each bone relative to the world. This set of features has the size of b ? 2, being b the number of bones within the skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Motion descriptor generation from a TCN</head><p>In order to generate motion representations based on the temporal context and not only static pose features, we use a Temporal Convolutional Network (TCN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. The TCN processes, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, streams of pose features M = {X, P, B}, and obtains motion embeddings z (or descriptors). This motion generation works in an online fashion, creating embeddings z n = T CN (M n?w:n ) that encode, at the time n, all the motion from the last w frames. This receptive field (memory) w is implicitly defined by the TCN hyperparameters (details in Section 4.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">One-shot action recognition</head><p>We formulate the one-shot action recognition as a simple similarity evaluation between the anchor embedding z a calculated to describe an anchor action and a stream of target embeddings z T extracted from a full video sequence.</p><p>For the anchor action description, we use the embedding associated to the last frame of the anchor action, i.e. z a , assuming it encodes all the relevant previous motion information. Then, the evaluation distance at time n is given by the distance between the anchor embedding and the target embedding z T (n) computed at time n:</p><formula xml:id="formula_3">d 1 (n) = D(z a , z T (n))<label>(3)</label></formula><p>For the embedding distance computation D we have explored several options. The cosine distance (cos) and the Jensen-Shannon divergence (JS) are the two best performing ones:</p><formula xml:id="formula_4">Dcos(z1, z2) = 1 ? z1 ? z2 z1 z2 ,<label>(4)</label></formula><formula xml:id="formula_5">DJS(z1, z2) = KL(z1 ||z1,2) + KL(z2 ||z1,2) 2<label>(5)</label></formula><p>where z 1 and z 2 are two motion embeddings, KL is the Kullback-Leibler divergence andz 1,2 is the pointwise mean of z 1 and z 2 . Both functions are bounded between 0 and 1, being this last value the lowest similarity between two movement descriptors.</p><p>The final action recognition is performed by thresholding the calculated distance. If d 1 (n) is below the acceptance threshold ?, we consider that the anchor action has been detected by frame n. This threshold value is set by evaluating the precision/recall curve over an evaluation dataset, as detailed in Section 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Improving action recognition in the wild</head><p>Real action recognition applications (e.g. real medical therapies) involve artifacts that hinder the motion description and recognition. These issues are intensified in the oneshot recognition set-up, where the available labeled data is limited. In the following, we describe different improvements for the action recognition described previously, to get better performance in the wild.</p><p>Extended anchor action representation. Anchor actions are not only scarce but frequently also hard to consistently segment in a video sequence. Therefore, using just the last embedding generated for them can lead to noisy action descriptions. In order to get a better anchor action representation, we use a set of descriptors z A = {z 1 , ..., z m } composed by the ones generated at their last m frames. The distance between a target embedding z T (n) and the anchor action is then set as the minimum distance to each element of the set of anchor embeddings (z A ):</p><formula xml:id="formula_6">d m (n) = min ?za?z A D(z a , z T (n))<label>(6)</label></formula><p>Few-shot recognition. In case more than one anchor sequence is available, the set of anchor embeddings z A can be easily augmented with the embeddings generated for the different anchor sequences.</p><p>Dynamic threshold. Most challenging scenarios can include actions consisting of maintaining a static pose or performing subtle movements. The descriptors generated from this type of actions are very similar to the descriptors from target idle poses (e.g. when no specific action is being performed). If information of idle positions is available, we propose to use it to set a dynamic threshold that can better (a) NTU RGB+D 120 dataset frames.</p><p>(b) Therapy dataset frames. discriminate idle poses from actual actions. New threshold value is set to be the minimum between the original threshold ? and the 10 th percentile P 10% of all the distances computed between a given idle target sub-sequence (identified within the target sequence) and the anchor action representation:</p><formula xml:id="formula_7">? = min(?, P 10% n=a...b {d(n)}),<label>(7)</label></formula><p>where a and b are the initial and final time steps of the idle interval, and d is computed as in Eq. (4) or Eq. (6) depending on the anchor representation used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section details experimental validation of the proposed action recognition approach, including implementation and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup 4.1.1 Datasets</head><p>The proposed method is designed for one-shot online action recognition in real scenarios. In particular, our motivation is to automate the analysis of medical therapies. Since the available data in this setting is scarce, real therapy data is only used for evaluation, while a large public action dataset (NTU RGB+D 120) is used to learn the action encoding model as well as to validate the motion representation framework in a public benchmark.</p><p>The NTU RGB+D 120 dataset <ref type="bibr" target="#b9">[10]</ref> (see <ref type="figure" target="#fig_3">Fig. 5a</ref>) is so far the largest dataset available for skeleton-based action classification. It contains 114,480 action samples belonging to 120 different categories with different complexities. Action sequences are recorded at 30 fps with different users, setups and points of view.</p><p>The therapy dataset 2 (see <ref type="figure" target="#fig_3">Fig. 5b</ref>) has been acquired in real medical settings and consists of 57 different imitation games where the patient has to imitate the therapist <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> action and gestures. Each imitation game is composed of at least one anchor action and one patient imitation, but frequently more anchors (up to 3) or imitations appear in the games. Due to the nature of this data, patient motion is characterized by strong artifacts such as highly variable length imitations, uncommon resting poses and poor quality imitations. Due to limited computational resources, sessions are recorded with a variable low frame rate of 12 fps on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation and training details</head><p>Due to the nature of the therapy dataset, we have specified the memory length w of our TCN to be 32 frames long by using a convolutional kernel size of 4, stacks of 2 residual blocks, and dilations of 1, 2 and 4 for the layers within each convolutional block. We also use skip connections and 256 filters in each convolutional layer, which finally generates motion descriptors of size 256.</p><p>This TCN is pretrained for a classification task (categorical cross-entropy loss) in the NTU RGB+D 120 dataset by adding an output classification layer to the TCN backbone and applying the following random data augmentation to the original coordinates:</p><p>? Movement speed variation. Joint coordinates are randomly scaled by interpolation along the time dimension to simulate varying movement speeds.</p><p>? Skip frames. Since the training dataset is recorded at more than twice the frame rate of the evaluation dataset, for each training sample we just use one out of either two or three frames, discarding the rest.</p><p>? Horizontal flip. Coordinates that correspond to the left or right part of the body are randomly flipped. Vertical and depth dimensions remain as originally.</p><p>? Random cropping. Actions longer than the receptive field are randomly cropped to fit the TCN memory length. Shorter ones are pre-padded with zeros.</p><p>Finally, after calculating and stacking all the pose features M the input training data has a size of 32 ? 423.  <ref type="table">Table 1</ref>: NTU RGB+D 120 Dataset One-shot evaluation</p><p>After training, the output classification layer is discarded to extract the 256-dimensional motion descriptors straight from the TCN backbone. <ref type="table">Table 1</ref> shows the classification accuracy of our framework for one-shot action recognition on the NTU-120 oneshot 3D action recognition problem, where we outperform the results of previous action recognition models <ref type="bibr" target="#b9">[10]</ref>. For this benchmark, we use the same implementation described in Section 4.1.2 and the same training procedure, but with the train/test splits specified in the original paper <ref type="bibr" target="#b9">[10]</ref>. Additionally, we also set the frame skipping to 2 both for training and evaluation and we suppress the random horizontal flipping during training. Descriptors are evaluated with the cosine distance, but other distance functions such as the Jensen-Shannon divergence report identical or comparable performance. Since this problem is about classifying action segments and not identifying anchor actions in the wild, classification is performed by assigning, for each evaluation sample, the class with the lowest similarity distance among the set of anchor action segments. Note that, unlike the other action recognition models that work in an offline fashion, our model works in an online and real-time fashion and only uses half of the available data (alternate frames).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Validation of the system on a generic one-shot action recognition benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Validation and discussion on real therapies data</head><p>This experiment evaluates the performance of our action recognition approach on the imitation games from the therapy dataset. Here, the actions from the therapist are taken as anchor to later detect their imitation at each time-step of the patient motion stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Quantitative results</head><p>Metrics. Action recognition is evaluated with common metrics, i.e. precision, recall and F1. However, since each target motion descriptor z T (n) does not refer only to a single time-step n but to the motion from the w previous frames, we have defined the following terms as follows:</p><p>? True Positive (TP). An action is correctly detected when the ground truth action is being executed or has been recently executed within the TCN receptive field w. A groundtruth action referenced by many detections only counts as one TP.</p><p>? False Positive (FP). An action is incorrectly detected when no groundtruth action has been recently executed. Consecutive detections inside the TCN receptive field w only count as one FP.</p><p>? False Negative (FN). A groundtruth action is missed if it has not been referenced by any action detection. Each miss-detected action counts as a single FN.</p><p>Precision-Recall trade off. The threshold ? selected to be used over the distance scores to perform action recognition is the one that optimizes the trade-off between precision and recall defined by the F1 metric. In our experiments, we calculate different thresholds to optimize the results achieved when using one descriptor per anchor (m = 1) and with extended anchor representations (m = 3). As seen in the precision-recall curves from <ref type="figure" target="#fig_4">Fig. 6</ref>, the best trade-off is achieved at the cost of lowering the precision of the framework, especially when working with single anchor descriptors.</p><p>Analysis of variations of our approach. The effect of the different variations proposed to make our action recognition more robust are summarized in <ref type="table">Table 2</ref>. First three rows show the performance of the one-shot version, i.e., using as anchor the last action performed by the therapist in each imitation game. The use of extended anchor representations (m = 3) allows us to have a more restrictive threshold ?, which reduces false positives, increasing the detection precision. The increase in precision is also achieved by  <ref type="table">Table 2</ref>: Comparison of the studied variations of our action recognition approach on the evaluation therapy dataset.</p><p>making the threshold dynamic. New threshold values can be set differently in each imitation game by evaluating the anchor representation and the target motion up the end of the anchor execution (period in which the patient is not performing any specific action). The dynamic threshold avoids false positives related to "static" anchor actions, at no recall cost. Finally, as expected, the performance improves when more anchor actions (up to 3 in the experiments) are available to run a few-shot recognition. Regarding the embedding similarity computation, both the cosine distance and the JS divergence report similar performance when using extended anchor representations. However, the JS divergence performs better in simpler set-ups and the cosine distance excels when using a dynamic threshold.</p><p>The results can be further scrutinized in <ref type="table" target="#tab_3">Table 3</ref>, that compares the recognition performance by action category. Extended anchor representations (m = 3) and few-shot recognition significantly overcome the limitations of the simple one-shot recognition for challenging classes (last 8 rows of the table refer to actions with softer and subtler arm movements or actions that consist in staying on specific positions). This is further improved when using a dynamic threshold (DT).</p><p>Influence of different pose feature sets. <ref type="table" target="#tab_4">Table 4</ref> reports the influence of different pose features proposed in Section 3.2. Using only normalized coordinates achieves lower performance than using just the original body coordinates. Normalized coordinates gain in invariance, but they lose certain discriminative power. The proposed geometric feature set achieves good performance on its own. However, the best performance comes from the combination of normalized coordinates and geometric features, which suggests that the geometric features work better along with the invariance provided by normalized coordinates. <ref type="figure" target="#fig_5">Figure 7</ref> shows the timeline of an imitation game sampled from the therapy dataset. This exercise involves the action of raising and moving the arms, which is performed once   (anchor) by the therapist and repeated twice (target) by the patient. Detection results from the timeline represent both the location and quality of both action repetitions. The latter is estimated according to the calculated similarity between anchor and target motion. The first action imitation is performed quickly (worse quality) and the second one slower and more detailed (better quality). For a better understanding, the whole execution of the experiment is shown in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Time performance</head><p>Temporal Convolutional Networks are frequently lightweight models, making them suitable for lowresource environments like in the therapies described in Section 4.1.1. With the settings described in Section 4.1.2, the action recognition (except the skeleton extraction from RGB-D data) takes 0.08 ms per time-step using the GPU and 0.1 ms just using the CPU 4 . Time performance has been calculated with the therapy dataset skeleton sequences. Note that due to the TCN parallel computing, the longer the motion sequences are, the faster per-frame processing we can get.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This work presents a novel skeleton-based framework for one-shot action recognition in the wild. Our method generates motion descriptors robust different motion artifacts and variable kinematics. We achieve accurate action recognition by combining the proposed set of pose features and an efficient architecture based on a TCN that en-3 https://sites.google.com/a/unizar.es/filovi/ <ref type="bibr" target="#b3">4</ref> Performing speed has been calculated with a NVIDIA GeForce RTX 2080 Ti (GPU) and a Intel Core i7-9700K (CPU) codes these pose features. Besides, we demonstrate the effectiveness of several simple steps included in our method to boost the action recognition performance in challenging real world scenarios, i.e. with limited reference data and noisy repetitions of the actions to be recognized. Our base one-shot recognition approach is evaluated on the public NTU RGB+D 120 dataset, outperforming previous action recognition methods, by using only half of the available data. We also demonstrate the suitability of our framework to analyze videos from real therapies with autistic people. These recordings are characterized by having extreme motion artifacts. Evaluation results provide both quantitative and qualitative measures about the actions recognized, which is essential for the patient evaluation and monitoring. Moreover, our approach can run online, being able to provide immediate feedback to the therapist and making the sessions more dynamic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Motion descriptor generation. Input skeleton coordinates X n are normalizedX n and pose features M are calculated. Pose features are processed by a TCN to generate motion descriptors z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Skeleton representation. W and H refer to the original and transformed skeleton coordinate systems respectively. H axis are aligned with the vectors (dashed lines) that cross the human hip and spine. ? and ? angles refer to the elevation and azimuth calculation in a bone from the leg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) skeletons from jump up action (b) skeletons from moving heavy objects action Pose normalization. Left plots show original skeleton coordinates of different actions from the NTU-120 RGB-D dataset. Right plots represent the same skeletons after applying the proposed pose normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Sample frames from the training (a) dataset (kicking something, handshaking and shoot at the basket actions) and evaluation (b) datasets (big, give and high gestures).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Precision/recall curves for the two evaluation distances used: cosine distance (cos) and Jensen-Shannon divergence (js) . Solid lines refer to regular one-shot evaluations (m = 1) and dashed lines refer to few-shot recognition with extended anchor representations (m = 3). Red crosses refer to the optimal F1 values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Action recognition on one imitation game from the therapy dataset. Central horizontal line represents the execution timeline, with one anchor action (top/purple) performed by the therapist, followed by two imitations (bottom/blue) performed by the patient. Dashed line segments indicate the groundtruth action time segmentations. Vertical small solid segments, ranging from green to orange, refer to time steps where the action has been detected, and the color gives an estimation of the repetition quality. supplementary video 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>few-shot stands for the use of 1, 2 or 3 anchor sequences depending on the available reference data for each test, as explained in Section 4.1.1</figDesc><table><row><cell>Recognition Modality</cell><cell>m</cell><cell>Dynamic Threshold</cell><cell cols="3">Cosine distance ? Precision Recall</cell><cell>F1</cell><cell cols="3">Jensen-Shannon divergence ? Precision Recall</cell><cell>F1</cell></row><row><cell>one-shot</cell><cell>1</cell><cell></cell><cell>0.48</cell><cell>0.655</cell><cell>0.922</cell><cell>0.697</cell><cell>0.50</cell><cell>0.689</cell><cell>0.902</cell><cell>0.714</cell></row><row><cell>one-shot</cell><cell>3</cell><cell></cell><cell>0.40</cell><cell>0.773</cell><cell>0.837</cell><cell>0.724</cell><cell>0.48</cell><cell>0.765</cell><cell>0.853</cell><cell>0.728</cell></row><row><cell>one-shot</cell><cell>3</cell><cell></cell><cell>? 0.40</cell><cell>0.803</cell><cell>0.837</cell><cell cols="2">0.751 ? 0.48</cell><cell>0.779</cell><cell>0.853</cell><cell>0.739</cell></row><row><cell>few-shot*</cell><cell>1</cell><cell></cell><cell>0.48</cell><cell>0.784</cell><cell>0.827</cell><cell>0.731</cell><cell>0.50</cell><cell>0.785</cell><cell>0.846</cell><cell>0.744</cell></row><row><cell>few-shot*</cell><cell>3</cell><cell></cell><cell>0.40</cell><cell>0.760</cell><cell>0.876</cell><cell>0.753</cell><cell>0.48</cell><cell>0.749</cell><cell>0.892</cell><cell>0.755</cell></row><row><cell>few-shot*</cell><cell>3</cell><cell></cell><cell>? 0.40</cell><cell>0.790</cell><cell>0.876</cell><cell cols="2">0.781 ? 0.48</cell><cell>0.763</cell><cell>0.892</cell><cell>0.765</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Per-class F1 evaluation comparison of different variations of our action recognition approach (using the cosine distance).</figDesc><table><row><cell>Coordinates</cell><cell>Normalized Coordinates</cell><cell>Geometric Features</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.711</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.689</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.757</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.695</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.781</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>F1 evaluation of different pose features running our top-performing action recognition (few-shot, cosine distance, extended anchor representations and dynamic threshold).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code, learned models and supplementary video can be found in: https://sites.google.com/a/unizar.es/filovi/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://vislab.isr.tecnico.ulisboa.pt/datasets/#autism</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been funded by FEDER/Ministerio de Ciencia, Innovaci?n y Universidades/Agencia Estatal de Investigaci?n RTC-2017-6421-7 and PGC2018-098817-A-I00, DGA T45 17R/FSE, the Office of Naval Research Global project ONRG-NICOP-N62909-19-1-2027, Universidad de Zaragoza, Fundaci?n Bancaria Ibercaja and Fundaci?n CAI IT 17/19, and the Funda??o para a Ci?ncia e Tecnologia (FCT) project UIDB/50009/2020 and PhD scholarship SFRH/BD/145040/2019.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Universal Language Model Finetuning for Text Classification</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a 3d human pose distance metric from geometric pose descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1676" to="1689" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot learning for real-time action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Sean Ryan Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Odone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Skeleton based zero shot action recognition in joint pose-language semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavan</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshaan</forename><surname>Mazagonwalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11344</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot-learning gesture recognition using hog-hof features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Hagara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2513" to="2532" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatiotemporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="528" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interaction relational network for mutual action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive social games with a social robot (IOGIOCO): Communicative gestures training for preschooler children with autism spectrum disorder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Annunziata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Geminiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Brazzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Caglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Pedrocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Olivieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">I Congresso Annuale Rete IRCCCS Neuroscienze e Neuroriabilitazione</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Copyrobot: Interactive mirroring robotics game for asd children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Geminiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Olivieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Pedrocchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XV Mediterranean Conference on Medical and Biological Engineering and Computing -MEDICON 2019</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2014" to="2027" />
		</imprint>
		<respStmt>
			<orgName>Nuno Neves, and Paulo de Carvalho</orgName>
		</respStmt>
	</monogr>
	<note>Jorge Henriques</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict &amp; cluster: Unsupervised skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9631" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Make skeleton-based action recognition model smaller, faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Asia</title>
		<meeting>the ACM Multimedia Asia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1963" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatio-temporal phrases for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weina</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="707" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
