<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiftFormer: 3D Human Pose Estimation using attention models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Llopart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies UK</orgName>
								<address>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirin</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies UK</orgName>
								<address>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LiftFormer: 3D Human Pose Estimation using attention models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the 3D position of human joints has become a widely researched topic in the last years. Special emphasis has gone into defining novel methods that extrapolate 2-dimensional data (keypoints) into 3D, namely predicting the root-relative coordinates of joints associated to human skeletons. The latest research trends have proven that the Transformer Encoder blocks aggregate temporal information significantly better than previous approaches. Thus, we propose the usage of these models to obtain more accurate 3D predictions by leveraging temporal information using attention mechanisms on ordered sequences human poses in videos. Our method consistently outperforms the previous best results from the literature when using both 2D keypoint predictors by 0.3 mm (44.8 MPJPE, 0.7% improvement) and ground truth inputs by 2mm (MPJPE: 31.9, 8.4% improvement) on Human3.6M. It also achieves state-of-the-art performance on the HumanEva-I dataset with 10.5 P-MPJPE (22.2 % reduction). The number of parameters in our model is easily tunable and is smaller (9.5M) than current methodologies (16.95M and 11.25M) whilst still having better performance. Thus, our 3D lifting model's accuracy exceeds that of other end-to-end or SMPL approaches; and is comparable to many multi-view methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation from monocular images is an important research avenue that has gained much momentum lately. The reason behind this is that human pose estimation allows for better gesture/action recognition, game/device control and more precise 3D avatar generation, among other applications. However, currently, estimating precisely the 3D pose of humans requires multi-camera viewpoints, depth sensors or tracking motion capture suits, none of which are available to the general public. Human pose estimation started by predicting sets of 2D joints given a monocular image (either in a top-down or bottom-up fashion), but were soon replaced by architectures that estimated the 3D coordinates (either absolute coordinates w.r.t the camera frame or relative coordinates to the pelvis joint, also known as root). Other methods like SMPL try to parameterize the full human body at a higher computational cost. This work focuses on the estimation of 3D keypoints, corresponding to the most notable joints in the human body, from 2D keypoints. This is commonly known in the literature as lifting from 2D to 3D. Even though this corresponds to only a smaller part of the problem of keypoint estimation, there are inherently multiple 3D poses that can be represented by a set of 2D coordinates. Previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4]</ref> have tackled this problem by predicting the absolute coordinates w.r.t the camera. However, the majority of the research in this field still deals with root-relative estimations, that is, predicting the joint coordinates w.r.t the pelvis joint <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Additionally, the literature proves that leveraging temporal information results in substantial performance improvements. Whilst early approaches employed the use of Recurrent Neural Networks (RNNs), these methods were soon overshadowed by fully convolutional architectures <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>. Lately, the research has shifted to the introduction of attention models <ref type="bibr" target="#b23">[24]</ref>. This work argues that adding attention mechanisms enhances the models temporal coherency by exploiting long-range temporal relationships across frames/poses. An example of another well established attention-based model is the Transformer <ref type="bibr" target="#b40">[41]</ref> and its variants <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b18">[19]</ref>, which have been used in a wide array of tasks, not only in Natural Language Processing (NLP), where it was first introduced, but also in the field of speech recognition <ref type="bibr" target="#b43">[44]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref> and image inpainting <ref type="bibr" target="#b4">[5]</ref>, among others. Furthermore, the ALBERT paper <ref type="bibr" target="#b20">[21]</ref> shows how to reduce the model size of the Transformer, whilst maintaining the same performance, by sharing weights in the multi-attention heads.</p><p>The successful implementation in so many diverse tasks of the Transformer, as a self-attention model, sparks the question of how well it would perform in a lifting scenario. Similarly to <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b23">[24]</ref>, the Transformer Encoder architecture can be used to upscale the dimensionality of sequences of keypoints via its own self-attention. Hence, this paper proposes the Liftformer model, which leverages this type of attention model to predict 3D temporally cohesive joint coordinates from 2D input keypoints. The LiftFormer model not only surpasses all previous lifting literature, but does so with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Human pose estimation, from both single images and videos, has been researched largely in the last years. Originally, most work was based on pre-engineered models, with a large number of constraints, to solve the high degree of dependencies among human joints <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35]</ref>. With the development of Convolutional Neural Networks (CNNs), novel pose estimators were created which reconstructed 3D poses end-to-end directly from RGB images or intermediate 2D predictions. This research rapidly surpassed the accuracy of previous hand-made estimators.</p><p>End-to-end models Many works have pursued the usage of features extracted from RGB images as a way to obtain 3D root-relative pose predictions. Tekin et al. <ref type="bibr" target="#b39">[40]</ref> leverages both 2D heatmaps and image features by building a twostream network which aggregates both features through a trainable fusion strategy. Sun et al. <ref type="bibr" target="#b38">[39]</ref> argues that regression methods do not take advantage of the actual structure of human skeletons. For this reason they propose working with bones, instead of joints, and exploiting the joint connection structure by defining a compositional loss function. Pavlakos et al. <ref type="bibr" target="#b29">[30]</ref> enhances datasets and trained joint regressors using ordinal depth relations. Luvizon et al. <ref type="bibr" target="#b24">[25]</ref> propose a multitask framework for simultaneous 2D/3D pose estimation and action recognition. To achieve pose estimation they use a differentiable soft-argmax for volumetric heatmaps, which has now become one of the most common ways to solve this task. Zhou et al. <ref type="bibr" target="#b45">[46]</ref> use Part-Centric Heatmap Triplets to represent the relative depth information of endjoints. That is, three polarized heatmaps, corresponding to the different state of the local depth ordering of the partcentric joint pairs. Zhao et al. <ref type="bibr" target="#b44">[45]</ref> employ Graph convolutions that take as input the concatenation of pooled features from the backbone of a 2D keypoint predictor and the predictions themselves. However, the accuracy of their results is not that good compared to other state-of-the-art (SOTA) research. Yang et al. <ref type="bibr" target="#b42">[43]</ref> add adversarial learning to a SOTA 3D detector to discriminate humanly plausible poses, which are then aggregated to visual features for Action Recognition.</p><p>Moon et al. <ref type="bibr" target="#b27">[28]</ref> is one of the few papers that deal with absolute 3D coordinates, that is, coordinates relative to the camera itself and not to the root (i.e subject's pelvis). They employ two networks: the former predicts the 3D root relative coordinates by obtaining volumetric heatmaps (introduced by <ref type="bibr" target="#b30">[31]</ref>) and applying a 3D soft-argmax. The latter predicts a correction factor that multiplies a pre-defined depth distance based on the size of the human within the image and the intrinsic camera parameters.</p><p>Lifitng: Two step pose estimation Formally, two-step pose estimation models break down the complexity of 3D pose estimation by dividing the problem into two steps: i. They employ common 2D pose estimators to obtain accurate 2D poses of humans in images, usually in a top-down fashion by first predicting bounding boxes around each human instance and then procesing only those regions of interest.</p><p>ii. They then lift these joints to 3D by predicting their depth relative to the root (pelvis).</p><p>Martinez et al. <ref type="bibr" target="#b25">[26]</ref> were the first to introduce the concept of lifting 2D to 3D, where they proved that using simple and computationally inexpensive models (a residual block architecture) one is able to accurately predict the depth of previously predicted 2D keypoints. Another idea was propsed by Fang et al. <ref type="bibr" target="#b11">[12]</ref> who introduced grammar models and the usage of Bi-RNNs to explicitly incorporate a set of knowledge regarding human body configuration. The RNNS don't model temporal data, but instead model a chain-like grammar for single images. Similarly to <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b25">[26]</ref>, Chang et al. <ref type="bibr" target="#b3">[4]</ref> propose PoseLifter, a method to lift 2D to absolute 3D coordinates using residual blocks.</p><p>Video pose estimation In the last years, given that lifting approaches have performed so well in the past, there has been a significant and successful trend to aggregate multiple poses to produce more temporally cohesive predictions.</p><p>Hossain et al. <ref type="bibr" target="#b14">[15]</ref> define their pose lifter as a sequenceto-sequence model composed of layer-normalized LSTM units with shortcut connections. Similarly, Lee et al. <ref type="bibr" target="#b21">[22]</ref> used a Stacked Hourglass <ref type="bibr" target="#b28">[29]</ref> to predict 2D poses which are then temporally aggregated using p-LSTMs. In contrast, Dabral et el. <ref type="bibr" target="#b8">[9]</ref> temporally aggregated 2D poses, predicted with standard 2D keypoint estimators, by using a simple Fully Connected Network (FCN) with ReLU activations.</p><p>Pavllo et al. <ref type="bibr" target="#b31">[32]</ref> use Temporal Convolutions Networks (TCN) to lift 2D keypoints to 3D. The 2D keypoints are obtained from common object detectors, like Stacked Hourglass (SH), Mask-RCNN <ref type="bibr" target="#b13">[14]</ref> and Cascaded Pyramid Network (CPN) <ref type="bibr" target="#b5">[6]</ref> pretrained on COCO <ref type="bibr" target="#b22">[23]</ref> and then fine-tuned on Humans3.6M <ref type="bibr" target="#b17">[18]</ref>. They prove that using ground truth 2D predictions as inputs produce the best result, followed by CPN detections, MRCNN and, finally, SH. They also investigate how larger receptive fields achieve higher accuracy. This is also demonstrated in Liu et al. <ref type="bibr" target="#b23">[24]</ref>. They are also amongst the first to introduce the concept of attention into the 3D keypoint estimation field. They employ attention by dividing their model into Temporal attention, which calculates the positive cosine similarity between 2D joints, and kernel attention, which is a combination of TCN and Linear projection units. They prove how attention is beneficial for lifting from 2D to 3D.</p><p>Other approaches work by leveraging the original image features when lifting. Cheng et al. <ref type="bibr" target="#b7">[8]</ref> build complex models that focus on occlusions within the time domain. They employ MRCNN to detect humans and SH to detect the 2D keypoints. They clearly state that "by using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints". For this reason, they refine their 2D predictions with optical flow followed by a dilated 2D TCN, which removes occlusions. Likewise, the occluded 3D ground truths are removed by applying a Cylindrical Man model. Finally, the refined 2D keypoints are passed through a 3D TCN and matched with the modified 3D GTs. Their models achieve SOTA but at the price of a very convoluted system.</p><p>Cheng et al. <ref type="bibr" target="#b6">[7]</ref> improves the state-of-the-art by using a discriminator to assess if the generated poses are valid. Specifically, they use the Kinematic Chain Space (KCS) model, defined in <ref type="bibr" target="#b41">[42]</ref>, and expand it temporally (TKCS). They also use multi-scale 2D heatmaps, instead of keypoints directly, extracted from HRNet <ref type="bibr" target="#b37">[38]</ref>. By embedding the differently scaled features into 3D keypoints, KCS and TKCS, and then passing the results into a trained discriminator, they are able to achieve SOTA.</p><p>Attention models Vaswani et al. <ref type="bibr" target="#b40">[41]</ref> demonstrated how attention-based <ref type="bibr" target="#b1">[2]</ref> models, such as the Transformer, can achieve state-of-the-art results in machine translation tasks. These attention methods aggregate information from an entire input sequence into each of its elements. Their computational efficiency and improved long-term memory makes them better mechanisms at dealing with long input sequences than the previous RNNs or LSTMs. For this reason, Transformers have now been widely used in many NLP tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>, but have also been present in other fields like speech recognition (Transformer Transducer <ref type="bibr" target="#b43">[44]</ref>), object detection (DETR <ref type="bibr" target="#b2">[3]</ref>) and image inpainting (Image GPT <ref type="bibr" target="#b4">[5]</ref>), among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LiftFormer architecture</head><p>We propose the usage of a Transformer architecture for lifting. To this end, we employ the base Transformer Encoder presented in <ref type="bibr" target="#b40">[41]</ref> as our baseline, that is 512 dimension hidden layers, 8 multi-attention heads and 6 encoder blocks. The general idea is to pass 2D keypoint sequences through several Transformer Encoder blocks to generate one 3D pose prediction, corresponding to the center of the input sequence/receptive field, and then repeat the process for all timesteps in a sliding window fashion. Thus, during training, the model leverages temporal data from past and future frames to be able to create temporally cohesive predictions.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, certain modifications must take place so that the input and output dimensionality match. Namely, the input to the Transformer Encoder is reprojected from an input dimension of <ref type="bibr">[N, 34]</ref> to <ref type="bibr">[N, 512]</ref>, where N is the receptive field and 34 corresponds to 17 joints times 2 (number of coordinates). The reason for this is so that the input dimensionality matches that of the hidden layers inside the Transformer Encoder block. These input sequences can either come from 2D predictors, which estimate the 2D poses from frames, or directly from the datasets 2D ground truths. Hence, these predictions are in the image space.</p><p>Then, temporal encoding is aggregated by adding a vector to the input embeddings. This is so that the model can make use of the temporal order in the sequence. In the original Transformer paper <ref type="bibr" target="#b40">[41]</ref>, this is referred to as positional encoding which is used to inject some information about the relative or absolute position of the tokens in the sequence. We employ the same idea, using sine and cosine functions to create these temporal embeddings which are then summed to the re-projected inputs. The injected temporal embeddings must therefore have the same dimensionality as the inputs. The resulting features are then sent to the Transformer Encoder, which processes them.</p><p>Since the Decoder part of the Transformer is not being used, and due to the residual connections within the selfattention, the dimension of the output will be exactly the same as the input, that is <ref type="bibr">[N, 512]</ref>. Similarly to the BERT model <ref type="bibr" target="#b10">[11]</ref> during classification tasks, we choose the output token corresponding to the middle of the receptive field, which corresponds to the pose to be lifted within the input sequence. This is because during training, half of the receptive field corresponds to past poses, and the other half to future poses. The center pose within the receptive field is exactly the pose we are trying to lift.</p><p>Finally, we reproject the output embeddings into the desired dimensionality, from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">512]</ref> to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">51]</ref> using again a 1D convolutional layer, where 51 corresponds to 17 joints times 3 (x, y and z coordinates). The loss is then calculated using MPJPE against the datasets 3D ground truths and the error is back-propagated During inference, the model architecture could stay the same or work in a causal fashion, where only past frames will be used in the receptive field. Hence, the model will work in a sliding window fashion so that all frames get their 3D reconstructions.</p><p>One of the benefits of this architecture is that you can modify the receptive field and the number of multi-attention heads, without affecting the models size. Additionally, several ablation studies have been performed which show how modifying the Transformer's hyper-parameters change model size and accuracy results (Section 4.3).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weight sharing</head><p>Lan et al. <ref type="bibr" target="#b20">[21]</ref> propose sharing parameters across layers to build more efficient models whilst maintaining the accuracy. They show how sharing either all parameters in each Encoder block or only the FFN parameters, will considerably hurt the overall performance. On the other hand, sharing only the attention layers parameters seems to barely affect the final accuracy and, in some cases, even improve it. In all cases, the total number of parameters is considerably decreased. More information is shown in the ablation studies in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Protocols</head><p>We evaluate on two motion capture datasets: Human3.6M <ref type="bibr" target="#b17">[18]</ref> and HumanEva <ref type="bibr" target="#b36">[37]</ref> , which have been commonly used in the literature. For both datasets we follow the evaluation procedure of previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref>, among others.</p><p>Human3.6M consists of 3.6 million frames of 11 different subjects, however only 7 of them are annotated. The subjects performed up to 15 different types of actions, which were recorded from 4 different viewpoints. We train on 17 joints with subjects S1, S5, S6, S7 and S8. The evaluation is done on S9 and S11.</p><p>HumanEva is a much smaller dataset in comparison, with only 3 subjects and recorded from 3 viewpoints. We evaluate on 2 actions (Walk, Box) and report all our results for models trained on all actions and 15 joints.</p><p>In our experiments, we consider three evaluation protocols, all of which are relative to the root: Protocol 1 is the mean Euclidean distance between predicted root-relative joint positions and ground truth joint positions (MPJPE in mm). Protocol 2 is the error after applying a similarity transformation (Procrustes alignment) with the ground truth in translation, rotation and scale (P-MPJPE in mm). Protocol 3 is the mean per joint velocity, which evaluates the smoothness and stability of predictions over time (MPJVE in mm/s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Previous work in lifting pose estimation has utilized ground truth human bounding boxes, followed by 2D pose estimators. The most common 2D estimation models are Stacked Hourglass <ref type="bibr" target="#b28">[29]</ref>, Mask-RCNN <ref type="bibr" target="#b13">[14]</ref> or Cascaded Pyramid Networks <ref type="bibr" target="#b5">[6]</ref>. Since our approach is detector agnostic, we have investigated several 2D detectors that do not rely on ground truth data. Namely, we have used SH and CPN as detectors for the Human3.6M dataset and Mask-RCNN for Hu-manEva. However, we have also trained with ground truth 2D poses, to see the reach of our approach.</p><p>Following Pavllo et al. <ref type="bibr" target="#b31">[32]</ref>   <ref type="bibr" target="#b31">[32]</ref>  <ref type="table" target="#tab_2">(n=243 GT)</ref> -      384x384 input resolution. In practice, all 2D predictions have been processed one time first, the results of which are used as input to train our model. Similarly to the latest work in the field <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref>, we have divided our tests into 3 sizes of receptive fields (i.e n=27, 81, 243). The only applied data augmentation, for both training and testing, is horizontally flipping the poses. Following the requirements to train the Transformers attention layers, we use the Adam optimizer <ref type="bibr" target="#b35">[36]</ref>. All training runs last 80 epochs, for Human3.6M, and 1000 epochs, for HumanEva. We use a Noam learning rate schedule which linearly increases for the first training steps <ref type="bibr" target="#b12">[13]</ref> (1000 iterations, with a learning rate factor of 12) and then decreases proportionally to the inverse square root of the step number. The batch sizes are proportional to the receptive field value, such that for b = 5120, 3072, 1536 for n = 27, 81, 243, respectively.</p><formula xml:id="formula_0">- - - - - - - - - - - - - -</formula><p>In terms of hardware, 8 NVIDIA V100 GPUs have been used to train and evaluate the system, with parallel optimization. Typically, the training time for each receptive field is 8, 14 and 40 hours, respectively, taking the batch sizes into account. <ref type="table" target="#tab_1">Tables 1 and 2</ref> show results for Protocol 1 of our LiftFormer model, trained on both CPN predictions and GT, respectively, for 3 values of receptive fields <ref type="bibr">(27, 81 and 243)</ref>. Using both CPN and GT predictions as inputs, our model surpasses the current SOTA for all lifting monocular methods <ref type="bibr" target="#b23">[24]</ref> by 0.3mm (0.7%) and 2.8mm (8.1%), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art</head><p>Interestingly, weight sharing does indeed produce better results for GT inputs and a receptive field of 243. Also, our model does not rely on additional data, like MPII <ref type="bibr" target="#b33">[34]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref>, as some other approaches do.</p><p>Additionally, when using GTs, LiftFormer is still better than other end-to-end models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref> with 42.9, 40.1 and 39.9 mm MPJPE, respectively, which not only leverage temporal data, but also features extracted from the original RGB images themselves, optical flow or occlusion enhanced heatmaps.  <ref type="table">Table 6</ref>: Ablation study on the dimensionality of hidden layers, multi-attention heads and encoder blocks in the Transformer model. Inputs are CPN 2D predictions, with a receptive field of 27 and no weight sharing is used.</p><p>Our model also outperforms other SMPL-based approaches like SPIN <ref type="bibr" target="#b19">[20]</ref> or ENAS <ref type="bibr" target="#b32">[33]</ref>, with 41.1mm and 42.4mm MPJPE, respectively, and multi-view methods, like DeepFuse <ref type="bibr" target="#b15">[16]</ref> with 37.5mm MPJPE. <ref type="table" target="#tab_5">Table 3</ref> reinforces the results of LiftFormer under Protocol 2. When using CPN predictions, our model slightly surpasses all previous lifting methods <ref type="bibr" target="#b23">[24]</ref> (0.6mm, 1.7%). When using GT, our two best models (with receptive fields of 243 and 81, and no weight sharing) achieve 23.1 and 25.4 mm P-MPJPE, which consistently outperform all previous best models <ref type="bibr" target="#b31">[32]</ref> by 4.1mm (15%) and 1.8mm (6.6%), respectively. It is worth noticing how weight sharing decreases the performance slightly in this scenario. <ref type="table" target="#tab_6">Table 4</ref> shows that our model reduces the MPJVE of the single-frame and temporal baseline <ref type="bibr" target="#b31">[32]</ref> by 88.8% and 53.36%, respectively. This results in more temporally coherent poses with a smoother transition between timesteps. <ref type="table" target="#tab_8">Table 5</ref> displays results on the HumanEva dataset and shows how our model correctly generalizes to smaller datasets. The performance is comparable to other approaches, even when using MRCNN detections, which have been proven to perform worse than CPN detections. Our model outperforms previous state-of-the-art methods, when using GT 2D keypoints, by 3mm (22.2%). Interestingly enough, smaller receptive fields (n=27) seem to perform better in this scenario, similarly to <ref type="bibr" target="#b23">[24]</ref>.</p><p>Overall, our models outperform the latest SOTA lifting approaches in terms of performance, whilst keeping the models size relatively small. For a receptive field of 243 and GT 2D inputs, Pavllo et al. <ref type="bibr" target="#b31">[32]</ref> has 16.95M parameters and 37.8 MPJPE: Liu et al. <ref type="bibr" target="#b23">[24]</ref> has 11.25M and 34.7 MPJPE; and our models have 18.96M and 32.5 MPJPE. The next section will show that even when reducing our models parameters, we still outperform both previous approaches. <ref type="table">Table 6</ref> displays the results using CPN predictions and 27 receptive fields. It shows how increasing the dimensionality of the hidden layers in the Transformers attention leads to an increased accuracy of the lifted poses. However, this difference becomes negligible when increasing the dimension value from 512 to 768, at the cost of increasing the models parameters by 75% (19M to 34M). For this reason, most of the reported results of this work use the value 512.  <ref type="table">Table 7</ref>: Ablation study on the weight sharing of attention layers using GT 2D predictions as inputs. B -number of encoder blocks. hd -hidden dimensionality of Transformer Encoder layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>One of the multiple benefits of using these self-attention models is that the total number of parameters is independent of the number of multi-attention heads. This is because the linear layers in these heads are applied before the splitting and after the concatenation of the heads, so the number of parameters is not affected by it. Since the number of attention heads does not seem to influence the performance, we stick to a baseline with 8 multi-heads.</p><p>Finally, the number of blocks is clearly proportional to the model size. We see how increasing the blocks produces better accuracy, but the improvement is negligible when going from 6 to 8 blocks, which corresponds to a significant increase in the models size. <ref type="table">Table 7</ref> demonstrates how using the weight sharing technique for the multi-attention head drastically reduces the number of parameters. We see how weight shared models, given 2D GT inputs, barely decrease their performance, and in the case of 243 receptive field, it actually improves the MPJPE.</p><p>In particular for Protocol 1, <ref type="table" target="#tab_2">Table 2</ref> shows that for a receptive field of 27 our model obtains the same results with and without weight sharing. For a receptive field of 81 using weight sharing decreases slightly the performance. Finally, for a receptive field of 243, the performance is slightly improved.</p><p>However, for Protocol 2, <ref type="table" target="#tab_5">Table 3</ref> shows that weight sharing decreases slightly the performance for all receptive fields. Thus, weight sharing does not necessarily improve nor deteriorate the performance, but depends on specific cases.</p><p>For the sake of reducing parameters, we show how we can make really small models, of 9.5 M and 2.4M parameters which are still competitive, by simply tuning some of the Transformer's hyper-parameters.</p><p>For the 9.5M model, we see how we still have a better accuracy and smaller model size than <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b23">[24]</ref>. They obtain 37.8 and 34.8 MPJPE, respectively, whereas we achieve 33.0 MPJPE. This is with model sizes of 16.95M, 11.25M and 9.5M, respectively. Even for the smallest case (2.4M parameters, 2 encoder blocks, weight sharing and 256 hidden dimensionality), our model is only 5.6mm worse than our best previous result, and we still outperform Pavllo's et al. <ref type="bibr" target="#b31">[32]</ref> results in terms of model size (2.4 vs 16.95M) and accuracy (37.5 vs 37.8 MPJPE),  <ref type="table">Table 8</ref>: Accuracy of 3D predictions in terms of 2D detectors under Protocol 1 with MPJPE (mm). PT -pre-trained, FTfine-tuned, GT -Ground truth, SH -stacked hourglass, CPN -cascaded pyramid network. <ref type="table">Table 8</ref> displays the results based on the 2D predictors: pre-trained Stacked Hourglass on MPII (PT SH), fine-tuned Stacked Hourglass on Human3.6M (SH FT), fine-tuned Cascaded Pyramid Network (CPN FT) and ground truths (GT). As demonstrated by previous research, the quality of the initial 2D predictions is a clear indication of the final 3D accuracy. More interesting is the fact that our model outperforms all previous lifting approaches independently of the 2D detector used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have proposed the usage of a self-attention Transformer Encoder model to estimate the depth of 2D keypoints in monocular videos. The self-attention architecture allows the model to produce temporally coherent poses by exploiting the long-range temporal information across frames/poses. When using 2D predictions (Mask-RCNN and CPN) as inputs, our model outperforms all previous lifting approaches and is comparable to methodologies that use both keypoints and features extracted from the original RGB images. For ground truth inputs, our model outperforms all previous models substantially, achieving results comparable to SMPL or multi-view approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our Transformer Encoder model takes a sequence of 2D keypoints (bottom) as input and generates 3D pose estimates (top) using self-attention on long-term information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative results for several actions in the Humans3.6M dataset. From left to right: Original RGB image with 2D keypoint predictions using CPN. 3D reconstruction using LiftFormer (n=243). Ground truth 3D keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dir. Disc. Eat Greet Phone Photo Pose Purch. Hossain et al. ECCV'18 [15] 48.4 50.7 57.2 55.2 Luvizon et al. CVPR'18 [25] 49.2 51.6 47.6 50.5 Liu et al. CVPR'20 [24] 41.8 44.8 41.1 44.9</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell cols="2">Martinez et al. ICCV'17 [26] 51.8 56.2 58.1 59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell cols="2">74.0 94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell></cell><cell></cell><cell>63.1</cell><cell>72.6</cell><cell>53.0</cell><cell>51.7</cell><cell cols="2">66.1 80.9</cell><cell>59.0</cell><cell>57.3</cell><cell>62.4</cell><cell>46.6</cell><cell>49.6</cell><cell>58.3</cell></row><row><cell>Zhao et al. CVPR'19 [45]</cell><cell>47.3 60.7 51.4 60.5</cell><cell>61.1</cell><cell>49.9</cell><cell>47.3</cell><cell>68.1</cell><cell cols="2">86.2 55.0</cell><cell>67.8</cell><cell>61.0</cell><cell>60.6</cell><cell>42.1</cell><cell>45.3</cell><cell>57.6</cell></row><row><cell></cell><cell></cell><cell>51.8</cell><cell>60.3</cell><cell>48.5</cell><cell>51.7</cell><cell cols="2">61.5 70.9</cell><cell>53.7</cell><cell>48.9</cell><cell>57.9</cell><cell>44.4</cell><cell>48.9</cell><cell>53.2</cell></row><row><cell>Chang et al. [4]</cell><cell>44.8 48.2 48.5 51.5</cell><cell>54.5</cell><cell>64.4</cell><cell>47.9</cell><cell>47.8</cell><cell cols="2">60.7 76.4</cell><cell>52.5</cell><cell>50.8</cell><cell>55.3</cell><cell>39.0</cell><cell>42.2</cell><cell>52.5</cell></row><row><cell>Lee et al. ECCV'18 [22]</cell><cell>40.2 49.2 47.8 52.6</cell><cell>50.1</cell><cell>75.0</cell><cell>50.2</cell><cell>43.0</cell><cell cols="2">55.8 73.9</cell><cell>54.1</cell><cell>55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>Dabral et al. ECCV'18 [9]</cell><cell>44.8 50.4 44.7 49.0</cell><cell>52.9</cell><cell>61.4</cell><cell>43.5</cell><cell>45.5</cell><cell cols="2">63.1 87.3</cell><cell>51.7</cell><cell>48.5</cell><cell>52.2</cell><cell>37.6</cell><cell>41.9</cell><cell>52.1</cell></row><row><cell>Pavllo et al. CVPR'19 [32]</cell><cell>45.2 46.7 43.3 45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell cols="2">57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell></cell><cell></cell><cell>47.4</cell><cell>54.1</cell><cell>43.4</cell><cell>42.2</cell><cell cols="2">56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>Liftformer (n=27 CPN)</cell><cell>45.7 49.2 47.1 47.2</cell><cell>50.72</cell><cell>57.7</cell><cell>46.3</cell><cell>44.5</cell><cell cols="2">58.2 64.9</cell><cell>49.3</cell><cell>45.6</cell><cell>50.0</cell><cell>35.8</cell><cell>37.2</cell><cell>48.6</cell></row><row><cell>Liftformer (n=81 CPN)</cell><cell>43.8. 46.4 44.1 44.6</cell><cell>48.0</cell><cell>54.9</cell><cell>43.7</cell><cell>42.5</cell><cell cols="2">54.9 62.9</cell><cell>45.8</cell><cell>44.1</cell><cell>47.3</cell><cell>33.1</cell><cell>34.0</cell><cell>46.0</cell></row><row><cell>Liftformer (n=243 CPN)</cell><cell>42.2 44.5 42.6 43.0</cell><cell>46.9</cell><cell>53.9</cell><cell>42.5</cell><cell>41.7</cell><cell cols="2">55.2 62.3</cell><cell>44.9</cell><cell>42.9</cell><cell>45.3</cell><cell>31.8</cell><cell>31.8</cell><cell>44.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Protocol 1 with MPJPE (mm): Reconstruction error on Human3.6M. Input 2D joints are acquired by detection. CPN -Cascaded Pyramid Network.</figDesc><table><row><cell></cell><cell cols="5">Dir. Disc. Eat Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Wandt et al. CVPR'19 [42]</cell><cell>50.0 53.5 44.7 51.6</cell><cell>49.0</cell><cell>58.7</cell><cell>48.8</cell><cell>51.3</cell><cell cols="2">51.1 66.0</cell><cell>46.6</cell><cell>50.6</cell><cell>42.5</cell><cell>38.8</cell><cell>60.4</cell><cell>50.9</cell></row><row><cell cols="2">Martinez et al. ICCV'17 [26] 37.7 44.4 40.3 42.1</cell><cell>48.2</cell><cell>54.9</cell><cell>44.4</cell><cell>42.1</cell><cell cols="2">54.6 58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>Zhao et al. CVPR'19 [45]</cell><cell>37.8 49.4 37.6 40.9</cell><cell>45.1</cell><cell>41.4</cell><cell>40.1</cell><cell>48.3</cell><cell cols="2">50.1 42.2</cell><cell>53.5</cell><cell>44.3</cell><cell>40.5</cell><cell>47.3</cell><cell>39.0</cell><cell>43.8</cell></row><row><cell cols="2">Hossain et al. ECCV'18 [15] 35.2 40.8 37.2 37.4</cell><cell>43.2</cell><cell>44.0</cell><cell>38.9</cell><cell>35.6</cell><cell cols="2">42.3 44.6</cell><cell>39.7</cell><cell>39.7</cell><cell>40.2</cell><cell>32.8</cell><cell>35.5</cell><cell>39.2</cell></row><row><cell>Lee et al. ECCV'18 [22]</cell><cell>32.1 36.6 34.4 37.8</cell><cell>44.5</cell><cell>49.9</cell><cell>40.9</cell><cell>36.2</cell><cell cols="2">44.1 45.6</cell><cell>35.3</cell><cell>35.9</cell><cell>37.6</cell><cell>30.3</cell><cell>35.5</cell><cell>38.4</cell></row><row><cell>Pavllo et al. CVPR'19 [32]</cell><cell>35.2 40.2 32.7 35.7</cell><cell>38.2</cell><cell>45.5</cell><cell>40.6</cell><cell>36.1</cell><cell cols="2">48.8 47.3</cell><cell>37.8</cell><cell>39.7</cell><cell>38.7</cell><cell>27.8</cell><cell>29.5</cell><cell>37.8</cell></row><row><cell>Liu et al. CVPR'20 [24]</cell><cell>34.5 37.1 33.6 34.2</cell><cell>32.9</cell><cell>37.1</cell><cell>39.6</cell><cell>35.8</cell><cell cols="2">40.7 41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell>Liftformer (n=27 GT)</cell><cell>37.5 41.0 36.2 35.9</cell><cell>37.5</cell><cell>40.3</cell><cell>41.8</cell><cell>34.8</cell><cell cols="2">42.5 43.5</cell><cell>37.2</cell><cell>38.5</cell><cell>35.7</cell><cell>30.6</cell><cell>31.6</cell><cell>37.7</cell></row><row><cell>Liftformer (n=81 GT)</cell><cell>34.2 37.6 31.5 33.1</cell><cell>34.8</cell><cell>37.5</cell><cell>37.6</cell><cell>33.0</cell><cell cols="2">41.5 43.0</cell><cell>34.2</cell><cell>33.8</cell><cell>32.8</cell><cell>26.2</cell><cell>27.3</cell><cell>34.5</cell></row><row><cell>Liftformer (n=243 GT)</cell><cell>33.2 35.3 29.8 31.1</cell><cell>32.9</cell><cell>35.1</cell><cell>35.5</cell><cell>31.5</cell><cell cols="2">37.2 38.1</cell><cell>32.6</cell><cell>33.1</cell><cell>30.9</cell><cell>24.3</cell><cell>26.1</cell><cell>32.5</cell></row><row><cell>Liftformer (n=27 GT + WS)</cell><cell>37.3 41.4 35.9 35.8</cell><cell>37.6</cell><cell>40.7</cell><cell>41.3</cell><cell>36.0</cell><cell cols="2">41.8 42.9</cell><cell>37.2</cell><cell>38.8</cell><cell>36.5</cell><cell>30.9</cell><cell>31.6</cell><cell>37.7</cell></row><row><cell>Liftformer (n=81 GT + WS)</cell><cell>34.3 38.7 33.2 33.4</cell><cell>35.3</cell><cell>38.4</cell><cell>38.8</cell><cell>32.9</cell><cell cols="2">41.7 43.4</cell><cell>35.1</cell><cell>35.7</cell><cell>33.2</cell><cell>27.4</cell><cell>28.1</cell><cell>35.3</cell></row><row><cell cols="2">Liftformer (n=243 GT + WS) 31.8 34.6 30.1 30.4</cell><cell>32.4</cell><cell>34.3</cell><cell>34.6</cell><cell>31.1</cell><cell cols="2">36.9 38.9</cell><cell>32.2</cell><cell>32.1</cell><cell>30.4</cell><cell>24.4</cell><cell>24.9</cell><cell>31.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Protocol 1 with MPJPE (mm): Reconstruction error on Human3.6M. Input 2D joints from ground truth. GT -Ground truth. WS -Weight sharing between attention layers</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>methodology, SH was pretrained on MPII and fine-tunned on Humans3.6M. Both Mask-RCNN and CPN are pretrained on COCO [23] and then finetunned on the 2D poses of Human3.6M, since the keypoints in each dataset are defined differently. More specifically, Mask-RCNN uses a ResNet-101 with FPN. Since CPN requires bounding boxes, Mask-RCNN is employed first to detect humans. It then uses a ResNet-50 with Dir. Disc. Eat Greet Phone Photo Pose Purch. Pavllo et al. CVPR'19 [32] (n=243 CPN) 34.1 36.1 34.4 37.2 Pavllo et al. CVPR'19</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Martinez et al. ICCV'17 [26]</cell><cell>39.5 43.2 46.4 47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell cols="2">56.5 69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Hossain et al. ECCV'18 [15]</cell><cell>35.7 39.3 44.6 43.0</cell><cell>47.2</cell><cell>54.0</cell><cell>38.3</cell><cell>37.5</cell><cell cols="2">51.6 61.3</cell><cell>46.5</cell><cell>41.4</cell><cell>47.3</cell><cell>34.2</cell><cell>39.4</cell><cell>44.1</cell></row><row><cell>Wandt et al. CVPR'19 [42]</cell><cell>33.6 38.8 32.6 37.5</cell><cell>36.0</cell><cell>44.1</cell><cell>37.8</cell><cell>34.9</cell><cell cols="2">39.2 52.0</cell><cell>37.5</cell><cell>39.8</cell><cell>34.1</cell><cell>40.3</cell><cell>34.9</cell><cell>38.2</cell></row><row><cell>Chang et al. [4]</cell><cell>32.1 34.9 43.4 36.9</cell><cell>35.4</cell><cell>35.1</cell><cell>30.8</cell><cell>34.3</cell><cell cols="2">57.3 40.4</cell><cell>44.9</cell><cell>35.1</cell><cell>24.9</cell><cell>46.6</cell><cell>30.0</cell><cell>37.7</cell></row><row><cell>Dabral et al. ECCV'18 [9]</cell><cell>28.0 30.7 39.1 34.4</cell><cell>37.1</cell><cell>28.9</cell><cell>31.2</cell><cell>39.3</cell><cell cols="2">60.6 39.3</cell><cell>44.8</cell><cell>31.1</cell><cell>25.3</cell><cell>37.8</cell><cell>28.4</cell><cell>36.3</cell></row><row><cell></cell><cell></cell><cell>36.4</cell><cell>42.2</cell><cell>34.4</cell><cell>33.6</cell><cell cols="2">45.0 52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Liu et al. CVPR'20 [24]</cell><cell>32.3 35.2 33.3 35.8</cell><cell>35.9</cell><cell>41.5</cell><cell>33.2</cell><cell>32.7</cell><cell cols="2">44.6 50.9</cell><cell>37.0</cell><cell>32.4</cell><cell>37.0</cell><cell>25.2</cell><cell>27.2</cell><cell>35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Protocol 2 with P-MPJPE (mm): Reconstruction error on Human3.6M with similarity transformation. CPN -Cascaded Pyramid Network. GT -Ground truth. WS -Weight sharing.</figDesc><table><row><cell></cell><cell cols="16">Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell cols="5">Pavllo et al. [32] -Single-frame 12.8 12.6 10.3 14.2</cell><cell>10.2</cell><cell>11.3</cell><cell>11.8</cell><cell>11.3</cell><cell cols="2">8.2 10.2</cell><cell>10.3</cell><cell>11.3</cell><cell>13.1</cell><cell>13.4</cell><cell>12.9</cell><cell>11.6</cell></row><row><cell>Pavllo et al. [32] -Temporal</cell><cell>3.0</cell><cell>3.1</cell><cell>2.2</cell><cell>3.4</cell><cell>2.3</cell><cell>2.7</cell><cell>2.7</cell><cell>3.1</cell><cell>2.1</cell><cell>2.9</cell><cell>2.3</cell><cell>2.4</cell><cell>3.7</cell><cell>3.1</cell><cell>2.8</cell><cell>2.8</cell></row><row><cell>Liftformer (n=27)</cell><cell>1.6</cell><cell>1.7</cell><cell>1.3</cell><cell>1.9</cell><cell>1.3</cell><cell>1.5</cell><cell>1.6</cell><cell>1.8</cell><cell>1.0</cell><cell>1.4</cell><cell>1.2</cell><cell>1.5</cell><cell>2.2</cell><cell>1.9</cell><cell>1.7</cell><cell>1.6</cell></row><row><cell>Liftformer (n=81)</cell><cell>1.4</cell><cell>1.5</cell><cell>1.1</cell><cell>1.7</cell><cell>1.1</cell><cell>1.4</cell><cell>1.4</cell><cell>1.7</cell><cell>0.9</cell><cell>1.3</cell><cell>1.1</cell><cell>1.2</cell><cell>2.0</cell><cell>1.7</cell><cell>1.4</cell><cell>1.4</cell></row><row><cell>Liftformer (n=243)</cell><cell>1.3</cell><cell>1.4</cell><cell>1.0</cell><cell>1.6</cell><cell>1.1</cell><cell>1.3</cell><cell>1.3</cell><cell>1.6</cell><cell>0.9</cell><cell>1.2</cell><cell>1.0</cell><cell>1.2</cell><cell>1.9</cell><cell>1.6</cell><cell>1.3</cell><cell>1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Protocol 3 with MPJVE (mm/s): Velocity error over the 3D poses</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>ICCV'17 [26] 19.7 17.4 46.8 26.9 18.2 18.6 24.6 Pavlakos et al. CVPR'17 [30] 22.3 19.5 29.7 28.9 21.9 23.8 24.4 Lee et al. ECCV'18 [22] 18.6 19.9 30.5 25.7 16.8 17.7 21.5 Liu et al. CVPR'20 [24] 13.1 9.8 26.8 16.9 12.8 13.3 15.4 Zhou et al. ICCV'19 [46] 13.5 9.9 17.1 24.5 14.8 14.4 15.2 Pavllo et al. CVPR'19 [32] 13.9 10.2 46.6 1 20.9 13.1 13.8 14.4 Cheng et al. ICCV'19 [8] 11.7 10.1 22.8 18.7 11.4 11.0 14.3 Cheng et al. arXiv'20 [7] 10.6 11.8 19.3 15.8 11.5 12.2 13.5</figDesc><table><row><cell></cell><cell></cell><cell>Walk</cell><cell></cell><cell></cell><cell>Jog</cell><cell></cell></row><row><cell></cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg</cell></row><row><cell>Martinez et al. Liftformer (243 MRCNN)</cell><cell cols="7">17.6 11.1 48.1 1 30.0 14.6 15.0 25.7</cell></row><row><cell>Liftformer (81 MRCNN)</cell><cell cols="7">14.8 10.1 47.5 1 23.8 13.5 14.0 15.2</cell></row><row><cell>Liftformer (27 MRCNN)</cell><cell>14.0</cell><cell>9.3</cell><cell cols="5">46.6 1 20.1 12.6 12.8 13.8</cell></row><row><cell>Liftformer (243 GT)</cell><cell>12.0</cell><cell>7.6</cell><cell cols="3">18.1 18.1 8.8</cell><cell cols="2">8.9 12.3</cell></row><row><cell>Liftformer (81 GT)</cell><cell>10.6</cell><cell>7.3</cell><cell cols="3">15.9 15.4 7.9</cell><cell cols="2">9.0 11.0</cell></row><row><cell>Liftformer (27 GT)</cell><cell>9.4</cell><cell>6.7</cell><cell cols="3">16.4 14.0 7.6</cell><cell cols="2">8.8 10.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Protocol</figDesc><table><row><cell>2 with P-MPJPE (mm): Reconstruction</cell></row><row><cell>error on HumanEva. GT -Ground truth (15 joints). MRCNN</cell></row><row><cell>-Mask-RCNN (17 joints) Note 1 : The high error on Walk of</cell></row><row><cell>S3 is due to corrupted mocap data, hence the average is not</cell></row><row><cell>calculated with this value.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiview pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the British Machine Vision Conference (BMVC)</title>
		<meeting>eeding of the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PoseLifter: Absolute 3D Human Pose Lifting Network from a Single Noisy 2D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generative Pretraining from Pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/image-gpt/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11822</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2019)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence IEEE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask-RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep-Fuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV) (2020)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV) (2020)</meeting>
		<imprint>
			<biblScope unit="page" from="429" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the IEEE Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2019)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Propagating LSTM: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision</title>
		<meeting>the IEEE European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</meeting>
		<imprint>
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
	<note>Little</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on 3D Vision</title>
		<meeting>the IEEE International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="506" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2019)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV) (2016)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV) (2016)</meeting>
		<imprint>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A unified deep framework for joint 3d pose estimation and action recognition from a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salmane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khoudour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crouzil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zegers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1825</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV) (2012)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV) (2012)</meeting>
		<imprint>
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the convergence of Adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3941" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Advances in Neural Information Processing Systems</title>
		<meeting>the IEEE Advances in Neural Information Processing Systems<address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2020)</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2020)</meeting>
		<imprint>
			<biblScope unit="page" from="7829" to="7833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HEMlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2019)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2019)</meeting>
		<imprint>
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
