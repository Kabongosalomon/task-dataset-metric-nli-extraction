<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Burst Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhat</forename><surname>Goutam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danelljan</forename><surname>Martin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Burst Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAW Input burst</head><p>Single Image SR Burst SR (Ours) HR Reference <ref type="figure">Figure 1</ref>. Our network generates a super-resolved RGB image from an input burst consisting of multiple noisy RAW frames. In contrast to the single image baseline, our approach combines information from multiple frames to obtain a more detailed reconstruction of the scene. The results shown are for super-resolution by a factor of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>While single-image super-resolution (SISR) has attracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame superresolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increasing popularity of burst photography, have made MFSR an important problem for real-world applications.</p><p>We propose a novel architecture for the burst superresolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise optical flow. The information from all frames are then adaptively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth. We perform comprehensive experimental analysis, demonstrating the effectiveness of the proposed architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-resolution (SR) is the task of generating a highresolution (HR) image, given one or several low-resolution (LR) observations. It is a widely studied problem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b54">54]</ref> with numerous practical applications. In recent years, the SR community has mainly focused on the single image super-resolution (SISR) task, where an HR image is estimated from a single LR input. Due to the ill-posed nature of the SISR problem, these methods are limited to adding high frequency details through learned image priors.</p><p>The multi-frame super-resolution (MFSR), on the other hand, aims to reconstruct the original HR image using multiple LR images. If the input images have sub-pixel shifts with respect to each other, due to e.g. camera motion, they provide different LR samplings of the underlying scene. MFSR approaches can thus exploit this additional signal information to generate a higher quality image, compared to the SISR approaches (see <ref type="figure">Fig. 1</ref>). The MFSR problem naturally arises in the increasingly popular mobile burst photography, where the images have different sub-pixel shifts due to natural hand tremors <ref type="bibr" target="#b48">[48]</ref>. This opens up the possibility of using MFSR to overcome the resolution constraint in mobile cameras imposed by the cost and size restrictions.</p><p>Despite the aforementioned advantages, MFSR has received little attention in recent years. This is in stark contrast to SISR, where deep learning has led to significant advancements in SR performance. Compared to the SISR case, the MFSR problem imposes significant challenges when developing deep learning based solutions. Firstly, a MFSR architecture must be able to align the noisy input frames with sub-pixel accuracy in order to enable fusion. Secondly, it should be able to effectively fuse the information from the aligned frames, while being robust to alignment errors. Furthermore, the lack of benchmark datasets for the general MFSR task has led to a limited interest in the MFSR problem. We address these issues by proposing a novel deep learning based approach for the MFSR problem, along with a real-world dataset.</p><p>Our network directly operates on noisy RAW bursts captured from a hand-held camera and generates a denoised, demosaicked, and super-resolved image as output. This is achieved by developing a novel attention-based fusion module which can adaptively merge an arbitrary number of input frames in order to produce a high quality output. Our approach is not limited to simple motions between the images, such as translation or homography. Instead, we estimate dense pixel-wise optical flow to align the deep feature encoding of each input frame. The aligned representations of each frame are then merged by computing element-wise fusion weights. This allows the network to adaptively select the reliable and informative content from each image, while discarding, e.g., misaligned regions.</p><p>The conventional approach in SISR is to train and evaluate models on synthetically generated data. However, this has been shown to not generalize to real-world images due to inaccuracies in data generation model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>. Accurately modelling the image formation process for MFSR is further challenging due to the additional complexity introduced by camera motion. We therefore introduce the BurstSR dataset: the first real-world burst super-resolution dataset. Our dataset consists of 200 RAW bursts captured using a hand held mobile camera. Furthermore, we provide a high quality HR ground truth for each burst using a DSLR with zoom lens. We believe that our BurstSR dataset can serve as a valuable benchmark and source of training data to stimulate future research in MFSR. Contributions: Our main contributions are summarized as follows. (i) We introduce the first real world burst super-resolution dataset consisting of RAW bursts and corresponding HR ground truths. (ii) We propose a novel MFSR architecture which can perform joint denoising, demosaicking, and SR using bursts captured from a handheld camera. (iii) Our architecture employs an attention-based fusion method to adaptively merge the input images to generate high quality HR output (iv) We further address mis-alignment issues encountered when training on real world data by introducing a loss function which can internally correct these mis-alignments.</p><p>We perform comprehensive experiments on a synthetic dataset, as well as the BurstSR test set, in order to validate our contributions. Our approach demonstrates promising SR performance on real world bursts, significantly outperforming alternative methods in a user study. We also provide a detailed ablative study, analysing the impact of key components in the proposed MFSR architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single Image Super-Resolution: SISR is a widely studied task with a variety of proposed methods, for example based on the frequency domain <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>, interpolation techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>, sparse representations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50]</ref> or patch and examples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. Dong et al. <ref type="bibr" target="#b6">[7]</ref> were the first to train a deep CNN to directly map the input LR image to the HR output. A number of approaches have subsequently improved upon this work using more effective network architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b54">54]</ref> and loss functions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b51">51]</ref>. Multi-Frame Super-Resolution: Compared to SISR approaches which solely rely on image priors to perform super-resolution, MFSR methods aim to merge multiple aliased images of the same scene to reconstruct a higher resolution output. The MFSR problem was first addressed by Tsai and Huang <ref type="bibr" target="#b43">[43]</ref>, who proposed a frequency domain based method that assumes known translations between input images. Later, Peleg et al. <ref type="bibr" target="#b36">[36]</ref> and Irani and Peleg <ref type="bibr" target="#b18">[19]</ref> introduced the iterative back-projection approach. They estimate an initial HR image and simulate the imaging process to generate the LR images. The reconstruction error between the generated and input LR images is then minimized iteratively to refine the HR image. Hardie et al. <ref type="bibr" target="#b13">[14]</ref> extended this approach with an improved observation model and a regularization term. Farsui et al. <ref type="bibr" target="#b8">[9]</ref> proposed a joint multi-frame demosaicking and super-resolution approach using a maximum a posteriori estimation framework. Zomet et al. <ref type="bibr" target="#b55">[55]</ref> use information from multiple sensors to perform super-resolution. Recently, Wronski et al. <ref type="bibr" target="#b48">[48]</ref> proposed a MFSR method for hand-held cameras, where a kernel regression technique is employed to merge aligned input frames robustly. Unlike in SISR, only a few deep learning based approaches have been proposed for MFSR. Ustinova and Lempitsky <ref type="bibr" target="#b44">[44]</ref> proposed a multiframe network for face super-resolution. Deudon et al. <ref type="bibr" target="#b5">[6]</ref> developed HighRes-net, a MFSR network for satellite imagery. HighRes-net aligns each input frame to a reference frame implicitly, and merges them using a recursive fusion method. Another approach for satellite imagery, namely DeepSUM <ref type="bibr" target="#b34">[34]</ref>, assumes only translation motion between frames and utilizes 3D convolution for fusion. In contrast to these previous approaches which are focused on remote sensing, we tackle the general problem of burst SR from any handheld camera. Learning real world super-resolution: SR approaches are commonly trained using synthetically generated LR images. However, such a training strategy has been shown to not generalize well to real-world images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>. A few recent works have tried to address this issue by learning real world degradation models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">47]</ref>. Another approach is to learn camera specific SR models directly using real world data. Such a strategy allows the network to learn the characteristics of the particular sensor, leading to improved performance <ref type="bibr" target="#b53">[53]</ref>. This is however challenging due to difficulties in collecting paired training data for SR. Zhang et al. <ref type="bibr" target="#b53">[53]</ref> address this by using LR-HR pairs captured using a zoom lens for training. In order to handle the spatial and color mis-alignments between LR-HR pairs, a novel contextual bilateral loss is employed for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Burst Super-Resolution Network</head><p>In this section, we describe our burst super-resolution network. Our network inputs multiple noisy, RAW, lowresolution (LR) images captured in a single burst. The architecture processes and combines the information in individual images to generate a high-resolution (HR) RGB image as output. Thus, our network performs joint denoising, demosaicking, and SR. Since the images in a burst are captured in a rapid sequence from a hand-held device, they include small inter-frame offsets. This ensures multiple aliased versions of the same scene, providing additional signal information for SR. Consequently, by effectively merging the information from the whole burst, our network can better reconstruct the underlying scene to generate a higher quality output, compared to single frame approaches.</p><p>An overview of our architecture is shown in <ref type="figure">Figure 2</ref>. Our network takes a RAW burst sequence {b i } N i=1 of any arbitrary size N as input. Here, each image b i ? R W ?H is the RAW sensor data obtained from the camera. The images in the burst are first encoded independently in order to obtain deep feature representations {e i } N i=1 . Next, we align and warp each of the feature maps to a common reference frame b 1 using the offsets estimated by an alignment network. The aligned feature maps are then combined by our fusion module to obtain a merged feature map?. We propose an attention-based fusion approach that predicts element-wise fusion weights. This allows the network to adaptively select the most useful information from each image in the burst. The merged feature map is then passed to the decoder module which outputs the final RGB image y ? R sW ?sH?3 , where s is the super-resolution factor. We detail each network module of our architecture in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder</head><p>The encoder module E independently maps each input burst image b i to a deep feature representation e i . To ensure translational invariance, we first pack each 2?2 block in the raw Bayer pattern along the channel dimension, obtaining a 4 channel imageb i ? R W 2 ? H 2 ?4 at half the initial resolution. This LR image is passed through the encoder, consisting of an initial convolutional layer followed by a series of residual blocks. In order to achieve a high-dimensional encoding that allows more effective fusion of several frames, we radically expand the feature dimensionality with a final convolutional layer. The resulting D-dimensional encoding E(b i ) = e i ? R W 2 ? H 2 ?D thus achieves a rich embedding of the input image. We use D = 512 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Alignment Module</head><p>One of the important challenges in burst SR is that the pixel-wise displacement between the images is unknown. The displacements stem from both global camera motion and scene variations. In order to achieve an effective fusion of multiple frames, the information first needs to be aligned. We address this problem by explicitly aligning the individual image embeddings e i to a common reference LR image, called the base frame. For convenience, we let the first imageb 1 denote the base frame. Camera motion is often modelled using a homography when imaging static and distant scenes. However, we found these assumptions to seldom hold in the real-world scenario. Thus, we allow greater flexibility in our alignment module by computing dense pixel-wise optical flow f i ? R W 2 ? H 2 ?2 between every burst imageb i and the reference imageb 1 . Pixel-wise flow can capture global camera motion while also accounting for any object motion in the scene. The estimated flow vectors f i are then used to warp the feature maps e i to the base frame using a bilinear kernel</p><formula xml:id="formula_0">e i = ?(e i , f i ) , f i = F (b i ,b 1 )<label>(1)</label></formula><p>Here, ? denotes the warping operation, F is the flow estimator, while? i is the warped feature map. The warped feature</p><formula xml:id="formula_1">maps {? i } N i=1 , as well as the computed flow vectors {f i } N i=1</formula><p>are then passed to the fusion module. Here, the flow vectors f 1 for the base frame is set to 0. While any state-of-the-art optical flow network <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> can be employed as our flow estimator F , we use the PWC-Net <ref type="bibr" target="#b39">[39]</ref> approach due to it's high accuracy and speed. Since PWC-Net is trained to operate on RGB images, we discard one of the two green channels inb i to generate input RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion Module</head><p>The fusion module combines information across the individual burst images to generate a merged feature embedding?. In order to be able to operate on bursts of arbitrary  <ref type="figure">Figure 2</ref>. An overview of our burst super-resolution architecture. Each image bi in the input burst is first passed independently through the encoder. The resulting feature maps are then warped to the base frame (b1) coordinates using the flow vectors fi predicted by the alignment module. The aligned feature maps are then merged using an attention-based fusion module, using fusion weights computed by the weight predictor. The merged feature map? is then passed through to decoder module to obtain the super-resolved RGB image as output. sizes, the fusion module must be able to merge any number of input frames. Consequently, it is infeasible to e.g. directly concatenate the input feature maps along the channel dimension. We further found simple pooling operations such as element-wise max or average pool across the burst to provide unsatisfactory results. This is because the fusion module needs be able to merge adaptively based on e.g. image content, noise levels, etc. For instance, it can be beneficial to have uniform fusion weights for textureless regions in order to perform denoising. On the other hand, it is preferable to have low fusion weights for any mis-aligned frame in order to avoid ghosting artifacts. We therefore propose an attention-based fusion approach, where elementwise fusion weights are predicted by a weight predictor network W . This provides flexibility to the network to effectively extract the useful information from each image, while also being able to process arbitrary number of input images.</p><p>The weight predictor network W utilizes both the aligned feature maps? i and the flow vectors f i to estimate the unnormalized attention weightsw i ? R W 2 ? H 2 ?D for each embedding? i . We first project? i to a lower dimension feature map? p i for computational efficiency. To compute the attention weights for? i , we use the projected base frame feature map? p 1 , as well as the residual r i =? p i ?? p 1 between? p i and? p 1 . The base frame map? p 1 contains information about the local image content. This is informative to determine e.g. whether to use uniform fusion weights to achieve denoising, or perform edge-aware fusion in order to avoid over smoothing edges. On the other hand, the residual r i can provide an estimate of alignment errors and thus help assign low fusion weights to misaligned regions. Additionally, we use the flow vectors f i for weight estimation as they provide the sub-pixel sampling location of the image data. We obtain the sub-pixel offset by computing modulo 1 of the flow vectors f i and pass it through a small CNN to obtain the flow featuresf i . The reference frame features e p 1 , the feature residual r i , and the flow featuresf i are concatenated along the channel dimension and passed through a residual network to obtain the raw fusion weightsw i . The raw fusion weights are then normalized across the burst using a softmax function to obtain the final attention weights w i . The merged feature map? is then be obtained as the following weighted sum,</p><formula xml:id="formula_2">e = N i=1 w i ? e i , w i = ew i j ew j ,w i = W(? 1 , r i ,f i ). (2)</formula><p>Here, ? denotes element-wise multiplication. The merged feature map? is then passed to the decoder module to generate the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoder</head><p>The decoder module generates the output highresolution RGB image from the fused feature map?. We first project the input feature map to 128 channels and pass it through a residual network. Next, we upsample this to the desired resolution sH ? sW using sub-pixel convolution <ref type="bibr" target="#b38">[38]</ref>. We use a convolution layer to increase the feature dimension to 2 2 s 2 D , obtaining a tensor of shape</p><formula xml:id="formula_3">H 2 ? W 2 ? 2 2 s 2 D .</formula><p>The feature vectors at each spatial location are then re-arranged into a 2s?2s?D map to obtain a higher resolution feature map of shape H ? W ? D . Here, D is the output feature dimension of the sub-pixel convolution layer. Compared to performing na?ve upsampling using e.g. bilinear interpolation, sub-pixel convolution allows us to effectively decode the sub-pixel information encoded in the different feature channels. In order to avoid checkerboard artifacts, we use the ICNR initialization <ref type="bibr" target="#b0">[1]</ref> for the sub-pixel convolution layer and additionally apply Gaussian smoothing to its output. The upsampled feature map is then passed through another set of residual blocks, followed by a conv layer to obtain the high resolution RGB image y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BurstSR Dataset</head><p>The aim of this work is to propose a burst SR method for real-world photography applications. In order to validate the performance of our approach, it is essential to train and evaluate our models on real data. Hence, we collect a new dataset, called BurstSR. To the best of our knowledge, it is the first real world burst super-resolution dataset. The BurstSR dataset consists of 200 RAW burst sequences, and corresponding high-resolution ground truths. Each burst sequence contains 14 RAW images captured using identical camera settings (e.g. exposure, ISO). All bursts are captured using a handheld smartphone camera. Our dataset therefore contains natural hand tremors, resulting in small random offsets between the images within a burst that are essential for MFSR <ref type="bibr" target="#b43">[43]</ref>. For each burst sequence, we also capture a high-resolution image using a DSLR camera mounted on a tripod to serve as ground truth. Our BurstSR dataset will be released upon publication. We believe that it can serve as an important training set and benchmark for the community, in order to raise the interest in the important MFSR problem.</p><p>We capture the burst images in our datset using a handheld Samsung Galaxy S8 smartphone camera. In order to capture and store RAW bursts, we developed a custom app using Camera2 API. On pressing the shutter, the app runs the camera's auto-focus, auto-exposure, and auto-whitebalance algorithms to determine the camera settings. These settings are then used to capture a fixed number of RAW images. The corresponding ground truth images for each burst are collected using a Canon 5D Mark IV DSLR camera mounted on a tripod. We use a zoom lens with a focal length of 70mm to obtain images with ? 4 times higher spatial resolution compared to burst images captured from the phone camera. The images are taken using a smaller aperture size (F18) to have a wider depth of field. Other capture settings are automatically determined by the camera. We hold the phone camera just above the DSLR when taking bursts in order to minimize misalignments between the two images. Additionally, we use a timer on the DSLR to synchronize the capture time between the two cameras. In order to minimize the effect of any error in temporal synchronization, we try to capture static scenes with little (e.g. leaves moving due to wind) or no motion. We collect 200 bursts in total, which are split into train, validation, and test sets consisting of 160, 20, and 20 sequences, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training</head><p>In this section, we describe our training pipeline in detail. Due to the high cost and effort associated with collecting real-world paired data for MFSR, it is impractical to obtain large scale real world datasets for training our model from scratch. We therefore exploit methods for synthetic data generation to first pre-train our networks. The resulting model serves as a strong initialization, which is then finetuned on our BurstSR dataset to perform real-world SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic data training</head><p>We generate synthetic RAW bursts for pre-training our model using the sRGB images from the training split of Zurich RAW to RGB dataset <ref type="bibr" target="#b16">[17]</ref>. Given a sRGB image, we apply the inverse camera pipeline described in <ref type="bibr" target="#b1">[2]</ref> to obtain raw sensor values. Next, we generate a synthetic burst of size N by applying random translations and rotations to the converted RGB image. The translation and rotation values are sampled independently from the range [-24, 24] pixels and [-1, 1] degrees, respectively. The transformed images are then downsampled by the desired super-resolution factor s to obtain the low resolution RGB burst. We use bilinear kernel for both image translation/rotation and downsampling. Next, we add shot and read noise to the burst images, as described in <ref type="bibr" target="#b1">[2]</ref>. We then discard two color channels per pixel according to the Bayer CFA to obtain the mosaicked RAW burst. We extract 96 ? 96 crops from the resulting RAW burst for our training. Our network is trained in a fully supervised manner by minimizing the L 1 loss between the network prediction and the ground truth image. The loss is computed in the linear sensor space, before any post processing e.g. gamma compression or tone-mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real data training</head><p>In order to reconstruct the HR image using multiple aliased LR observations, a MFSR model needs to learn the image formation process in a camera. However, due to differences in the image formation process in a real camera and the one modelled by our synthetic pipeline, a network trained on only synthetic data is thus expected to have suboptimal performance when applied to real data. Hence, we fine-tune the pre-trained synthetic data model on our real world BurstSR dataset in order to adapt the model to the particular camera sensor. Data Processing: Here, we describe the pipeline used to pre-process the collected BurstSR data for training. Since the images captured using phone and DSLR cameras have different field of views (FOV), we first crop out matching field of view from each image in the burst. This is done by estimating a homography between the first image in the burst and DSLR image using SIFT <ref type="bibr" target="#b27">[28]</ref> and RANSAC <ref type="bibr" target="#b9">[10]</ref>. Next, we extract 160 ? 160 crops from the burst images in a sliding window manner, with a stride of 80 pixels. For each crop, we again estimate homography between the crop and the corresponding region in DSLR image to perform local alignment. The aligned DSLR image region is then downsampled to 160s ? 160s to obtain the ground truth crop. In order to filter out crops with incorrect alignment, we discard phone-DSLR pairs which have a normalized cross correlation of less than 0.9 between them. Training loss: There are several challenges when training our model on real bursts due to the unavoidable misalignments between the input burst and the ground truth. Firstly, even though we align the burst images to DSLR using homography, there can still be misalignments between the pair due to perspective shifts, error in homography estimation, etc. Secondly, since the burst and ground truth images are captured using two different sensors, there is a color mis-match between the two. Thus, it is not feasible to train the model by directly computing a pixel-wise error between the network prediction y and the ground truth y GT .</p><p>In order to handle the spatial mis-alignment issue, we first estimate the optical flow f Pred,GT between the prediction and ground truth using PWC-Net. The estimated flow is then used to warp the network prediction to the ground truth co-ordinates. Next, we estimate a global color mapping between the burst and the ground truth in order to handle the color mis-match. We first downsample the ground truth image to the same resolution as the input burst images.</p><p>The estimated flow f Pred,GT is then used to align the first image in the burst to the downsampled ground truth. In order to minimize the effect of small mis-alignments, we apply Gaussian smoothing on both the images to obtain the processed burst imageb 1 and ground truth image? GT . Given this aligned input-ground truth pair, we estimate a pixelwise color mapping C between the two images. We assume that the color mapping is linear and model it as a 3 ? 3 color correction matrix, which is computed by minimizing a least squares loss. Using the estimated color correction matrix, we can map the network prediction to the same color space as the ground truth and compute pixel-wise error. Our training loss (y, y GT ) is thus computed as (y, y GT ) = n m n ? L 1 (? n , y n GT ) ,? = C(?(y, f Pred,GT ))</p><p>(3) Here,? is the aligned and color mapped network prediction. The summation is over all pixel coordinates n in the image. The factor m n is a binary masking variable used to filter out image regions which are not aligned correctly. It is set to 0 in regions where the error R = ? GT ? C(b 1 ) 2 after color mapping the processed burst imageb 1 is greater than a threshold. Note that the images? GT andb 1 have lowerresolution compared to the model prediction y. Thus the error map R is upsampled to the same resolution as y, before computing the mask m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training details</head><p>We use pre-trained PWC-Net weights for our flow estimator F . All other modules are initialized using <ref type="bibr" target="#b14">[15]</ref>. Our model is first trained using the synthetic data for 100k iterations, and then fine-tuned on the BurstSR dataset for an additional 40k iterations. We use the ADAM <ref type="bibr" target="#b22">[23]</ref> optimizer for out training. Data augmentation is performed using random cropping and flipping. Our entire training takes 30 hours on a single Nvidia V100 GPU. All our networks are trained using a burst size of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We perform comprehensive qualitative and quantitative evaluation of our approach. All our experiments are performed for super-resolution by a factor s = 4. Additional details and results are provided in the suppl. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Analysis of our approach</head><p>Here, we analyze the impact of different components in the proposed burst SR architecture. We report results on a synthetically generated test set containing 300 bursts, as well as our BurstSR validation dataset. The synthetic test set is generated using the pipeline described in Sec 5.1, using sRGB images from the test set of the Zurich RAW to RGB dataset <ref type="bibr" target="#b16">[17]</ref>. We evaluate the networks trained using only the synthetic training data on this set. Since an accurate ground truth HR image is naturally available, the synthetic test set allows us to evaluate the impact of different architectural choices. We also report results on our BurstSR validation set, using the models fine-tuned on BurstSR training set. Since the input burst and HR ground truth in BurstSR are captured using different cameras, there exists spatial and color misalignments between them. We therefore align the network prediction to the ground truth and perform color transformation as described in Sec 5.2. The resulting image is then compared with the ground truth in order to compute performance metrics. We report the standard fidelity based metrics PSNR and SSIM <ref type="bibr" target="#b46">[46]</ref>, as well as the learned perceptual score LPIPS [52] on both datasets. All metrics are computed in linear sensor space. Note that the images in our BurstSR dataset are generally underexposed, leading to high PSNR scores for all methods. Unless specified, all the methods are evaluated using a burst size of 8. Impact of using multiple frames: Here, we investigate the impact of using multiple frames for SR by comparing our MFSR approach with a single image baseline. We train a SISR network with exactly the same encoder and decoder architecture as employed in our approach. In order to ensure that the SISR performance is not limited by model capacity, we increased the depth of the single image network until its performance saturated. We compare this single image baseline with our multi-frame approach, evaluated using bursts of different sizes. The result of this comparison is shown in <ref type="table" target="#tab_0">Table 1</ref>. Even when using only 4 input frames, our approach significantly outperforms the single image baseline with an improvement of 0.76 dB in PSNR on the synthetic set. Note that although our model is trained using a fixed burst size of 8, it generalizes to bursts with varying input sizes, providing a consistent improvement with increasing burst size. This shows that our approach can effectively utilize the information from multiple frames in order to improve SR performance. When using bursts of size 14, our approach obtains an improvement of 2.67 dB in PSNR on the synthetic set, clearly demonstrating the advantages of using multiple frames for SR. Impact of alignment module: We analyse the impact of the alignment module in our architecture by evaluating a baseline network which does not perform any explicit alignment. We directly concatenate the encoded base frame features to all other frames, and pass the resulting feature maps through additional residual blocks, before merging them. The result of this comparison is shown in <ref type="table">Table 2</ref>. Our approach, performing explicit sub-pixel alignment using a flow estimator, outperforms the baseline No Alignment with an improvement of 1.02 dB in PSNR on the BurstSR validation set. Interestingly, the No Alignment network only obtains a slight improvement over the SISR baseline. These results show that accurate alignment of input frames is essential in order to benefit from multiple frames. Analysis of fusion architecture: We compare our proposed attention-based fusion module with 4 different alternatives. i) MaxPool: The encoded feature maps are merged by performing element-wise max pooling across the burst. ii) AvgPool: The merged feature map is computed as element-wise mean across the burst. iii) Concatenate: The encoded feature maps are concatenated along the channel dimension to obtain the merged features. Note that this architecture is constrained to operate on bursts of fixed size. iv) RecMerge: The recursive fusion strategy proposed in <ref type="bibr" target="#b5">[6]</ref>. Pairs of encoded feature maps are concatenated and  <ref type="table">Table 4</ref>. Impact of different inputs used by the weight predictor.</p><p>passed through a small network to merge them. This process is repeated recursively until a single merged feature map is obtained. All four baseline networks employ the same encoder, decoder, and alignment modules as used in our approach to ensure a fair comparison. The result of this analysis is shown in <ref type="table">Table 3</ref>. We observe that MaxPool and AvgPool approaches obtain poor results, indicating that simple pooling operations are insufficient to perform effective merging. Both Concatenate and RecMerge achieve better results with PSNR of 37.80 dB and 37.55 dB respectively, on the synthetic set. Our attention-based fusion obtains the best results on both the synthetic set as well as BurstSR, showing that it can effectively merge the information from the input frames. Analysis of weight predictor network: Here, we analyse the impact of different inputs used by our weight predictor network to determine the element-wise fusion weights. We evaluate 4 different versions of the weight predictor, using different sets of inputs, i) Only Feature: Only the projected feature map? p i is used. ii) Only Residual: Only the feature residual r i =? p i ?? p 1 is used. iii) Residual+Base: Both the feature residual r i and the base frame features? p 1 are used. iv) Residual+Base+Flow: The feature residual r i , base frame features? p 1 , as well as the flow featuresf i are used. The result of this comparison is shown in <ref type="table">Table 4</ref>. Compared to using only the input feature? p i , using the residuals r i instead leads to better performance. Additionally using the base frame features? p 1 improves the performance further by 0.27 dB in PSNR on the synthetic set. The best results are obtained when using the feature residual r i , the base frame features? p 1 , and the flow featuresf i together, showing that they each provide complementary information to the weight predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison with other approaches</head><p>In this section, we evaluate our proposed burst superresolution network on the test set of our BurstSR dataset. We compare our approach with three methods: i) Single Image Our SISR baseline network; ii) DeepJoint+RRDB Single Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepJoint+RRDB</head><p>HighRes-net Ours HR Reference <ref type="figure">Figure 3</ref>. Qualitative comparison of our approach on real world bursts from the BurstSR test set. Our approach can effectively merge information from multiple frames to reconstruct high-frequency image details.  <ref type="table">Table 5</ref>. Comparison of our method with existing SR approaches on the BurstSR test set. We report the results of our user study, as well as the standard quality metrics PSNR, LPIPS, and SSIM.</p><formula xml:id="formula_4">MOR ? %Top ? PSNR ? LPIPS ? SSIM ?<label>DeepJoint+RRDB</label></formula><p>A two stage network which performs single frame demosaicking and denoising using DeepJoint <ref type="bibr" target="#b11">[12]</ref> and superresolves the resulting RGB image using the RRDB <ref type="bibr" target="#b45">[45]</ref> network; and iii) HighRes-net A recent deep learning based MFSR approach <ref type="bibr" target="#b5">[6]</ref> proposed for remote sensing applications. HighRes-net performs implicit registration of the input frames, without using any independent alignment module. Fusion is performed in a recursive manner. We use pretrained weights for the DeepJoint and RRDB networks. The Single Image baseline, as well as HighRes-net, are trained to perform joint denoising, demosiacking, and SR using the exact training pipeline used by our approach. In order to ensure a fair comparison, we increased the depth of the original HighRes-net network to have the same number of residual blocks as in our approach.</p><p>We conducted a user study on Amazon Mechanical Turk to compare the four approaches. We obtain the HR prediction for each of our network on the 20 test images. Next, we extract 15 random 200 ? 200 crops from each of our 20 test images. Each of the 300 crops are then resized to 400 ? 400 using nearest neighbor interpolation. We show the participants the ground truth HR image, as well as the network predictions. The participants are asked to rank the predictions from the 4 approaches according to the visual quality w.r.t. the provided DSLR reference image. The network predictions were anonymized and randomized in order to avoid any bias. We obtained 5 independent rankings for each crop. The mean ranking (MOR) over all the crops, as well as the percentage of times a method was ranked first (%Top) are shown in <ref type="table">Table 5</ref>. Our approach obtains a MOR of 1.81, significantly better than all other approaches. Furthermore, our approach is ranked as the best among all methods 53.6% of the times, more than 2.5 times the second best method. We also report the PSNR, LPIPS, and SSIM scores on the test set, computed as described in Sec. 6.1. A qualitative comparison is also provided in <ref type="figure">Fig. 3</ref>. Our approach obtains the best results in terms of all three metrics, outperforming HighRes-net by 1.18 dB in terms of PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We address the problem of real-world multi-frame superresolution. We introduce a new dataset BurstSR containing RAW burst sequences captured from a handheld camera, and corresponding high-resolution ground truths obtained using a zoom lens. We further propose a multi-frame superresolution network which can adaptively combine the information from multiple input images using an attentionbased fusion. Our approach obtains promising results on real world bursts, outperforming both single frame as well as multi-frame alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We provide additional details and analysis of our approach in this supplementary material. In Section A, we provide additional details about our network architecture. We analyse the impact of sub-pixel shifts in the input images for MFSR in Section B, while the impact of training dataset for real world SR is analysed in Section C. Section D provides a qualitative analysis of the impact of our training loss (3) used to train our networks on the BurstSR dataset. Additional qualitative comparison with existing super-resolution approaches are provided in Section E</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Here, we provide additional details about our burst super-resolution network architecture. Encoder: The encoder module maps the packed RAW imageb i to a 64 dimensional feature embedding using a convolution layer. The resulting feature map is processed by 9 residual blocks, before being passed to another convolution layer which expands the feature dimensionality to 512. An illustration of the Encoder module is provided in <ref type="figure" target="#fig_1">Figure 4</ref>. Weight Predictor: The weight predictor module computes the un-normalized element-wise fusion weights for each aligned feature embedding? i . It first projects the feature embeddings? i and? 1 to 64 dimensional feature maps? p i and? p 1 respectively, using a convolution layer with shared weights. Additionally, the weight predictor module also extracts flow featuresf i using the flow vectors f i . The modulo 1 of the flow vectors, f i mod 1, is first passed through a convolution layer, followed by a residual block to obtain 64 dimensional flow featuresf i . The flow featuresf i , the projected feature embedding? p i , and the residual? p i ?? p 1 are then concatenated along the channel dimension, and passed through a convolution layer. The output 128 dimensional feature map is processed by 3 residual blocks, before being passed to a final convolution layer which predicts raw element-wise fusion weightsw. An illustration of the weight predictor module is provided in <ref type="figure">Figure 5</ref>. Decoder: The decoder module projects the merged feature map? to a 64 dimensional feature space. The projected features are then passed through 5 residual blocks, before being passed to the sub-pixel convolution layer, which upsamples the feature map by a factor 2s. The sub-pixel convolution layer first increases the feature dimensionality to 2 2 s 2 32 using a convolution layer. The feature vectors at each spatial location are then re-arranged into a 2s ? 2s ? 32 map to obtain a 32 dimensional feature map with 2s times higher resolution compared to the input. The upsampled feature map is then processed by 4 residual blocks, before being passed to a convolution layer which predicts the output RGB image. An illustration of the Decoder module is provided in <ref type="figure">Figure 6</ref>.   <ref type="table">Table 7</ref>. Impact of fine-tuning on real data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of input shifts</head><p>Here, we investigate the importance of having sub-pixel shifts in the input images for MFSR. We train and evaluate a baseline network No Shifts on synthetic bursts generated without any simulated camera motion. That is, all the images in the burst are identical except having different independent noise. We also include our SISR baseline for comparison. While the No Shifts network can exploit the burst images in order to obtain better denoising, its performance improvement over the SISR baseline is limited to &lt; 0.6 dB (see <ref type="table" target="#tab_3">Table 6</ref>). In contrast, our approach obtains a significant improvement of 1.61 dB in PSNR over No Shifts when operating on burst with sub-pixel shifts. These results show that the majority of performance gains of our approach over the SISR baseline is obtained by effective fusion of information contained in the different aliased samplings of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact of training dataset</head><p>We analyse the impact of pre-training our model on the synthetic data, as well as fine-tuning on the real data. We compare our approach with two baselines, i) a network Only Synthetic trained using only the synthetic data, and ii) a network Only BurstSR trained using only the realworld BurstSR dataset. The results on the BurstSR validation set are shown in <ref type="table">Table 7</ref>. The network trained only using synthetic data fails to generalize to the real world images, obtaining a PSNR of 44.52 dB. In contrast, the network trained from scratch on BurstSR performs much better with a PSNR of 47.14 dB. The best results are obtained when combining both the strategies: pre-training first using large scale synthetic data, and finetuning the resulting network on real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Impact of our training loss</head><p>In this section, we analyze the impact of our training loss, defined in Eq. (3) in the main paper, which is used to train our model on the real-world BurstSR dataset. Our loss aligns the network prediction to the ground truth image in order to handle spatial misalignments between the input burst and the ground truth. Furthermore, it also handles the color mismatch between the input-ground truth pair by estimating the color mapping function between the two. We compare the network trained using our loss (3) with a network trained using direct pixel-wise loss without performing any explicit spatial alignment and color space correc-tion. Additionally, we also include a network trained only on synthetic data for comparison. The results of this analysis on the BurstSR validation set are shown in <ref type="figure">Figure 7</ref>. Compared to using direct pixel-wise loss, the network training using our loss (3) can generate sharper images with better details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Examples</head><p>Here, we provide additional qualitative comparison of our approach with the approaches described in Section 6.2 of the main paper; (i) Single Image baseline, (ii) Deep-Joint <ref type="bibr" target="#b11">[12]</ref>+RRDB <ref type="bibr" target="#b45">[45]</ref>, and (iii) HighRes-net <ref type="bibr" target="#b5">[6]</ref>. Visual examples from the BurstSR test set are shown in <ref type="figure">Figure 8</ref>. Compared to the other methods, our approach can best reconstruct the high frequency image details with high fidelity to the high-resolution ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Only Synthetic Direct Pixel-wise loss Our loss (3)</head><p>HR Reference <ref type="figure">Figure 7</ref>. Qualitative comparison of a network trained on BurstSR dataset using our training loss (3) with a network trained using direct pixel-wise loss on the BurstSR validation set. A network trained only on the synthetic dataset is also included for comparison. Note that there is a color shift between the predictions of the networks, as the networks are trained using different output color spaces. Hence, we encourage the reader to focus on image details, e.g. sharp edges, presence of artifacts and not on the color space differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepJoint+RRDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Image</head><p>HighRes-net Ours HR Reference <ref type="figure">Figure 8</ref>. Qualitative comparison of our approach with existing super-resolution approaches on the BurstSR test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The network architecture employed for the Encoder module E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>The network architecture employed for the Weight Predictor module W . The network architecture employed for the Decoder module D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>LPIPS ? SSIM ? PSNR ? LPIPS ? SSIM ? Comparison of the baseline SISR network with our multiframe approach, evaluated using different number of input frames. LPIPS ? SSIM ? PSNR ? LPIPS ? SSIM ?</figDesc><table><row><cell></cell><cell cols="2">Synthetic data</cell><cell></cell><cell></cell><cell>BurstSR</cell><cell></cell></row><row><cell cols="2">PSNR ? Single Image 36.42</cell><cell>0.123</cell><cell>0.913</cell><cell>46.41</cell><cell>0.041</cell><cell>0.979</cell></row><row><cell>Burst-2</cell><cell>34.90</cell><cell>0.133</cell><cell>0.893</cell><cell>46.10</cell><cell>0.040</cell><cell>0.977</cell></row><row><cell>Burst-4</cell><cell>37.18</cell><cell>0.092</cell><cell>0.927</cell><cell>47.06</cell><cell>0.033</cell><cell>0.981</cell></row><row><cell>Burst-8</cell><cell>38.61</cell><cell>0.084</cell><cell>0.941</cell><cell>47.52</cell><cell>0.031</cell><cell>0.983</cell></row><row><cell>Burst-14</cell><cell>39.09</cell><cell>0.084</cell><cell>0.945</cell><cell>47.76</cell><cell>0.030</cell><cell>0.984</cell></row><row><cell></cell><cell cols="2">Synthetic data</cell><cell></cell><cell></cell><cell>BurstSR</cell><cell></cell></row><row><cell cols="2">PSNR ? Ours 38.61</cell><cell>0.084</cell><cell>0.941</cell><cell>47.52</cell><cell>0.031</cell><cell>0.983</cell></row><row><cell>No Alignment</cell><cell>36.66</cell><cell>0.119</cell><cell>0.915</cell><cell>46.50</cell><cell>0.040</cell><cell>0.979</cell></row><row><cell>Single Image</cell><cell>36.42</cell><cell>0.123</cell><cell>0.913</cell><cell>46.41</cell><cell>0.041</cell><cell>0.979</cell></row><row><cell cols="7">Table 2. Comparison of our approach performing explicit align-</cell></row><row><cell cols="7">ment with a baseline which does not employ an alignment module.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>LPIPS ? SSIM ? PSNR ? LPIPS ? SSIM ? LPIPS ? SSIM ? PSNR ? LPIPS ? SSIM ?</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Synthetic data</cell><cell></cell><cell></cell><cell>BurstSR</cell></row><row><cell cols="3">PSNR ? Ours 38.61</cell><cell cols="2">0.084</cell><cell>0.941</cell><cell>47.52</cell><cell>0.031</cell><cell>0.983</cell></row><row><cell>MaxPool</cell><cell>36.24</cell><cell></cell><cell cols="2">0.116</cell><cell>0.912</cell><cell>46.74</cell><cell>0.039</cell><cell>0.980</cell></row><row><cell>AvgPool</cell><cell>35.45</cell><cell></cell><cell cols="2">0.131</cell><cell>0.902</cell><cell>46.53</cell><cell>0.040</cell><cell>0.979</cell></row><row><cell>Concatenate</cell><cell>37.80</cell><cell></cell><cell cols="2">0.098</cell><cell>0.928</cell><cell>47.17</cell><cell>0.034</cell><cell>0.981</cell></row><row><cell>RecMerge</cell><cell>37.55</cell><cell></cell><cell cols="2">0.098</cell><cell>0.927</cell><cell>47.12</cell><cell>0.033</cell><cell>0.981</cell></row><row><cell cols="8">Table 3. Analysis of different fusion approaches for merging the</cell></row><row><cell cols="5">information from input frames.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Synthetic data</cell><cell></cell><cell>BurstSR</cell></row><row><cell cols="4">PSNR ? Only Feature 37.46</cell><cell>0.101</cell><cell>0.927</cell><cell>47.11</cell><cell>0.034</cell><cell>0.981</cell></row><row><cell>Only Residual</cell><cell></cell><cell>38.14</cell><cell></cell><cell>0.093</cell><cell>0.935</cell><cell>47.46</cell><cell>0.031</cell><cell>0.982</cell></row><row><cell>Residal+Base</cell><cell></cell><cell>38.41</cell><cell></cell><cell>0.085</cell><cell>0.939</cell><cell>47.46</cell><cell>0.030</cell><cell>0.983</cell></row><row><cell cols="2">Residal+Base+Flow</cell><cell>38.61</cell><cell></cell><cell>0.084</cell><cell>0.941</cell><cell>47.52</cell><cell>0.031</cell><cell>0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>PSNR ? LPIPS ? SSIM ? Impact of sub-pixel shifts in the input burst for MFSR. Results are shown on the synthetic test set.</figDesc><table><row><cell>Ours</cell><cell>38.61</cell><cell>0.084</cell><cell>0.941</cell></row><row><cell>No Shifts</cell><cell>37.00</cell><cell>0.106</cell><cell>0.920</cell></row><row><cell>Single Image</cell><cell>36.42</cell><cell>0.123</cell><cell>0.913</cell></row><row><cell>Ours</cell><cell>47.52</cell><cell>0.031</cell><cell>0.983</cell></row><row><cell>Only Synthetic</cell><cell>44.52</cell><cell>0.081</cell><cell>0.967</cell></row><row><cell>Only BurstSR</cell><cell>47.14</cell><cell>0.037</cell><cell>0.981</cell></row></table><note>PSNR ? LPIPS ? SSIM ?</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was supported by a Huawei Technologies Oy (Finland) project, the ETH Z?rich Fund (OK), an Amazon AWS grant, and an Nvidia hardware grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1707.02937</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11028" to="11037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a GAN to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>:I-I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft edge smoothness prior for alpha channel super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Highres-net: Recursive fusion for multi-frame super-resolution of satellite imagery. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michel Deudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalaitzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Goytom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arefin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiframe demosaicing and super-resolution from undersampled color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Daniel Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-resolution image reconstruction from a sequence of rotated and translated frames and its application to an infrared imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Bognar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="260" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cubic splines for image interpolation and digital filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="508" to="517" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Replacing mobile camera isp with a single deep learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP Graph. Model. Image Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust wavelet-based superresolution reconstruction: Theory and algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5835" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">New edge directed interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)</title>
		<meeting>2000 International Conference on Image Processing (Cat. No.00CH37101)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="311" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometry constrained sparse coding for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1648" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference, Glasgow</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised learning for real-world super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Namhyuk Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se-Young</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwantae</forename><surname>Kheradmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Navarrete</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kalpesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Prajapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Hyeok</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangquan</forename><surname>Muhammad Umer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<title level="m">NTIRE 2020 challenge on real-world image superresolution: Methods and results. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Hyung</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guisik</forename><surname>Seung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sefi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bell-Kligler</surname></persName>
		</author>
		<title level="m">AIM 2019 challenge on real-world image super-resolution: Methods and results. 2019 IEEE/CVF International Conference on Computer Vision Workshop (IC-CVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DeepSUM: Deep neural network for superresolution of unregistered multitemporal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bordone Molini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3644" to="3656" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A wavelet-based interpolation-restoration method for superresolution (wavelet superresolution). Circuits, Systems and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="321" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving image resolution using subpixel motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shmuel Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="226" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discrete cosine transform based regularized high-resolution image reconstruction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical Engineering</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1348" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GOCor: Bringing globally optimized correspondence volumes into your neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiframe image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep multi-frame face superresolution. arXiv: Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ESR-GAN: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DeFlow: Learning complex image degradations from unpaired data with conditional flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Handheld multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wronski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Garcia-Dorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Kai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bilevel sparse coding for coupled feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zoom to learn, learn to zoom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qi Feng Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3757" to="3765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-sensor super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE Workshop on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
	<note>Proceedings.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
