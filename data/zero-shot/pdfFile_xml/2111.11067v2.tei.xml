<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intell. Info. Processing</orgName>
								<orgName type="department" key="dep2">School of CS</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Collaborative Innovation Center on Intelligent Visual Computing</orgName>
								<address>
									<addrLine>3 Meta AI 4 Baidu Apollo</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intell. Info. Processing</orgName>
								<orgName type="department" key="dep2">School of CS</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Collaborative Innovation Center on Intelligent Visual Computing</orgName>
								<address>
									<addrLine>3 Meta AI 4 Baidu Apollo</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intell. Info. Processing</orgName>
								<orgName type="department" key="dep2">School of CS</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Collaborative Innovation Center on Intelligent Visual Computing</orgName>
								<address>
									<addrLine>3 Meta AI 4 Baidu Apollo</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision Transformers</term>
					<term>CNNs</term>
					<term>Semi-Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the training of Vision Transformers for semisupervised image classification. Transformers have recently demonstrated impressive performance on a multitude of supervised learning tasks. Surprisingly, we show Vision Transformers perform significantly worse than Convolutional Neural Networks when only a small set of labeled data is available. Inspired by this observation, we introduce a joint semisupervised learning framework, Semiformer, which contains a transformer stream, a convolutional stream and a carefully designed fusion module for knowledge sharing between these streams. The convolutional stream is trained on limited labeled data and further used to generate pseudo labels to supervise the training of the transformer stream on unlabeled data. Extensive experiments on ImageNet demonstrate that Semiformer achieves 75.5% top-1 accuracy, outperforming the state-ofthe-art by a clear margin. In addition, we show, among other things, Semiformer is a general framework that is compatible with most modern transformer and convolutional neural architectures. Code is available at https://github.com/wengzejia1/Semiformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision transformers (ViT) have achieved remarkable performance recently on a variety of supervised computer vision tasks <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18]</ref>. Their success is largely fueled by high capacity models with self-attention layers trained on massive data. However, it is not always feasible to collect sufficient annotated data in many real world applications. When only a small number of labeled samples are provided, semi-supervised learning (SSL) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">40]</ref> is a powerful paradigm to achieve better performance by leveraging a huge amount of unlabeled data. Despite the success of Vision Transformers in fully supervised scenarios, the understanding of its effectiveness in SSL is still an empty space.</p><p>We perform a series of studies with Vision Transformers (ViT) <ref type="bibr" target="#b8">[8]</ref> in the semisupervised learning (SSL) setting on ImageNet. Surprisingly, the results show that simply training a ViT using a popular SSL approach, FixMatch <ref type="bibr" target="#b23">[23]</ref> leads to much worse performance than a CNN trained even without FixMatch. We believe this results from the fact that pseudo labels from CNNs are more accurate, possibly due to their encoded inductive bias.</p><p>To validate our hypothesis, we use CNNs to produce pseudo labels for the joint semi-supervised training of CNNs and transformers. By doing so, we are able to significantly improve the top-1 accuracy of the ViT by 8+% (c.f. Conv-labeled and Vanilla in <ref type="figure">Fig. 1</ref>). This highlights that labels derived from CNNs are also helpful for training transformers under the SSL setting. While pseudo labels from CNNs are effective, the final ViT is still slightly weaker than the "teacher" CNN. We posit that simply performing pseudo labeling (PL) with CNNs to derive supervisory signals for transformers is not sufficient. Instead, we hypothesize that a joint knowledge sharing mechanism at the architecture level is required to fully explore knowledge in CNNs.</p><p>In light of these, we introduce a novel semi-supervised learning framework for Vision Transformers, which we term as Semiformer. In particular, Semiformer composes of a convolutional stream and a transformer stream. It leverages labels produced by CNNs as supervisory signals to train the CNN and the transformers jointly using a popular SSL strategy. The two streams are further connected with a cross-stream feature interaction module, enabling streams to complement each other. Benefited from more accurate labels and the interaction design, Semiformer can be readily used for SSL.</p><p>We conduct extensive experiments to evaluate Semiformer. In particular, Semiformer achieves 75.5% top-1 accuracy on ImageNet and outperforms the state-of-the-art using 10% of labeled samples. We also show Semiformer outperforms alternative methods by clear margins under different labeling ratios. In addition, we empirically demonstrate Semiformer is a generic framework compatible with modern CNN and transformer architectures. We also pro-vide qualitative evidence that Semiformer is better than ViTs in the SSL setting.</p><p>Contributions. Our contributions are three-folded:</p><p>1. We are the first to investigate the application of Vision Transformers for semi-supervised learning. We reveal that Vision Transformers perform poorly when labeled samples are limited, yet they can be improved by utilizing unlabeled data together with the help from Convolutional neural networks.</p><p>2. We propose a generic framework Semiformer for the semi-supervised learning of Vision Transformers, which not only explores predictions as supervisory signals but also feature-level clues from CNNs to improve the ViTs in the low-data learning regime.</p><p>3. We perform extensive experiments and studies to evaluate Semiformer. Semiformer achieves 75.5% top-1 accuracy on ImageNet and outperforms state-of-the-art methods in semi-supervised learning. Additional ablation studies are further conducted to understand its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Vision Transformers. A variety of Vision Transformers <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b37">37]</ref> have refreshed the state-of-the-art performance on ImageNet, demonstrating their powerful representation capability in solving vision tasks. Among them, the Vision Transformer (ViT) <ref type="bibr" target="#b8">[8]</ref> is the first to prove that purely using the transformer structure can perform well on image classification tasks. It divides each image into a sequence of patches and then applies multiple transformer layers <ref type="bibr" target="#b27">[27]</ref> to model their global relations. T2T-ViT <ref type="bibr" target="#b37">[37]</ref> recursively aggregates neighboring tokens into one token for better modeling of local structures such as edges and lines among neighboring pixels, which outperforms ResNets <ref type="bibr" target="#b13">[13]</ref> and also achieves comparable performance to light CNNs by directly training on ImageNet. Swin Transformer <ref type="bibr" target="#b18">[18]</ref> creates a shifted windowing scheme cooperated with stacked local transformers for better information interaction among patches. With the continuous improvements of Vision Transformers, transformer based networks have achieved higher accuracy on medium-scale and large-scale datasets. Although transformers have been proven effective at solving visual tasks, it is known inferior to some CNNs when training from scratch on smallsized datasets mainly because ViTs lack image-specific inductive bias <ref type="bibr" target="#b8">[8]</ref>.</p><p>Touvron et al . <ref type="bibr" target="#b25">[25]</ref> distill the knowledge of CNNs to ViTs, easing the training process of transformers to be more data efficient. The hard distillation idea is similar to the pseudo label approach in SSL. However, it differs from our work in that the teacher model in distillation is pre-trained in a fully supervised setting and frozen while we also use the pseudo labels to continuously updating the convolutional stream in our framework.</p><p>Semi-supervised learning. Effective supervised learning using deep neural networks usually requires annotating a large amount of data. However, creating such large datasets is costly and labor-intensive. A promising solution is SSL, which leverages unlabeled data to improve model performance. Existing SSL methods are designed from the aspects of pseudo labeling where model predictions are converted to hard labels (e.g., <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36]</ref>), and consistency regularization where the model is constrained to have consistent outputs under different perturbations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b34">34]</ref>. FixMatch <ref type="bibr" target="#b23">[23]</ref> combines these two classic semi-supervised learning strategies. It predicts hard pseudo labels under weak perturbations and guides the model to learn on unlabeled data with strong perturbations. Our work is built upon FixMatch to explore the potential of semi-supervised Vision Transformers. The noisy student <ref type="bibr" target="#b35">[35]</ref> extends the idea of self-training and distillation with larger student models and add noise to the student. <ref type="bibr" target="#b39">[39]</ref> applies transformers to automated speech recognition using semi-supervised learning. Their superior performance is obtained by large scale pre-training and iterative self-training using the noisy student training approach.</p><p>As the advances of self-supervised learning approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">5]</ref>, a new trend for semi-supervised learning becomes first utilizing the large scale unlabeled data for self-supervised pre-training and then use the labeled data for fine-tuning. Chen et al . <ref type="bibr" target="#b6">[6]</ref> show that a big ResNet pre-trained using SimCLRv2 can achieve competitive semi-supervised performance after fine-tuning.</p><p>Joint modeling of CNNs and Transformers. CNNs and Transformers use two different ways to enforce geometric structure priors. A convolution operator is applied on patches of an image, which naturally results in a local geometric inductive bias. However, a Vision Transformer model utilizes the global self-attention to learn the relationships between global image elements <ref type="bibr" target="#b8">[8]</ref>. From a complementary point of view, combining the advantages of CNNs in processing local visual structures and the advantages of transformer in processing global relationships is potentially a better approach for image modeling.</p><p>One research direction is to imitate the CNN operations into a Vision Transformer or vice versa <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37]</ref>. For example, Pooling-based Vision Transformer (PiT) <ref type="bibr" target="#b15">[15]</ref> applies pooling operations to shrink the feature maps and gradually increases the channel dimension at the same time, similar to the practice of CNN. PyramidViT <ref type="bibr" target="#b30">[30]</ref> and CvT <ref type="bibr" target="#b32">[32]</ref> also adopt a similar hierarchical design. T2T-ViT <ref type="bibr" target="#b37">[37]</ref> designs a progressive tokenization module to aggregate neighboring tokens. <ref type="bibr" target="#b33">[33]</ref> replaces the ViT stem by a small number of stacked convolutions and observes it improves the stability of model training. They also keep the network deep and narrow, inspired by CNNs.</p><p>Probably the most relevant approaches are <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b31">31</ref>] that aim to find ways to combine convolution and transformer into a single model. For example, the non-local network <ref type="bibr" target="#b31">[31]</ref> adds self-attention layers to CNN backbones. Speech-Conformer <ref type="bibr" target="#b12">[12]</ref> attempts to use convolution to enhance the capabilities of the transformer, while ConVit <ref type="bibr" target="#b9">[9]</ref> introduces gated positional self-attention (GPSA) module which becomes equipped with a "soft" convolutional inductive bias. Vi-sualConformer <ref type="bibr" target="#b19">[19]</ref> decouples CNN and Vision Transformer streams and design a module for feature communication across streams. However, these studies are all focused on supervised learning while we propose a generic framework for training semi-supervised vision transformers. Another major difference lies in that, even</p><formula xml:id="formula_0">Trans Trans Conv Conv Trans (a) (b) (c) Conv Conv Trans Trans Fig. 2.</formula><p>We explore a variety of ways to apply vision transformer into semi-supervised learning task. Dotted line refers to weights sharing. u refers to the input image, g and g+ refer to weak and strong data augmentation.?T and?C refer to pseudo labels produced by transformer and convolutional streams. fT (?) and fC (?) represent model predictions of transformer and convolutional streams respectively.</p><p>though we follow the same direction of fusing convolutions and transformers, our approach does not treat the combined architecture as an entirety, e.g., the pseudo labels have to be generated by the convolutional stream only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Study with Vision Transformers for SSL</head><p>We start by presenting two frameworks that use pseudo labels for SSL. Although the two attempts are surprisingly unsatisfactory, their results reveal two important lessons which eventually inspire us to develop our framework. Below we provide the details of the two studies and our learned lessons. Unlabeled data improves Vision Transformers. A natural approach to leverage unlabeled data is to do pseudo labeling through Vision Transformers. Our first hypothesis is that a Vision Transformer can be improved when the total number of input-output training pairs increases (though many of them are pseudo labels). We verify this with a Vanilla framework, which uses the same architecture (e.g., CNN or Transformer) and builds upon FixMatch <ref type="bibr" target="#b23">[23]</ref> for SSL. In particular, FixMatch uses two types of augmentations, a strong one and a weak one. The pseudo label of the unlabeled data is obtained by applying the model on weakly augmented images. And the model is trained using the strongly augmented inputs with the pseudo labels.</p><p>Results in Tab. 1 show that after adding the other 90% images from the ImageNet as unlabeled training data, the transformer-based model can have an accuracy improvement by 10.4%, which is greater than the accuracy improvement of CNN's 8.3%. This validates our hypothesis, i.e., large-scale data helps the Vision Transformer to learn better even when many of them are pseudo labeled. However, despite the score increases, the performance of Vision Transformers in semi-supervised learning is still unsatisfactory, even inferior to the accuracy of fully supervised CNN training on only 10% of the labeled data.</p><p>Pseudo labels from CNNs are more accurate. We suspect that the weak performance of Vanilla is due to the inaccurate pseudo labels generated by the transformer. Vision Transformer contains less image-specific inductive bias, leading to poor performance on small-scale data and thus requires more data for representation learning. In contrast, CNNs are shown to possess strong image-specific inductive bias due to its convolution and pooling design. A natural question is: what if we use a CNN to generate pseudo labels for Vision Transformer?</p><p>We introduce a new SSL framework, Conv-labeled, which uses labels from CNNs for the SSL of CNN and transformers jointly, as illustrated in <ref type="figure">Fig. 2(b)</ref>. As is seen in Tab. 1, the Conv-labeled approach results in 67.2% top-1 accuracy using the predictions from the ViT on ImageNet, improving the Vanilla approach by 8.2%, which suggests that CNNs provide better pseudo labels.</p><p>Conv-based pseudo labeling is not enough. Although the ViT's performance is boosted by a CNN pseudo-label generator, the final performance of the ViT (67.2%) is still worse than the CNN (68.5%), observed from Tab. 1. This suggests that the knowledge from the CNN is not yet fully utilized through the simple pseudo labeling approach. One major problem here is that the two models are mostly decoupled except for the unilateral supervision given by the CNN. On the one hand, knowledge from the CNN is not directly injected into the transformer model. On the other hand, the CNN does not gain any information from the ViT. This motivates us to consider jointly modeling both a convolution network and a transformer, which becomes the proposed Semiformer framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach: Semiformer</head><p>We introduce Semiformer (illustrated in <ref type="figure">Fig. 2</ref>(c) and <ref type="figure" target="#fig_0">Fig. 3</ref>), which jointly fuses a CNN architecture and a Transformer for semi-supervised learning.</p><p>Notation. We use f (x; ?) to represent the mapping function of our Semiformer, given the input x and the model parameter are the vectorized output probability for each label from the transformer stream and the convolutional stream, respectively, and ? is omitted for simplicity. Additionally, a weak data augmentation function g(?) and a strong data augmentation function g + (?) are used in our approach. We assume the semi-supervised dataset contains N l labeled examples and N u unlabeled examples. We use index i for labeled data, index j for unlabeled and index k for the label space. Loss for labeled data. Formally, the total loss for labeled data is</p><formula xml:id="formula_1">L l = N l i=1</formula><p>Lxent(yi, fT (g+(xi))) + Lxent(yi, fC (g+(xi))) <ref type="bibr" target="#b0">(1)</ref> where x i is the i-th labeled example and y i is its corresponding one-hot label vector. L xent is the cross-entropy loss function, i.e., L xent (p, q) = k p k log q k . Loss for unlabeled data. Given an unlabeled image u j , we first perform strong data augmentation g + (?) and weak data augmentation g(?), according to FixMatch <ref type="bibr" target="#b23">[23]</ref>, to obtain the two views of the same input image. However, only the prediction output of the convolutional stream f C (g(u j )) is used to generate the pseudo label. The probability p j of an unlabeled input u j becomes p j = f C (g(u j )) .</p><p>(</p><p>We define the pseudo labels as the class with maximum probability, i.e.,p j = arg max k p jk . We use? j to represent the one-hot vector corresponding to pseudo labelp j . These pseudo labels will in turn be used to calculate the cross entropy loss to back-propagate both the convolutional and the transformer streams with the strongly augmented inputs g + (u j ). A filtering by threshold max k p jk ? ? , equivalent to ?? j , p j ? ? ? , is applied to remove pseudo labels without sufficient certainty. The remaining pseudo labels are used to guide the semi-supervised learning. The total loss for unlabeled data becomes</p><formula xml:id="formula_3">L u = Nu j=1 (L xent (? j , f T (g + (u j ))) + L xent (? j , f C (g + (u j )))) ?[?? j , p j ? ? ? ] ,<label>(3)</label></formula><p>where ?[?] is the delta function whose value is 1 when the condition is met and 0 otherwise. Total loss. The total training loss is the sum of both labeled and unlabeled losses such that</p><formula xml:id="formula_4">L = L l + ?L u ,<label>(4)</label></formula><p>where ? is a trade-off. A more detailed study of ? can be found in Sec. 5.4. Stream fusion. Let M T be the Vision Transformer feature map in a certain layer with the shape (d T , h T , w T ) representing depth, height and width, respectively. Let M T,i be the i-th patch feature according to M T with the shape (d T , 1, 1). So, M T,i corresponds to a specific area of the original image and we denote the CNN sub-feature map who also corresponds to the same area as M C,i with the shape (d C , h C , w C ). Motivated by <ref type="bibr" target="#b19">[19]</ref>, we exchange information between patch features and its related CNN sub-feature map, described as Eq. <ref type="formula" target="#formula_5">(5)</ref> and Eq. <ref type="formula" target="#formula_6">(6)</ref>:</p><formula xml:id="formula_5">M T,i += layernorm(pooling(align(M C,i ))),<label>(5)</label></formula><p>M C,i += batchnorm(upsample(align(M T,i ))),</p><p>where the align operator refers to mapping features to the same dimensional space, pooling refers to downsampling, upsample refers to upsampling, layernorm refers to layer normalization and BN refers to batch normalization. Specifically, a Conv1x1 layer is used for embedding dimension alignment (the align operator). The average pooling and spatial interpolation methods are used for spatial dimension alignment, i.e., the pooling operator and the upsample operator, respectively. To summarize, our framework consists of two parts, including carrying out a hard-way distillation manner by a convolutional stream to guide the transformer's learning from unlabeled data, and carrying out feature-level information interaction between the two streams so that the CNN's knowledge can be injected into the transformer and the convolutional stream can also be enhanced with a better global spatial information organization capability.</p><p>Inference. During training, we use the pseudo labels derived by the convolutional stream to train both the CNN and the Vision Transformer in a semisupervised setting. For inference, we simply average combine predictions from the both streams as final scores, which is slightly better than using the transformer stream alone, as will be shown empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Datasets and evaluation metrics. To evaluate the effectiveness of Semiformer, we mainly conduct experiments on ImageNet <ref type="bibr" target="#b7">[7]</ref>, which contains 1,000 classes and 1.3M images. In addition, we provide experimental results on Places205. Unlike ImageNet that contains generic categories, Places205 is a place-focused dataset, which contains 2.5M images annotated into 205 classes. We use top-1 accuracy as our evaluation metric. Through all experiments, following <ref type="bibr" target="#b23">[23]</ref>, we mainly select 10% labeled samples and leave the other 90% samples as unlabeled data, unless specified otherwise.</p><p>Models. The Semiformer framework emphasizes how to complement the characteristics of the CNN and the ViT to achieve improved results. For the convolutional stream, we use a ResNet-like model and a personalized Con-vMixer <ref type="bibr" target="#b26">[26]</ref>, while within transformer stream, we experiment with both a slightly modified ViT-S <ref type="bibr" target="#b25">[25]</ref> and the PiT-S <ref type="bibr" target="#b15">[15]</ref> as backbone networks.</p><p>Implementation details. The initial learning rate is set to 10 ?3 and is decayed towards 10 ?5 following the cosine decay scheduler. We use 5 epochs to warm-up our models and another 25 epochs to train models on the labeled data before starting the semi-supervised learning process. In the training of ViT-ConvMixer model, the batch size of each GPU is 84, while in the training of ViT-Conv and PiT-Conv model, the batch size is 108 per GPU. We train models with 600 epochs using 32 NVIDIA V100 GPUs to produce our best top-1 accuracy by setting the number ratio of labeled and unlabeled images in each batch as 1:7. In order to avoid gains brought by data augmentation, we do not apply mixup, cutmix and repeat augmentation in our SSL process. We choose random augmentation, random erasing and color jitter as the strong data augmentation, and use random flipping and random cropping as the weak data augmentation. The value of ? which is the balance factor between loss terms is set as 4.0. In the semi-supervised learning with 5% ImageNet labeled samples, we reduce the number ratio of labeled and unlabeled images per batch to 1:9. All the experiments share the same G.T. data split.</p><p>For ablation studies and discussion, we train 300 epochs to speed up the experiments and we set the number ratio of labeled and unlabeled images in each batch as 1:5 and use the label smoothing trick on ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>Comparisons with state-of-the-art. We first compare with state-of-the-art semi-supervised methods, such as UDA <ref type="bibr" target="#b34">[34]</ref>, FixMatch <ref type="bibr" target="#b23">[23]</ref>, S4L <ref type="bibr" target="#b38">[38]</ref>, MPL <ref type="bibr" target="#b20">[20]</ref> and CowMix <ref type="bibr" target="#b10">[10]</ref>, as well as recent self-supevised methods. Experimental results in Tab. 2 show that our approach achieves better results by clear margins compared with alternative methods. For example, Semiformer is better than S4L <ref type="bibr" target="#b38">[38]</ref> and CowMix [10] by 2.3% and 1.6% with only 11% and 67% of parameters of their models, respectively. In addition, while we follow the design the Self-supervised pretraining CPC <ref type="bibr" target="#b14">[14]</ref> ResNet-161 305M 71.5 SimCLR <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref> ResNet-50 24M 65.6 SimCLR <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref> ResNet-50 (2?) 94M 71.7 BYOL <ref type="bibr" target="#b11">[11]</ref> ResNet-50 24M 68.8 BYOL <ref type="bibr" target="#b11">[11]</ref> ResNet-50 (2?) 94M 73.5 DINO <ref type="bibr" target="#b2">[3]</ref> ViT-S 21M 72.2</p><p>Semi-supervised methods UDA <ref type="bibr" target="#b34">[34]</ref> ResNet-50 24M 68.8 FixMatch <ref type="bibr" target="#b23">[23]</ref> ResNet-50 24M 71.5 S4L <ref type="bibr" target="#b38">[38]</ref> ResNet-50 (4?) 375M 73.2 MPL <ref type="bibr" target="#b20">[20]</ref> ResNet-50 24M 73.9 CowMix <ref type="bibr" target="#b10">[10]</ref> ResNet-152 60M 73.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semiformer</head><p>ViT-S + Conv 40M 75.5</p><p>principle of FixMatch to generate pseudo labels, the knowledge sharing mechanism in Semiformer brings about 4% performance gain compared to FixMatch. Although MPL has a smaller model size, training MPL is computationally expensive as it requires meta updates. In addition, MPL uses complicated data augmentations, i.e., AutoAugment, while we only use basic augmentations. Similarly, CowMix <ref type="bibr" target="#b10">[10]</ref> introduces a new data augmentation strategy for SSL. We would like to point that Semiformer is a generic SSL framework that explores pseudo labels and knowledge in CNNs to promote the results of transformers. We believe it is in tandem with more advanced pseudo label generation strategies like MPL <ref type="bibr" target="#b20">[20]</ref> and more complex augmentation methods <ref type="bibr" target="#b10">[10]</ref>. In addition to SSL methods, we also compare with self-supervised learning results such as <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b14">14]</ref>, which firstly learn representations with self-supervised methods and then perform finetuning on limited data. We see that Semiformer also performs favorably compared to this line of methods.</p><p>Effectiveness of Semiformer with different backbones. We evaluate the performance of Semiformer instantiated with different CNN and transformer backbones using 10% of labeled samples. We compare with the supervised training baseline (Sup.), the Vanilla method where the pseudo label generator share the same backbone used for SSL, and Conv-labeled that trains transformers with labels produced by CNNs. The results are summarized in Tab. 3. As the Vanilla results shown in the second block of Tab. 3, CNNs obviously achieve higher image classification accuracy than Vision Transformers under the SSL setting, verifying that labels from CNNs are more accurate. ConvMixer <ref type="bibr" target="#b26">[26]</ref>  <ref type="table">Table 3</ref>. Ablation Study: The effectiveness of Semiformer with various backbones and comparisons with alternative methods (i.e., vanilla and conv-labeled). All models are trained with 300 epochs and without pseudo label smoothing. For Conv-labeled and Semiformer, A/B in the last column: A indicates scores from the transformer stream only and B indicates averaged predictions from CNNs and transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone PiT-S and ViT-S to reach an accuracy of 67.2% and 67.8% respectively, with the same CNN architecture. The improved accuracy is close to that of the Vanilla semi-supervised CNN, suggesting the quality of the pseudo labels makes a difference to the semi-supervised learning process of Vision Transformers. Results in the last block of Tab. 3 show that Semiformer significantly improves the performance of Vision Transformers. This highlights the effectiveness of Semiformer in exploring the interactions of CNNs and transformers. Taking the combination of ViT-S and Conv as an example, after applying the featurelevel interaction to accomplish the dual information exchange, the accuracy of ViT-S is improved by 5.2% from 67.2% to 72.4%, revealing the efficacy of our Semiformer framework. We also observe that Semiformer is a versatile framework compatible with modern CNN and transformer architectures. In addition, by further combining the predictions from both the convolutional and transformer streams, we observe consistent performance gains under all settings for Conv-labeled and Semiformer.</p><p>The ratio of labeled samples. we further experiment with 5% and 20% of labeled samples for SSL and compare with alternative methods. Except that we decrease the number of labeled and unlabeled images in each batch from 1:5 to 1:9 for 5% labeled samples, all the experimental settings are kept the  same as those of using 10% labeled samples. Tab. 4 presents the results. Vision transformer performs poorly when only 5% labels are available, with an accuracy of only 28.6%, which is 15.6% lower than the Conv accuracy of 44.2%. With the increase of the number of labeled samples, the performance gain of ViTs is more significant than that of CNNs. For example, with ViTs, the accuracy increases by 20% and 14.3% respectively, when the number labeled samples grows from 5% to 10% and from 10% to 20%, respectively, suggesting that the training of Vision Transformers is more sensitive to the number of labels. In addition, we see that pseudo labels from CNNs are more accurate and help ViT learn better. Extension to Places205. We also conduct experiments on Places205 to further evaluate the effectiveness of Semiformer. As Places205 is roughly 2 times larger than ImageNet, we use 5% of labeled samples to assure the semi-supervised experiments on 5% Places205 and 10% ImageNet have approximately the same number of labeled samples. We see from Tab. 5 that Semiformer consistently produces the best results. For example, Semiformer is 1.3% and 6.9% better than Conv-labeled and Vanilla-ViT-S, respectively. Similar trends can be observed by comparing across Tab. 4 and Tab. 5, which further confirms the efficacy of Semiformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Results</head><p>We visualize in <ref type="figure" target="#fig_1">Fig. 4</ref> the attention maps of ViT and Semiformer. Thanks to the guidance of pseudo label generator CNN and its supplementary help of injecting the local information extraction ability, Semiformer can retain more local information of images and can correctly focus on the key local positions of the images. For example, when analyzing the <ref type="figure" target="#fig_1">Fig. 4(a)</ref> which corresponds to the class of bow, Semiformer is particularly more focused on the man's hand holding the bow, the man's head and the quiver carried by the person, and those attended areas are critical for identifying the bow category. In addition, Semiformer covers essential objects precisely. In <ref type="figure" target="#fig_1">Fig. 4(f)</ref>, we can see the attention map of Semiformer not only covers the animal completely, but also covers the contours more tightly. And for images with many small objects, for instance, <ref type="figure" target="#fig_1">Fig. 4</ref>(e), Semiformer shows stronger ability to concentrate on key local areas and coverage the essential areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>What model should be used to produce pseudo labels? Although models in our Semiformer framework interact with each other, the CNN model still outperforms the vision transformer especially in the early training stage, making it important to retain the CNN hard-way distillation mode. To verify this, we replace the teacher stream which is responsible for generating pseudo labels. We use the following three strategies to produce pseudo labels: CNNs only, transformers only, and averaged predictions from CNNs and transformers. As shown in Tab. 6, using the CNNs as the teacher network brings the highest accuracy, i.e. 73.5%, while using the transformer stream to generate pseudo labels performs worst (i.e., 67.4%) . As the quality of pseudo labels generated by vision transformers are limited, we do not get better results by simply averaging CNN and Vision Transformer outputs as pseudo labels under the same setting. This further confirms the effectiveness of our pseudo labeling strategy.</p><p>Does Semiformer performs well because of larger models? To clear up the confusion on the relationship between the number of parameters and accuracy, we ablate on the model architecture of Semiformer using different backbones. We experiment with different versions of ResNet <ref type="bibr" target="#b13">[13]</ref> including ResNet-50 (R50), ResNet-101 (R101), ResNet-152 (R152). Results are presented in Tab. 7. We observe that by adding more layers to ResNet, the top-1 accuracy of semisupervised learning does gradually increase. However, it is still lower than that of Semiformer. Even though the ResNet152 model contains 18M more parameters than Semiformer, its accuracy is still 1.7% worse than that of Semiformer, which proves the performance gain of Semiformer does not come from model sizes. We further instantiate the two streams of Semiformer with the same backbone, i.e. C+C and V+V respectively, and modify the stream connection correspondingly. Note that this is different from Vanilla as the two streams exchange information. Tab. 7 reveals that these combinations are significantly worse than Semiformer. For example, Semiformer outperforms V+V by 13.9% with 7M fewer parameters, which again shows the effectiveness of Semiformer is not due to extra parameters.</p><p>The impact of hyper-parameters. The default set of hyperparameters are: label and unlabeled data ratio is 1:5, confidence threshold is 0.7 and ? is set as 4. Based on the default setting, we control other variables unchanged and observe how the accuracy rate changes after independently changing the following three factors: different confidence threshold (0.65, 0.7, 0.75, 0.8); different ? value <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4)</ref>; different proportion of the number of labeled and unlabeled data (1:3, 1:5, 1:7). Semiformer offers the best results with 0.7 confidence threshold, 1:7 labeled-unlabeled ratio, and ? = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented Semiformer, the first framework to train Vision Transformers for semi-supervised learning. We found directly training a Vanilla transformer on semi-supervised data is ineffective. The proposed framework combines a CNN and a Vision Transformer using a cross fusion approach. The optimal semi-supervised learning performance is achieved by using only the convolutional stream to generate the pseudo labels. The final fused framework achieves 75.5% top-1 accuracy on ImageNet and outperforms the state-of-the-art in semisupervised image classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>?. f T (x) and f C (x) t e x i t s h a 1 _ b a s e 6 4 = " n x p J s b F 1 h Q v S v U + j j K 8 h J 6 m a e V o = " &gt; A A A C z 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k W o I C U t o i 6 L R X D Z Q l / Q l p K k 0 x r M Y 0 w m S i m K W 3 / A r f 6 V + A f 6 F 9 4 Z U 1 C L 6 I Q k Z 8 6 9 5 8 z c e y 3 u O p E w j N e U N j e / s L i U X s 6 s r K 6 t b 2 Q 3 t 5 p R E I c 2 a 9 i B G 4 R t y 4 y Y 6 / i s I R z h s j Y P m e l Z L m t Z l x U Z b 1 2 z M H I C v y 7 G n P U 8 c + Q 7 Q 8 c 2 B V H d y l m e 9 + s H + l W / s t / P 5 o y C o Z Y + C 4 o J y C F Z 1 S D 7 g i 4 G C G A j h g c G H 4 K w C x M R P R 0 U Y Y A T 1 8 O E u J C Q o + I M t 8 i Q N q Y s R h k m s Z f 0 H d G u k 7 A + 7 a V n p N Q 2 n e L S G 5 J S x x 5 p A s o L C c v T d B W P l b N k f / O e K E 9 5 t z H 9 r c T L I 1 b g g t i / d N P M / + p k L Q J D n K g a H K q J K 0 Z W Z y c u s e q K v L n + p S p B D p w 4 i Q c U D w n b S j n t s 6 4 0 k a p d 9 t Z U 8 T e V K V m 5 t 5 P c G O / y l j T g 4 s 9 x z o J m q V A 8 K p R q h 7 n y a T L q N H a w i z z N 8 x h l n K O K B n l z P O I J z 1 p N u 9 H u t P v P V C 2 V a L b x b W k P H / t o k t U = &lt; / l a t e x i t &gt; CE(pT , qC) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y k A u 6 i 6 i 3 H / p 8 a B A W c K e C b v M h E E = " &gt; A A A C z 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k W o I C U p o i 6 L Q X D Z g n 1 A W 0 q S T m t o X i Y T p R T F r T / g V v 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v V b o O j H X t N e M M j e / s L i U X c 6 t r K 6 t b + Q 3 t x p x k E Q 2 q 9 u B G 0 Q t y 4 y Z 6 / i s z h 3 u s l Y Y M d O z X N a 0 R o a I N 6 9 Z F D u B f 8 H H I e t 6 5 t B 3 B o 5 t c q I 6 x l k x 7 B k H 6 l X P 2 O / l C 1 p J k 0 u d B X o K C k h X N c i / o I M + A t h I 4 I H B B y f s w k R M T x s 6 N I T E d T E h L i L k y D j D L X K k T S i L U Y Z J 7 I i + Q 9 q 1 U 9 a n v f C M p d q m U 1 x 6 I 1 K q 2 C N N Q H k R Y X G a K u O J d B b s b 9 4 T 6 S n u N q a / l X p 5 x H J c E v u X b p r 5 X 5 2 o h W O A E 1 m D Q z W F k h H V 2 a l L I r s i b q 5 + q Y q T Q 0 i c w H 2 K R 4 R t q Z z 2 W Z W a W N Y u e m v K + J v M F K z Y 2 2 l u g n d x S x q w / n O c s 6 B R L u l H p X L t s F A 5 T U e d x Q 5 2 U a R 5 H q O C c 1 R R J + 8 Q j 3 j C s 1 J T b p Q 7 5 f 4 z V c m k m m 1 8 W 8 r D B 9 K i k s Q = &lt; / l a t e x i t &gt; CE(pC, qC) Diagram of the Semiformer framework.For an unlabeled image, its weaklyaugmented version (top) is fed into the model. The prediction of CNN is used for generating pseudo labels with a confidence threshold (dotted line). Then we compute the model's prediction for the strong augmented version of the same image (bottom). We expect both transformer and convolutional streams to match the pseudo label via cross-entropy losses. Streams complement each other with the feature-level modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Attention map of ViTs and Semiformer using samples from ImageNet. Compared to ViTs where the attention scores are scattered, Semiformer focuses more on critical objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Three semi-supervised vision transformers using 10% labeled and 90% unlabeled data (colored in green) vs. fully supervised vision transformers (colored in blue) using 10% and 100% labeled data. Our approach Semiformer achieves competitive performance, 75.5% top-1 accuracy.</figDesc><table><row><cell>Top-1 Accuracy (%)</cell><cell>50 60 70 80 90</cell><cell cols="2">48.6 Vision Transformer 59.0 Semi Vision Transformer</cell><cell>67.2</cell><cell>75.5</cell><cell>80.0</cell></row><row><cell></cell><cell>40</cell><cell>Supervised (10%)</cell><cell>Vanilla</cell><cell cols="2">Conv-labeled Semiformer</cell><cell>(100%) Supervised</cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, still</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results and comparisons with two different SSL frameworks, and comparisons with the supervised baselines.</figDesc><table><row><cell>Architecture</cell><cell>Method</cell><cell>Top-1 Acc (%)</cell></row><row><cell>CNN</cell><cell>Sup. only (10%) Vanilla</cell><cell>60.2 68.5</cell></row><row><cell></cell><cell>Sup. only (10%)</cell><cell>48.6</cell></row><row><cell>Transformer</cell><cell>Vanilla</cell><cell>59.0</cell></row><row><cell></cell><cell>Conv-labeled</cell><cell>67.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The results of Semiformer and comparisons with state-of-the-art methods. Semiformer achieves 75.5% top-1 accuracy and outperforms all Convolutional neural network based methods, while still keeping a reasonable parameter size. Here, the params does not include the final classifier.</figDesc><table><row><cell>Method</cell><cell>Architecture</cell><cell>Params</cell><cell>Top-1 Acc(%)</cell></row><row><cell>Sup. (10%)</cell><cell>ViT-S Conv</cell><cell>23M 13M</cell><cell>48.6 60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>achieves the best results among all Vanilla models, offering a top-1 accuracy of 69.3%. This possibly results from the fact ConvMixer integrates the architectural advantages of both transformers and CNNs. Results in the third block of Tab. 3 show that using CNNs instead of Vision Transformers to generate pseudo labels significantly improves the performance of the Vision Transformer, allowing</figDesc><table><row><cell></cell><cell></cell><cell>Pseudo labels</cell><cell>Top-1 Acc(%)</cell></row><row><cell>Sup. (10%)</cell><cell>ViT-S PiT-S</cell><cell>--</cell><cell>48.6 50.0</cell></row><row><cell></cell><cell>Conv</cell><cell>Conv</cell><cell>68.5</cell></row><row><cell>Vanilla</cell><cell>ConvMixer ViT-S</cell><cell>ConvMixer ViT-S</cell><cell>69.3 59.0</cell></row><row><cell></cell><cell>PiT-S</cell><cell>PiT-S</cell><cell>63.0</cell></row><row><cell></cell><cell>ViT-S + Conv</cell><cell>Conv</cell><cell>67.2 / 70.2</cell></row><row><cell>Conv-labeled</cell><cell>PiT-S + Conv</cell><cell>Conv</cell><cell>67.8 / 70.5</cell></row><row><cell></cell><cell>ViT-S + ConvMixer</cell><cell>ConvMixer</cell><cell>66.7 / 70.2</cell></row><row><cell></cell><cell>ViT-S + Conv</cell><cell>Conv</cell><cell>72.4 / 73.5</cell></row><row><cell>Semiformer</cell><cell>PiT-S + Conv</cell><cell>Conv</cell><cell>70.8 / 71.6</cell></row><row><cell></cell><cell>ViT-S + ConvMixer</cell><cell>ConvMixer</cell><cell>72.9 / 73.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>SSL with different ratios of labeled samples on ImageNet.</figDesc><table><row><cell>Dataset</cell><cell>Ratio</cell><cell>Sup.</cell><cell>ViT-S Vanilla</cell><cell>Sup.</cell><cell>Conv Vanilla</cell><cell cols="2">ViT-S + Conv Conv-labeled Semiformer</cell></row><row><cell></cell><cell>5 %</cell><cell cols="2">28.6 45.7 (?17.1)</cell><cell cols="2">44.2 61.3 (?17.1)</cell><cell>62.0</cell><cell>66.3 (?4.3)</cell></row><row><cell>ImageNet</cell><cell>10 %</cell><cell cols="2">48.6 59.0 (?10.4)</cell><cell>60.2</cell><cell>68.5 (? 8.3)</cell><cell>70.2</cell><cell>73.5 (?3.3)</cell></row><row><cell></cell><cell>20 %</cell><cell cols="2">52.9 69.8 (?16.9)</cell><cell cols="2">63.5 73.6 (?10.1)</cell><cell>74.8</cell><cell>78.1 (?3.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Top-1 Accuracy of Semiformer on 5% labeled subset of Places205.</figDesc><table><row><cell>Dataset</cell><cell>Ratio</cell><cell>Sup.</cell><cell>ViT-S Vanilla</cell><cell>Sup.</cell><cell>Conv Vanilla</cell><cell cols="2">ViT-S + Conv Conv-labeled Semiformer</cell></row><row><cell>Places205</cell><cell>5 %</cell><cell cols="2">36.0 46.9 (?3.9)</cell><cell cols="2">44.3 51.6 (?7.3)</cell><cell>52.5</cell><cell>53.8 (?1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Results by different pseudo labels. Model size analysis. V and C refer to ViT-S and CNN, respectively. R represnets ResNet. Acc(%) 68.3 70.8 71.8 68.5 59.0 66.9 59.6 73.5</figDesc><table><row><cell cols="2">PL Type Acc@1(%)</cell><cell cols="2">Architecture R50 R101 R152 C</cell><cell>V C+C V+V V+C</cell></row><row><cell>CNN</cell><cell>73.5</cell><cell>Params</cell><cell cols="2">24M 43M 58M 13M 23M 35M 47M 40M</cell></row><row><cell>Trans Fusion</cell><cell>67.4 71.1</cell><cell>Top-1</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<title level="m">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised learning (chapelle, o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<editor>. et al.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tnn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Milking cowmask for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12022</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<title level="m">Conformer: Convolution-augmented transformer for speech recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021) 1, 3, 4</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICMLW</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contextual transformer networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Conformer: Local features coupling global representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>CVPR (2021)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2020) 1, 4, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (2021)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792</idno>
		<title level="m">Patches are all you need? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient video transformers with spatial-temporal token selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bevt: Bert pretraining of video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep co-training with task decomposition for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pushing the limits of semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
