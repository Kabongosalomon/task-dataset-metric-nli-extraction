<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Methods for Aggregated Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Thomas</surname></persName>
							<email>xavier.thomas1@learner.manipal.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
							<email>dhruvm@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
							<email>pentland@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
							<email>dubeya@fb.com</email>
						</author>
						<title level="a" type="main">Adaptive Methods for Aggregated Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain generalization involves learning a classifier from a heterogeneous collection of training sources such that it generalizes to data drawn from similar unknown target domains, with applications in large-scale learning and personalized inference. In many settings, privacy concerns prohibit obtaining domain labels for the training data samples, and instead only have an aggregated collection of training points. Existing approaches that utilize domain labels to create domain-invariant feature representations are inapplicable in this setting, requiring alternative approaches to learn generalizable classifiers. In this paper, we propose a domain-adaptive approach to this problem, which operates in two steps: (a) we cluster training data within a carefully chosen feature space to create pseudodomains, and (b) using these pseudo-domains we learn a domain-adaptive classifier that makes predictions using information about both the input and the pseudo-domain it belongs to. Our approach achieves state-of-the-art performance on a variety of domain generalization benchmarks without using domain labels whatsoever. Furthermore, we provide novel theoretical guarantees on domain generalization using cluster information. Our approach is amenable to ensemble-based methods and provides substantial gains even on large-scale benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of domain generalization addresses learning a classifier from a random subset of training domains, with the objective of generalizing to unseen test domains <ref type="bibr" target="#b5">[6]</ref>. Research on this problem has seen an explosion in interest recently, with a majority of approaches focusing on learning domain-invariant feature representations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. The general idea behind such methods is to learn feature representations that reduce domainspecific variance, which can, under suitable assumptions, be shown to minimize generalization error (see, e.g., <ref type="bibr" target="#b4">[5]</ref>). * Manipal Institute of Technology ? Facebook AI Research, Meta ? MIT From a practical perspective, the most relevant application of domain generalization is in large-scale learning <ref type="bibr" target="#b21">[22]</ref>, where data is gathered from multiple (often varying) sources, and we wish to learn a model that generalizes to new sources of data, e.g., learning an image classifier from data obtained from a collection of mobile devices. Each data source exhibits unique properties, and hence it is desirable to eliminate domain-specific variation (i.e., changes across users) to reduce spurious error.</p><p>In addition to this challenge of generalization, environments with data from multiple participants additionally present a second challenge of privacy-preservation. When data is collected from various sources (e.g., users), it is desirable to eliminate sensitive information to ensure privacy of the participants. In the domain generalization setting, a straightforward approach to achieve this is to discard domain information altogether, i.e., aggregating all points into an anonymized dataset. However, in this problem setting, existing domain-invariant approaches present an insurmountable challenge: without domain information, it is not possible to construct appropriate regularization penalties to learn invariant features. Furthermore, recent work <ref type="bibr" target="#b16">[17]</ref> suggests that domain information may not be even necessary, as simple fine-tuning matches domain invariant methods when model selection is done properly. These reasons make domain generalization in the aggregate setting (i.e., without domain labels) an interesting open problem.</p><p>In this paper, we demonstrate that when fine-tuning from pre-trained models (as is standard in computer vision), it indeed is possible to generalize to new domains without access to the domain partitions. The fundamental insight for our approach is to recover latent domains via clustering, and subsequently bootstrap from these latent domains using domain-adaptive learning <ref type="bibr" target="#b12">[13]</ref>. Specifically, we provide an algorithm that recovers domain information in an unsupervised manner, by carefully removing class-specific noise from features. We then use this carefully selected feature space to partition inputs and learn a domain-adaptive classifier with state-of-the-art performance. Our precise contributions are summarized as follows.</p><p>? We extend domain-adaptive domain generalization to an algorithm that simultaneously assigns latent domain labels to an aggregated training set via unsupervised clustering, and then runs regular fine-tuning on an augmented input space to produce a classifier that adapts to the domain corresponding to any input.</p><p>? Additionally, we extend the theory of domain generalization via kernel mean embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> to approaches that utilize an approximate clustering of the domain space, and provide novel generalization bounds under this setting, which are applicable in tasks beyond those considered within this paper.</p><p>? On a set of standard and even large-scale (1M+ points) domain generalization benchmarks, we demonstrate that even when the training data is aggregated, it is possible to obtain competitive performance in the domain generalization task by domain-adaptive classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work draws from several lines of research in computer vision and machine learning, as discussed below.</p><p>Domain Generalization. First proposed in the work of Blanchard et al. <ref type="bibr" target="#b5">[6]</ref>, domain generalization is a problem gaining rapid attention in the machine learning and computer vision communities. A broad category of approaches can be summarized by domain-invariant representation learning, i.e., learning representations that eliminate domain-specific variations within the dataset. This approach was first examined in the context of domain adaptation by Ben-David et al. <ref type="bibr" target="#b4">[5]</ref>, which was used to construct a domain-adversarial neural network in the work of Ganin et al. <ref type="bibr" target="#b14">[15]</ref>. Building on the work of <ref type="bibr" target="#b14">[15]</ref>, several algorithms have been proposed for domain generalization <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> via adversarial feature learning. Key differences within these approaches are based on the penalty formulation used to ensure invariant feature learning. For example, Li et al. <ref type="bibr" target="#b25">[26]</ref> utilize a maximum mean discrepancy (MMD) regularization, Sun et al. <ref type="bibr" target="#b36">[37]</ref> use a correlation alignment, and Li et al. <ref type="bibr" target="#b24">[25]</ref> propose class-conditional adversarial learning.</p><p>In contrast to these approaches, Arjovsky et al. <ref type="bibr" target="#b1">[2]</ref> propose invariant risk minimization (IRM), a training method that optimizes for a robust loss function in order to provably reduce out-of-distribution error. A similar robust design philosophy has been explored in the work of Sagawa et al. <ref type="bibr" target="#b35">[36]</ref> via distributionally robust optimization, and a straightforward but effective interpolation strategy known as MixUp <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. Generalization via assuming a causal structure has also been explored in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>. While these approaches have seen improvements on domain generalization benchmarks, recent work by Gulrajani and Lopez-Paz <ref type="bibr" target="#b16">[17]</ref> suggests that improvements obtained by domain-invariant approaches are largely dependent on hyperparameter settings and the model selection technique used, as naive ERM (vanilla training using the training data) outperforms several of these approaches when initialized properly. Teney et al. <ref type="bibr" target="#b37">[38]</ref> for the task of Visual Question Answering shows that partitioning the data into well-chosen environments can lead to better generalization, by capturing patterns that are stable across environments and discarding spurious ones.</p><p>Our setting departs from the ones considered within the above line of work as we do not assume access to the domain labels, which are imperative for learning invariant features in the methods highlighted previously. Similar to <ref type="bibr" target="#b16">[17]</ref>, our core algorithm is also vanilla ERM, and our improvements arise from using a more expressive class of functions. Kernel Mean Embeddings. Our design philosophy utilizes kernel mean embeddings <ref type="bibr" target="#b29">[30]</ref>, the technical tool used originally by Blanchard et al. <ref type="bibr" target="#b5">[6]</ref> to study the domain generalization problem. Kernel mean embeddings provide a rigorous and realizable mechanism to "project" probability distributions on to reproducing kernel Hilbert spaces (RKHS), and have been shown to be effective in generalization across a variety of problems, including multi-task learning <ref type="bibr" target="#b10">[11]</ref> and reinforcement learning <ref type="bibr" target="#b11">[12]</ref>. We further the analysis of learning with kernel mean embeddings from that presented in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b5">6]</ref> by providing novel generalization bounds in the approximate setting, where we learn embeddings in an unsupervised manner via bootstrapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The domain generalization setting typically assumes a space of all relevant data distributions D, i.e., each domain D ? D defines a probability distribution over the space X ? Y. Further, we assume that D is endowed with a "mother" probability distribution P that determines how a domain is sampled. A training domain D(n) is obtained by first sampling a domain D ? D following P and then sampling n points from X ? Y following D. The training set is constructed by sampling N such domains ( D i (n)) N i=1 and aggregating them. A test domain is constructed by sampling a domain D T (n T ) identically but discarding the labels. Since we are working in the aggregated setting, we assume that the training set only consists of a set of n ? N points {x i , y i } n?N i=1 from all the sampled domains without the corresponding domain labels.</p><p>Domain-Adaptive Classification. Our approach at a high level follows the domain-adaptive paradigm introduced in <ref type="bibr" target="#b12">[13]</ref>, building on the kernel mean embedding approach for domain generalization <ref type="bibr" target="#b5">[6]</ref>. Consider a family of functions F. For each domain D ? D, we have that the optimal classifier within F under a loss function can be given by</p><formula xml:id="formula_0">f D = arg min f ?F E (x,y)?D [ (f (x), y)].</formula><p>Additionally, the universal optimal classifier f (over all domains) within the class F can be given by f = arg min f ?F E D?P E (x,y)?D [ (f (x), y)]. Unless we make regularity assumptions on the geometry of D, once can see that the test error of f on an randomly chosen domain D ? D can be arbitrarily worse.</p><p>The primary motivation behind domain-adaptive classification is to expand the function class to consider functions over X ? D, i.e., a function F that takes in both the input sample x and the domain D while making predictions, and attempt to ensure uniformly low error, i.e., ensure that for any arbitrarily sampled test domain D T , the adaptive classifier f T = F (?, D T ) incurs low risk. The central challenge in such an approach is to learn a function over each training domain. For this, we need an approach to represent each domain itself as vector, which brings us to the framework of kernel mean embeddings.</p><p>Kernel Mean Embeddings. Kernel mean embeddings (KMEs) are a straightforward technique to compute a function over probability distributions, which also provide rigorous convergence guarantees <ref type="bibr" target="#b29">[30]</ref>. The approach outlined in <ref type="bibr" target="#b12">[13]</ref> trains such an adaptive network by representing the domain D via its kernel mean embedding ? D . Specifically, we can learn a domain-adaptive function F (x, D) as F = f (x, ? D ), where f is a neural network that takes in the joint input, i.e., the input x and the KME of the domain D computed via a separate feature extractor ?(?), i.e.,</p><formula xml:id="formula_1">? D = 1 n ? n i=1 ?(x i ), where x 1 , ..., x n are sampled i.i.d. from D.</formula><p>In <ref type="bibr" target="#b12">[13]</ref>, the authors adopt this approach to learn a CNN classifier. The authors learnt the feature ? by first training a domain classifier, and then discarding the softmax layer to use its features. The network F in this case is then a CNN that first produces image features ? image (x), which are concatenated with the domain embeddings ? D , followed by an MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Aggregated Domain Generalization</head><p>The aforementioned adaptive approach (and moreover, any approach building on using domain labels to create </p><formula xml:id="formula_2">?(x) ? F image (x; ?) ?(x) ? PROJECTION(?(x)) onto [dstart, d end ] ?x ? arg min k?[K] ? k ? ?(x) 2 . return F (x) = F mlp CONCAT ?(x), ?x ; W .</formula><p>domain-invariant classifiers) is not feasible when the domain labels are unavailable, and we only have access to aggregated data with the knowledge that it comprises inputs from several sources. We assume that we are provided with a training set D tr with T = n?N samples (n from each of the N domains), but the dataset is aggregated, i.e., the domain labels are discarded. During testing, we are provided with n T samples from a fresh domain D T sampled i.i.d. from P. The objective, once again, is to construct a classifier that achieves uniform low risk on the test domains.</p><p>To counter the lack of labels, we rely on a bootstrappingstyle approach, i.e., where we cluster the training data into K pseudo-domains { D 1 , D 2 , ..., D K }, and then compute the kernel mean embedding of each pseudo-domain itself, and perform adaptive classification. At a high level, our training involves two steps in every epoch: (A) Cluster Training Data. We compute a clustering { D k } K k=1 (also known as pseudo-domains) of the input data using an appropriately chosen feature ?. Once these clusters are obtained, we obtain the corresponding pseudo mean embeddings { ? k } K k=1 for each pseudo-domain, i.e.,</p><formula xml:id="formula_3">? k = (1/| D k |) ? x? D k ?(x)</formula><p>. After clustering, we compute the augmented inputs, i.e., where each point x in the training data is augmented with the corresponding ? x from the cluster it belongs to, to produce the augmented dataset ((x, ? x ), y) x? Dtr . To avoid heavy computation, this clustering step is not performed every epoch but at a logarithmic schedule. Hence, if training progresses for T epochs, we recompute clusters O(log(T )) times. (B) ERM using augmented pseudo-embeddings. We learn a function F which involves a feature extractor F image with weights ? (a CNN, e.g., ResNet <ref type="bibr" target="#b17">[18]</ref>), followed by a fully-connected classifier F MLP with weights W. The feedforward operation for an input (x, ? x ) involves three steps: (a) obtaining the features F image (x; ?); (b) concatenating the feature F image (x) with the mean embedding ? x to form the augmented input F joint (x); (c) computing class probabilities by feeding F joint (x) through the linear layer F MLP with weights W, followed by a softmax operation. We use the standard cross-entropy error (ERM) to optimize the backward pass, and note that there are no gradients through ? x , i.e., the centroids ? x are fixed features, treated as additional inputs (until they are recomputed). The complete algorithm is summarized in Algorithm 1.</p><p>Testing. During testing, for any input x T , we identify the nearest pseudo embedding ? x T and return F (x T , ? x T ). In contrast to prior work on adaptive domain generalization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, inference is carried out one sample at a time.</p><p>We now provide more details on how we select the feature ? and the corresponding theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Obtaining Pseudo-Domains via Clustering</head><p>Selecting ?. Selecting an appropriate feature embedding ? to perform clustering is imperative for our bootstrapbased approach. Our central idea is to recover domainspecific features from the existing network F image itself, by carefully selecting relevant directions of importance.</p><p>We first fine-tune a pre-trained network (trained on a large dataset, e.g., ILSVRC12 <ref type="bibr" target="#b9">[10]</ref> as per standard practice), for 1 epoch on our target data. We then discard the final (classification) layer and set F image to be the remaining feature extractor. Now, we use F image itself to provide us with the relevant features ?. However, these features primarily contain information about the prediction problem, i.e., class-specific variance, however, we only want to extract domain-specific variance so that the resulting clustering separates the domains well. For this, we assume that the features from F image can be decomposed into three broad categories: the first are class-specific features, next are domain/image-specific features, and finally, we have noise. For some parameters d start , d end ? d, we assume that the fist d start principal eigenvectors capture class-specific variance and the last d end principal eigenvectors are primarily noise. Hence, to obtain a domain-specific clustering, we only need to consider the central [d start , d end ] portion of the spectrum. To obtain these features, in each clustering round, we first compute the image features ? = F image (X) and project ? on to its central [d start , d end ] principal components by first performing a PCA operation (to obtain the principal components), and then truncation (to remove the irrelevant feature directions). This provides us with the resulting "projected" features?. Finally, we cluster? into K clusters (pseudo-domains), and provide the resulting centroids of each cluster { ? k } K k=1 as the KMEs for each of the K pseudo-domains.</p><p>Note that the filtering of irrelevant feature directions is a critical component of the algorithm, as demonstrated via ablation experiments in Section 5.4 as well. However, we see that the algorithm is robust to the precise values of [d start , d end ] as long as they are within a reasonable range.</p><p>A Remark on Scalability. Centralized data operations such as clustering and finding nearest neighbors are known to be expensive and difficult to scale, however, recent work <ref type="bibr" target="#b20">[21]</ref> has demonstrated that by careful quantization and efficient implementation, one can scale even to billion-scale datasets. We employ a similar approach and our experiments demonstrate that even for large-scale settings, our algorithm scales effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Guarantees</head><p>In this section we provide some theoretical guarantees for our proposed algorithm. The theoretical results are within the framework of Blanchard et al. <ref type="bibr" target="#b5">[6]</ref>, where our key contributions are to analysing when the true kernel mean embeddings and obtained embeddings are mismatched. At a high level, our function class F is defined over the space X ? D (i.e., the joint input-domain space), however, we embed domains into R d via a mean embedding ? : D ? R d , and therefore, any function f ? F is written as f (x, ? D ), where ? D denotes the kernel mean embedding of D. Background. We assume a compact input space X and assume the outputs to lie in the space Y = [?1, 1]. For any Lipschitz loss function , then the empirical loss on any domain D with n samples is 1</p><formula xml:id="formula_4">ni (x,y)?D (f (x, ? D ), y).</formula><p>We can define the average training error over N domains with n samples as,</p><formula xml:id="formula_5">L N (f, ?) 1 n ? N i?[N ] (x,y)?Di (f (x, ? Di ), y).</formula><p>Similarly, our benchmark is to compare the above with the expected risk, i.e., the error obtained by f in the limit of infinite samples.</p><formula xml:id="formula_6">L(f, ?) E D?P E (x,y)?D [ (f (x, ? D ), y] .</formula><p>Following <ref type="bibr" target="#b5">[6]</ref>, the space F we consider is defined by product kernels, i.e., the kernel ? can be decomposed into a product of two separate kernels k P (which depends on the domain via ?) and k X (which depends on the inputs x).</p><p>We defer more kernel assumptions to the Appendix.</p><p>Results. Our key result is a uniform bound on the excess risk of using K pseudo-domain centroids ? instead of the true kernel mean embeddings ?, since computing the true kernel mean embeddings would require the domain labels. We make two assumptions on the data distribution P and feature space ? to obtain our generalization bound. We state the assumptions informally, and provide detailed technical explanations with examples in the Appendix.</p><p>Assumption 1 (d -Expressivity, Informal). We assume that the feature space ? is expressive with a parameter d d for the distribution P, i.e., it requires on average d dimensions to cover the domain space D. Specifically, if we assume that there exists an optimal clustering (with infinite samples) into K partitions within P whose centroids are given by ? , then, we assume that for any domain D ? D,</p><formula xml:id="formula_7">min k ? D ? ? ,[k] 2 ? O 1 K d .</formula><p>The above assumption implies that ? is able to cover the entire domain space with only d d dimensions under the distribution P. If ? is completely aligned with the domains themselves (i.e., we can easily separate the domains), we expect d ? 1, and in the worst case, d = d (i.e., no information). Next, we present a standard assumption on the clustering approximation.</p><p>Assumption 2 (Cluster Approximation, Informal). Let the optimal clustering (with infinite samples) of P under ? be given by ? , and let the optimal clustering (with n ? N samples) from P be given by ? . We assume that with high</p><formula xml:id="formula_8">probability, ? ? ? 2 ? O 1 n?N .</formula><p>This is a standard assumption can be satisfied by most practical data distributions; for a thorough treatment of statistical clustering, see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. Armed with these assumptions, we present our primary generalization bound. Theorem 1. Let ? and P be such that Assumptions 3 and 4 are true. Let E f = L(f, ?) ? L N (f, ?) denote the generalization error for any f ? F. Then, with probability at</p><formula xml:id="formula_9">least 1 ? ?, sup f ?F E f = O 1 K 1 d + log ( KN /?) n + log ( nKN /?) N .</formula><p>Remark 1 (Discussion). The generalization bound above admits an identical dependency on the number of domains N and points per domain n as in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, and cannot be improved in general. We see an additional term</p><formula xml:id="formula_10">K ? 1 d</formula><p>which can be decomposed as follows. We see that as K ? ? (we select a larger clustering), the additional term goes to 0. Its rate of decrease, however, depends on d , i.e., the effective dimensionality of ?. If ? contains ample information about P (or P is concentrated in ?), then we can expect d d (it is at most d by a covering bound). To the best of our knowledge, ours is the first analysis on domainadaptive classification with kernel mean embeddings that considers the misspecification introduced by using approximate clustering solutions.</p><p>A proof for Theorem 1 can be found in the Appendix. In addition to the main result, we believe that the intermediary characterizations of aggregated clustering and expressivity can be of value beyond the domain generalization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our experiments are performed using DOMAINBED <ref type="bibr" target="#b16">[17]</ref>, and the compared algorithms are run using hyperparameter ranges suggested therein. Our baseline experiments are done on a ResNet-50 <ref type="bibr" target="#b17">[18]</ref> neural network pre-trained on the ImageNet LSVRC12 <ref type="bibr" target="#b9">[10]</ref> training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Clustering. We use the Facebook AI Similarlity Search (FAISS) <ref type="bibr" target="#b19">[20]</ref> platform to perform clustering using the kmeans++ algorithm. To accelerate clustering, we quantize the feature space which introduces noise within the matching process. We set K as an integer multiple of the number of classes in the problem (which depends on the dataset). We set the default clustering schedule T clust = {1, 2, 4, 8, ...}, i.e., the first round of clustering occurs at epoch 1, then at epoch 2, then at epoch 4, and so on until training converges. Training. The feature extractor weights ? we use are a ResNet-50 network pre-trained on ILSVRC12 <ref type="bibr" target="#b9">[10]</ref> where we truncate the network after the pool5 layer. This layer is then concatenated with a d end ?d start dimensional pseudodomain embedding, and passed through the fully-connected layer (W), with output dimension set to the number of Algorithm VLCS <ref type="bibr" target="#b13">[14]</ref> PACS <ref type="bibr" target="#b22">[23]</ref> OffHome <ref type="bibr" target="#b38">[39]</ref> DNet <ref type="bibr" target="#b31">[32]</ref> TerraInc <ref type="bibr" target="#b2">[3]</ref> Average Model Selection: Leave-One-Domain-Out Validation Algorithms that require domain labels MLDG <ref type="bibr" target="#b23">[24]</ref> 76. <ref type="bibr" target="#b7">8</ref>  classes. Note that W is simply a fully-connected layer and not an MLP, as we did not find any notable performance improvements with the latter. We use weight decay with standard cross-entropy loss for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmark Comparisons</head><p>We experiment on 5 datasets: VLCS <ref type="bibr" target="#b13">[14]</ref>, PACS <ref type="bibr" target="#b22">[23]</ref>, Office-Home <ref type="bibr" target="#b38">[39]</ref>, Domain-Net <ref type="bibr" target="#b31">[32]</ref> and Terra Incognita <ref type="bibr" target="#b2">[3]</ref>). The variations within these datasets are on the nature of images (sketch vs. photographs) as well as synthetic (rotations and translations), providing a wide array of testing scenarios. Our default paradigm for model selection is the leave-one-domain-out cross validation method, where for a dataset with N domains, we run N experiments, wherein the i th round we leave the i th dataset out from training and only performing testing exclusively on it. Each domain is split 80 ? 20 at random into a trainingvalidation split, following standard procedure outlined in DOMAINBED <ref type="bibr" target="#b16">[17]</ref>. We do a random search for the hyperparameters, which include neural network as well as clustering hyperparameters, and denote the specific hyperaparameters within the tables as AdaClust(d start ? d end , K/n c ), i.e., spectrum range and clusters per class (where n c = |Y|). We report the full hyperparameter ranges in the Appendix.</p><p>Results <ref type="table" target="#tab_1">(Table 1</ref>). The algorithms we compare with are standard ERM (i.e., fine-tuning), domain adaptive classification <ref type="bibr" target="#b12">[13]</ref>, and a suite of domain-invariant approaches preimplemented in the DOMAINBED suite. We observe that our algorithm, titled Adaptive Clustering, performs competitively with all invariant approaches, as well as domainadaptive classification, despite not utilizing any labels at all. Another line of research that provides competitive perfor-mance is based on validation-based model averaging, e.g., SWAD <ref type="bibr" target="#b6">[7]</ref>, which provides improvements orthogonal to our algorithm. This can be highlighted by running our algorithm with validation-based model averaging via SWAD, which is noted as Adaptive Clustering (SWAD), and provides improvements over regular SWAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Large-Scale Comparisons</head><p>In addition to the small-scale benchmarks from DO-MAINBED, we also provide comparisons on a real-world, large-scale benchmark dataset to examine real-world performance. We use the GeoYFCC <ref type="bibr" target="#b12">[13]</ref> dataset, which is a real-world domain generalization benchmark with 1.1M total examples across 1.2K output categories and 62 total domains. We follow their outlined train/val/test splits and use traditional cross-validation on the heldout validation set to select hyperparameters. Note that given the size of the dataset, extensive hyperparameter tuning as employed by most algorithms is not possible, therefore it is a good benchmark to compare with domain-invariant approaches as well. We observe that our algorithm outperforms domain invariant approaches, matches the performance of the domainadaptive DA-ERM <ref type="bibr" target="#b12">[13]</ref> up to 0.1%, without domain labels. The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Clustering Ablations</head><p>Varying K <ref type="figure" target="#fig_1">(Figure 2A</ref>). We study the performance while changing the number of clusters K while keeping d start = n c and d end = ? d + n c fixed, where n c denotes the number of output classes for that dataset, and we try various ? d . We compare performance for K = n c , 2n c , 4n c , 8n c and   <ref type="table">Table 3</ref>. A comparison with various embedding approaches. AdaClust-Random refers to using random cluster embeddings (instead of nearest-neighbors), and AdaClust-NoPCA refers to clustering without using PCA. We see that both clustering and PCA are necessary for optimal adaptive clustering.</p><p>16n c averaged on the PACS and VLCS datasets. We observe a fairly stable performance increase up to K = 8n c , after which it deteriorates, as we believe the recovered embeddings are too noisy when the number of clusters is large.</p><p>Varying d start and d end ( <ref type="figure" target="#fig_1">Figure 2B</ref>). We examine the impact of the indices d start and d end on generalization. For any d start , d end pair, let ? d = d end ? d start denote the spectrum width. We set K = n c and vary , and for any fixed d start , we observe a small decrease as d end is increase beyond 512, potentially due to the addition of noisy features in later eigenvectors. We observe the best performance for all d start to be in the range 8 ? 128, which is in alignment with our hypothesis of introducing class-independent variance (see Section 5.5 for a qualitative analysis).</p><p>Varying Clustering Schedule (T clust ) ( <ref type="figure" target="#fig_1">Figure 2C</ref>). One expects that more rounds of clustering will improve performance at a higher clustering cost. To examine this, we compare 3 clustering schedules with K = n c , d start = 8, d end = 520 on the PACS dataset. We see no major differences in performance (validation accuracy within 0.2%) between T clust = O(T ) (cluster every epoch) and T clust = O(log T ) (doubling scheme with cost O(log T )). However, if we cluster only a constant number of times (i.e., T clust = O(1)) (where we perform clustering only once during the beginning, one halfway, and one at the end), we see very small improvements compared to baseline performances. The small deterioration due to a logarithmic T clust enables us to scale to large datasets without much difficulty.</p><p>Varying Clustering Algorithm. We compare the performance due to different clustering subroutines, as it is known that different clustering algorithms often partition identical data differently <ref type="bibr" target="#b33">[34]</ref>. We examine this effect by testing out 4 different clustering algorithms on the PACS dataset, while keeping K = n c , d start ? [0, 512, 1024], d end = d start + 512. We observe that kmeans++ clustering performs the best overall, whereas the other approaches (spectral clustering, agglomerative clustering and GMM) perform worse, and hence we select kmeans++. This is also backed by the convergence guarantees obtained for kmeans++ with minimal assumptions on the data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41]</ref>. The complete summary can be found in the Appendix. Random Embeddings and Removing PCA <ref type="table">(Table 3)</ref>. Finally, we compare two variants of the embeddings that are formed via other unsupervised approaches. We compare with random embeddings, i.e., select any domain embedding from the clustering at random (instead of the nearest), to examine the impact of additional cluster information. We also compare with a version where we do not run PCA, and cluster using the original features from F image . We observe that in both cases, the performance is significantly worse than that with Adaptive Clustering, which indicates  <ref type="table">Table 4</ref>. A comparison with vanilla ERM on different backbone networks. We see that the improvements with Adaptive Clustering improve as the expressivity of the backbone model increases.</p><p>that both the clustering, and PCA are crucial for success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Backbone Ablations</head><p>A central assumption in our approach is that we are pretraining from a large-scale model that contains the relevant information such that the domains are well-separated via ?. To examine this, we study the validation performance of our models on four different datasets: PACS, VLCS, OfficeHome and TerraIncognita, while we vary the backbone model used to train the network. The results are summarized in <ref type="table">Table 4</ref> for three models: ResNet-50 on ILSVRC12 data <ref type="bibr" target="#b17">[18]</ref>, the Microsoft Vision model <ref type="bibr" target="#b0">[1]</ref> that is trained on four distinct visual tasks, and the Facebook SWSL model <ref type="bibr" target="#b42">[43]</ref> that is weakly-supervised on 1B+ images. Since all models use the same architecture (ResNet-50), this comparison exclusively compares the large-scale nature of the problem. We observe an expected performance trend, with the Facebook SWSL model providing the best performance across datasets. More importantly, we see that compared with vanilla ERM, the relative improvement obtained by Adaptive Clustering increases with model size, highlighting that our algorithm is able to capture the relevant directions of variance residing in the pre-trained models. We set the hyperparameters as (8 ? 1032, 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Analysis</head><p>We hypothesize that the variance in fine-tuned model feature space ? follows the specific structure that the first few eigenvalues correspond to class-specific variance, followed by other structural variations, including domainspecific variance, followed eventually by noise inherited from the training process. We test this hypothesis with two sets of experiments. The first of these is measuring predictability of both the class and domain labels directly from the projected features ?. We consider d start ? [0, <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref> and fix K = N ? n c , d end = d start + 256, and train an MLP directly to predict the class and domain labels from ?, on the PACS dataset. We train a 1-layer MLP with SGD via leave-one-domain-out cross validation. Specifically, this looks at the alignment of features with all 256?dimensional "slices" of the spectrum. The results of this ablation are summarized in <ref type="figure" target="#fig_0">Figure 2(D1)</ref>. We observe that the domain predictive power of ? gradually increases as we remove the first few components, followed by a flat decline, whereas the class predictability of ? decreases dramatically as the window is shifted.</p><p>Next, to compare the clustering quality, we examine the normalized mutual information (NMI) <ref type="bibr" target="#b34">[35]</ref> to examine the overlap between class partitions and domain partitions on the PACS dataset (the network is trained on all domains). We report the renormalized NMI (since ? contains more information about the classes compared to domain labels) of the produced clusterings with both the class and domain labels. We observe that once again, as d start is shifted, the NMI with respect to domain partitions remains stable whereas the class NMI deteriorates rapdily, in line with our hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>Domain generalization is an increasingly relevant problem in real-world settings that contain data restrictions motivated by privacy, such as the one studied in this paper. Our central contributions in this regard are as follows. First, we demonstrate that even when domain labels are not available, one can leverage large-scale pre-trained models to bootstrap for the domain generalization problem, and obtain competitive performance. Next, we extend the analysis of domain generalization via kernel mean embeddings to handle approximate embedding spaces such as the one presented in this paper. On their own, our contributions can provide interesting starting points for forays into other relevant problems, such as multi-task and multi-agent learning.</p><p>Additionally, our contributions shed more light into the practical feasibility of domain-invariant learning: as discussed in <ref type="bibr" target="#b16">[17]</ref>, it is unclear whether explicitly modeling domain invariance outperforms naive ERM. Our research provides another argument in favor of naive ERM over modeling feature invariance, as we can see both experimentally and theoretically that whenever we have large pre-trained models to begin with, one can achieve competitive performance via ERM. Furthermore, as suggested by <ref type="bibr" target="#b12">[13]</ref>, invariant approaches are difficult to scale to large-scale benchmarks, given their careful model selection requirements.</p><p>There are many follow-up directions that our work presents. First, we only explore clustering to partition the training data, whereas one can consider alternative approaches to compute the pseudo-domain embedding, including random projection hashing <ref type="bibr" target="#b32">[33]</ref> or unsupervised random forests <ref type="bibr" target="#b30">[31]</ref> to compute paritions faster. Alternatively, one can explore utilizing domain labels (whenever applicable) to accelerate the cluster discovery process.</p><p>On the theoretical aspect, relaxing the expressivity assumption is a viable first step as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details for Domain-Specific Filtering</head><p>Consider an initial feature ? 0 (these can be features from a CNN pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>, for instance), which are gradually being fine-tuned on our training data D tr . One can assume that the pre-final layer activations at any epoch t, denoted as ? t will gradually adapt from ? 0 so that they are discriminatory with respect to the classification task. Alternatively stated, this implies that the largest principal components of ? t will gradually capture the class-specific variations within the data. Specifically, if the eigendecomposition of the feature ? is given as follows:</p><formula xml:id="formula_11">? t (?) = d i=1 ? t i ? e t i (?), V t S t V t = 1 n ? N ? 1 ? t (X) ? t (X).</formula><p>Where S t and V t are obtained by diagonalization, and S t is a diagonal matrix containing the eigenvalues of the scaled covariance matrix. Then, we compute the end-truncated eigenvectorsV t by only considering the middle [d start , d end ] columns of V t to create a matrixV t ? R d?(dend?dstart) . The "projected" data points can then be obtained as:</p><formula xml:id="formula_12">? t (X) = ? t (X)V t .</formula><p>Hence, any x can be projected by first computing its feature ? t (x) followed by a projection to get ? t (x) = ? t (x)V t . The central idea with this step is to recover the "Goldilocks zone" of useful domain-dependent variance, such that when d start and d end are selected appropriately, the projected space is a good separator of different domains. Since we initialize ? 0 via a pre-trained network, trained originally on a large-scale dataset such as ImageNet <ref type="bibr" target="#b9">[10]</ref>, we hope that the diverse information present in these datasets allows us to obtain a useful embedding. As we see in our experiments, the algorithm is not too sensitive to the specific choice of d start and d end , and is quite robust given a good starting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Theoretical Guarantees</head><p>In this section we provide some theoretical guarantees for our proposed algorithm. The theoretical results are within the framework of Blanchard et al. <ref type="bibr" target="#b5">[6]</ref>, where our key contributions are to analysing when the true kernel mean embeddings and obtained embeddings are mismatched. At a high level, our function class F is defined over the space X ? D (i.e., the joint input-domain space), however, we embed domains into R d via a mean embedding ? : D ? R d , and therefore, any function f ? F is written as f (x, ? D ), where ? D denotes the kernel mean embedding of D. Background. We assume a compact input space X and assume the outputs to lie in the space Y = [?1, 1]. For any L -Lipschitz loss function : R ? Y ? R + , then the empirical loss on any domain D with n samples is 1 ni (x,y)?D (f (x, ? D ), y). We can define the average training error over N domains with n samples as,</p><formula xml:id="formula_13">L N (f, ?) 1 n ? N i?[N ] (x,y)?Di (f (x, ? Di ), y).</formula><p>Similarly, our benchmark is to compare the above with the expected risk, i.e., the error obtained by f in the limit of infinite samples.</p><formula xml:id="formula_14">L(f, ?) E D?P E (x,y)?D [ (f (x, ? D ), y] .</formula><p>Following <ref type="bibr" target="#b5">[6]</ref>, the space F we consider is defined by product kernels. Consider a P.S.D. kernel ? over the product space D X ? X 1 with associated RKHS H ? . We select f ? such that</p><formula xml:id="formula_15">f ? = arg min f ?H? 1 nN N i=1 n j=1 (f (x ij , ?), y ij ) training error + ? ? f 2 H? regularization .</formula><p>Here, ? denotes the centroids obtained by pseudo-domain clustering, and ? is a kernel on D X ? X defined as,</p><formula xml:id="formula_16">?((x, ?), (x , ? )) = f ? (k P (?, ? ), k X (x, x )).</formula><p>Where K P and K X are kernels defined over P X and X respectively, and f ? is Lipschitz in both arguments, with constants L P and L X with respect to the first and second argument respectively. Moreover, K P is defined with the use of yet another kernel K that is a necessarily non-linear. For a feature vector ?, the kernel mean embedding ? D of a domain D is the image of D in the RKHS of a distinct kernel k X specified by ?, i.e.,</p><formula xml:id="formula_17">D ? ? D X k X (x, ?)dD X (x) = X ?(x)dD X (x).</formula><p>We assume k X , k X and K are bounded, i.e., k X (?, ?) ? B 2 k , k X (?, ?) ? B 2 k and K(?, ?) ? B 2 K . Results. Our key result is a uniform bound on the excess risk of using K pseudo-domain centroids ? instead of the true kernel mean embeddings ?, since computing the true kernel mean embeddings would require access to the domain labels. We introduce three constructs that are critical for our analysis. The first are the optimal centroids ? = {? k } K k=1 that are an optimal covering of the joint space X ? = {?(x)|x ? X } in expectation, i.e.,</p><formula xml:id="formula_18">? = arg min ??B d (1) K E D?P E x?D min k?[K] ?(x) ? ? [k] 2 .</formula><p>We denote the R.H.S. cost as C(?) for brevity. Note that since X ? ? B d (1) (as ?(?) 2 ? 1), C(? ) ? 3 K 1/d even in the worst case, by a simple covering argument. In a nutshell, ? denotes the best possible aggregated clustering. Next, we define the optimal domain-wise centroids, ? = {? k } K k=1 that denote the optimal covering of the data space partitioned by domains, i.e, ? = arg min</p><formula xml:id="formula_19">??B d (1) K E D?P min k?[K] E x?D [?(x)] ? ? [k] 2 .</formula><p>We denote the R.H.S. cost as C(?) for brevity. Note that these centroids are different from the ones earlier, since these consider the centroids that are closest to the domain-wise embeddings for each D ? D. Nevertheless, since D ? ? B d (1) (as ?(?) 2 ? 1), C( ? ) ? 3 K 1/d even in the worst case, by a simple covering argument. Finally, we define the K optimal sample centroids ? = { ? k } K k=1 for any set X = {x ij } N,n i,j that denote the optimal covering of the training data, i.e., ? = arg min</p><formula xml:id="formula_20">??B d (1) K N i=1 n j=1 min k?[K] ?(x ij ) ? ? [k] 2 .</formula><p>Note that all three of ? , ? and ? are independent of the clustering algorithms, and only depend on the data distribution P and feature ?. We now define some terms based on this notation.</p><p>Definition 1 (Bad Neighbors). We define any pair of points as bad neighbors if they belong to the same cluster in ? but not in ? or vice-versa. Specifically, if, for any point (x, D) we denote the nearest centroid in ? as ? x and nearest centroid in ? as? D , then any pair of points (x, D) and (x , D ) are bad neighbors if either of the following are true:</p><formula xml:id="formula_21">{? x = ? x and ? D = ? D } or {? x = ? x and ? D = ? D } .</formula><p>The above definition identifies points that lie within different domain clusters (or data clusters) but are in the same data cluster (or domain cluster), and we use this definiton to rigorously define how expressive the feature embedding ? is in separating the domains.</p><p>Assumption 3 (d ?Expressivity). Let P 0 denote the probability that two i.i.d. sampled points (x, D) and (x , D ) from P are bad neighbors. We then assume that the feature space ? is such that there exists a constant d d that satisfies,</p><formula xml:id="formula_22">C(? ) = O 1 K 1 d and P 0 = O ? P K 1 d , where ? P = E D?P E x?D [ ?(x) 2 2 ] 1/2 denotes the effective standard deviation of ?.</formula><p>Remark 2 (Expressivity). The expressivity condition is how we formalize the notion of domain-dependent variance present within the feature space ?. It implies that ? is able to cover the entire domain space with only d d dimensions under the distribution P. If ? is completely aligned with the domains themselves (i.e., we can easily separate the domains), we expect d ? 1, and in the worst case, d = d (i.e., no information).</p><p>Next, we present a standard assumption on the clustering approximation.</p><p>Assumption 4 (Cluster Sampling Approximation). We assume that there exist absolute constants C such that with probability at least 1 ? ?, ? ? (0, 1),</p><formula xml:id="formula_23">max k,k ?[K] ? ,[k] ? ? ,[k ] 2 ? C ? log nKN ? nN .</formula><p>This is a standard assumption can be satisfied by most practical data distributions; for a thorough treatment of statistical clustering, see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. Armed with these assumptions, we present our primary generalization bound. </p><formula xml:id="formula_24">E f = O K ?1 pd + ? + log( KN /?) n + log( nKN /?) N .</formula><p>Remark 3 (Discussion). The generalization bound above admits an identical dependency on the number of domains N and points per domain n as in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, and cannot be improved in general. We see an additional term K ? 1 d which can be decomposed as follows. We see that as K ? ? (we select a larger clustering), the additional term goes to 0. Its rate of decrease, however, depends on d , i.e., the effective dimensionality of ?. If ? contains ample information about P (or P is concentrated in ?), then we can expect d d (it is at most d by a covering bound). To the best of our knowledge, ours is the first analysis on domain-adaptive classification with kernel mean embeddings that considers the misspecification introduced by using approximate clustering solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Theorem 1</head><p>We begin by providing some assistive lemmas. Lemma 1. For any set of K centroids ? = {? 1 , ..., ? K }, ? ? R dK such that ?k ? [K], ? k 2 ? 1, and feature ?(?) ? R d with ?(?) 2 ? 1, let the expected and empirical clustering cost be given as,</p><formula xml:id="formula_25">C(?) = E D?P [E x?D [c(x, ?)]] , C(?) = 1 n ? N N i=1 n j=1 c(x ij , ?),</formula><p>where c(x, ?) = min k?[K] ?(x) ? ? k 2 . Then, we have with probability at least 1 ? ?,</p><formula xml:id="formula_26">C(?) ? C(?) ? 2 log(2N/?) n + 2 log(1/?) N .</formula><p>Proof. Observe that for any ?(x) ? B d (1) and fixed <ref type="bibr" target="#b1">2]</ref>, and hence c(x, ?) is sub-Gaussian with variance proxy at most 2. Since x are sampled i.i.d. from D, we have, by Hoeffding's inequality, for any domain D i with probability at least 1 ? ? ,</p><formula xml:id="formula_27">? ? B d (1) K , c(x, ?) = min k?[K] ?(x) ? ? k 2 is bounded in [0,</formula><formula xml:id="formula_28">E x?Di [c(x, ?)] ? 1 n n j=1 c(x ij , ?) + 2 log(1/? ) n .</formula><p>Furthermore, for any domain D, the variable E x?D [c(x, ?)] is also 2 sub-Gaussian (by the boundedness of c), and therefore, we have, with probability at least 1 ? ? ,</p><formula xml:id="formula_29">E D?P [E x?D [c(x, ?)]] ? 1 N N i=1 E x?Di [c(x, ?)] + 2 log(1/? ) N .</formula><p>We can bound the first term in the R.H.S. by taking a union bound over the first result and setting ? = ?/2N , ? = ?/2. We have, with probability at least 1 ? ?,</p><formula xml:id="formula_30">E D?P [E x?D [c(x, ?)]] ? 1 n ? N N i=1 n j=1 c(x ij , ?) + 2 log(2N/?) n + 2 log(1/?) N .</formula><p>Substituting shorthand notations provides us the result. We can derive the opposite direction in an identical manner.</p><p>Lemma 2. For any point x ? X , let ? x = arg min k?[K] ?(x) ? ? k 2 denote the optimal cluster it is closest to. Then we have that,</p><formula xml:id="formula_31">E D?P [E x?D [ ? x ? E x?D [?(x)] 2 ]] = O ? P K 1 d .</formula><p>Proof. For any domain D, let? D denote the centroid nearest to D from ?. We have that,</p><formula xml:id="formula_32">E D?P [E x?D [ ? x ? E x?D [?(x)] 2 ]] = E D?P E x?D ? x ?? D +? D ? E x?D [?(x)] 2 ? E D?P E x?D ? x ?? D 2 + E D?P E x?D ? D ? E x?D [?(x)] 2 ? E D?P E x?D ? x ?? D 2 + C K 1 d .</formula><p>The last inequality follows from Assumption 3. Now, to bound the first term, for any centroid ?, let X ? denote the partition of X it covers (and similarly for D). Observe that we can write the centroids ? x = y?X ?x xp(y) and</p><formula xml:id="formula_33">? D = D ?D? D ? D p(D )</formula><p>, as they are the average features within those regions. Substituting this gives us, for any</p><p>x, D ? X ? D, the inner term in the summation,</p><formula xml:id="formula_34">? x ?? D 2 = D ?D x ?X ?(x ) [1{x ? X ?x } ? 1{D ? D ? D }] p(x , D ) 2 ? D ?D x ?X ?(x ) [1{x ? X ?x } ? 1{D ? D ? D }] p(x , D ) 2 (Jensen) = D ?D x ?X ?(x ) 2 ? |1{x ? X ?x } ? 1{D ? D ? D }| p(x , D ) ? D ?D x ?X ?(x ) 2 p(x , D ) ? D ?D x ?X |1{x ? X ?x } ? 1{D ? D ? D }| p(x , D ) (Cauchy-Schwarz) = ? P ? D ?D x ?X 1{x ? X ?x ? D ? D ? D }p(x , D ) = ? P ? P x ,D {x ? X ?x ? D ? D ? D }.</formula><p>Replacing this in the expectation, we have that, for some absolute constant C , since x and x (resp. D and D ) are independent,</p><formula xml:id="formula_35">E D?P [E x?D [? P ? P x ,D {x ? X ?x ? D ? D ? D }]] = ? P ? P 0 ? ? P ? C K 1 d .</formula><p>Putting it together gives us the final result.</p><p>We are now ready to prove Theorem 1.</p><p>Proof. We begin by decomposing the LHS. L N (f, ? ) ? L N (f, ?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>We first bound the term 3 . For any K clustering, let the set of points assigned to cluster k be given by n k . Observe that, since the loss function L is Lipschitz in f , we have,</p><formula xml:id="formula_36">sup f ?B?(R) |L N (f, ? ) ?L N (f, ?)| ? L nN ? sup f ?B?(R) N i=1 n j=1 f (x ij , ? x ) ? f (x ij , ? x ) ? L nN K i=1 n k j=1 sup f ?B?(R) f (x ij , ? x ) ? f (x ij , ? x ) .</formula><p>Now, by the reproducing property of ? (and the corresponding RKHS), we have that for any x ? X that has an optimal empirical centroid ? x , and estimated empirical centroid ? x ,</p><formula xml:id="formula_37">sup f ?B?(R) f (x, ? x ) ? f (x, ? x ) ? f ? sup f ? (k P ( ? x , ?), k X (x, ?)) ? f ? (k P ( ? x , ?), k X (x, ?)) ? R ? sup f ? (k P ( ? x , ?), k X (x, ?)) ? f ? (k P ( ? x , ?), k X (x, ?)) (Since f ? ? R) ? RL P ? sup K( ? x , ?) ? K( ? x , ?) (Since f ? is Lipschitz) ? RL P ? sup ? K ( ? x ) ? ? K ( ? x ) (Triangle inequality) ? RL P ? sup ? x ? ? x ? .</formula><p>(1-H?lder assumption)</p><p>By replacing this result above, we have that, with probability at least 1 ? ?,</p><formula xml:id="formula_38">L nN K i=1 n k j=1 sup f ?B?(R) f (x ij , ? x ) ? f (x ij , ? x ) ? L nN RL P K i=1 n k j=1 ? x ? ? x ? = L RL P nN K i=1 n k ? ? x ? 1 n k n k j=1 ?(x ij ) ? ? L RL P nN K i=1 n k j=1 ? x ? ?(x ij ) 2 ( ? ? ? ? 2 )</formula><p>? L RL P ? C( ? ) (H?lder's inequality) ? L RL P ? C(?) ( ? minimizes C) ? L RL P ? C(?) + 2 log(2N/?) n + 2 log(1/?) N (Lemma 1) ? L RL P ? C K 1/d + 2 log(2N/?) n + 2 log(1/?) N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Assumption 3)</head><p>Now, we bound term 2 . By a similar decomposition as earlier, we have that with probability at least 1 ? ?,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Geo-YFCC Hyperparameters</head><p>We do a grid search on number of epochs in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25)</ref> and fixed best performance at 6 epochs. We have a batch size of 80 for all methods (AdaClust, ERM, DA-ERM, MMD, CORAL). We fix the learning rate as 0.04 and weight-decay of 1e ? 5 over a system of 64 GPUs. We tune the value of the loss weight in the range (0, 1.5) for MMD and CORAL. The reported results are with the loss weight (? = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Varying the Clustering Algorithm</head><p>The ablations on varying the clustering algorithm are summarized in <ref type="table">Table 6</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Training Pipeline. The shaded orange area is only run when the epoch t ? Tclust; we reuse previous presudo-domain embeddings otherwise. The paths in green denote where there are gradient flows, and the paths in red denote only feed-forward operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Ablation Studies. (A) Comparison of average performance on PACS and VLCS when varying the number of clusters K (dstart is fixed); (B) Comparison on PACS when varying the spectrum starting index dstart (K is fixed); (C) Varying the clustering schedule Tclust; (D) Qualitative ablations on predictability of ? and mutual information of clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 .</head><label>2</label><figDesc>Let ? and P be such that C(? ) = o K ? 1 d for d ? d, and Assumptions 3 and 4 are true. Let E f = L(f, ?) ? L N (f, ?) denote the generalization error for any f ? F = B ? (1). Then, with probability at least 1 ? ?, sup f ?F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>sup f ?B?(R) L(f, ?) ? L N (f, ?) = sup f ?B?(R) L(f, ?) ? L N (f, ?) + L N (f, ?) ? L N (f, ? ) + L N (f, ? ) ? L N (f, ?) ? sup f ?B?(R) L(f, ?) ? L N (f, ?) 1 + sup f ?B?(R) L N (f, ?) ? L N (f, ? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>sup f ?B?(R) L N (f, ?) ? L N (f, ? ) ? L nN RL P K i=1 n k j=1 ? x ? ? x ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Aggregated training data Dtr, clustering schedule T clust K : #clusters, dstart: starting component, d end : last component. Output. Classifier F image with weights ?, F MLP with weights W and clustering centroids { ? k } K k=1 . Initialize. Run 1 epoch of SGD using pre-trained ? 0 on training data Dtr and obtain ? 1 , discard final layer weights. Initialize new W. Network with weights ?, W and embeddings { ? k } K k=1 . Output. Prediction for any point x.</figDesc><table><row><cell>Algorithm 1 Training Pseudocode</cell></row><row><cell>TRAINING</cell></row><row><cell>Input. for Round t = 1 to T do</cell></row><row><cell>if t ? T clust then</cell></row><row><cell>// do clustering in this epoch</cell></row><row><cell>?t ? FEATURE EXTRACTION(?t, Dtr).</cell></row><row><cell>?t ? SVD + TRUNCATION(?t, dstart, d end ). { ? k } K k=1 , { D k } K k=1 ? CLUSTERING(?t, K).</cell></row><row><cell>Create augmented dataset.</cell></row><row><cell>end if</cell></row><row><cell>for each batch (x, ?x, y) do</cell></row><row><cell>?(x) ? F image (x; ?t). // compute image feature?</cell></row><row><cell>y ? F mlp (CONCAT(?(x), ?x); Wt). // compute predictions</cell></row><row><cell>Jt ? CROSSENTROPY(?, y). // compute loss</cell></row><row><cell>? t+1 , W t+1 ? SGD STEP(Jt, ?t, Wt). // gradient descent</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>INFERENCE</cell></row><row><cell>Input.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">? 0.4 82.6 ? 0.6</cell><cell>67.7 ? 0.3</cell><cell>42.2 ? 0.6</cell><cell>46.0 ? 0.4</cell><cell>63.0 ? 0.4</cell></row><row><cell>CORAL [37]</cell><cell cols="2">77.3 ? 0.3 83.3 ? 0.5</cell><cell>68.6 ? 0.1</cell><cell>42.1 ? 0.7</cell><cell>47.7 ? 0.3</cell><cell>63.8 ? 0.3</cell></row><row><cell>MMD [25]</cell><cell cols="2">76.1 ? 0.7 83.1 ? 0.7</cell><cell>67.3 ? 0.2</cell><cell>38.7 ? 0.5</cell><cell>45.8 ? 0.6</cell><cell>62.2 ? 0.6</cell></row><row><cell>C-DANN [26]</cell><cell cols="2">73.8 ? 1.1 81.4 ? 1.3</cell><cell>64.2 ? 0.5</cell><cell>39.5 ? 0.2</cell><cell>40.9 ? 0.7</cell><cell>60.1 ? 0.9</cell></row><row><cell>Mixup [40]</cell><cell cols="2">78.2 ? 0.6 83.9 ? 0.8</cell><cell>68.3 ? 0.4</cell><cell>40.2 ? 0.4</cell><cell>46.2 ? 0.5</cell><cell>63.3 ? 0.7</cell></row><row><cell>DA-ERM [13]</cell><cell cols="2">78.0 ? 0.2 84.1 ? 0.5</cell><cell>67.9 ? 0.4</cell><cell>43.6 ? 0.3</cell><cell>47.3 ? 0.5</cell><cell>64.1 ? 0.8</cell></row><row><cell></cell><cell cols="4">Algorithms that do not require domain labels</cell><cell></cell><cell></cell></row><row><cell>ERM [17]</cell><cell cols="2">76.7 ? 0.9 83.2 ? 0.7</cell><cell>67.2 ? 0.5</cell><cell>41.1 ? 0.8</cell><cell>46.2 ? 0.3</cell><cell>62.9 ? 0.6</cell></row><row><cell>IRM [2]</cell><cell cols="2">76.9 ? 0.5 82.8 ? 0.5</cell><cell>67.0 ? 0.4</cell><cell>35.7 ? 1.9</cell><cell>43.8 ? 1.0</cell><cell>61.2 ? 0.8</cell></row><row><cell>DRO [36]</cell><cell cols="2">77.3 ? 0.2 83.3 ? 0.3</cell><cell>66.8 ? 0.1</cell><cell>33.0 ? 0.5</cell><cell>42.0 ? 0.6</cell><cell>60.4 ? 0.4</cell></row><row><cell>RSC [19]</cell><cell cols="2">75.3 ? 0.5 87.7 ? 0.2</cell><cell>64.1 ? 0.4</cell><cell>41.2 ? 1.0</cell><cell>45.7 ? 0.3</cell><cell>62.8 ? 0.6</cell></row><row><cell cols="3">AdaClust(512 ? 1024, 5) 78.9 ? 0.6 87.0 ? 0.3</cell><cell>67.7 ? 0.5</cell><cell>43.3 ? 0.5</cell><cell>48.1 ? 0.1</cell><cell>64.9 ? 0.7</cell></row><row><cell></cell><cell cols="5">Model Selection: Validation-Based Model Averaging (SWAD) [7]</cell><cell></cell></row><row><cell>ERM</cell><cell>78.8 ? 0.1</cell><cell>87.8? 0.2</cell><cell>69.8 ? 0.1</cell><cell>46.0 ? 0.1</cell><cell>50.2 ? 0.3</cell><cell>66.5 ? 0.2</cell></row><row><cell>AdaClust(8 ? 520, 5)</cell><cell cols="2">79.6 ? 0.1 89.2 ? 0.4</cell><cell>69.4 ? 0.2</cell><cell>46.7 ? 0.2</cell><cell>50.6 ? 0.1</cell><cell>67.2 ? 0.2</cell></row></table><note>. Benchmark Comparisons. All implementations are using DOMAINBED [17]. Experiments are repeated thrice with random seeds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>? 1032, 1) 28.4 / 56.2 23.4 / 48.9 Comparison on the Geo-YFCC [13] dataset. AdaClust matches the performance of DA-ERM without domain labels.</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell>Train Top-1/5</cell><cell cols="2">Test Top-1/5</cell><cell></cell></row><row><cell cols="4">Algorithms Using Domain Labels</cell><cell></cell><cell></cell></row><row><cell>MMD</cell><cell></cell><cell cols="3">25.4 / 50.9 21.8 / 46.2</cell><cell></cell></row><row><cell>CORAL [37]</cell><cell></cell><cell cols="3">25.4 / 50.9 21.7 / 46.2</cell><cell></cell></row><row><cell>DA-ERM [13]</cell><cell></cell><cell cols="3">28.2 / 55.9 23.5 / 49.0</cell><cell></cell></row><row><cell cols="4">Algorithms without Domain Labels</cell><cell></cell><cell></cell></row><row><cell>ERM</cell><cell></cell><cell cols="3">28.4 / 56.4 22.5 / 48.1</cell><cell></cell></row><row><cell>AdaClust(8 Algorithm</cell><cell cols="2">VLCS PACS</cell><cell>OH</cell><cell>TI</cell><cell>Avg.</cell></row><row><cell>ERM</cell><cell>77.4</cell><cell>84.0</cell><cell>64.8</cell><cell>46.0</cell><cell>68.0</cell></row><row><cell>AdaClust-Random</cell><cell>76.5</cell><cell>83.1</cell><cell>63.6</cell><cell>45.2</cell><cell>67.1</cell></row><row><cell>AdaClust-NoPCA</cell><cell>77.5</cell><cell>84.2</cell><cell>65.3</cell><cell>46.9</cell><cell>68.5</cell></row><row><cell>AdaClust(8 ? 1032, 5)</cell><cell>78.2</cell><cell>86.2</cell><cell>65.2</cell><cell>48.1</cell><cell>69.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 6. A comparison with various clustering approaches.</figDesc><table><row><cell></cell><cell>Algorithm</cell><cell cols="2">VLCS PACS</cell><cell>OH</cell><cell>TI</cell><cell>Avg.</cell></row><row><cell></cell><cell>ERM</cell><cell>77.4</cell><cell>84.0</cell><cell cols="2">64.8 46.0</cell><cell>68.0</cell></row><row><cell></cell><cell>AdaClust-GMM</cell><cell>76.8</cell><cell>84.9</cell><cell cols="2">64.8 47.0</cell><cell>68.3</cell></row><row><cell></cell><cell>AdaClust-Agglomerative</cell><cell>77.8</cell><cell>85.7</cell><cell cols="2">65.9 46.3</cell><cell>68.9</cell></row><row><cell></cell><cell>AdaClust(k-means++)</cell><cell>78.2</cell><cell>86.2</cell><cell cols="2">65.2 48.1</cell><cell>69.4</cell></row><row><cell>Condition</cell><cell>Parameter</cell><cell></cell><cell cols="3">Default value Random distribution</cell></row><row><cell></cell><cell>learning rate</cell><cell></cell><cell cols="2">0.00005</cell><cell>10 Uniform(?5,?3.5)</cell></row><row><cell>Basic hyperparameters</cell><cell>batch size</cell><cell></cell><cell>32</cell><cell></cell><cell>2 Uniform(3,5.5)</cell></row><row><cell></cell><cell>weight decay</cell><cell></cell><cell>0</cell><cell></cell><cell>10 Uniform(?6,?2)</cell></row><row><cell></cell><cell>lambda</cell><cell></cell><cell>1.0</cell><cell></cell><cell>10 Uniform(?2,2)</cell></row><row><cell></cell><cell cols="2">generator learning rate</cell><cell cols="2">0.00005</cell><cell>10 Uniform(?5,?3.5)</cell></row><row><cell></cell><cell cols="2">generator weight decay</cell><cell>0</cell><cell></cell><cell>10 Uniform(?6,?2)</cell></row><row><cell>C-DANN</cell><cell cols="2">discriminator learning rate discriminator weight decay</cell><cell cols="2">0.00005 0</cell><cell>10 Uniform(?5,?3.5) 10 Uniform(?6,?2)</cell></row><row><cell></cell><cell>discriminator steps</cell><cell></cell><cell>1</cell><cell></cell><cell>2 Uniform(0,3)</cell></row><row><cell></cell><cell>gradient penalty</cell><cell></cell><cell>0</cell><cell></cell><cell>10 Uniform(?2,1)</cell></row><row><cell></cell><cell>adam ? 1</cell><cell></cell><cell>0.5</cell><cell></cell><cell>RandomChoice([0, 0.5])</cell></row><row><cell>IRM</cell><cell cols="3">lambda iterations of penalty annealing 500 100</cell><cell></cell><cell>10 Uniform(?1,5) 10 Uniform(0,4)</cell></row><row><cell>Mixup</cell><cell>alpha</cell><cell></cell><cell>0.2</cell><cell></cell><cell>10 Uniform(0,4)</cell></row><row><cell>DRO</cell><cell>eta</cell><cell></cell><cell>0.01</cell><cell></cell><cell>10 Uniform(?1,1)</cell></row><row><cell>MMD</cell><cell>gamma</cell><cell></cell><cell>1</cell><cell></cell><cell>10 Uniform(?1,1)</cell></row><row><cell>MLDG</cell><cell>beta</cell><cell></cell><cell>1</cell><cell></cell><cell>10 Uniform(?1,1)</cell></row><row><cell>all</cell><cell>dropout</cell><cell></cell><cell>0</cell><cell></cell><cell>RandomChoice([0, 0.1, 0.5])</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>DOMAINBED hyperparameters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We assume there exist sets of probability distributions D X and D Y|X such that for any sample D ? D there exist samples D X ? D X and D Y |X ? D Y|X such that D = D X ? D Y |X (this characterization is applicable under suitable assumptions, see Section 3 of<ref type="bibr" target="#b5">[6]</ref>).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(Assumption 4)</p><p>Finally, we arrive to bounding term 1 . This term can further be decomposed as follows.</p><p>Here, the first term A measures the difference in expected error when f uses ? instead of the true kernel mean embeddings ?. We see that this can be bound by Assumption 3.</p><p>(1-H?lder assumption) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DomainBed Hyperparameters</head><p>We follow Gulrajani and Lopez-Paz <ref type="bibr" target="#b16">[17]</ref> for hyperparameters. These values are summarized in <ref type="table">Table 5</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">microsoftvision: Downloads pretrained Microsoft Vision models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A framework for statistical clustering with a constant time approximation algorithms for k-median clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Swad: Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08604</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">k-means++: few more steps yield constant approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davin</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Grunau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Portmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?clav</forename><surname>Rozhon</surname></persName>
		</author>
		<idno>PMLR, 2020. 7</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1909" to="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A causal framework for distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rune</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">Emil</forename><surname>Jakobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Gnecco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task learning for contextual bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urun</forename><surname>Aniket Anand Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clay</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4848" to="4856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Provably efficient cooperative multi-agent reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04972</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive methods for real-world domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<title level="m">search of lost domain generalization</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="124" to="140" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><forename type="middle">Nitin</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
		<title level="m">Rachel Cummings, et al. Advances and open problems in federated learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03463</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Domain generalization via conditional invariant representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08479</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards a statistical theory of clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PASCAL workshop on Statistics and Optimization of Clustering</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Domain generalization using causal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07500</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09522</idno>
		<title level="m">Bharath Sriperumbudur, and Bernhard Sch?lkopf. Kernel mean embedding of distributions: A review and beyond</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised random forest manifold alignment for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuru</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random features for largescale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining and knowledge discovery handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="269" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maimon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining and knowledge discovery handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="321" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worstcase generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">(IEEE) Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Heterogeneous domain generalization via domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A constant-factor bi-criteria approximation guarantee for k-means++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="604" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improve unsupervised domain adaptation with mixup training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
