<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NODIS: Neural Ordinary Differential Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuren</forename><surname>Cong</surname></persName>
							<email>cong@tnt.uni-hanover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Processing</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Ackermann</surname></persName>
							<email>ackermann@tnt.uni-hanover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Processing</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
							<email>liao@tnt.uni-hanover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Processing</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
							<email>michael.yang@utwente.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Scene Understanding Group</orgName>
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<email>rosenhahn@tnt.uni-hanover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Processing</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NODIS: Neural Ordinary Differential Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Image Understanding</term>
					<term>Scene Graph</term>
					<term>Visual Rela- tionship Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic image understanding is a challenging topic in computer vision. It requires to detect all objects in an image, but also to identify all the relations between them. Detected objects, their labels and the discovered relations can be used to construct a scene graph which provides an abstract semantic interpretation of an image. In previous works, relations were identified by solving an assignment problem formulated as (Mixed-)Integer Linear Programs. In this work, we interpret that formulation as Ordinary Differential Equation (ODE). The proposed architecture performs scene graph inference by solving a neural variant of an ODE by end-to-end learning. The connection between (Mixed-)Integer Linear Program and ODEs in combination with the endto-end training amounts to learning how to solve assignment problems with image-specific objective functions. Intuitive, visual explanations are provided for the role of the single free variable of the ODE modules which are associated with time in many natural processes. The proposed model achieves results equal to or above state-of-the-art on all three benchmark tasks: scene graph generation (SGGEN), classification (SGCLS) and visual relationship detection (PREDCLS) on Visual Genome benchmark. The strong results on scene graph classification support the claim that assignment problems can indeed be solved by neural ODEs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper investigates the problem of semantic image understanding. Given an image, the objective is to detect objects within, label them and infer the relations which might exist between objects. These data provide rich semantic information about the image content. So called scene graphs contain all these information and constitute abstract representations of images <ref type="bibr" target="#b14">[15]</ref>. Nodes of a scene graph represent objects detected in an image, while edges represent relationships between objects. Applications range from image retrieval <ref type="bibr" target="#b14">[15]</ref> to high-level vision tasks such as visual question answering <ref type="bibr" target="#b38">[39]</ref>, image captioning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44]</ref> and visual arXiv:2001.04735v3 [cs.CV] 18 Jul 2020 <ref type="figure">Fig. 1</ref>: Visualization of the main contributions made in this work. Left: state-ofthe-art works rely on multiple embedding layers which amounts to an evenlyspaced, discrete sampling. Right: the proposed module relies on ordinary differential equations (ODE), thus it learns a continuous approximation of the trajectory onto the embedding manifolds.</p><p>reasoning <ref type="bibr" target="#b34">[35]</ref>. The community has been very active in the past years to investigate these problems. Results on public benchmarks such as the Visual Genome database <ref type="bibr" target="#b19">[20]</ref> have improved drastically within the past few years <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>A naive approach to infer scene graphs is to use a standard object detector, classify the objects, and use a separate network to label the relationships between the objects. This, however, ignores the conditional information shared between the objects. It has therefore become standard practice to also use relationship information for object classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Scene graph estimation relies on two classification problems, both for objects and for relationships. Classification, or more generally, assignment problems can be solved by (Mixed) Integer Linear Programs, (M)ILP, for instance in <ref type="bibr" target="#b35">[36]</ref>. Powerful solvers are able to even infer globally optimal solutions for many types of problems. A drawback of (M)ILPs is that the optimization objective and all constraints must be defined in advance. Objective functions usually include multiple terms and weights per term. Furthermore, backpropagation through an (M)ILP solver is not possible, thus such pipelines cannot adapt to data.</p><p>For many other problems in which adaption to data is less important, (M)ILPs have been successfully employed. In <ref type="bibr" target="#b7">[8]</ref>, network flows and transport problem are modeled by Partial Differential Equations (PDEs). The arising systems of PDEs are transformed to systems of Ordinary Differential Equations (ODEs) by finite differences. Adding constraints such as minimal or maximal capacities, integral constraints and an objective, for instance energy minimization, and further applying a piece-wise linear approximation, a solution of the ODE can be obtained by means of an (M)ILP. That implies that for properly constructed (M)ILPs there are systems of ODEs which include the solution of the (M)ILPs, since dropping the optimization objective -thereby switching from an optimization problem to solving an equation system -only increases the number of solutions. While backpropagating through an (M)ILP is generally not possible, backpropa-gation through an ODE is possible using a recent seminal work Neural Ordinary Differential Equations <ref type="bibr" target="#b2">[3]</ref>.</p><p>Using this link, we propose to solve the labeling problem by means of neural ordinary differential equations. This module can be interpreted such that it can learn to perform the same task as an (M)ILP. Since it allows end-to-end training, it adapts to the data, thus it learns a near-optimal objective per image. A further question is which role the time-variable of the ODE plays in our architecture. We will provide visual explanations of that variable, namely that it controls how many objects are classified and how many relationships are classified (cf. <ref type="figure">Figs. 1 and 4</ref>). In our experimental evaluation, we demonstrate results that are equal to state-of-the-art or above in all three benchmark problems: scene graph generation (SGGEN), scene graph classification (SGCLS) and visual relationship prediction (PREDCLS).</p><p>The contributions made in this work can be summarized as follows: (1) We propose to use ODE modules for scene graph generation. (2) It can be interpreted that it learns the optimal assignment function per image. (3) Intuitive, visual explanations are provided for the role of the single free variable of the ODE modules which are associated with time in many natural processes. <ref type="bibr" target="#b3">(4)</ref> The proposed method achieves state-of-the-art results. Our code is published on GitHub 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Context for Visual Reasoning: Context has been used in semantic image understanding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. For scene graph generation, context information has been recently proposed and is still being investigated. Message passing has been used to include object context in images in several works, for instance by graphical models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b41">42]</ref>, by recurrent neural networks (RNN) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b39">40]</ref>, or by an iterative refinement process <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19]</ref>. Context from language priors <ref type="bibr" target="#b29">[30]</ref> has been proved to be helpful for visual relationships detection and scene graph generation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b24">25]</ref>. Scene Graph Generation: Scene graphs are proposed in <ref type="bibr" target="#b14">[15]</ref> for the task of image retrieval and also potential for many applications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18]</ref>. They consist of not only detected objects but also the object classes and the relationships between the detected objects. The estimation of scene graphs from images is attracting more and more attention in computer vision <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b13">14]</ref>. Several of these methods use message passing to capture the context of the two related objects <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40]</ref>, or of the objects and their relationships <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>. The general pipeline for message passing is to train some shared matrices to transform the source features into a semantic domain and then to assemble them to update the target features. In some works, an attention mechanism is implemented to weight the propagated information to achieve further improvement <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>. Graph CNNs <ref type="bibr" target="#b15">[16]</ref> have been used to propagate information between object and relationship nodes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4]</ref>. Some works also introduce generative models for scene understanding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. Contrastive Training: Contrastive losses <ref type="bibr" target="#b0">[1]</ref> have been applied for scene understanding <ref type="bibr" target="#b49">[50]</ref>. They have also been applied for image captioning, visual question answering (VQA) and vector embeddings <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr">34]</ref>. They are based on the idea that it can be easier to randomly select similar and dissimilar samples. Such losses can then be used even if no label information is available. Losses: In Visual Genome <ref type="bibr" target="#b19">[20]</ref>, many semantically meaningful relations are not labeled. Furthermore, relations that can have multiple labels, for instance on and sit, are usually labeled only once. Networks for visual relationship detection which use cross-entropy as training loss may encounter difficulties during training, since almost identical pairs of objects can have either label, or even none at all. To overcome the problem of such contradicting label information, margin-based <ref type="bibr" target="#b20">[21]</ref> and contrastive <ref type="bibr" target="#b49">[50]</ref> losses have been proposed. Support Inference: Inferring physical support, i.e. which structures for instance carry others, e.g. floor ? table, was investigated for object segmentation <ref type="bibr" target="#b35">[36]</ref>, instance segmentation <ref type="bibr" target="#b51">[52]</ref>, and also for scene graph inference <ref type="bibr" target="#b42">[43]</ref>. Proposed model: The proposed model uses neither iterations, except for LSTMcells, nor message passing, nor a Graph CNN. As most recent works do, language priors are included. We further use a standard loss based on cross-entropy. Unlike most state-of-the-art algorithms, we propose to use a new module which has never before been used for semantic image analysis. The ODE module <ref type="bibr" target="#b2">[3]</ref> can be interpreted as learning a deep residual model. In contrast to residual networks, the ODE-module continuously models its solutions according to a pre-defined, problem-specific precision.</p><p>This idea is motivated by works on gas, water, energy and transport networks which need to be modeled by systems of Partial Differential Equations (PDE). In <ref type="bibr" target="#b7">[8]</ref>, it was proposed to simplify such systems to obtain systems of Ordinary Differential Equations (ODE), add one or multiple optimization objectives and then use a Mixed-Integer Linear Program to compute the solution. That implies that, given an (M)ILP, we can always find a system of ODEs that can generate the same solution among others. Using a neural ODE <ref type="bibr" target="#b2">[3]</ref>, we can thus learn a function of the system of ODEs. Due to the end-to-end training, that system further encodes near-optimal assignment functions per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first define scene graphs, then we define straight-forward ILP models for object and relationship classification in Sec. 3.2. Neural Ordinary Differential Equations <ref type="bibr" target="#b2">[3]</ref> are briefly explained in Sec. 3.3. Both models are combined in Sec. 3.4 and 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scene Graphs</head><p>A scene graph <ref type="bibr" target="#b14">[15]</ref> is an abstract semantic representation of all objects in an image and the relationships between them. The set of objects consists of a set of bounding boxes along with a label for each object. The relationships between two objects, also known as predicates, consists of bounding boxes which usually cover both object bounding boxes, and labels for each relation. As in <ref type="bibr" target="#b48">[49]</ref>, we include a particular predicate to indicate that there is no relation between two objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models for Object and Relationship Detection</head><p>Assume a graph G obj = (U obj , E obj ) whose nodes represent the detected yet unlabeled objects in an image. Each node is further assigned to a label l ? L obj . Each label l is associated with a score ? u,l with u ? U . That score can represent an agreement with some given feature map. The integer variables x u,l ? {0, 1} indicate that node u is given label l. We assume that each object can only belong to a single class.</p><p>We further include a term ? u,u ,l,l which models statistical prior knowledge about co-occurrences of object pairs u and u , u = u , and their corresponding labels l and l . We then arrive at the following Integer Linear Problem (ILP) for object classification</p><formula xml:id="formula_0">max u?U obj l?L obj ? u,l x u,l + w obj u, u ?U obj l, l ? u,u ,l,l x u,l x u ,l (1a) s.t. x u,l ? {0, 1}, l?L obj x u,l ? 1 (1b)</formula><p>where w obj is a scalar weight which balances the two terms. How to exactly determine the scores ? u,l , ? u,u ,l,l and the weight w obj constitutes a hyper-parameter search. The large search space makes this problem by itself challenging. Some of the parameters remain invariant for all images. Assume a further graph G pred = (V pred , E pred ) whose nodes represent all possible subject-object pairs. Each node is assigned to a label k ? L pred . Each label k is associated with a score ? v,k with v ? V pred . The integer variables x v,k ? {0, 1} indicate that node v is given label k, for instance the subjectobject pair dog-street with a label sit or walk. The number of labels per node is limited by T v , and the total number of labels by K. The latter arises from the recall-at-K metric commonly used in visual relationship detection and scene graph estimation tasks.</p><p>Here, we also include a term ? v,v ,k,k which models statistical prior knowledge about co-occurrences of subject-object pairs v with pairs v , v = v , and their corresponding relationship labels k and k . We then arrive at the following Integer Linear Problem (ILP) for relationship classification</p><formula xml:id="formula_1">max v?V pred k?L pred ? v,k x v,k + w pred v, v ?V pred k, k ? v,v ,k,k x v,k x v ,k (2a) s.t. x v,k ? {0, 1}, k?L x v,k ? T v , v?V pred k?L pred x v,k ? K,<label>(2b)</label></formula><p>where w pred is a scalar weight which balances the two terms. Denote by ?(t) the function in the single variable t that assigns continuous scalar weights which express label strengths. The values that the label weights take on vary with t. Likewise, let ?(t) be a function in t that assigns scalar weights according to co-occurrence. From <ref type="bibr" target="#b7">[8]</ref>, we can see that the optimization problems defined by Eqs. <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> correspond to ordinary differential equation (ODE) models defined by</p><formula xml:id="formula_2">d dt f = f (x(t), t).<label>(3)</label></formula><p>In other words, we can use Eqs. <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> to obtain a solution of problem (3) subject to several constraints. A disadvantage of the optimization problem in Eqs. <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> is that it is not possible to backpropagate through the ILPs. Thus, feature-generating neural networks cannot be trained to adapt to given data when using the ILPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Ordinary Differential Equations [3]</head><p>For many problems it is hard to explicitly define functions ?(t) and ?(t). In <ref type="bibr" target="#b2">[3]</ref>, it was proposed to parameterize f (t) by a function f ? (t) based on neural network with parameters ?. The idea is then to use f ? (t) to solve an ODE. Thus, starting with an input x(0) at time t = 0, the output at time T can be computed by a standard ODE-solver</p><formula xml:id="formula_3">d dt f ? = f ? ( x(t), t) .<label>(4)</label></formula><p>The dynamics of the system can thereby be approximated up to the required precision. Importantly, model states at times 0 &lt; t &lt; T need not be evenly spaced.</p><p>For back-propagation, the derivatives for the adjoint a(t) = ??L/?x(t)</p><formula xml:id="formula_4">d dt a = ?a(t) ? ?t f ( x(t), t, ? )<label>(5)</label></formula><p>and</p><formula xml:id="formula_5">d d? L = 0 T a(t) ? ?? f ( x(t), t, ? ) dt<label>(6)</label></formula><p>have to be computed for loss L. This computation, along with the reverse computation of x(t) starting at time T can be done by a single call of the ODE-solver backwards in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Assignments by Neural Ordinary Differential Equations</head><p>While Eqs. <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> can be used to obtain solutions of problem <ref type="formula" target="#formula_2">(3)</ref>, this does not allow to train previous modules if used in a neural network. Furthermore, the hyper-parameters of the ILP must be determined in advance and cannot be easily adapted to the data. On the other hand, we can construct an ODE from Eqs. <ref type="bibr" target="#b0">(1)</ref> or <ref type="formula">(2)</ref>, respectively. Since a manual construction would be hard, we can use a neural ODE layer to learn the optimal assignment function. In contrast to an ILP, the neural ODE both allows to train a feature generating network in front of it, and can adapt to each image. The latter implies that the hyper-parameters of the corresponding, yet unknown ILP vary with each image. ODEs involve the variable t which is usually associated with time in many natural processes. Here, we are posed with the question what this variable represents for object or relationship classification. It will be demonstrated in Sec. 4.4 that the two variables of two neural ODE layers control how many objects and relationships are classified correctly (cf. <ref type="figure">Fig. 4</ref>). In other words, specifying particular values of the variable of the relationship module determines the connectivity of estimated scene graph, whereas such values of the object layer determine if too few or too many objects are correctly labeled.</p><p>As outlined in <ref type="figure" target="#fig_0">Fig. 2</ref>, we use two separate ODE layers. The visual features of detected objects, their positional features and the prior object distributions are concatenated into a vector x v and processed by an ODE layer in the object classifier</p><formula xml:id="formula_6">d dt 1 f ?u = f ?u ( x u (t 1 ), t 1 ) .<label>(7)</label></formula><p>In the following, this ODE layer will be denoted by O-ODE.</p><p>The word embedding resulting from the object classifier, the spatial mask with the union box and the visual features of two detected and classified object are concatenated and pre-processed to yield vector x v,v before being processed by an ODE layer for predicate classification (P-ODE )</p><formula xml:id="formula_7">d dt 2 f ?v = f ?v ( x v,v (t 2 ), t 2 ) .<label>(8)</label></formula><p>The variables t 1 and t 2 in Eqs. <ref type="bibr" target="#b6">(7)</ref> and (8) control how many objects or relations are labeled. In other words, graphs constructed using different t and t , either for Eq. <ref type="bibr" target="#b6">(7)</ref> or <ref type="bibr" target="#b7">(8)</ref>, result in scene graphs with differently many objects or relations correctly labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Architecture</head><p>Our model is built on Faster-RCNN <ref type="bibr" target="#b32">[33]</ref> which provides proposal boxes, feature maps and primary object distributions. There are two fundamental modules in the model: object classifier and predicate classifier. Each of them contains a neural ordinary differential equation layer, Object ODE (O-ODE) and Predicate ODE (P-ODE). For both of them we use bidirectional LSTMs as the approximate function in the ODE solver. The data in the model are organized as sequences with random order.</p><p>In the object classifier, the feature maps, bounding box information and primary object distribution from Faster-RCNN are concatenated and fed through a fully connection layer. The object class scores are computed by the O-ODE and a following linear layer. The predicate classifier uses feature maps, spatial and semantic information of object pairs to predict the predicate categories. The spatial masks are forwarded into a group of convolution layers so that the output has the same size as the feature maps of the union box (512 ? 7 ? 7) and can be added per element. Global average pooling is applied on the feature maps of the subject box, object box and union box. The features of the subject, object, and union boxes are concatenated as (3 ? 512)-dimensional visual vectors. Two 200-dimensional semantic embedding vectors are generated from the subject and object classes predicted by the object classifier and concatenated as 400dimensional semantic vectors.</p><p>The visual vectors and semantic vectors of object pairs can be pre-processed by three methods before the P-ODE: FC-Layer: The (3 ? 512)-dimensional visual vectors and 400-dimensional semantic vectors are forwarded into two independent fully connection layers that both have 512 neurons. Then, the outputs are concatenated together as 1024-dimensional representation vectors for the P-ODE. GCNN: The visual vectors and semantic vectors are first concatenated. Then, we use a graph convolutional neural network (GCNN) to infer information about context. Since the number of object pairs in each image is variable, we set each element on the diagonal of the adjacency matrix to 0.8. The weight of 0.2 is uniformly distributed among the remaining entries of each row. The output vectors of the GCNN are passed into the P-ODE. LSTM: Similar as for the first variant, the (3 ? 512)-dimensional visual vectors and 400-dimensional semantic vectors are fed into two single layer LSTMs. Both of them have the output dimension 512. We concatenate the two outputs for the P-ODE.</p><p>The final class scores of the relations are computed by the P-ODE followed by two linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we firstly clarify the experimental settings and implementation details. Then, we show quantitative and qualitative results on the Visual Genome (VG) benchmark dataset <ref type="bibr" target="#b19">[20]</ref> in terms of scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset, Settings, and Evaluation</head><p>Dataset We validated our methods on the VG benchmark dataset <ref type="bibr" target="#b19">[20]</ref> for the task of scene graph generation. However, there are varying data pre-processing strategies and dataset splits in different works. For fair comparison, we adopted the data split as described in <ref type="bibr" target="#b40">[41]</ref> which is the most widely used. According to the data pre-processing strategy, the most-frequent 150 object categories and 50 predicate types are selected. The training set has 75651 images while the test set has 32422 images.</p><p>Settings For comparison with prior works, we use Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> with VGG16 <ref type="bibr" target="#b36">[37]</ref> as the backbone network for proposing object candidates and extracting visual features. We adopted the code base and a pre-trained model provided by <ref type="bibr" target="#b48">[49]</ref>. As in NeuralMotifs <ref type="bibr" target="#b48">[49]</ref>, the input image is resized to 592 ? 592, bounding box scales and dimension ratios are scaled, and a ROIAlign layer <ref type="bibr" target="#b10">[11]</ref> is used to extract features within the boxes of object proposals and the union boxes of object pairs from the shared feature maps.</p><p>Evaluation There are three standard experiment settings for evaluating the performance of scene graph generation: (1) Predicate classification (PREDCLS): predict relationship labels of object pairs given ground truth bounding boxes and labels of objects. (2) Scene graph classification (SGCLS): given ground truth bounding boxes of objects, predict object labels and relationship labels.</p><p>(3) Scene graph detection (SGGEN): predict boxes, labels of object proposals and relation labels of object pairs given an image. Only when the labels of the subject, relation, and object are correctly classified, and the boxes of subject and object have more than 50% intersection-over-union (IoU) with the ground truth, it is counted as a correctly detected entity. The most widely adopted re-call@K metrics (K = <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">100]</ref>) for relations are used to evaluate the system performance.</p><p>Training We train our model with the sum of the cross entropy losses for objects and predicates. We collect all annotated relationships in the image and add negative relationships so that the relation sequences in the batch have identical length if there are sufficiently many ground truth boxes. We randomly select one if the object pairs are annotated with multiple relationships. For SGCLS and SGGEN, ground truth object labels are provided to the semantic part at the training stage. For fair comparison we use the same pre-trained Faster-RCNN as <ref type="bibr" target="#b48">[49]</ref> and freeze its parameters. An ADAM optimizer was used with batch size 6, initial learning rate 10 ?4 , and cross-entropy as loss both for object and relationship classification. We choose dopri5 as ODE solver and set the absolute tolerance atol = 0.01 and the relative tolerance rtol = 0.01. The integral time t end is set to <ref type="bibr">1.5 (cf. Sec. 4.4)</ref>. Please confer to the supplementary for a more detailed explanation of our model architecture.  <ref type="bibr" target="#b40">[41]</ref> using graph constraints. All numbers in %. We use the same object detection backbone provided by <ref type="bibr" target="#b48">[49]</ref> for fair comparison. Bold blue numbers indicate results better than competitors by &gt; 0.5. Regarding GCL <ref type="bibr" target="#b49">[50]</ref>, cf. to the text. Methods in the lower part use the same data split as <ref type="bibr" target="#b40">[41]</ref>. Results for MSDN <ref type="bibr" target="#b24">[25]</ref> are from <ref type="bibr" target="#b48">[49]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results and Comparison</head><p>Our results using graph constraints are shown in Tab. 1. The middle block indicates methods that all use the same data split that is used in <ref type="bibr" target="#b40">[41]</ref>. Two methods that use different splits are listed in the top section of the table. For MSD-net <ref type="bibr" target="#b24">[25]</ref> in this part, we show the results reported in <ref type="bibr" target="#b48">[49]</ref>. We used bold blue numbers to indicate the best result in any column that was at least 0.5% larger than the next best competing method. Since GCL <ref type="bibr" target="#b49">[50]</ref> used a different VGG backbone than other works, and also larger input images than all other methods (1024 ? 1024 compared to 592 ? 592), results cannot be fairly compared with other methods, so we did not highlight best results in this row (SGGEN-R@100 and PREDCLS-R@20).</p><p>The bottom part of Tab. 1 shows results of the proposed ODE layers using three different front-ends: (1) One in which the function used inside the ODE layer is taken to be a linear layer (ours-FC), (2) the second in which a Graph-CNN is used (ours-GCNN), (3) and the third in which a bidirectional LSTM is used (ours-LSTM).</p><p>As can be seen from Tab. 1, most recent methods (MotifNet <ref type="bibr" target="#b48">[49]</ref>, VC-TREE <ref type="bibr" target="#b37">[38]</ref> and CMAT <ref type="bibr" target="#b1">[2]</ref>) including ours are very similar in performance with respect to the scores in SGGEN and PREDCLS. The only exception among state-of-the-art works is GCL <ref type="bibr" target="#b49">[50]</ref> in two out of nine scores, yet by using a more powerful front-end.</p><p>For SGCLS, however, the proposed ODE layer turns out to be very effective. Apparently both the version using a linear layer inside the ODE layer (ours-FC) and the one using the bidirectional LSTM (ours-LSTM) perform very similar. Their scores improve state-of-the-art (CMAT <ref type="bibr" target="#b1">[2]</ref>) between ? 2% (SGCLS-R@20), ? 3% (SGCLS-R@50) and ? 3% (SGCLS-R@100). Compared with GCL <ref type="bibr" target="#b49">[50]</ref> our results improve by up to 5% (SGCLS-R@100) although our VGG was trained on coarser images. This demonstrates the effectiveness of the proposed ODE layer. It is sufficient to use a simple linear layer, since the ODE layer is so powerful that it produces similar outputs to those of a function based upon a more complicated and slower bidirectional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>For ablation studies, we consider several variants of the proposed network architecture. Results are shown in Tab. 2. For model-1 (first row), we removed the ODE networks and directly classified the output of the feature generating networks. This measures the impact of both ODE modules. In model-2 (second row), we removed only the ODE layer for object classification (O-ODE) in the proposed network. The third model shows the results when the relationship ODE (P-ODE) is removed. Finally, the fourth rows shows the results if both ODE modules are present. As can be seen, removing the ODE layer for predicate classification has a negligible effect since the PREDCLS scores hardly change. Removing the ODE layer for object classification has a strong, negative effect, however. This study shows that the ODE module can have a very positive impact. It further confirms our main claim that classification/assignment problems can be solved by means of neural ODEs.</p><p>Regarding the scores on PREDCLS, we conjecture that the noise in VG (missing relationship labels, incorrect labels) is so strong that scores cannot improve anymore. This hypothesis is supported by the fact that results in the past two years have not improved since <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Neural ODE Analysis</head><p>We propose two neural ordinary differential equations: Object ODE (O-ODE) and Predicate ODE (P-ODE) in the object classifier and predicate classifier respectively. Here, we tune the hyper-parameter, integral time t end in the ODE solver, to understand how that variables influences neural ordinary differential equations and final performance. Moreover, we extract the hidden states at different points in time and generate the scene graph to visualize how features are refined in the ODE space. Since the O-ODE and P-ODE both work in the setting SGCLS, SGCLS Re@50 is used as performance indicator. Because different images contain different number of objects, we randomly sample 5000 images from the training set to compute the average number of function evaluation (NFE) for a forward pass and the computation time per image. This experiment is implemented on a single GTX 1080Ti GPU. We vary the integral time t end from 0.05 to 3.00 (x-axes).</p><p>According to the left plot in <ref type="figure" target="#fig_1">Fig. 3</ref>, SGCLS Re@50 increases gradually from 56.5 to 62.0 until T = 1.50 and then decreases; (middle plot) the average forward NFE of O-ODE increases from 8.0 to 26.2 and from 8.0 to 24.0 for P-ODE; (right plot) the average forward time of the P-ODE layer increases from 0.06 seconds to 0.21 seconds whereas the average forward time of the O-ODE layer is negligibly small (&lt; 10 ?4 seconds), thus we omit these measurements. The P-ODE requires much more time than the O-ODE due to the large amount of object pairs. Considering the above points, we set t end = 1.5 for all other experiments.</p><p>To provide an intuitive interpretation of the parameters t 1 and t 2 in the O-ODE and the P-ODE, respectively, we use a model trained on SGGEN with the hyper-parameter t end = 1.5 and evaluate it using different latent vectors corresponding to different points between 0 and 2.5. The hidden states at t = 2.0 and t = 2.5 imply extrapolation. The detection results and scene graphs generated by the different hidden states are shown in <ref type="figure">Fig. 4</ref>. The features are more and more refined by the neural ODE layers if t 1 and t 2 increase, hence more objects and relationships are correctly classified. After t end = 1.5, increasingly many relationships are misclassified due to the extrapolation. The orange boxes and edges indicate the objects and predicates in the ground truth that are not detected, purple that the objects and predicates are predicted correctly. Blue edges show false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>Qualitative results for scene graph generation (SGGEN) are shown in <ref type="figure">Fig. 5</ref>. The images include object detections. The purple color indicates correctly detected and classified objects and relations, whereas orange means failure. The blue color indicates false negatives, i.e. relationships that are not in the ground truth. Most of the errors stem from the object detection stage. Whenever an object is not detected, relationships connecting this object are also not present in the scene graph. The cause of these errors is the Faster R-CNN detector which is used in all previous works.</p><p>There are a few false positives (blue links) which are semantically meaningful, for instance eye-of-man in the upper left example. In other words, the ground truth lacks this particular relationship in such cases. Several false positives result from semantically indistinguishable classes, for instance has and of in the lower left example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5:</head><p>Qualitative results from our model in the scene graph generation setting. Purple boxes denote correctly detected objects while orange boxes denote ground truth objects that are not detected. Purple edges correspond to correctly classified relationships at the R@20 setting while orange edges denote ground truth relationships that are not detected. Blue edges denote detected relationships that do not exist in ground truth annotations (false positives).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented Neural Ordinary Differential Equations for Scene Understanding (NODIS). The idea of this work is based on the fact that Mixed-Integer Linear Programs can be used to solve problems defined by ordinary differential equations; therefore, given a particular (M)ILP, we can find a system of ODEs that can produce the solution of the (M)ILP within a time series. Since it is not possible to manually define the system of ODE to solve, we draw on recent advances in machine learning and use a trainable function approximator instead of an explicitly defined system of ODEs. In other words, the proposed network learns the optimal function to solve the assignment problem, whereas previous works manually define modules to do so.</p><p>We use this newly defined module for object classification and relationship classification. The proposed model using ODE layers shows large improvements on SGCLS, between 2% and 3% compared with the best SOTA in that category. We believe that ODE layers can be valuable improvements for neural architectures in semantic image understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the proposed method: (a) given an image, many object regions are proposed, (b) those regions are classified by an Object ODE (O-ODE), (c) after pre-processing the features of object pairs, a Predicate ODE (P-ODE) is applied to predict predicates, (d) a scene graph is generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The effect of integral time in the ODE solver on SGCLS Re@50, average forward NFE (number of function evaluation) and average forward running time per image of O-ODE and P-ODE. Because of different number of objects in different images the experimental results are based on 5000 samples from the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Fig. 4 :</head><label>14</label><figDesc>We evaluate the model trained with the hyper-parameter t end = 1.5, i.e. during training the hidden vector at t = 1.5 is used for classifications. The upper row visualizes detection results while the lower row shows the corresponding scene graphs. The red box indicates the state corresponding to t = 1.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison on VG test set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study demonstrating the effect if both ODE layers are removed (first row), only the layer for object classification (second row), only the layer for predicate classification (third row), or if both are present (last row).</figDesc><table><row><cell cols="3">Model O-ODE P-ODE</cell><cell>SGGEN</cell><cell>SGCLS</cell><cell>PREDCLS</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">R@50 R@100 R@50 R@100 R@50 R@100</cell></row><row><cell>1</cell><cell>-</cell><cell>-</cell><cell cols="2">27.1 30.4 35.3 36.1 65.4 67.4</cell></row><row><cell>2</cell><cell>-</cell><cell></cell><cell cols="2">27.3 30.6 35.9 36.6 65.9 67.9</cell></row><row><cell>3</cell><cell></cell><cell>-</cell><cell cols="2">27.3 30.7 41.4 42.5 65.4 67.5</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell cols="2">27.7 31.0 41.9 42.9 66.0 67.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/yrcong/NODIS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially supported by the DFG grant COVMAP (RO 2497/12-2) and EXC 2122.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counterfactual critic multi-agent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene graph prediction with limited labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combinatorial and Continuous Codels for the Optimization of Traffic Flows on Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>F?genschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-translationrelation network for scalable scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gkanatsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting attention for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporally consistent horizon lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consac: Robust multi-model fitting by conditional sample consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referring relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining deep learning and qualitative spatial reasoning to learn complex structures from sparse examples with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph cut based inference with cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vip-cnn: Visual phrase guided convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Natural language guided visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure inference net: Object detection using scene-level context and instance-level relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning convolutional neural networks for object detection with very little training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Elsevier</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On support relations and semantic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing (ISPRS)</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shuffle-then-assemble: Learning object-agnostic visual relationship features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in humanobject interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graphical contrastive losses for scene graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 4</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards context-aware interaction recognition for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Indoor scene parsing with instance segmentation, semantic labeling and support relationship inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
