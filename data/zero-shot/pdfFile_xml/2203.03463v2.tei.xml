<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Sketch Induction for Paraphrase Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hosking</surname></persName>
							<email>tom.hosking@ed.ac.ukhao.tang@ed.ac.ukmlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Sketch Induction for Paraphrase Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a generative model of paraphrase generation, that encourages syntactic diversity by conditioning on an explicit syntactic sketch. We introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity. This hierarchy of codes is learned through end-to-end training, and represents fine-to-coarse grained information about the input. We use HRQ-VAE to encode the syntactic form of an input sentence as a path through the hierarchy, allowing us to more easily predict syntactic sketches at test time. Extensive experiments, including a human evaluation, confirm that HRQ-VAE learns a hierarchical representation of the input space, and generates paraphrases of higher quality than previous systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans use natural language to convey information, mapping an abstract idea to a sentence with a specific surface form. A paraphrase is an alternative surface form of the same underlying semantic content. The ability to automatically identify and generate paraphrases is of significant interest, with applications in data augmentation <ref type="bibr">(Iyyer et al., 2018)</ref>, query rewriting, <ref type="bibr">(Dong et al., 2017)</ref> and duplicate question detection <ref type="bibr" target="#b30">(Shah et al., 2018)</ref>.</p><p>While autoregressive models of language (including paraphrasing systems) predict one token at a time, there is evidence that in humans some degree of planning occurs at a higher level than individual words <ref type="bibr" target="#b15">(Levelt, 1993;</ref><ref type="bibr" target="#b23">Martin et al., 2010)</ref>. Prior work on paraphrase generation has attempted to include this inductive bias by specifying an alternative surface form as additional model input, either in the form of target parse trees <ref type="bibr">(Iyyer et al., 2018;</ref><ref type="bibr" target="#b6">Chen et al., 2019a;</ref><ref type="bibr" target="#b14">Kumar et al., 2020)</ref>, exemplars <ref type="bibr" target="#b25">(Meng et al., 2021)</ref>, or syntactic codes  <ref type="figure">Figure 1</ref>: The generative models underlying our approach. Given some semantic content z sem , we predict a hierarchical set of syntactic codes q d that describe the output syntactic form at increasing levels granularity. These are combined to give a syntactic embedding z syn , which is fed to the decoder along with the original semantic content to generate the output sentence y. During training, the encoder is driven by a paraphrase x sem and a syntactic exemplar x syn . <ref type="bibr" target="#b31">(Shu et al., 2019;</ref><ref type="bibr">Hosking and Lapata, 2021)</ref>. Most of these approaches suffer from an 'all or nothing' problem: the target surface form must be fully specified during inference. However, predicting the complete syntactic structure is almost as difficult as predicting the sentence itself, negating the benefit of the additional planning step.</p><p>In this paper, we propose a generative model for paraphrase generation, that combines the diversity introduced by an explicit syntactic target with the tractability of models trained end-to-end. Shown in <ref type="figure">Figure 1</ref>, the model begins by assuming the existence of some semantic content z sem . Conditioned on this semantic information, the model predicts a syntactic 'sketch' in the form of a hierarchical set of discrete codes q 1:D , that describe the target syntactic structure with increasing granularity. The sketch is combined into an embedding z syn , and fed along with the original meaning z sem to a de-coder that generates the final output utterance y. Choosing a discrete representation for the sketch means it can be predicted from the meaning as a simple classification task, and the hierarchical nature means that the joint probability over the codes admits an autoregressive factorisation, making prediction more tractable.</p><p>The separation between z sem and z syn is induced by a training scheme introduced in earlier work <ref type="bibr">(Hosking and Lapata, 2021;</ref><ref type="bibr">Huang and Chang, 2021)</ref> and inspired by prior work on separated latent spaces <ref type="bibr" target="#b7">(Chen et al., 2019b;</ref><ref type="bibr" target="#b1">Bao et al., 2019)</ref>, whereby the model must reconstruct a target output from one input with the correct meaning, and another input with the correct syntactic form. To learn the discretized sketches, we propose a variant of Vector-Quantized Variational Autoencoders (VQ-VAE, or VQ) that learns a hierarchy of embeddings within a shared vector space, and represents an input encoding as a path through this hierarchy. Our approach, which we call Hierarchical Refinement Quantized Variational Autoencoders or HRQ-VAE, leads to a decomposition of a dense vector into embeddings of increasing granularity, representing high-level information at the top level before gradually refining the encoding over subsequent levels.</p><p>Our contributions are summarized as follows:</p><p>? We propose a generative model of natural language generation, HRQ-VAE, that induces a syntactic sketch to account for the diversity exhibited by paraphrases. We present a parameterization of our generative model that is a novel method for learning hierarchical discretized embeddings over a single latent encoding space. These embeddings are trained end-to-end and jointly with the encoder/decoder. ? We use HRQ-VAE to induce hierarchical sketches for paraphrase generation, demonstrating that the known factorization over codes makes them easier to predict at test time, and leads to higher quality paraphrases.</p><p>2 Latent Syntactic Sketches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>Let y be a sentence, represented as a sequence of tokens. We assume that y contains semantic content, that can be represented by a latent variable z sem . Types of semantic content might include the description of an image, or a question intent. How-ever, the mapping from semantics to surface form is not unique: in general, there is more than one way to express the semantic content. Sentences with the same underlying meaning z sem but different surface form y are paraphrases. Standard approaches to paraphrasing (e.g., Bowman et al. 2016) map directly from z sem to y, and do not account for this diversity of syntactic structure. Following recent work on syntax-guided paraphrasing <ref type="bibr" target="#b6">(Chen et al., 2019a;</ref><ref type="bibr">Hosking and Lapata, 2021)</ref>, and inspired by evidence that humans plan out utterances at a higher level than individual words <ref type="bibr" target="#b23">(Martin et al., 2010)</ref>, we introduce an intermediary sketching step, depicted in <ref type="figure">Figure 1b</ref>. We assume that the output sentence y is generated as a function both of the meaning z sem and of a syntactic encoding z syn that describes the structure of the output. Moreover, since natural language displays hierarchical organization in a wide range of ways, including at a syntactic level (constituents may contain other consituents), we also assume that the syntactic encoding z syn can be decomposed into a hierarchical set of discrete latent variables q 1:D , and that these q d are conditioned on the meaning z sem . This contrasts with popular model architectures such as VAE <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> which use a flat internal representation in a dense Euclidean vector space.</p><p>Intuitively, our generative model corresponds to a process where a person thinks of a message they wish to convey; then, they decide roughly how to say it, and incrementally refine this decision; finally, they combine the meaning with the syntactic sketch to 'spell out' the sequence of words making up the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Factorization and Objective</head><p>The graphical model in <ref type="figure">Figure 1b</ref> factorizes as</p><formula xml:id="formula_0">p(y, z sem ) = q 1:D ,zsyn p(y|z sem , z syn ) ? p(z syn |q 1:D ) ?p(z sem ) ? p(q 1 |z sem ) D d=2 ?p(q d |q &lt;d , z sem ). (1)</formula><p>Although q 1:D are conditionally dependent on z sem , we assume that z sem may be determined from y without needing to explicitly calculate q 1:D or z syn . We also assume that the mapping from discrete codes q 1:D to z syn is a deterministic func-tion f q?z (?). The posterior therefore factorises as ?(z sem , z syn |y) = ?(z sem |y) ? ?(z syn |y)</p><formula xml:id="formula_1">? ?(q 1 |z syn ) ? D d=2 ?(q d |q &lt;d , z syn ). (2)</formula><p>The separation between z sem and q 1:D , such that they represent the meaning and form of the input respectively, is induced by the training scheme. During training, the model is trained to reconstruct a target y using z sem derived from an input with the correct meaning (a paraphrase) x sem , and q 1:D from another input with the correct form (a syntactic exemplar) x syn . Hosking and Lapata (2021) showed that the model therefore learns to encode primarily semantic information about the input in z sem , and primarily syntactic information in q 1:D . Exemplars are retrieved from the training data following to the process described in <ref type="bibr">Hosking and Lapata (2021)</ref>, with examples in Appendix C. The setup is shown in <ref type="figure">Figure 1a</ref>; in summary, during training we set ?(z sem |y) = ?(z sem |x sem ) and ?(q d |y, q &lt;d ) = ?(q d |x syn , q &lt;d ). The final objective is given by</p><formula xml:id="formula_2">ELBO = E ? ? log p(y|z sem , q 1:D )) ? log p(q 1 |z sem ) ? D d=2 log p(q d |q &lt;d , z sem ) + KL ?(z sem |x sem )||p(z sem ) ,<label>(3)</label></formula><p>where q d ? ?(q d |x syn ) and z sem ? ?(z sem |x sem ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Parameterisation</head><p>We assume a Gaussian distribution for z sem , with prior p(z sem ) ? N (0, 1).</p><p>The encoders ?(z sem |x sem ) and ?(z syn |x syn ) are Transformers <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>, and we use an autoregressive Transformer decoder for p(y|z sem , z syn ). The mapping f q?z (?) from q 1:D to z syn and the posterior network ?(q d |q &lt;d , z syn ) are more complex, and form a significant part of our contribution.</p><p>Our choice of parameterization is learned endto-end, and ensures that the sketches learned are hierarchical both in the shared embedding space and in the information they represent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Refinement Quantization</head><p>Let z syn ? R D be the output of the encoder network ?(z syn |y), that we wish to decompose as a sequence of discrete hierarchical codes. Recall that q d ? [1, K] are discrete latent variables corresponding to the codes at different levels in the hierarchy, d ? [1, D]. Each level uses a distinct codebook, C d ? R K?D , which maps each discrete code to a continuous embedding</p><formula xml:id="formula_3">C d (q d ) ? R D .</formula><p>The distribution over codes at each level is a softmax distribution, with the scores s d given by the distance from each of the codebook embeddings to the residual error between the input and the cumulative embedding from all previous levels,</p><formula xml:id="formula_4">s d (q) = ? x ? d?1 d =1 C d (q d ) ? C d (q) 2 .<label>(4)</label></formula><p>Illustrated in <ref type="figure">Figure 2</ref>, these embeddings therefore represent iterative refinements on the quantization of the input. The posterior network ?(q d |q &lt;d , z syn ) iteratively decomposes an encoding vector into a path through a hierarchy of clusters whose centroids are the codebook embeddings. Given a sequence of discrete codes q 1:D , we deterministically construct its continuous representation with the composition function f q?z (?),</p><formula xml:id="formula_5">z syn = f q?z (q 1:D ) = D d=1 C d (q d ).<label>(5)</label></formula><p>HRQ-VAE can be viewed as an extension of VQ-VAE (van den Oord et al., 2017), with two significant differences: (1) the codes are hierarchically ordered and the joint distribution p(q 1 , . . . , q D ) admits an autoregressive factorization; and (2) the HRQ-VAE composition function is a sum, compared to concatenation in VQ or a complex neural network in VQ-VAE 2 <ref type="bibr" target="#b28">(Razavi et al., 2019)</ref>. Under HRQ, latent codes describe a path through the learned hierarchy within a shared encoding space. The form of the posterior ?(q d |q &lt;d , z syn ) and the composition function f q?z (?) do not rely on any particular properties of the paraphrasing task; the technique could be applied to any encoding space.</p><p>Initialisation Decay Smaller perturbations in encoding space should result in more fine grained changes in the information they encode. Therefore, we encourage ordering between the levels of hierarchy (such that lower levels encode finer grained information) by initialising the codebook with a decaying scale, such that later embeddings have a smaller norm than those higher in the hierarchy. Specifically, the norm of the embeddings at level d is weighted by a factor (? init ) d?1 . <ref type="figure">Figure 2</ref>: An illustration of how HRQ-VAE maps an input encoding vector z to a decomposition of hierarchical discretized encodings. HRQ-VAE compares the input to a jointly learned codebook of embeddings that become increasingly granular at lower depths of hierarchy. In this simplified example, with a depth of 3 and a codebook size of 3, the nearest top-level (colours) embedding to z is e red ; then, the residual error ? 1 = z?e red is compared to the 2 nd level of embeddings (shapes), with the nearest being e . Finally, the residual error ? 2 is compared to the 3 rd level codebook (patterns), where the closest is e stripes . The quantized encoding of z is then z ? e red +e +e stripes .</p><p>Depth Dropout To encourage the hierarchy within the encoding space to correspond to hierarchical properties of the output, we introduce depth dropout, whereby the hierarchy is truncated at each level during training with some probability p depth . The output of the quantizer is then given by</p><formula xml:id="formula_6">z syn = D d=1 C d (q d ) d d =1 ? d ,<label>(6)</label></formula><p>where ? h ? Bernoulli(1 ? p depth ). This means that the model is sometimes trained to reconstruct the output based only on a partial encoding of the input, and should learn to cluster similar outputs together at each level in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sketch Prediction Network</head><p>During training the decoder is driven using sketches sampled from the encoder, but at test time exemplars are unavailable and we must predict a distribution over syntactic sketches p(q 1:D |z sem ). Modelling the sketches as hierarchical ensures that this distribution admits an autoregressive factorization. We use a simple recurrent network to infer valid codes at each level of hierarchy, using the semantics of the input sentence and the cumulative embedding of the predicted path so far as input, such that q d is sampled from</p><formula xml:id="formula_7">p(q d |z sem , q &lt;d ) = Softmax(MLP d (z sem , z &lt;d )), where z &lt;d = d?1 d =1 C d (q d ).</formula><p>This MLP is trained jointly with the encoder/decoder model, using the outputs of the posterior network ?(q d |x syn , q &lt;d ) as targets. To generate paraphrases as test time, we sample from the sketch prediction model p(q d |z sem , q &lt;d ) using beam search and condition generation on these predicted sketches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Setup</head><p>We use the Gumbel reparameterisation trick <ref type="bibr" target="#b9">(Jang et al., 2016;</ref><ref type="bibr" target="#b20">Maddison et al., 2017;</ref><ref type="bibr" target="#b32">S?nderby et al., 2017)</ref> for the discrete codes and the standard Gaussian reparameterisation for the semantic representation. To encourage the model to use the full codebook, we decayed the Gumbel temperature ? , according to the schedule given in Appendix A. We approximate the expectation in Equation <ref type="formula" target="#formula_2">(3)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets A paraphrase is 'an alternative surface form in the same language expressing the same semantic content as the original form' <ref type="bibr" target="#b21">(Madnani and Dorr, 2010)</ref>, but it is not always clear what counts as the 'same semantic content'. Our approach requires access to reference paraphrases; we evaluate on three English paraphrasing datasets which have clear grounding for the meaning of each sentence: Paralex <ref type="bibr">(Fader et al., 2013)</ref>, a dataset of question paraphrase clusters scraped from WikiAnswers; Quora Question Pairs (QQP) 1 sourced from the community question answering forum Quora; and MSCOCO 2017 <ref type="bibr" target="#b17">(Lin et al., 2014)</ref>, a set of images that have been captioned by multiple annotators. For the question datasets, each paraphrase is grounded to the (hypothetical) answer they share. We use the splits released by Hosking and Lapata (2021). For MSCOCO, each caption is grounded by the image that it describes. We evaluate on the public validation set, randomly selecting one cap- tion for each image to use as input and using the remaining four as references.</p><p>Model Configuration Hyperparameters were tuned on the Paralex development set, and reused for the other evaluations. We set the depth of the hierarchy D = 3, and the codebook size K = 16. The Transformer encoder and decoder consist of 5 layers each, and we use the vocabulary and token embeddings from BERT-Base <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. We use an initialisation decay factor of ? init = 0.5, and a depth dropout probability p depth = 0.3. A full set of hyperparameters is given in Appendix A, and our code is available at https://github.com/tomhosking/hrq-vae.</p><p>Comparison Systems As baselines, we consider three popular architectures: a vanilla autoencoder (AE) that learns a single dense vector representation of an input sentence; a Gaussian Variational AutoEncoder <ref type="bibr">(VAE, Bowman et al., 2015)</ref>, which learns a distribution over dense vectors; and a Vector-Quantized Variational AutoEncoder (VQ-VAE, <ref type="bibr" target="#b36">van den Oord et al., 2017)</ref>, that represents the full input sentence as a set of discrete codes. All three models are trained to generate a sentence from one of its paraphrases in the training data, and are not trained with an autoencoder objective. We implement a simple tf-idf baseline <ref type="bibr" target="#b10">(Jones, 1972)</ref>, retrieving the question from the training set with the highest cosine similarity to the input. Finally, we include a basic copy baseline as a lower bound, that simply uses the input sentences as the output.</p><p>We also compare to a range of recent paraphrasing systems. Latent bag-of-words <ref type="bibr">(BoW, Fu et al., 2019)</ref> uses an encoder-decoder model with a discrete bag-of-words as the latent encoding. SOW/REAP (Goyal and Durrett, 2020) uses a two stage approach, deriving a set of feasible syntactic rearrangements that is used to guide a second encoder-decoder model. BTmPG <ref type="bibr" target="#b18">(Lin and Wan, 2021)</ref> uses multi-round generation to improve diversity and a reverse paraphrasing model to preserve semantic fidelity. We use the results after 10 rounds of paraphrasing. Separator (Hosking and Lapata, 2021) uses separated, non-hierarchical encoding spaces for the meaning and form of an input, and an additional inference model to predict the target syntactic form at test time. All comparison systems were trained and evaluated on our splits of the datasets.</p><p>As an upper bound, we select a sentence from the evaluation set to use as an oracle syntactic exemplar, conditioning generation on a sketch that is known to represent a valid surface form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our experiments were designed to test two primary hypotheses: (1) Does HRQ-VAE learn hierarchical decompositions of an encoding space? and (2) Does our choice of generative model enable us to generate high quality and diverse paraphrases? <ref type="figure" target="#fig_1">Figure 3</ref> shows a t-SNE (van der Maaten and Hinton, 2008) plot of the syntactic encodings z syn for 10,000 examples from Paralex. The encodings are labelled by their quantization, so that colours indicate top-level codes q 1 , shapes denote q 2 , and patterns q 3 . The first plot shows clear high level structure, with increasingly fine levels of substructure visible as we zoom into each cluster. This confirms that the discrete codes are ordered, with lower levels in the hierarchy encoding more fine grained information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Probing the Hierarchy</head><p>To confirm that intermediate levels of hierarchy represent valid points in the encoding space, we generate paraphrases using oracle sketches, but truncate the sketches at different depths. Masking one level (i.e., using only q 1 , q 2 ) reduces performance by 2.5 iBLEU points, and two levels by 5.5.  <ref type="table">Table 1</ref>: Top-1 paraphrase generation results, without access to oracle sketches. HRQ-VAE achieves the highest iBLEU scores, indicating the best tradeoff between quality and diversity. Paired bootstrap resampling <ref type="bibr" target="#b13">(Koehn, 2004)</ref> indicates that HRQ-VAE significantly improves on all other systems (p&lt; 0.05).</p><formula xml:id="formula_8">Paralex QQP MSCOCO System BLEU ? Self-B ? iBLEU ? BLEU ? Self-B ? iBLEU ? BLEU ? Self-B ? iBLEU ?<label>Copy</label></formula><p>(iBLEU is an automatic metric for assessing paraphrase quality; see Section 5.2). Although encodings using the full depth are the most informative, partial encodings still lead to good quality output, with a gradual degradation. This implies both that each level in the hierarchy contains useful information, and that the cluster centroids at each level are representative of the individual members of those clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Paraphrase Generation</head><p>Metrics Our primary metric is iBLEU <ref type="bibr" target="#b33">(Sun and Zhou, 2012)</ref>,</p><formula xml:id="formula_9">iBLEU = ?BLEU(outputs, ref erences) ?(1 ? ?)BLEU(outputs, inputs),<label>(7)</label></formula><p>that measures the fidelity of generated outputs to reference paraphrases as well as the level of diversity introduced. We use the corpus-level variant. Following the recommendations of Sun and Zhou <ref type="formula">(2012)</ref>, we set ? = 0.8, with a sensitivity analysis shown in Appendix A. We also report BLEU(outputs, ref erences) as well as Self-BLEU(outputs, inputs). The latter allows us to examine the extent to which models generate paraphrases that differ from the original input.</p><p>To evaluate the diversity between multiple candidates generated by the same system, we report pairwise-BLEU (Cao and Wan, 2020),</p><formula xml:id="formula_10">P-BLEU = E i =j [BLEU(outputs i , outputs j )].</formula><p>This measures the average similarity between the different candidates, with a lower score indicating more diverse hypotheses. Separator What are some of the best ways to defrost chicken? HRQ-VAE How do you thaw frozen lobster tails? MSCOCO Set of toy animals sitting in front of a red wooden wagon. VAE Two stuffed animals sitting in front of a toy train. BTmpG A herd of sheep grazing in a field of grass. SOW/REAP A close up of a close up of a street Latent BoW A toy wagon with a toy horse and a toy wagon. Separator A toy model of a toy horse and buggy. HRQ-VAE A group of stuffed animals sitting next to a wooden cart.  <ref type="table">Table 1</ref>, the results of the automatic evaluation highlight the importance of measuring both paraphrase quality and similarity to the input: the Copy baseline is able to achieve high BLEU scores despite simply duplicating the input. The VAE baseline is competitive but tends to have a hi gh Self-BLEU score, indicating that the semantic preservation comes at the cost of low syntactic diversity. HRQ-VAE achieves both higher BLEU scores and higher iBLEU scores than the comparison systems, indicating that it is able to generate  <ref type="table">Table 3</ref>: Examples of model output, for a range of different sketches. The left hand side shows the sketch (i.e., the values of the codes q 1:D ), with the corresponding model output on the right. q 1 primarily specifies the wh-word (e.g., outputs with q 1 = 13 are all 'what' questions), while q 2 , q 3 correspond to more fine grained details, e.g., the outputs with q 3 = 6 all use the article 'a' when referring to 'body'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Evaluation Shown in</head><p>higher quality paraphrases without compromising on syntactic diversity.</p><p>The examples in <ref type="table" target="#tab_3">Table 2</ref> demonstrate that HRQ is able to introduce significant syntactic variation while preserving the original meaning of the input. However, there is still a gap between generation using predicted sketches and 'oracle' sketches (i.e., when the target syntactic form is known in advance), indicating ample scope for improvement.</p><p>Worked Example Since the sketches q 1:D are latent variables, interpretation is difficult. However, a detailed inspection of example output reveals some structure. <ref type="table">Table 3</ref> shows the model output for a single semantic input drawn from Paralex, across a range of different syntactic sketches. It shows that q 1 is primarily responsible for encoding the question type, with q 1 = 13 leading to 'what' questions and q 1 = 2 'how' questions. q 2 and q 3 encode more fine grained details; for example, all outputs shown with q 3 = 6 use the indefinite article 'a'.</p><p>We also examine how using increasingly granular sketches refines the syntactic template of the output. <ref type="table">Table 4</ref> shows the model output for a single semantic input, using varying granularities of sketch extracted from the exemplar. When no sketch is specified, the model defaults to a canonical phrasing of the question. When only q 1 is specified, the output becomes a 'how many' question, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Two types of fat in body? Exemplar How many states are in the USA? No sketch What are the different types of fats in the body?</p><p>q1 How many types of fats are there in the body? q1, q2 How many fats does the body have? q1, q2, q3 How many fat are in the body? <ref type="table">Table 4</ref>: Model output for varying sketch granularities. When no sketch is used, the model defaults to the most common phrasing of the question. As more detail is included, the output converges towards the exemplar.  Table 5: Top-3 generation results. P-BLEU indicates the similarity between the different candidates, while iBLEU scores reported are the mean across the 3 candidates. HRQ-VAE is able to generate multiple high quality paraphrases with more diversity between them than comparison systems. when a full sketch is included, the output closely resembles the exemplar.</p><formula xml:id="formula_11">Paralex QQP MSCOCO Model i B L E U ? P -B L E U ? i B L E U ? P -B L E U ? i B L E U ? P -B L E U ?<label>VAE</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Multiple Paraphrases</head><p>We evaluated the ability of our system to generate multiple diverse paraphrases for a single input, and compared to the other comparison systems capable of producing more than one output. For both HRQ-VAE and Separator, we used beam search to sample from the sketch prediction network as in the top-1 case, and condition generation on the top-3 hypotheses predicted. For BTmPG, we used the paraphrases generated after 3, 6 and 10 rounds. For the VAE, we conditioned generation on 3 different samples from the encoding space. The results in <ref type="table">Table 5</ref> show that HRQ-VAE is able to generate multiple high quality paraphrases for a single input, with lower similarity between the candidates than other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human Evaluation</head><p>In addition to automatic evaluation we elicited judgements from crowdworkers on Amazon Mechanical Turk. They were shown a sentence and two paraphrases, each generated by a different system, and asked to select which one was preferred along three dimensions: the dissimilarity of the paraphrase compared to the original sentence; how  <ref type="figure">Figure 4</ref>: Results of our human evaluation. Although the VAE baseline is the best at preserving sentence meaning, it is the worst at introducing variation to the output. HRQ-VAE offers the best balance between dissimilarity and meaning preservation, and is more fluent than both Separator and Latent BoW.</p><p>well the paraphrase reflected the meaning of the original; and the fluency of the paraphrase (see Appendix B). We evaluated a total of 300 sentences sampled equally from each of the three evaluation datasets, and collected 3 ratings for each sample. We assigned each system a score of +1 when it was selected, ?1 when the other system was selected, and took the mean over all samples. Negative scores indicate that a system was selected less often than an alternative. We chose the four best performing models for our evaluation: HRQ-VAE, Separator, Latent BoW, and VAE. <ref type="figure">Figure 4</ref> shows that although the VAE baseline is the best at preserving question meaning, it is also the worst at introducing variation to the output. HRQ-VAE better preserves the original question intent compared to the other systems while introducing more diversity than the VAE, as well as generating much more fluent output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablations</head><p>To confirm that the hierarchical model allows for more expressive sketches, we performed two ablations. We compared to the full model using oracle sketches, so that code prediction performance was not a factor. We set the depth D = 1 and K = 48, giving equivalent total capacity to the full model (D = 3, K = 16) but without hierarchy. We also removed the initialisation scaling at lower depths, instead initialising all codebooks with the same scale. <ref type="table">Table 6</ref> shows that a non-hierarchical model with the same capacity is much less expressive.</p><p>We also performed two ablations against the model using predicted sketches; we removed depth dropout, so that the model is always trained on a full encoding. We confirm that learning the code-  <ref type="table">Table 6</ref>: Changes in iBLEU score for a range of ablations from our full model. All components lead to an improvement in paraphrase quality across datasets.</p><p>books jointly with the encoder/decoder leads to a stronger model, by first training a model with a continuous Gaussian bottleneck (instead of the HRQ-VAE); then, we recursively apply k-means clustering <ref type="bibr" target="#b19">(Lloyd, 1982)</ref>, with the clustering at each level taking place over the residual error from all levels so far, analogous to HRQ-VAE. The results of these ablations shown in <ref type="table">Table 6</ref> indicate that our approach leads to improvements over all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Hierarchical VAEs VQ-VAEs were initially proposed in computer vision <ref type="bibr" target="#b36">(van den Oord et al., 2017)</ref>, and were later extended to be 'hierarchical' <ref type="bibr" target="#b28">(Razavi et al., 2019)</ref>. However, in vision the term refers to a 'stacked' version architecture, where the output of one variational layer is passed through a CNN and then another variational layer that can be continuous <ref type="bibr" target="#b35">(Vahdat and Kautz, 2020)</ref> or quantized <ref type="bibr" target="#b40">(Williams et al., 2020;</ref><ref type="bibr" target="#b16">Li?vin et al., 2019;</ref><ref type="bibr" target="#b39">Willetts et al., 2021)</ref>. Unlike these approaches, we induce a single latent space that has hierarchical properties. Other work has looked at using the properties of hyperbolic geometry to encourage autoencoders to learn hierarchical representations. <ref type="bibr" target="#b24">Mathieu et al. (2019)</ref> showed that a model endowed with a Poincar? ball geometry was able to recover hierarchical structure in datasets, and <ref type="bibr">Sur?s et al. (2021)</ref> used this property to deal with uncertainty in predicting events in video clips. However, their work was limited to continuous encoding spaces, and the hierarchy discovered was known to exist a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntax-controlled Paraphrase Generation</head><p>Prior work on paraphrasing has used retrieval techniques <ref type="bibr" target="#b2">(Barzilay and McKeown, 2001)</ref>, Residual LSTMs <ref type="bibr" target="#b27">(Prakash et al., 2016</ref><ref type="bibr">), VAEs (Bowman et al., 2016</ref>, VQ-VAEs <ref type="bibr" target="#b29">(Roy and Grangier, 2019)</ref> and pivot languages <ref type="bibr" target="#b22">(Mallinson et al., 2017)</ref>.</p><p>Syntax-controlled paraphrase generation has seen significant recent interest, as a means to explicitly generate diverse surface forms with the same meaning. However, most previous work has required knowledge of the correct or valid surface forms to be generated <ref type="bibr">(Iyyer et al., 2018;</ref><ref type="bibr" target="#b6">Chen et al., 2019a;</ref><ref type="bibr" target="#b14">Kumar et al., 2020;</ref><ref type="bibr" target="#b25">Meng et al., 2021)</ref>. It is generally assumed that the input can be rewritten without addressing the problem of predicting which template should be used, which is necessary if the method is to be useful. <ref type="bibr">Hosking and Lapata (2021)</ref> proposed learning a simplified representation of the surface form using VQ, that could then be predicted at test time. However, the discrete codes learned by their approach are not independent and do not admit a known factorization, leading to a mismatch between training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a generative model of paraphrasing, that uses a hierarchy of discrete latent variables as a rough syntactic sketch. We introduce HRQ-VAE, a method for mapping these hierarchical sketches to a continuous encoding space, and demonstrate that it can indeed learn a hierarchy, with lower levels representing more fine-grained information. We apply HRQ-VAE to the task of paraphrase generation, representing the syntactic form of sentences as paths through a learned hierarchy, that can be predicted during testing. Extensive experiments across multiple datasets and a human evaluation show that our method leads to high quality paraphrases. The generative model we introduce has potential application for any natural language generation task; z sem could be sourced from a sentence in a different language, from a different modality (e.g., images or tabular data) or from a task-specific model (e.g., summarization or machine translation). Furthermore, HRQ-VAE makes no assumptions about the type of space being represented, and could in principle be applied to a semantic space, learning a hierarchy over words or concepts.  to this value, and our model outperforms all comparison systems on all datasets for 0.7 ? ? ? 0.9. Models were trained on a single GPU, with training taking between one and three days depending on the dataset. We use SacreBLEU <ref type="bibr" target="#b26">(Post, 2018)</ref> to calculate BLEU scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Human Evaluation</head><p>Annotators were recruited from the UK and USA via Amazon Mechanical Turk, and were compensated for their time above a living wage in those countries. A full Participant Information Sheet was provided, and the study was approved by an internal ethics committee. Annotators were asked to rate the outputs according to the following criteria:</p><p>? Which system output is the most fluent and grammatical?</p><p>? To what extent is the meaning expressed in the original sentence preserved in the rewritten version, with no additional information added?</p><p>? Does the rewritten version use different words or phrasing to the original? You should choose the system that uses the most different words or word order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Exemplar Retrieval Process</head><p>Our approach requires exemplars during training to induce the separation between latent spaces. We follow the approach introduced by Hosking and  Lapata <ref type="formula">(2021)</ref>. During training, we retrieve exemplars x syn from the training data following a process which first identifies the underlying syntax of Y, and finds a question with the same syntactic structure but a different, arbitrary meaning. We use a shallow approximation of syntax, to ensure the availability of equivalent exemplars in the training data. An example of the exemplar retrieval process is shown in <ref type="table" target="#tab_9">Table 8</ref>; we first apply a chunker (FlairNLP, <ref type="bibr" target="#b0">Akbik et al., 2018)</ref> to Y, then extract the chunk label for each tagged span, ignoring stopwords. This gives us the template that Y follows. We then select a question at random from the training data with the same template to give x syn . If no other questions in the dataset use this template, we create an exemplar by replacing each chunk with a random sample of the same type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis of Code Properties</head><p>We define two features of sentences: (1) the presence of common auxiliary verbs that roughly indicate the tense of the sentence (present, future, etc.); and (2) the presence of different question or 'wh-' words 2 . We calculate the distributions of these features for each code q d at different levels, with the results shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Each column represents the distribution over the feature for a specific code. <ref type="figure" target="#fig_4">Figure 6a</ref> shows clear evidence that the sentences are (at least partly) clustered at the top level based on the verb used, while <ref type="figure" target="#fig_4">Figure 6b</ref> shows that level 2 encodes the question type.  Each column represents the distribution over the feature for a specific code. The plots show that level 1 is a strong predictor of verb tense, and level 2 predicts question type, giving some insight into what syntactic features each level has learned to encode. We have reordered the columns of the plot to improve readability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>: t-SNE visualisation of the syntactic encodings z syn for 10k examples from Paralex: colours indicate top-level codes q 1 , shapes indicate the second level, and patterns are used to label the third level. Deeper levels in the hierarchy represent finer grained information in encoding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>iBLEU scores for all comparison systems, for a range of values of ?. Distribution of wh-words for each code within level 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Plots showing the conditional distributions of two different sentence features, auxiliary verb and question type, for different values of the latent codes q d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>by sampling from the training set and updating via backpropagation (Kingma and Welling, 2014). The full model was trained jointly by optimizing the ELBO in Equation (3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Examples of generated paraphrases. HRQ-VAE is able to preserve the original meaning, while introducing significant syntactic variation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella Lapata. 2017. Learning to paraphrase for question answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 875-886, Copenhagen, Denmark. Association for Computational Linguistics.</figDesc><table><row><cell>Encoder/decoder</cell><cell></cell></row><row><cell cols="2">Embedding dimension D 768</cell></row><row><cell>Encoder layers</cell><cell>5</cell></row><row><cell>Decoder layers</cell><cell>5</cell></row><row><cell>Feedforward dimension</cell><cell>2048</cell></row><row><cell>Transformer heads</cell><cell>8</cell></row><row><cell>Semantic/syntactic dim</cell><cell>192/594</cell></row><row><cell cols="2">Depth D Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 3 Codebook size K 16</cell></row><row><cell cols="2">2013. Paraphrase-driven learning for open question Optimizer Adam (Kingma</cell></row><row><cell cols="2">answering. In Proceedings of the 51st Annual Meet-and Ba, 2015)</cell></row><row><cell cols="2">ing of the Association for Computational Linguis-Learning rate 0.01</cell></row><row><cell cols="2">Batch size tics (Volume 1: Long Papers), pages 1608-1618, 64 Token dropout 0.2 (Xie et al., Sofia, Bulgaria. Association for Computational Lin-2017) guistics. Decoder Beam search</cell></row><row><cell cols="2">Beam width Yao Fu, Yansong Feng, and John P Cunningham. 2019. 4 Code predictor Paraphrase generation with latent bag of words. In Num. hidden layers 2 Advances in Neural Information Processing Systems, volume 32, pages 13645-13656. Curran Associates, Hidden layer size 3072</cell></row><row><cell>Inc.</cell><cell></cell></row><row><cell cols="2">Tanya Goyal and Greg Durrett. 2020. Neural syntactic</cell></row><row><cell cols="2">preordering for controlled paraphrase generation. In</cell></row><row><cell cols="2">Proceedings of the 58th Annual Meeting of the As-</cell></row><row><cell cols="2">sociation for Computational Linguistics, pages 238-</cell></row><row><cell cols="2">252, Online. Association for Computational Linguis-</cell></row><row><cell>tics.</cell><cell></cell></row><row><cell cols="2">Tom Hosking and Mirella Lapata. 2021. Factorising</cell></row><row><cell cols="2">meaning and form for intent-preserving paraphras-</cell></row><row><cell cols="2">ing. In Proceedings of the 59th Annual Meeting of</cell></row><row><cell cols="2">the Association for Computational Linguistics and</cell></row><row><cell cols="2">the 11th International Joint Conference on Natu-</cell></row><row><cell cols="2">ral Language Processing (Volume 1: Long Papers),</cell></row><row><cell cols="2">pages 1405-1418, Online. Association for Computa-</cell></row><row><cell>tional Linguistics.</cell><cell></cell></row><row><cell cols="2">Kuan-Hao Huang and Kai-Wei Chang. 2021. Generat-</cell></row><row><cell cols="2">ing syntactically controlled paraphrases without us-</cell></row><row><cell cols="2">ing annotated parallel pairs. In Proceedings of the</cell></row><row><cell cols="2">16th Conference of the European Chapter of the</cell></row><row><cell cols="2">Association for Computational Linguistics: Main</cell></row><row><cell cols="2">Volume, pages 1022-1033, Online. Association for</cell></row><row><cell>Computational Linguistics.</cell><cell></cell></row><row><cell cols="2">Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke</cell></row><row><cell cols="2">Zettlemoyer. 2018. Adversarial example generation</cell></row><row><cell cols="2">with syntactically controlled paraphrase networks.</cell></row><row><cell cols="2">In Proceedings of the 2018 Conference of the North</cell></row><row><cell cols="2">American Chapter of the Association for Computa-</cell></row><row><cell cols="2">tional Linguistics: Human Language Technologies,</cell></row><row><cell cols="2">Volume 1 (Long Papers), pages 1875-1885, New</cell></row><row><cell cols="2">Orleans, Louisiana. Association for Computational</cell></row><row><cell>Linguistics.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter values used for our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>InputHow heavy is a moose?Chunker output How [heavy]ADVP is a [moose]NP ? Template How ADVP is a NP ? Exemplar How much is a surgeon's income? Input What country do parrots live in Chunker output What [country]NP do [parrots]NP [live]VP in ? Template What NP do NP VP in ? Exemplar What religion do Portuguese believe in?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Examples of the exemplar retrieval process for training. The input is tagged by a chunker, ignoring stopwords. An exemplar with the same template is then retrieved from a different paraphrase cluster.Table reproducedwith permission from Hosking and Lapata (2021).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.kaggle.com/c/quora-question-pairs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">+ e t/10000 , 0.5).(8)Intuitively, this smoothly decays ? from an initial value of 2, with a half-life of 10k steps, to a minimum value of 0.5.We use ? = 0.8 when calculating iBLEU, but as shown inFigure 5our conclusions are not sensitive</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This analysis was performed for Paralex, which comprises entirely of questions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank our anonymous reviewers for their feedback. This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh. Lapata acknowledges the support of the European Research Council (award number 681760, "Translating Multiple Modalities into Text").</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>The hyperparameters given in <ref type="table">Table 7</ref> were selected by manual tuning, based on a combination of: (a) validation iBLEU scores with depth masking, (b) validation BLEU scores using oracle sketches, and (c) validation iBLEU scores using predicted syntactic codes.</p><p>The Gumbel temperature ? is decayed during training as a function of the step t, according to the following equation:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating sentences from disentangled syntactic and semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1602</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6008" to="6019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073012.1073020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1511.06349</idno>
		<editor>M. Dai, Rafal J?zefowicz, and Samy Bengio</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DivGAN: Towards diverse paraphrase generation via diversified generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.218</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2411" to="2421" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Controllable paraphrase generation with a syntactic exemplar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1599</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5972" to="5984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-task approach for disentangling syntax and semantics in sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1254</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2453" to="2464" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Sp?rck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntax-guided controlled generation of paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuram</forename><surname>Vadapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="330" to="345" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speaking: From Intention to Articulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Willem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levelt</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/6393.001.0001</idno>
		<imprint>
			<date type="published" when="1993" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards hierarchical discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Valentin Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Dittadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pushing paraphrase away from original sentence: A multi-round paraphrase generation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.135</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1548" to="1557" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1982.1056489</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on learning Representations. International Conference on Learning Representations</title>
		<meeting>the international conference on learning Representations. International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating phrasal and sentential paraphrases: A survey of data-driven methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonnie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorr</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00002</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="387" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Long Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Planning in sentence production: Evidence for the phrase as a default planning scope. Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Randi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meredith</forename><surname>Crowther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Franklin P Tamborello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Lung</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="177" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous hierarchical representations with poincar? variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charline</forename><forename type="middle">Le</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><forename type="middle">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Conrpg: Paraphrase generation using contexts as regularizer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural paraphrase generation with stacked residual LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2923" to="2934" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generating Diverse High-Fidelity Images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>A?ron van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised paraphrasing without translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1605</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6033" to="6039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation for duplicate question detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1056" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating diverse translations with sentence codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1177</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1823" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous relaxation training of discrete latent variable image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Casper Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Beysian DeepLearning workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint learning of a dual SMT system for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D?dac</forename><surname>Sur?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoshi</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Carl Vondrick. 2021. Learning the predictability of the future</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and koray kavukcuoglu</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Relaxedresponsibility hierarchical discrete vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Willetts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenia</forename><surname>Miscouridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Holmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical quantized autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4524" to="4535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>L?vy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
