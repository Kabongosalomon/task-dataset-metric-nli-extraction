<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Saunders</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aston University Birmingham</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vogiatzis</surname></persName>
							<email>g.vogiatzis@aston.ac.ukl.manso@aston.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Aston University Birmingham</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Manso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aston University Birmingham</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised monocular depth estimation has been a subject of intense study in recent years, because of its applications in robotics and autonomous driving. Much of the recent work focuses on improving depth estimation by increasing architecture complexity. This paper shows that state-of-the-art performance can also be achieved by improving the learning process rather than increasing model complexity. More specifically, we propose (i) only using invariant pose loss for the first few epochs during training, (ii) disregarding small potentially dynamic objects when training, and (iii) employing an appearance-based approach to separately estimate object pose for truly dynamic objects. We demonstrate that these simplifications reduce GPU memory usage by 29% and result in qualitatively and quantitatively improved depth maps. The code is available at https://github.com/kieran514/Dyna-DM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of depth estimation from visual data has recently received increased interest because of its emerging application in autonomous vehicles. Using cameras as a replacement to LiDAR and other distance sensors leads to cost efficiencies and can improve environment perception.</p><p>There have been several attempts at visual depth estimation, for example, using stereo images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> and optical flow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. Other methods focus on supervised monocular depth estimation, which requires large amounts of ground truth depth data. Examples of supervised monocular architectures can be found in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21]</ref>. Ground truth depth maps such as those available in the KITTI dataset <ref type="bibr" target="#b8">[9]</ref> are usually acquired using LiDAR sensors. Supervised methods can be very precise but require expensive sensors and post-processing to obtain clear ground truth. This paper focuses on monocular depth estimation, as it can provide the most affordable physical set-up and requires less training data.</p><p>Self-supervised methods exploit information acquired from motion. Examples of architectures using this method are given in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>. While supervised monocular depth estimation currently outperforms self-supervised methods, their performance is converging towards that of supervised ones. Additionally, research has shown that selfsupervised methods are better at generalizing across a variety of environments <ref type="bibr" target="#b25">[26]</ref> (e.g., indoor/outdoor, urban/rural scenes). Many works assume the entire 3D world is a rigid scene, thus ignoring objects that move independently. While convenient for computational reasons, in practice this assumption is frequently violated, leading to inexact pose and depth estimations. To account for each independently moving object, we would need to estimate the motion of the camera and each object separately. Unfortunately, slow-moving objects and deforming dynamic objects such as pedestrians make this a very challenging problem.</p><p>Attempts to account for potentially dynamic objects by detecting them and estimating their motion independently can be found in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b16">[17]</ref>. However, not all potentially dynamic objects in a scene will be moving at any given time (e.g., cars parked on the side of the road). Estimating each potentially dynamic objects' motion is not only computationally wasteful, but more importantly, can cause significant errors in depth estimation (see Section 4).</p><p>Self-supervised models are generally trained on the KITTI <ref type="bibr" target="#b8">[9]</ref> and CityScape <ref type="bibr" target="#b6">[7]</ref> datasets, which are known to have a large amount of potentially dynamic objects. Previous literature reports that truly dynamic objects are relatively infrequent (see <ref type="figure" target="#fig_0">Figure 1</ref>) and that static objects represent around 86% of the pixels in KITTI test images <ref type="bibr" target="#b26">[27]</ref>. In addition to being computationally wasteful, estimating static objects' motion can reduce accuracy. To address this and other issues, the paper provides the following contributions:</p><p>? A computationally efficient appearance-based approach to avoid estimating the motion of static objects.</p><p>? Experiments supporting the idea of removing small objects, as they tend to be far from the camera and their motion estimation does not bring significant benefits since they are "almost rigid".</p><p>? Finally, based on experimental results, the paper proposes limiting the use of invariant pose loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Depth Estimation</head><p>Self-supervised methods use image reconstruction between consecutive frames as a supervisory signal. Stereo methods use synchronized stereo image pairs to predict the disparity between pixels <ref type="bibr" target="#b9">[10]</ref>. For monocular video, the framework for simultaneous self-supervised learning of depth and ego-motion via maximising photometric consistency was introduced by Zhou et al. <ref type="bibr" target="#b29">[30]</ref>. It uses egomotion, depth estimation and an inverse warping function to find reconstructions for consecutive frames. This early model laid the foundation of self-supervised monocular depth estimation. Monodepth2 by Godard et al. <ref type="bibr" target="#b10">[11]</ref> presented further improvements by masking stationary pixels that would cause infinite-depth estimations. They cast the problem as the minimisation of image reprojection error while addressing occluded areas between two consecutive images. In addition, they employed multi-scale reconstruction, allowing for consistent depth estimations between the layers of the depth decoder. Monodepth2 is currently the most widely used baseline for depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Motion Estimation</head><p>The previous methods viewed dynamic objects as a nuisance, masking non-static objects to find ego-motion. Later methods such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref> showed that there is a performance improvement when using a separate object-motion estimation for each potentially dynamic object. Casser et al. <ref type="bibr" target="#b4">[5]</ref> estimated object motion for potentially dynamic objects given predefined segmentation maps, cancelling the camera motion to isolate object motion estimations. Furthermore, Insta-DM by Lee et al. <ref type="bibr" target="#b16">[17]</ref> improved previous models by using forward warping for dynamic objects and inverse warping for static objects to avoid stretching and warping dynamic objects. Additionally, this method uses a depth network (DepthNet) and pose network (PoseNet) based on a ResNet18 encoder-decoder structure following <ref type="bibr" target="#b21">[22]</ref>. Both methods treat all potentially dynamic objects in a scene as dynamic, therefore calculating object motion estimations for each individual object.</p><p>There have been many appearance-based approaches using optical flow estimations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. Recent methods have combined geometric motion segmentation and appearancebased approaches <ref type="bibr" target="#b28">[29]</ref>. These methods tackle the complex problem of detecting object motion, although this is a step forward for self-supervision, motion estimations can be jittery and inaccurate.</p><p>Recently, Safadoust et al. <ref type="bibr" target="#b22">[23]</ref> demonstrated detecting dynamic objects in a scene using an auto-encoder. While this seems promising for reducing computation and improving depth estimation, it tends to struggle with texture-less regions such as the sky and plain walls. More complex methods have approached the detection of dynamic objects using transformers <ref type="bibr" target="#b26">[27]</ref> resulting in state-of-the-art performance in detecting dynamic objects but requiring heavy computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose Dyna-DM, an end-to-end self-supervised framework that learns depth and motion using sequential monocular video data. It introduces the detection of truly dynamic objects using the S?rensen-Dice coefficient (Dice). We later discuss the use of invariant pose and the necessity of object motion estimations for objects at a large distance from the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-supervised monocular depth estimation</head><p>To estimate depth in a self-supervised manner we consider consecutive RGB frames (I 1 , I 2 ) in a video sequence. We then estimate depth (D 1 , D 2 ) for both of these images using the inverse of the Disparity Network's (DispNet) output. To estimate the motion in the scene, two trainable pose networks (PoseNet) with differing weights are used, one optimized for ego-motion and another to handle objectmotion. The initial background images are computed by removing all potentially dynamic objects' masks (M i 1 , M i 2 ), corresponding to (I 1 , I 2 ) respectively. These binary masks are obtained using an off-the-shelf instance segmentation <ref type="figure">Figure 2</ref>. The input image and the background mask are used to calculate the initial ego-motion. Before removing static objects we remove small objects (objects less than 0.75% of the image's pixel count) leading to an updated background mask. This mask is further processed using the theta filter, removing objects with a Dice value greater than 0.9. This results in an updated background mask with truly dynamic objects only. <ref type="figure">Figure 3</ref>. On the left, a dynamic object (in red) is warped using initial ego-motion, resulting in a warped source mask (green). Calculating the IoU and Dice coefficients between the target mask (blue) and warped source mask (green) a relatively small Dice value is observed. On the right, larger values for IoU and Dice were obtained following the same process.</p><p>model Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>. The next step is to estimate the ego-motion (P i=0 1?2 , P i=0 2?1 ) using these two background images. Following <ref type="bibr" target="#b16">[17]</ref>, let us define inverse and forward warping, as follows:</p><formula xml:id="formula_0">F f w (I 1 , D 1 , P 1?2 , K) ?? f w 1?2<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">F iw (I 1 , D 2 , P 2?1 , K) ?? iw 1?2 ,<label>(2)</label></formula><p>where K is the camera matrix. Furthermore, we forward warp the image and mask using ego-motion, that we previously calculated, resulting in a forward warped image? f w 1?2 and a forward warped maskM f w,i=n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1?2</head><p>. We feed the pixelwise product of the forward warped mask and image and the pixel-wise product of the target mask M i=n 2 and target image I 2 into the object motion network. Resulting in an object motion estimation for each potentially dynamic object</p><formula xml:id="formula_2">(P i=n 1?2 , P i=n 2?1 ),</formula><p>where n = 1, 2, 3 . . . represents each potentially dynamic object. Using these object motion estimates we can inverse warp each object to give? f w?iw,i=n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1?2</head><p>. The inverse warped background image is represented as? iw,i=0</p><p>1?2 . The final warped image is a combination of the background warped image and all of the warped potentially dynamic objects as follows:</p><formula xml:id="formula_3">I 1?2 =? iw,i=0 1?2 + n?(1,2,...)? f w?iw,i=n 1?2 .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Truly Dynamic Objects</head><p>To detect truly dynamic objects we first define an initial ego-motion network that uses pre-trained weights from Insta-DM's ego-motion network. The binary masks from Insta-DM will also be used as our initial binary mask that contains all potentially dynamic objects.</p><formula xml:id="formula_4">M initial 1,2 = (1?? n?(1,2,...) M i=n 1 )?(1?? n?(1,2,...) M i=n 2 ) (4)</formula><p>Then, similarly to section 3.1, we find the background image using pixel-wise multiplication ( ) and determine the initial ego-motion estimation between frames I 1 and I 2 .</p><formula xml:id="formula_5">P initial 1?2 = Ego(M initial 1,2 I 1 , M initial 1,2 I 2 ) (5)</formula><p>We then forward warp all of these masked potentially dynamic objects using this initial ego-motion.</p><formula xml:id="formula_6">M i=n 1?2 = F f w (M i=n 1 , D 1 , P initial 1?2 , K)<label>(6)</label></formula><p>Assuming perfect precision for pose and depth implies that the object mask will have been warped to the object mask in the target image if the object is represented by egomotion i.e., if the object is static. In other words, if an object is static, there will be a significant overlap between its warped mask (under initial ego-motion) and its mask on the target image. Conversely, if the object is truly dynamic, the warped and target masks will not match. This type of overlap or discrepancy can be captured using the S?rensen-Dice coefficient (Dice) or the Jaccard Index, also known as Intersection over Union (IoU):</p><formula xml:id="formula_7">Dice(M i=n 1?2 , M i=n 2 ) = 2|M i=n 1?2 ? M i=n 2 | |M i=n 1?2 | + |M i=n 2 | &lt; ? (7) IoU (M i=n 1?2 , M i=n 2 ) = |M i=n 1?2 ? M i=n 2 | |M i=n 1?2 ? M i=n 2 | &lt; ?<label>(8)</label></formula><p>Warping of dynamic and static objects using ego-motion is depicted in <ref type="figure">Figure 3</ref>. Stationary objects have greater Dice values than dynamic objects. Potentially dynamic objects that have Dice values lower than the selected value of theta will be classed as truly dynamic objects. Testing both IoU and Dice, we found that the Dice coefficient led to more accurate dynamic object detection, therefore the proposed solution is to use the Dice coefficient. We found optimal theta values around the range [0. <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Note however that the reasoning presented above is based on the assumption that depth and pose estimations are accurate. This is unrealistic in most circumstances so we can expect larger Dice values with dynamic objects. To mitigate this challenge we can use greater frame separation between the source and target frames. This is simply done by calculating the reconstruction between frames t and t+2 rather than frames t and t + 1. This extra distance between the frames gives dynamic objects more time to diverge from where ego-motion will warp them to be, which is beneficial for slow-moving objects. With modern camera frame rates, extra distance does not cause jitters, but leads to more consistency in depth and more exact pose. As these objects can be moving very slowly, calculating the reconstruction between larger frames can be beneficial in determining imperceptible object motion.</p><p>To remove static objects, the method decreasingly sorts all potentially dynamic objects based on their Dice values and selects the first 20 objects. These objects are the most likely to be static and tend to be larger objects, for theta less than 1, these objects with large Dice values will be removed first. The discrepancy between these metrics at theta equal to 1 can be explained by the metrics choosing different objects as the first 20 objects.</p><p>In summary, we filter using Dice scores to remove all static objects in the initial binary mask, providing an updated binary mask that only contains truly dynamic objects. We refer to this filtering process as theta filtering. This can be used to calculate an updated ego-motion estimation and object motion estimations for each truly dynamic object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Small Objects &amp; Invariant Pose</head><p>We improve this method further in two aspects: by removing small objects and revisiting the usefulness of using invariant pose. Firstly, small objects are objects that take a small pixel count in an image. We define these objects as less than 1% of an image's pixel count. Motion estimation for small objects tends to be either inaccurate or insignificant as these objects are frequently at a very large distance from the camera. The removal of small objects and static objects is depicted in <ref type="figure">Figure 2</ref>.</p><p>Secondly, when calculating poses in 3D space, some methods exploit the relationship between the forward and backward pose as a loss function known as invariant pose, as similarly shown in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12]</ref>.</p><formula xml:id="formula_8">L I = |P 1?2 + P 2?1 |<label>(9)</label></formula><p>We performed an ablation study to test the usefulness of calculating forward and backward pose versus considering a single direction, which would lead to a reduction in computing, significant enough to outweigh any accuracy improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Final Loss</head><p>Photometric consistency loss L pe will be our main loss function in self-supervision. Following Insta-DM <ref type="bibr" target="#b16">[17]</ref>, we apply consistency to the photometric loss function using a weighted valid mask V 1?2 (p) to handle invalid instance regions, view exiting and occlusion.</p><formula xml:id="formula_9">L pe = p?P V 1?2 (p) ? [(1 ? ?) ? |I 2 (p) ?? 1?2 (p)| 1 + ? 2 (1 ? SSIM (I 2 (p),? 1?2 (p)))]<label>(10)</label></formula><p>The geometric consistency loss L g is a combination of the depth inconsistency map D dif f 1?2 and the valid instance mask M 1?2 as shown in <ref type="bibr" target="#b16">[17]</ref>;</p><formula xml:id="formula_10">L g = p?PM 1?2 (p) ? D dif f 1?2 (p).<label>(11)</label></formula><p>To smooth textures and noise while holding sharp edges, we applying an edge aware smoothness loss L s proposed by <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_11">L s = p?P (?D 1 (p) ? e ??I1(p) ) 2<label>(12)</label></formula><p>Finally, as there will be trivial cases for infinite depth for moving objects that have the same motion as the camera as discussed in <ref type="bibr" target="#b10">[11]</ref>, we use object height constraint loss L h as proposed by <ref type="bibr" target="#b4">[5]</ref> given by;</p><formula xml:id="formula_12">L h = n?{1,2,... } 1 D ? |D M i=n ? f y ? p h h i=n | 1<label>(13)</label></formula><p>Where p h is a learnable height prior, h i=n is the learnable pixel height of the n th object andD is the mean estimate depth. The final objective function is a weighted combination of the previously mentioned loss functions:</p><formula xml:id="formula_13">L = ? I L I + ? pe L pe + ? g L g + ? s L s + ? h L h .<label>(14)</label></formula><p>We followed the path of reconstructing I 2 from I 1 , although the reconstruction of I 1 from I 2 would follow the same process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>This section describes the experiments carried out. They show that (1) invariant pose is not necessary as an extra supervisory signal after the first few epochs, (2) theta filtering removes most static objects assumed to be dynamic, leading to accuracy improvements, and (3) removing small objects leads to better reconstructions. All of these modifications to previous approaches are shown to significantly reduce the cost of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Testing: We will be using the KITTI benchmark following the Eigen et al. split <ref type="bibr" target="#b7">[8]</ref>. The memory usage and power consumption were recorded as the maximum memory used during training GPUm and maximum power used while training GPUp recorded using Weights &amp; Biases <ref type="bibr" target="#b3">[4]</ref>. For CityScape, as there is no ground truth data, we will record the loss in equation 15 to inform us of improvements when doing hyperparameter tuning.</p><formula xml:id="formula_14">L = ? pe L pe + ? g L g + ? s L s<label>(15)</label></formula><p>Implementation Details: Pytorch <ref type="bibr" target="#b19">[20]</ref> is used for all models, training the networks on a single NVIDIA RTX3060 GPU with the ADAM optimiser, setting ? 1 = 0.9 and ? 2 = 0.999. Input images have resolution 832?256 and are augmented using random scaling, cropping and horizontal flipping. The batch size is set to 1 and we will be training the network with an epoch size of 1000 with 100 epochs. Initially setting the learning rate to 10 ?4 , and using exponential learning rate decay with gamma = 0.975. Finally, the weights are set to (? I , ? pe , ? g , ? s , ? h ) = (0.1, 2, 1, 0.1, 0.02) with ? = 0.85 as defined in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Network Details: Following from <ref type="bibr" target="#b21">[22]</ref> we use Disp-Net, which is an encoder-decoder. This auto-encoder uses single-scale training as <ref type="bibr" target="#b2">[3]</ref> suggests faster convergence and better results. Ego-PoseNet, Obj-PoseNet and Initial Ego-PoseNet are all based on multiple PoseNet auto-encoders, but they do not share weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Invariant pose</head><p>As we are training from scratch using He weight initialization <ref type="bibr" target="#b14">[15]</ref>, we initially reap benefits from training using the forward-backward pose consistency. After a few epochs, this benefit diminishes and can lead to small reconstruction errors as the pose estimations will differ in the two directions. <ref type="table" target="#tab_0">Table 1</ref> shows the test scores with and without the invariant loss. It can be appreciated that after training on the CityScape and KITTI datasets, continuing to use the invariant pose to enforce consistency between backward and forward pose leads to minimal change in the validation metrics. Furthermore, this consistency check comes at a significant cost in terms of memory and power usage. The invariant pose requires the backward and forward pose estimation for the background and each potentially dynamic object, whereas without invariant pose we only require one direction, for example, calculating just the forward pose. Also, if these networks are initialised using pre-trained weights then accuracy improvements from invariant pose become even less significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Tuning the Theta parameter</head><p>This section focuses on selecting a value of the filtering parameter, theta, used to determine if an object is dynamic. Theta filtering removes all objects that are determined as static, leaving only object motion estimations for dynamic objects. Using the IoU measure (Jaccard index) as our measure, we iterative select theta values as described in <ref type="table">Table  2</ref>. This table demonstrates that a value of 0.9 leads to the greatest reduction in loss while also reducing memory and power usage.  <ref type="table">Table 2</ref>. Using the Jaccard index to remove static objects with varying theta values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant Pose</head><p>Furthermore, we explore an increased intra-frame distance to handle slow-moving objects. In <ref type="table">Table 3</ref>, again the optimal value of theta is shown to be 0.9. Now the loss is shown to be less than in <ref type="table">Table 2</ref>, suggesting that extra distance leads to improvements in detecting dynamic objects.  <ref type="table">Table 3</ref>. Using the Jaccard index and extra distance to determine if an object is dynamic based on three frames rather than two frames.</p><p>Replacing the Jaccard Index with the S?rensen-Dice coefficient, we get the optimal theta at 0.9 being as seen in <ref type="table">Table 4</ref>. This measure gives even greater reduction in loss, memory and power usage. <ref type="table">Tables 2, 3</ref>, 4, show loss increases when using ? values under 0.9. Arguably, doing so increases the probability of misclassifying dynamic objects as static.  <ref type="table">Table 4</ref>. Using extra distance and Dice coefficient to detect dynamic objects.</p><p>Using these methods, we must determine how many potentially dynamic objects we will process for each scene in advance. Previously, Insta-DM <ref type="bibr" target="#b16">[17]</ref> focused on a maximum of 3 potentially dynamic objects, whereas here we will use a maximum of 20.</p><p>We base our theta experimentation on the CityScape dataset as it is known to have more potentially dynamic objects than the KITTI dataset. This allows us to determine the optimal value of theta in a setting where this method is potentially more valuable. Now testing with the KITTI dataset we obtain the <ref type="table">Table 5</ref>. A theta value of 0.9 is again optimal in this dataset, also showing a reduction in power and memory usage as theta decreases. To explore more quantitative evidence, we took the first 500 potentially dynamic objects from the validation set and labelled them as either dynamic or non-dynamic. Then exporting the IoU and Dice coefficient values for each associated object. For these values, we iteratively modify theta and determine which value was optimal for detecting if the object was truly dynamic. Using a theta value of 0.9 with the Jaccard index the method was only accurate 65% of the time. Whereas, when using the Dice coefficient, with a theta value of 0.9, the method was accurate 74% of the time. Although ? = 0.9 seems to lead to the greatest loss reduction, the selection of this value is depended on the user as smaller values may lead to more misclassifications but lead to greater GPU memory and power reductions.  <ref type="table">Table 6</ref>. The table shows the removal of small objects in each image of size less than the first column's percentage value. <ref type="table">Table 6</ref> demonstrates the removal of objects smaller than a specific percentage of the images pixel count. We are using a theta value of 0.9 and the S?rensen-Dice coefficient with extra intra-frame distance. We observe that as the percent value increases we will be removing more objects and therefore reducing memory and power usage. But we see an increase in loss after 0.75% as objects greater than this could represent small objects than are close to the camera, like children. This would be greatly detrimental if we ig-  <ref type="table">Table 5</ref>. Training the KITTI dataset to analyse the optimal value of theta on the Eigen test set. nored these objects, therefore, as suggested by the increasing loss, we will keep this value low to only account for objects at a far distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Small Objects</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison Table</head><p>In summary, our method, which we refer to as Dyna-DM alleviates the need for forward/backward pose consistency and removes all objects that occupy less than 0.75% of the image's pixel count. Finally and most importantly, it determines which objects are dynamic using the S?rensen-Dice coefficient with a theta value of 0.9, therefore only calculating object motion estimations for these truly dynamic objects. We will be comparing our method with Monodepth2 <ref type="bibr" target="#b10">[11]</ref>, Struct2Depth <ref type="bibr" target="#b4">[5]</ref> and Insta-DM <ref type="bibr" target="#b16">[17]</ref>. Dyna-DM has been initialised by weights provided by Insta-DM <ref type="bibr" target="#b16">[17]</ref> and therefore, the invariant pose is not used ? I = 0. We train consecutively through CityScape and then the KITTI dataset for all methods and test with the Eigen test split. Insta-DM and Struct2Depth will be using a maximum number of dynamic objects of 13 and Dyna-DM will use 20. The results are reported in <ref type="table">Table 7</ref>.</p><p>Here we see significant improvements in all metrics except for a3. These metric improvements are accompanied by improvements in GPU usage. We most notably see a 29.6% reduction in memory usage when training comparing Dyna-DM to Insta-DM, this allows for us to improve the accuracy of our pose and depth estimations while requiring less computation. As we know that the CityScape dataset has more potentially dynamic objects than KITTI we can train Dyna-DM and Insta-DM on CityScape and test with the Eigen test split. <ref type="table" target="#tab_7">Table 8</ref> demonstrates even greater improvements in all metrics when comparing Dyna-DM and Insta-DM. Our model can remove most static objects from these object motion estimations while isolating truly dynamic objects, thereby leading to significant improvements in pose estimation. This further improves the reconstructions and leads to improved depth estimations. Estimated depth-maps from samples of the Eigen test split are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The figure shows clear qualitative improvements in depth estimations when compared to Insta-DM. Looking closely at the potentially dynamic objects in the images, we can see greater depth estimations, with sharper edges and clearer outlines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Dyna-DM reduces memory and power usage during training monocular depth-estimation while providing better accuracy in test time. Although not all dynamic objects are always classified accordingly (e.g., slow-moving cars) Dyna-DM detects significant movements which are the ones which would cause the greatest reconstruction errors. We believe that the best step forward is to make this approach completely self-supervised by detecting all dynamic objects, including debris, using an auto-encoder for safety and efficiency. This means we will have to be able to handle textureless regions and lighting issues which will be shown in future work. Other further improvements include using depth maps to determine objects' distances, and removing object motion estimations for objects at larger distances.  <ref type="table">Table 7</ref>. Dyna-DM uses no invariant pose loss and a theta value 0.9 with the S?rensen-Dice coefficient to detect dynamic objects, removing all objects ? 0.75% of the images pixel count. Training for the whole CityScape and KITTI datasets, we compare methods Monodepth2 <ref type="bibr" target="#b10">[11]</ref>, Insta-DM <ref type="bibr" target="#b16">[17]</ref> and Struct2Depth <ref type="bibr" target="#b4">[5]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CityScape Only</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Dynamic objects have been consistently ignored in selfsupervised monocular depth estimations. Our proposed method, Dyna-DM, isolates truly dynamic objects in a scene using an appearance-based approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Depth maps comparing Dyna-DM to Insta-DM showing qualitative improvements for Dyna-DM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AbsRel? SqRel? RMSE? RMSE log? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ? GPUm? GPUp ? I.P. To compare the usefulness of the pose invariant loss function, we compare the test scores with and without the invariant loss. Initially, we train on CityScape and then continue training with the KITTI dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">0.124</cell><cell>0.809</cell><cell>4.897</cell><cell>0.201</cell><cell>0.847</cell><cell>0.954</cell><cell>0.981</cell><cell>10.8GB 121W</cell></row><row><cell cols="2">w/o I.P.</cell><cell cols="2">0.127</cell><cell>0.844</cell><cell>4.880</cell><cell>0.200</cell><cell>0.842</cell><cell>0.953</cell><cell>0.982</cell><cell>7.4GB</cell><cell>107W</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell cols="3">Jaccard Index</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Loss ?</cell><cell cols="2">GPUm ?</cell><cell>GPUp ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="2">1.220</cell><cell cols="2">9.08GB</cell><cell>100W</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.95</cell><cell cols="2">1.217</cell><cell cols="2">8.90GB</cell><cell>94.7W</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell cols="2">1.216</cell><cell cols="2">8.54GB</cell><cell>87.4W</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.85</cell><cell cols="2">1.233</cell><cell cols="2">8.29GB</cell><cell>84.5W</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell cols="2">1.264</cell><cell cols="2">8.04GB</cell><cell>88.9W</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc>AbsRel? SqRel? RMSE? RMSE log? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ? GPUm? GPUp ?</figDesc><table><row><cell>Monodepth2</cell><cell>0.132</cell><cell>1.044</cell><cell>5.142</cell><cell>0.210</cell><cell>0.845</cell><cell>0.948</cell><cell>0.977</cell><cell>9GB</cell><cell>112W</cell></row><row><cell>Struct2Depth</cell><cell>0.141</cell><cell>1.026</cell><cell>5.290</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell><cell>10GB</cell><cell>116W</cell></row><row><cell>Insta-DM</cell><cell>0.121</cell><cell>0.797</cell><cell>4.779</cell><cell>0.193</cell><cell>0.858</cell><cell>0.957</cell><cell>0.983</cell><cell cols="2">9.48GB 107.9W</cell></row><row><cell cols="2">Dyna-DM (ours) 0.115</cell><cell>0.785</cell><cell>4.698</cell><cell>0.192</cell><cell>0.871</cell><cell>0.959</cell><cell>0.982</cell><cell cols="2">6.67GB 94.0W</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>AbsRel? SqRel? RMSE? RMSE log? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ? GPUm? GPUp ?</figDesc><table><row><cell>Insta-DM</cell><cell>0.178</cell><cell>1.312</cell><cell>6.016</cell><cell>0.257</cell><cell>0.728</cell><cell>0.916</cell><cell>0.966</cell><cell>10.58GB 110.8W</cell></row><row><cell cols="2">Dyna-DM (ours) 0.163</cell><cell>1.259</cell><cell>5.939</cell><cell>0.244</cell><cell>0.768</cell><cell>0.926</cell><cell>0.970</cell><cell>6.63GB 86.99W</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Using the same setup, we train for the whole CityScape dataset. Where we compare Dyna-DM to Insta-DM<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhancing self-supervised monocular depth estimation with traditional visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Andraghetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panteleimon</forename><surname>Myriokefalitakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pier</forename><forename type="middle">Luigi</forename><surname>Dovesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases, 2020. Software available from wandb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<idno>com. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8977" to="8986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning monocular depth in dynamic scenes via instance-aware projection consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02629</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning rigidity in dynamic scenes with a moving camera for 3d motion field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13413</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised monocular scene decomposition and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadra</forename><surname>Safadoust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G?ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using laplacian pyramid-based depth residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjae</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A study of the generalizability of self-supervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atharva</forename><surname>Tendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Rashedul</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning with Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100124</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Instance-aware multi-object self-supervision for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Voicila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Comport</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00809</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to segment rigid motions from two frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1266" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
