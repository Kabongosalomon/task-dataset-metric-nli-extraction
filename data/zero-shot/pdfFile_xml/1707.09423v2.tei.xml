<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
							<email>angli@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
							<email>morariu@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the visual relationship between two objects involves identifying the subject, the object, and a predicate relating them. We leverage the strong correlations between the predicate and the subj, obj pair (both semantically and spatially) to predict predicates conditioned on the subjects and the objects. Modeling the three entities jointly more accurately reflects their relationships compared to modeling them independently, but it complicates learning since the semantic space of visual relationships is huge and training data is limited, especially for longtail relationships that have few instances. To overcome this, we use knowledge of linguistic statistics to regularize visual model learning. We obtain linguistic knowledge by mining from both training annotations (internal knowledge) and publicly available text, e.g., Wikipedia (external knowledge), computing the conditional probability distribution of a predicate given a subj, obj pair. As we train the visual model, we distill this knowledge into the deep model to achieve better generalization. Our experimental results on the Visual Relationship Detection (VRD) and Visual Genome datasets suggest that with this linguistic knowledge distillation, our model outperforms the stateof-the-art methods significantly, especially when predicting unseen relationships (e.g., recall improved from 8.45% to 19.17% on VRD zero-shot testing set).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting visual relationships from images is a central problem in image understanding. Relationships are commonly defined as tuples consisting of a subject (subj), predicate (pred) and object (obj) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1]</ref>. Visual relationships represent the visually observable interactions between subject and object subj, obj pairs, such as person, ride, horse <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recently, Lu et al. <ref type="bibr" target="#b18">[19]</ref> introduce the visual relationship dataset (VRD) to study learning of a large number of visual   <ref type="figure">Figure 1</ref>. Linguistic Knowledge Distillation Framework. We extract linguistic knowledge from training annotations and a public text corpus (green box), then construct a teacher network to distill the knowledge into an end-to-end deep neural network (student) that predicts visual relationships from visual and semantic representations (red box). GT is the ground truth label and "+" is the concatenation operator. relationships from images. Lu et al. predict the predicates independently from the subjects and objects, and use the product of their scores to predict relationships present in a given image using a linear model. The results in <ref type="bibr" target="#b18">[19]</ref> suggest that predicates cannot be predicted reliably with a linear model that uses only visual cues, even when the ground truth categories and bounding boxes of the subject and object are given ( <ref type="bibr" target="#b18">[19]</ref> reports Recall@100 of only 7.11% for their visual prediction). Although the visual input analyzed by the CNN in <ref type="bibr" target="#b18">[19]</ref> includes the subject and object, predicates are predicted without any knowledge about the object categories present in the image or their relative locations. In contrast, we propose a probabilistic model to predict the predicate name jointly with the subject and object names and their relative spatial arrangement: P (R|I) = P (pred|I union , subj, obj)P (subj)P (obj). <ref type="bibr" target="#b0">(1)</ref> While our method models visual relationships more accurately than <ref type="bibr" target="#b18">[19]</ref>, our model's parameter space is also en-larged because of the large variety of relationship tuples. This leads to the challenge of insufficient labeled image data. The straightforward-but very costly-solution is to collect and annotate a larger image dataset that can be used to train this model. Due to the long tail distribution of relationships, it is hard to collect enough training images for all relationships. To make the best use of available training images, we leverage linguistic knowledge (LK) to regularize the deep neural network. One way to obtain linguistic knowledge is to compute the conditional probabilities P (pred|subj, obj) from the training annotations.</p><p>However, the number of subj, pred, obj combinations is too large for each triplet to be observed in a dataset of annotated images, so the internal statistics (e.g., statistics of the VRD dataset) only capture a small portion of the knowledge needed. To address this long tail problem, we collect external linguistic knowledge (P (pred|subj, obj)) from public text on the Internet (Wikipedia). This external knowledge consists of statistics about the words that humans commonly use to describe the relationship between subject and object pairs, and importantly, it includes pairs unseen in our training data. Although the external knowledge is more general, it can be very noisy (e.g., due to errors in linguistic parsing).</p><p>We make use of the internal and external knowledge in a teacher-student knowledge distillation framework <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, shown in <ref type="figure">Figure 1</ref>, where the output of the standard vision pipeline, called the student network, is augmented with the output of a model that uses the linguistic knowledge to score solutions; their combination is called the teacher network. The objective is formulated so that the student not only learns to predict the correct one-hot ground truth labels but also to mimic the teacher's soft belief between predicates.</p><p>Our main contribution is that we exploit the role of both visual and linguistic representations in visual relationship detection and use internal and external linguistic knowledge to regularize the learning process of an end-to-end deep neural network to significantly enhance its predictive power and generalization. We evaluate our method on the VRD <ref type="bibr" target="#b18">[19]</ref> and Visual Genome (VG) <ref type="bibr" target="#b12">[13]</ref> datasets. Our experiments using Visual Genome show that while the improvements due to training set size are minimal, improvements due to the use of LK are large, implying that with current dataset sizes, it is more fruitful to incorporate other types knowledge (e.g., LK) than to increase the visual dataset size-this is particularly promising because visual data is expensive to annotate and there exist many readily available large scale sources of knowledge that have not yet been fully leveraged for visual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work Knowledge Distillation in Deep Neural Networks:</head><p>Recent work has exploited the use of additional information (or "knowledge") to help train deep neural networks (DNN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>. Hinton et al. <ref type="bibr" target="#b8">[9]</ref> proposed a framework to distill knowledge, in this case the predicted distribution, from a large network into a smaller network. Recently, Hu et al. proposed a teacher-student framework to distill massive knowledge sources, including logic rules, into DNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Visual Relationship Detection: Visual relationships represent the interactions between object pairs in images. Lu et al. <ref type="bibr" target="#b18">[19]</ref> formalized visual relationship prediction as a task and provided a dataset with a moderate number of relationships. Before <ref type="bibr" target="#b18">[19]</ref>, a large corpus of work had leveraged the interactions between objects (e.g. object cooccurrence, spatial relationships) to improve visual tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>. To enable visual relationship detection on a large scale, Lu et al. <ref type="bibr" target="#b18">[19]</ref> decomposed the prediction of a relationship into two individual parts: detecting objects and predicting predicates. Lu et al. used the sub-image containing the union of two bounding boxes of object pairs as visual input to predict the predicates and utilized language priors, such as the similarity between relationships and the likelihood of a relationship in the training data, to augment the visual module.</p><p>Plummer et al. <ref type="bibr" target="#b24">[25]</ref> grounded phrases in images by fusing several visual features like appearance, size, bounding boxes, and linguistic cues (like adjectives that describe attribute information). Despite focusing on phrase localization rather than visual phrase detection, when evaluated on the VRD dataset, <ref type="bibr" target="#b24">[25]</ref> achieved comparable results with <ref type="bibr" target="#b18">[19]</ref>. Recently, there are several new attempts for visual relationship detection task: Liang et al. <ref type="bibr" target="#b17">[18]</ref> proposed to detect relationships and attributes within a reinforcement learning framework; Li et al. <ref type="bibr" target="#b16">[17]</ref> trained an end-to-end system boost relationship detection through better object detection; Bo et al. <ref type="bibr" target="#b1">[2]</ref> detected relationships via a relational modeling framework.</p><p>We combine rich visual and linguistic representations in an end-to-end deep neural network that absorbs external linguistic knowledge using the teacher-student framework during the training process to enhance prediction and generalization. Unlike <ref type="bibr" target="#b18">[19]</ref>, which detected objects independently from relationship prediction, we model objects and relationships jointly. Unlike <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref>, which do not use linguistic knowledge explicitly, we focus on predicting predicates using the linguistic knowledge that models correlations between predicates and subj, obj pairs, especially for the long-tail relationships. Unlike <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, which used either the teacher or the student as their final output, we combine both teacher and student networks, as they each have their own advantages: the teacher outperforms in cases with sufficient training data, while the student generalizes to cases with few or no training examples (the zero-shot case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>A straightforward way to predict relationship predicates is to train a CNN on the union of the two bounding boxes that contain the two objects of interest as the visual input, fuse semantic features (that encode the object categories) and spatial features (that encode the relative positions of the objects) with the CNN features (that encode the appearance of the objects), and feed them into a fully connected (FC) layer to yield an end-to-end prediction framework. However, the number of subj, pred, obj tuples is very large and the parameter space of the end-to-end CNN would be huge. While the subject, predicate, and object are not statistically independent, a CNN would require a massive amount of data to discover the dependence structure while also learning the mapping from visual features to semantic relationships. To avoid over-fitting and achieve better predictive power without increasing the amount of visual training data, additional information is needed to help regularize the training of the CNN. <ref type="figure">Figure 1</ref> summarizes our proposed model. Given an image, we extract three input components: the cropped images of the union of the two detected objects (BB-Union); the semantic object representations obtained from the object category confidence score distributions obtained from the detector; and the spatial features (SF) obtained from pairs of detected bounding boxes. We concatenate VGG features, semantic object vectors, and the spatial feature vectors, then train another FC layer using the ground truth label (GT) and the linguistic knowledge to predict the predicate. Unlike <ref type="bibr" target="#b18">[19]</ref>, which used the VGG features to train a linear model, our training is end-to-end without fixing the VGGnet. Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, we call the data-driven model the "student", and the linguistic knowledge regularized model the "teacher".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linguistic Knowledge Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Preliminary: Incorporating Knowledge in DNNs</head><p>The idea of incorporating additional information in DNNs has been exploited recently <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. We adapted Hu et al.'s teacher-student framework <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> to distill linguistic knowledge in a data-driven model. The teacher network is constructed by optimizing the following criterion:</p><formula xml:id="formula_0">min t?T KL(t(Y )||s ? (Y |X)) ? CE t [L(X, Y )],<label>(2)</label></formula><p>where t(Y ) and s ? (Y |X) are the prediction results of the teacher and student networks; C is a balancing term; ? is the parameter set of the student network; L(X, Y ) is a general constraint function that has high values to reward the predictions that meet the constraints and penalize the others. KL measures the KL-divergence of teacher's and student's prediction distributions. The closed-form solution of the optimization problem is:</p><formula xml:id="formula_1">t(Y ) ? s(Y |X)exp(CL(X, Y )) .<label>(3)</label></formula><p>The new objective which contains both ground truth labels and the teacher network is defined as:</p><formula xml:id="formula_2">min ??? 1 n n i=1 ?l(s i , y i ) + (1 ? ?)l(s i , t i ),<label>(4)</label></formula><p>where s i and t i are the student's and teacher's predictions for sample i; y i is the ground truth label for sample i; ? is a balancing term between ground truth and the teacher network. l is the loss function. More details can be found in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Knowledge Distillation for Visual Relationship Detection</head><p>Linguistic knowledge is modeled by a conditional probability that encodes the strong correlation between the pair of objects subj, obj and the predicate that humans tend to use to describe the relationship between them:</p><formula xml:id="formula_3">L(X, Y ) = log P (pred|subj, obj),<label>(5)</label></formula><p>where X is the input data and Y is the output distribution of the student network. P (pred|subj, obj) is the conditional probability of a predicate given a fixed subj, obj pair in the obtained linguistic knowledge set. By solving the optimization problem in Eq. 2, we construct a teacher network that is close to the student network, but penalizes a predicted predicate that is unlikely given the fixed subj, obj pairs. The teacher's output can be viewed as a projection of the student's output in the solution space constrained by linguistic knowledge. For example, when predicting the predicate between a "plate" and a "table", given the subject ("plate") and the object ("table"), and the conditional probability P (pred|plate, table), the teacher will penalize unlikely predicates, (e.g., "in") and reward likely ones (e.g., "on"), helping the network avoid portions of the parameter space that lead to poor solutions.</p><p>Given the ground truth label and the teacher network's output distribution, we want the student network to not only predict the correct predicate labels but also mimic the linguistic knowledge regularized distributions. This is accomplished using a cross-entropy loss (see Eq. 4).</p><p>One advantage of this LK distillation framework is that it takes advantage of both knowledge-based and data-driven systems. Distillation works as a regularizer to help train the data-driven system. On the other hand, since we construct the teacher network based on the student network, the knowledge regularized predictions (teacher's output) will also be improved during training as the student's output improves. Rather than using linguistic knowledge as a post-processing step, our framework enables the data-driven model to absorb the linguistic knowledge together with the ground truth labels, allowing the deep network to learn a better visual model during training rather than only having its output modified in a post-processing step. This leads to a data-driven model (the student) that generalizes better, especially in the zero-shot scenario where we lack linguistic knowledge about a subj, obj pair. While <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> used either the student or the teacher as the final output, our experiments show that both the student and teacher in our framework have their own advantages, so we combine them to achieve the best predictive power (see section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Linguistic Knowledge Collection</head><p>To obtain the linguistic knowledge P (pred|subj, obj), a straightforward method is to count the statistics of the training annotations, which reflect the knowledge used by an annotator in choosing an appropriate predicate to describe a visual relationship. Due to the long tail distribution of relationships, a large number of combinations never occur in the training data; however, it is not reasonable to assume the probability of unseen relationships is 0. To tackle this problem, one can apply additive smoothing to assign a very small number to all 0's <ref type="bibr" target="#b19">[20]</ref>; however, the smoothed unseen conditional probabilities are uniform, which is still confusing at LK distillation time. To collect more useful linguistic knowledge of the long-tail unseen relationships, we exploit text data from the Internet.</p><p>One challenge of collecting linguistic knowledge online is that the probability of finding text data that specifically describes objects and their relationships is low. This requires us to obtain the knowledge from a huge corpus that covers a very large domain of knowledge. Thus we choose the Wikipedia 2014-06-16 dump containing around 4 billion words and 450 million sentences that have been parsed to text by <ref type="bibr" target="#b23">[24]</ref> 1 to extract knowledge.</p><p>We utilize the scene graph parser proposed in <ref type="bibr" target="#b27">[28]</ref> to parse sentences into sets of subj, pred, obj triplets, and we compute the conditional probabilities of predicates based on these triplets. However, due to the possible mistakes of the parser, especially on text from a much wider domain than the visual relationship detection task, the linguistic knowledge obtained can be very noisy. Naive methods such as using only the linguistic knowledge to predict the predicates or multiplying the conditional probability with the data-driven model's output fail. Fortunately, since the teacher network of our LK-distillation framework is constructed from the student network that is also supervised by the labeled data, a well-trained student network can help correct the errors from the noisy external proba-bility. To achieve good predictive power on the seen and unseen relationships, we obtain the linguistic knowledge from both training data and the Wikipedia text corpus by a weighted average of their conditional probabilities when we construct the teachers' network, as shown in Eq. 4. We conduct a two-step knowledge distillation: during the first several training epoches, we only allow the student to absorb the knowledge from training annotations to first establish a good data-driven model. After that, we start distilling the external knowledge together with the knowledge extracted from training annotations weighted by the balancing term C as shown in Eq. 4. The balancing terms are chosen by a validation set we select randomly from the training set (e.g., in VRD dataset, we select 1,000 out of 4,000 images to form the validation set) to achieve a balance between good generalization on the zero-shot and good predictive power on the entire testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic and Spatial Representations</head><p>In <ref type="bibr" target="#b18">[19]</ref>, Lu et al. used the cropped image containing the union of two objects' bounding boxes to predict the predicate describing their relationship. While the cropped image encodes the visual appearance of both objects, it is difficult to directly model the strong semantic and spatial correlations between predicates and objects, as both semantic and spatial information is buried within the pixel values of the image. Meanwhile, the semantic and spatial representations capture similarities between visual relationships, which can generalize better to unseen relationships.</p><p>We utilize word-embedding <ref type="bibr" target="#b21">[22]</ref> to represent the semantic meaning of each object by a vector. We then extract spatial features similarly to the ones in <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_4">x min W , y min H , x max W , y max H , A A img ,<label>(6)</label></formula><p>where W and H are the width and height of the image, A and A img are the areas of the object and the image, respectively. We concatenate the above features of two objects as the spatial feature (SF) for a subj, obj pair. We predict the predicate conditioned on the semantic and spatial representations of the subject and object:</p><formula xml:id="formula_5">P (R|I) =P (pred|subj, obj, B s , B o , I) ? P (subj, B s |I)P (obj, B o |I),<label>(7)</label></formula><p>where subj and obj are represented using the semantic object representation, B s and B o are the spatial features, and I is the image region of the union of the two bounding boxes. For the BB-Union input, we use the same VGGnet <ref type="bibr" target="#b28">[29]</ref> in <ref type="bibr" target="#b18">[19]</ref> to learn the visual feature representation.</p><p>We adopt a pre-trained word2vec vectors weighted by confidence scores of each object category for the subject and the object, then concatenate the two vectors as the semantic representation of the subject and the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on Visual Relationship Detection <ref type="bibr" target="#b18">[19]</ref> and Visual Genome <ref type="bibr" target="#b12">[13]</ref> datasets for three tasks: Predicate detection: given an input image and a set of ground truth bounding boxes with corresponding object categories, predict a set of predicates describing each pair of objects. This task evaluates the prediction of predicates without relying on object detection. Phrase detection: given an input image, output a phrase subj, pred, obj and localize the entire phrase as one bounding box. Relationship detection: given an input image, output a relationship subj, pred, obj and both the subject and the object with their bounding boxes.</p><p>Both datasets have a zero-shot testing set that contains relationships that never occur in the training data. We evaluate on the zero-shot sets to demonstrate the generalization improvements brought by linguistic knowledge distillation.</p><p>Implementation Details. We use VGG-16 <ref type="bibr" target="#b28">[29]</ref> to learn the visual representations of the BB-Union of two objects. We use a pre-trained word2vec <ref type="bibr" target="#b21">[22]</ref> model to project the subjects and objects into vector space, and the final semantic representation is the weighted average based on the confidence scores of a detection. For the balancing terms, we choose C = 1 and ? = 0.5 to encourage the student network to mimic the teacher and the ground truth equally.</p><p>Evaluation Metric. We follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref> using Recall@n (R@n) as our evaluation metric (mAP metric would mistakenly penalize true positives because annotations are not exhaustive). For two detected objects, multiple predicates are predicted with different confidences. The standard R@n metric ranks all predictions for all object pairs in an image and compute the recall of top n. However, instead of computing recall based on all predictions, <ref type="bibr" target="#b18">[19]</ref> considers only the predicate with highest confidence for each object pair. Such evaluation is more efficient and forced the diversity of object pairs. However, multiple predicates can correctly describe the same object pair and the annotator only chooses one as ground truth, e.g., when describing a person "next to" another person, predicate "near" is also plausible. So we believe that a good predicted distribution should have high probabilities for all plausible predicate(s) and probabilities close to 0 for remaining ones. Evaluating only the top prediction per object pair may mistakenly penalize correct predictions since annotators have bias over several plausible predicates. So we treat the number of chosen predictions per object pair (k) as a hyper-parameter, and report R@n for different k's to compare with other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Since the number of predicates is 70, k = 70 is equivalent to evaluating all predictions w.r.t. two detected objects. <ref type="table">Table 1</ref>. Predicate Detection on VRD Testing Set: "U" is the union of two objects' bounding boxes; "SF" is the spatial feature; "W" is the word-embedding based semantic representations; "L" means using LK distillation; "S" is the student network; "T" is the teacher network and "S+T" is the combination of them. Part 1 uses the VRD training images; Part 2 uses the training images in VRD <ref type="bibr" target="#b18">[19]</ref> and images of Visual Genome (VG) <ref type="bibr" target="#b12">[13]</ref>  We first evaluate it on predicate prediction (as in <ref type="bibr" target="#b18">[19]</ref>).</p><p>Since <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> do not report results of predicate prediction, we compare our results with ones in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>. Part 1 of <ref type="table">Table 1</ref> shows the results of linguistic knowledge distillation with different sets of features in our deep neural networks. In addition to the data-driven baseline "Baseline: U only", we also compare with the baseline that only uses linguistic priors to predict a predicate, which is denoted as "Baseline: L only". The "Visual Phrases" method <ref type="bibr" target="#b25">[26]</ref> trains deformable parts models for each relationship; "Joint CNN" <ref type="bibr" target="#b5">[6]</ref> trains a 270-way CNN to predict the subject, object and predicate together. The visual only model and the full model of <ref type="bibr" target="#b18">[19]</ref> that uses both visual input and language priors are denoted as "VRD-V only" and "VRD-Full". S denotes using the student network's output as the final prediction; T denotes using the teacher network's output. T+S denotes that for subj, obj pairs that occur in the training data, we use the teacher network's output as the final prediction; for subj, obj pairs that never occur in training, we use the student network's output.</p><p>End-to-end CNN training with semantic and sparecall values using their code.  <ref type="figure">Figure 2</ref>. Visualization of predicate detection results: "Data-driven" denotes the baseline using BB-Union; "LK only" denotes the baseline using only the linguistic knowledge without looking at the image; "Full model student" denotes the student network with U+W+SF features; "Full model teacher" denotes the teacher network with U+W+SF features.</p><p>tial representations. Comparing our baseline, which uses the same visual representation (BB-Union) as <ref type="bibr" target="#b18">[19]</ref>, and the "VRD-V only" model, our huge recall improvement (R@100/50, k=1 increases from 7.11% <ref type="bibr" target="#b18">[19]</ref> to 34.82%) reveals that the end-to-end training with soft-max prediction outperforms extracting features from a fixed CNN + linear model method in <ref type="bibr" target="#b18">[19]</ref>, highlighting the importance of finetuning. In addition, adding the semantic representation and the spatial features improves the predictive power and generalization of the data-driven model <ref type="bibr" target="#b3">4</ref> .</p><p>To demonstrate the effectiveness of LK-distillation, we compare the results of using different combinations of features with/without using LK-distillation. In Part 1 of <ref type="table">Table  1</ref>, we train and test our model on only the VRD dataset, and use the training annotation as our linguistic knowledge. "Linguistic knowledge only" baseline ("Baseline: L only") itself has a strong predictive power and it outperforms the state-of-the-art method <ref type="bibr" target="#b18">[19]</ref> by a large margin (e.g., 51.34% vs. 47.87% for R@100/50, k=1 on the entire VRD test set), which implies the knowledge we distill in the datadriven model is reliable and discriminative. However, since, some subj, obj pairs in the zero-shot test set never occur in the linguistic knowledge extracted from the VRD train set, trusting only the linguistic knowledge without looking at the images leads to very poor performance on the zeroshot set of VRD, which explains the poor generalization of "Baseline: L only" method and addresses the need for combining both data-driven and knowledge-based methods as the LK-distillation framework we propose does.</p><p>The benefit of LK distillation is visible across all feature settings: the data-driven neural networks that absorb linguistic knowledge ("student" with LK) outperform the data-driven models significantly (e.g., R@100/50, k=1 is improved from 37.15% to 42.98% for "U+W" features on the entire VRD test set). We also observe consistent improvement of the recall on the zero-shot test set of datadriven models that absorb the linguistic knowledge. The student networks with LK-distillation yield the best generalization, and outperform the data-driven baselines and knowledge only baselines by a large margin.</p><p>Unlike <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, where either the student or the teacher is the final output, we achieve better predictive power by combining both: we use the teacher network to predict the predicates whose subj, obj pairs occur in the training data, and use the student network for the remaining. The setting "U+W+SF+LK: T+S" performs the best. <ref type="figure">Fig. 2</ref>(a) and 2(b) show a visualization of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Phrase and Relationship Detection</head><p>To enable fully automatic phrase and relationship detection, we train a Fast R-CNN detector <ref type="bibr" target="#b6">[7]</ref> using VGG-16 for object detection. Given the confidence scores of detected each detected object, we use the weighed word2vec vectors as the semantic object representation, and extract spatial features from each detected bounding box pairs. We then use the pipeline in <ref type="figure">Fig. 1</ref> to obtain the predicted predicate distribution for each pair of objects. According to Eq. 7, we use the product of the predicate distribution and the confi- <ref type="table">Table 2</ref>. Phrase and Relationship Detection: Distillation of Linguistic Knowledge. We use the same notations as in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase Detection</head><p>Relationship <ref type="table" target="#tab_6">Detection  R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50,  k=1  k=1  k=10  k=10  k=70  k=70  k=1  k=1  k=10  k=10  k=70  k=70</ref> Part  dence scores of the subject and object as our final prediction results. We also adopt the triplet NMS in <ref type="bibr" target="#b16">[17]</ref> to remove redundant detections. To compare with <ref type="bibr" target="#b18">[19]</ref>, we report R@n, k=1 for both phrase detection and relationship detection. For fair comparison with <ref type="bibr" target="#b24">[25]</ref> (denoted as "Linguistic Cues"), we choose k=10 as they did to report recall. In addition, we report the full recall measurement k=70. Evaluation results on the entire dataset and the zero-shot setting are shown in Part 1 of <ref type="table" target="#tab_4">Tables 2 and 3</ref>. Our method outperforms the state-of-the-art methods in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b24">[25]</ref> significantly on both entire testing set and zero-shot setting. The observations about student and teacher networks are consistent with predicate prediction evaluation. We also compare our method with the very recently introduced "VIP-CNN" in <ref type="bibr" target="#b16">[17]</ref> and "VRL" <ref type="bibr" target="#b17">[18]</ref> and achieve better or comparable results. For phrase detection, we achieve better results than <ref type="bibr" target="#b17">[18]</ref> and get similar result for R@50 to <ref type="bibr" target="#b16">[17]</ref>. One possible reason that <ref type="bibr" target="#b16">[17]</ref> gets better result for R@100 is that they jointly model the object and predicate detection while we use an off-the-shelf detector. For relationship detection, we outperform both methods, especially on the zero-shot set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Visual Genome Dataset</head><p>We also evaluate predicate detection on Visual Genome (VG) <ref type="bibr" target="#b12">[13]</ref>, the largest dataset that has visual relationship annotations. We randomly split the VG dataset into training (88,077 images) and testing set (20,000 images) and select the relationships whose predicates and objects occur in the VRD dataset. We conduct a similar evaluation on the dataset (99,864 relationship instances in training and 19,754 in testing; 2,056 relationship test instances are never seen in training). We use the linguistic knowledge extracted from VG and report predicate prediction results in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>Not surprisingly, we observe similar behavior as on the VRD dataset-LK distillation regularizes the deep model and improves its generalization. We conduct another experiment in which images from Visual Genome dataset augment the training set of VRD and evaluate on the VRD test set. From the Part 2 of Tables 1, 2 and 3, we observe that training with more data leads to only marginal improvement over almost all baselines and proposed methods. However, for all experimental settings, our LK distillation framework still brings significant improvements, and the combination of the teacher and student networks still yields the best performance. This reveals that incorporating additional knowledge is more beneficial than collecting more data 5 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distillation with External Knowledge</head><p>The above experiments show the benefits of extracting linguistic knowledge from internal training annotations and distilling them in a data-driven model. However, training annotations only represent a small portion of all possible relationships and do not necessarily represent the real world distribution, which has a long tail. For unseen long-tail relationships in the VRD dataset, we extract the linguistic knowledge from external sources: the Visual Genome annotations and Wikipedia, whose domain is much larger than any annotated dataset. In <ref type="table" target="#tab_6">Table 5</ref>, we show predicate detection results on the VRD test set using our linguistic knowledge distillation framework with different sources of knowledge. From Part 2 and Part 4 of <ref type="table" target="#tab_6">Table 5</ref>, we observe that using only the external knowledge, especially the very noisy one obtained from Wikipedia, leads to bad performance. However, interestingly, although the external knowledge can be very noisy (Wikipedia) and has a different distribution when compared with the VRD dataset (Visual Genome), the performance of the teacher network us- Our Method VRD-Full <ref type="figure">Figure 3</ref>. Performance with varying sizes of training examples. "Our Method" denotes the student network that absorbs linguistic knowledge from both VRD training annotations and the Wikipedia text. "VRD-Full" is the full model in <ref type="bibr" target="#b18">[19]</ref>.</p><p>ing external knowledge is much better than using only the internal knowledge (Part 1). This suggests that by properly distilling external knowledge, our framework obtains both good predictive power on the seen relationships and better generalization on unseen ones. Evaluation results of combining both internal and external linguistic knowledge are shown in Part 3 and Part 5 of <ref type="table" target="#tab_6">Table 5</ref>. We observe that by distilling external knowledge and the internal one, we improve generalization significantly (e.g., LK from Wikipedia boosts the recall to 19.17% on the zero-shot set) while maintaining good predictive power on the entire test set. <ref type="figure">Fig. 3</ref> shows the comparison between our student network that absorbs linguistic knowledge from both VRD training annotations and the Wikipedia text (denoted as "Our Method") and the full model in <ref type="bibr" target="#b18">[19]</ref> (denoted as "VRD-Full"). We observe that our method significantly outperforms the existing method, especially for the zeroshot (relationships with 0 training instance) and the fewshot setting (relationships with few training instances, e.g., ? 10). By distilling linguistic knowledge into a deep model, our data-driven model improves dramatically, which is hard to achieve by only training on limited labeled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a framework that distills linguistic knowledge into a deep neural network for visual relationship detection. We incorporated rich representations of a visual relationship in our deep model, and utilized a teacher-student distillation framework to help the data-driven model absorb internal (training annotations) and external (public text on the Internet) linguistic knowledge. Experiments on the VRD and the Visual Genome datasets show significant improvements in accuracy and generalization capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Teacher'Network'Output'</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Linguistic Knowledge</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Distillation in Deep Model</cell></row><row><cell></cell><cell cols="2">W o rd</cell><cell>e m b e d d in g</cell><cell>+ Person shirt</cell><cell>Student Network's Output</cell></row><row><cell>Object</cell><cell cols="3">Crop</cell></row><row><cell>Detection Training annotations (VRD, Visual Genome)</cell><cell></cell><cell cols="2">S p a ti a l fe a tu re</cell><cell>+</cell><cell>LK Distillation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Teacher Network's</cell></row><row><cell>External textual data</cell><cell>Parse</cell><cell cols="2">table / with / table car / has / engine person / has / hand</cell><cell>Output</cell></row><row><cell>(Wikipedia)</cell><cell></cell><cell></cell><cell>? ? bowl / in / hand</cell></row><row><cell cols="4">Linguistic Knowledge Collection</cell></row></table><note>? FC</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>dataset.</figDesc><table><row><cell></cell><cell cols="2">Entire Set</cell><cell></cell><cell></cell><cell>Zero-shot</cell><cell></cell></row><row><cell></cell><cell cols="6">R@100/50 2 R@100 R@50 R@100/50 R@100 R@50</cell></row><row><cell></cell><cell>k=1</cell><cell>k=70</cell><cell>k=70</cell><cell>k=1</cell><cell>k=70</cell><cell>k=70</cell></row><row><cell cols="2">Part 1: Training images VRD only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visual Phrases [26]</cell><cell>1.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint CNN [6]</cell><cell>2.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VRD-V only [19]</cell><cell>7.11</cell><cell cols="2">37.20 3 28.36</cell><cell>3.52</cell><cell>32.34</cell><cell>23.95</cell></row><row><cell>VRD-Full [19]</cell><cell>47.87</cell><cell>84.34</cell><cell>70.97</cell><cell>8.45</cell><cell>50.04</cell><cell>29.77</cell></row><row><cell>Baseline: U only</cell><cell>34.82</cell><cell>83.15</cell><cell>70.02</cell><cell>12.75</cell><cell>69.42</cell><cell>47.84</cell></row><row><cell>Baseline: L only</cell><cell>51.34</cell><cell>85.34</cell><cell>80.64</cell><cell>3.68</cell><cell>18.22</cell><cell>8.13</cell></row><row><cell>U+W</cell><cell>37.15</cell><cell>83.78</cell><cell>70.75</cell><cell>13.44</cell><cell>69.77</cell><cell>49.01</cell></row><row><cell>U+W+L:S</cell><cell>42.98</cell><cell>84.94</cell><cell>71.83</cell><cell>13.89</cell><cell>72.53</cell><cell>51.37</cell></row><row><cell>U+W+L:T</cell><cell>52.96</cell><cell>88.98</cell><cell>83.26</cell><cell>7.81</cell><cell>40.15</cell><cell>32.62</cell></row><row><cell>U+SF</cell><cell>36.33</cell><cell>83.68</cell><cell>69.87</cell><cell>14.33</cell><cell>69.01</cell><cell>48.32</cell></row><row><cell>U+SF+L:S</cell><cell>41.06</cell><cell>84.81</cell><cell>71.27</cell><cell>15.14</cell><cell>72.72</cell><cell>51.62</cell></row><row><cell>U+SF+L:T</cell><cell>51.67</cell><cell>87.71</cell><cell>83.84</cell><cell>8.05</cell><cell>41.51</cell><cell>32.77</cell></row><row><cell>U+W+SF</cell><cell>41.33</cell><cell>84.89</cell><cell>72.29</cell><cell>14.13</cell><cell>69.41</cell><cell>48.13</cell></row><row><cell>U+W+SF+L: S</cell><cell>47.50</cell><cell>86.97</cell><cell>74.98</cell><cell>16.98</cell><cell>74.65</cell><cell>54.20</cell></row><row><cell>U+W+SF+L: T</cell><cell>54.13</cell><cell>89.41</cell><cell>82.54</cell><cell>8.80</cell><cell>41.53</cell><cell>32.81</cell></row><row><cell>U+W+SF+L: T+S</cell><cell>55.16</cell><cell>94.65</cell><cell>85.64</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Part 2: Training images VRD + VG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline: U</cell><cell>36.97</cell><cell>84.49</cell><cell>70.19</cell><cell>13.31</cell><cell>70.56</cell><cell>50.34</cell></row><row><cell>U+W+SF</cell><cell>42.08</cell><cell>85.89</cell><cell>72.83</cell><cell>14.51</cell><cell>70.79</cell><cell>50.64</cell></row><row><cell>U+W+SF+L: S</cell><cell>48.61</cell><cell>87.15</cell><cell>75.45</cell><cell>17.16</cell><cell>75.26</cell><cell>55.41</cell></row><row><cell>U+W+SF+L: T</cell><cell>54.61</cell><cell>90.09</cell><cell>82.97</cell><cell>9.23</cell><cell>43.21</cell><cell>33.40</cell></row><row><cell>U+W+SF+L: T+S</cell><cell>55.67</cell><cell>95.19</cell><cell>86.14</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">4.1. Evaluation on VRD Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">4.1.1 Predicate Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Phrase and Relationship Detection: Distillation of Linguistic Knowledge -Zero Shot. We use the same notations as inTable 1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Phrase Detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relationship Detection</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="12">R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50,</cell></row><row><cell></cell><cell>k=1</cell><cell>k=1</cell><cell>k=10</cell><cell>k=10</cell><cell>k=70</cell><cell>k=70</cell><cell>k=1</cell><cell>k=1</cell><cell>k=10</cell><cell>k=10</cell><cell>k=70</cell><cell>k=70</cell></row><row><cell cols="2">Part 1: Training images VRD only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VRD -V only [19]</cell><cell>1.12</cell><cell>0.95</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.78</cell><cell>0.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VRD -Full [19]</cell><cell>3.75</cell><cell>3.36</cell><cell>12.57</cell><cell>7.56</cell><cell>12.92</cell><cell>7.96</cell><cell>3.52</cell><cell>3.13</cell><cell>11.46</cell><cell>7.01</cell><cell>11.70</cell><cell>7.13</cell></row><row><cell>Linguistic Cues [25]</cell><cell>-</cell><cell>-</cell><cell>15.23</cell><cell>10.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.43</cell><cell>9.67</cell><cell>-</cell><cell>-</cell></row><row><cell>VRL [18]</cell><cell>10.31</cell><cell>9.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.52</cell><cell>7.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>U+W+SF+L: S</cell><cell>10.89</cell><cell>10.44</cell><cell>17.24</cell><cell>13.01</cell><cell>17.24</cell><cell>12.96</cell><cell>9.14</cell><cell>8.89</cell><cell>16.15</cell><cell>12.31</cell><cell>15.89</cell><cell>12.02</cell></row><row><cell>U+W+SF+L: T</cell><cell>6.71</cell><cell>6.54</cell><cell>11.27</cell><cell>9.45</cell><cell>9.84</cell><cell>7.86</cell><cell>6.44</cell><cell>6.07</cell><cell>9.71</cell><cell>7.82</cell><cell>10.21</cell><cell>8.75</cell></row><row><cell cols="3">Part 2: Training images VRD + VG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U+W+SF+L: S</cell><cell>11.23</cell><cell>10.87</cell><cell>17.89</cell><cell>13.53</cell><cell>17.88</cell><cell>13.41</cell><cell>9.75</cell><cell>9.41</cell><cell>16.81</cell><cell>12.72</cell><cell>16.37</cell><cell>12.29</cell></row><row><cell>U+W+SF+L: T</cell><cell>7.03</cell><cell>6.94</cell><cell>11.85</cell><cell>9.88</cell><cell>10.12</cell><cell>8.97</cell><cell>6.89</cell><cell>6.56</cell><cell>10.34</cell><cell>8.23</cell><cell>10.53</cell><cell>9.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Predicate Detection on Visual Genome Dataset. Notations are the same as inTable 1.</figDesc><table><row><cell></cell><cell></cell><cell>Entire Set</cell><cell></cell><cell></cell><cell>Zero-shot</cell><cell></cell></row><row><cell></cell><cell cols="6">R@100/50 R@100 R@50 R@100/50 R@100 R@50</cell></row><row><cell></cell><cell>k=1</cell><cell>k=70</cell><cell>k=70</cell><cell>k=1</cell><cell>k=70</cell><cell>k=70</cell></row><row><cell>U</cell><cell>37.81</cell><cell>82.05</cell><cell>81.41</cell><cell>7.54</cell><cell>81.00</cell><cell>65.22</cell></row><row><cell>U+W+SF</cell><cell>40.92</cell><cell>86.81</cell><cell>84.92</cell><cell>8.66</cell><cell>82.50</cell><cell>67.72</cell></row><row><cell>U+W+SF+L: S</cell><cell>49.88</cell><cell>91.25</cell><cell>88.14</cell><cell>11.28</cell><cell>88.23</cell><cell>72.96</cell></row><row><cell>U+W+SF+L: T</cell><cell>55.02</cell><cell>94.92</cell><cell>91.47</cell><cell>3.94</cell><cell>62.99</cell><cell>47.62</cell></row><row><cell>U+W+SF+L: T+S</cell><cell>55.89</cell><cell>95.68</cell><cell>92.31</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Predicate Detection on VRD Testing Set: External Linguistic Knowledge. Part 1 uses the LK from VRD dataset; Part 2 uses the LK from VG dataset; Part 3 uses the LK from both VRD and VG dataset. Part 4 uses the LK from parsing Wikipedia text; Part 5 uses the LK from from both VRD dataset and Wikipedia. Notations are the same as as in Table 1.</figDesc><table><row><cell></cell><cell></cell><cell>Entire Set</cell><cell></cell><cell></cell><cell>Zero-shot</cell><cell></cell></row><row><cell cols="7">R@100/50 R@100 R@50 R@100/50 R@100 R@50</cell></row><row><cell></cell><cell>k=1</cell><cell>k=70</cell><cell>k=70</cell><cell>k=1</cell><cell>k=70</cell><cell>k=70</cell></row><row><cell>Part 1 LK: VRD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VRD-V only [19]</cell><cell>7.11</cell><cell>37.20</cell><cell>28.36</cell><cell>3.52</cell><cell>32.34</cell><cell>23.95</cell></row><row><cell>VRD-Full [19]</cell><cell>47.87</cell><cell>84.34</cell><cell>70.97</cell><cell>8.45</cell><cell>50.04</cell><cell>29.77</cell></row><row><cell>U+W+SF+L: S</cell><cell>47.50</cell><cell>86.97</cell><cell>74.98</cell><cell>16.98</cell><cell>74.65</cell><cell>54.20</cell></row><row><cell>U+W+SF+L: T</cell><cell>54.13</cell><cell>89.41</cell><cell>82.54</cell><cell>8.80</cell><cell>41.53</cell><cell>32.81</cell></row><row><cell>Part 2 LK: VG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U+W+SF+L: S</cell><cell>45.00</cell><cell>81.64</cell><cell>74.76</cell><cell>16.88</cell><cell>72.29</cell><cell>52.51</cell></row><row><cell>U+W+SF+L: T</cell><cell>51.54</cell><cell>87.00</cell><cell>79.70</cell><cell>11.01</cell><cell>54.66</cell><cell>45.25</cell></row><row><cell>Part 3 LK: VRD+VG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U+W+SF+L: S</cell><cell>48.21</cell><cell>87.76</cell><cell>76.51</cell><cell>17.21</cell><cell>74.89</cell><cell>54.65</cell></row><row><cell>U+W+SF+L: T</cell><cell>54.82</cell><cell>90.63</cell><cell>83.97</cell><cell>12.32</cell><cell>47.22</cell><cell>38.24</cell></row><row><cell>Part 4 LK: Wiki</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U+W+SF+L: S</cell><cell>36.05</cell><cell>77.88</cell><cell>68.16</cell><cell>11.80</cell><cell>64.24</cell><cell>49.19</cell></row><row><cell>U+W+SF+L: T</cell><cell>30.41</cell><cell>69.86</cell><cell>60.25</cell><cell>11.12</cell><cell>63.58</cell><cell>44.65</cell></row><row><cell>Part 5 LK: VRD+Wiki</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U+W+SF+L: S</cell><cell>48.94</cell><cell>87.11</cell><cell>77.79</cell><cell>19.17</cell><cell>76.42</cell><cell>56.81</cell></row><row><cell>U+W+SF+L: T</cell><cell>54.06</cell><cell>88.93</cell><cell>81.78</cell><cell>9.65</cell><cell>42.24</cell><cell>34.61</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The Wikipedia text file can be found on http://kopiwiki.dsd. sztaki.hu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In predicate detection, R@100,k=1 and R@50,k=1 are equivalent (also observed in<ref type="bibr" target="#b18">[19]</ref>) because there are not enough objects in ground truth to produce over 50 pairs.<ref type="bibr" target="#b2">3</ref> The recall of different k's are not reported in<ref type="bibr" target="#b18">[19]</ref>.We calculate those</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">More analysis on using different combinations of features can be found in the supplementary materials.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Details can be found in the supplementary materials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research was supported by the Office of Naval Research under Grant N000141612713: Visual Common Sense Reasoning for Multi-agent Activity Prediction and Recognition.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1704.03114</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part I</title>
		<meeting>Part I<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context based object categorization: A critical survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="712" to="722" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling the knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks with massive learned knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2946" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph cut based inference with co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part V, ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part V, ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="239" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating holistic 3d scene abstractions for text-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1611.09392</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">When are tree structures necessary for deep learning of representations? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>abs/1503.00185</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Costa: Co-occurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pataki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vajna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Marosi</surname></persName>
		</author>
		<idno>04/2012 2012. 4</idno>
		<title level="m">Wikipedia as text. ECRIM News, Special theme: Big Data</title>
		<imprint>
			<biblScope unit="page" from="48" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno>abs/1611.06641</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to share visual appearance for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1481" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Vision and Language (VL15)</title>
		<meeting><address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Portugal</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of context selection in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tree kernelbased relation extraction with context-sensitive structured parse tree information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
