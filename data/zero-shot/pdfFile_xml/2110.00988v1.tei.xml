<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Keypoint Communities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Zauss</surname></persName>
							<email>duncan.zauss@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">EPFL VITA</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">EPFL VITA</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">EPFL VITA</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Keypoint Communities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a fast bottom-up method that jointly detects over 100 keypoints on humans or objects, also referred to as human/object pose estimation. We model all keypoints belonging to a human or an object -the pose-as a graph and leverage insights from community detection to quantify the independence of keypoints. We use a graph centrality measure to assign training weights to different parts of a pose. Our proposed measure quantifies how tightly a keypoint is connected to its neighborhood. Our experiments show that our method outperforms all previous methods for human pose estimation with fine-grained keypoint annotations on the face, the hands and the feet with a total of 133 keypoints. We also show that our method generalizes to car poses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent large-scale datasets with fine-grained annotations of complex poses present a new challenge for pose estimation methods. Beyond detecting a coarse person bounding box and a small set of keypoints for large body joints, we now have large datasets that include over 100 extra finegrained keypoints in the face, the hands and the feet. Resolving these fine details will allow us to build robust representations of humans for downstream tasks like action recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2]</ref>, and intent prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Training our current pose estimation algorithms on poses with mixed coarse and fine keypoints presents a challenge as they assume a uniform importance of all keypoints in a pose. We introduce a principled keypoint weighting method to take into account the difference of the importance of coarse and fine-grained keypoints. <ref type="figure" target="#fig_0">Figure 1</ref> shows a complex person and car poses. For instance, the person pose contains coarse keypoints like the hips and shoulders and fine-grained keypoints like the ones along the eyebrows.</p><p>In <ref type="bibr" target="#b11">[12]</ref>, Jin et al. share a large scale annotation for complex human body poses and propose their method, Zoom-Net, that set the state-of-the-art for this type of complex human body pose. Their method first localizes a person with their major keypoints and then estimates the areas of the hands and face. On the estimated areas, it runs a separate head that is zoomed-in on that area to determine finegrained keypoint locations. In contrast, we propose a fast, bottom-up method that directly estimates all keypoints in parallel. Our method does not need predefined areas for fine-grained estimation and, therefore, generalizes to any pose, like a fine-grained car pose. Such a car pose is proposed in the ApolloCar3D dataset <ref type="bibr" target="#b34">[35]</ref> and we show that our method generalizes to this pose as well.</p><p>Complex, coarse and fine-grained poses present a challenge for current pose estimation methods that assume a uniform distribution of keypoints across a person or object. A cluster of fine-grained keypoints overly emphasizes that region and focuses the neural network optimization on that area reducing the importance of other regions that only have a single keypoint. We propose a method that quantifies how tightly connected these keypoints are and that rebalances the training weights such that all areas of a pose are equally well connected to the rest of the pose. We introduce the details in Section 3.</p><p>Our contributions are (i) a method to weigh the importance of keypoints and their connections in complex poses based on graph-based methods for community detection, (ii) an efficient implementation for fine-grained human poses and (iii) generalization from human poses to finegrained car poses. We show the impact of our contribution with state-of-the-art results on the challenging COCO WholeBody dataset <ref type="bibr" target="#b11">[12]</ref> and the ApolloCar3D dataset <ref type="bibr" target="#b34">[35]</ref>.</p><p>The software is open source and publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is an extensive literature on pose estimation. While many works have focused on human pose estimation, there are recent works that extend the method to animal pose estimation <ref type="bibr" target="#b22">[23]</ref> and car pose estimation <ref type="bibr" target="#b34">[35]</ref>. Recent datasets include more keypoints that represent finer details on human and car poses and are reviewed below.</p><p>Human Pose Estimation. The recent release of the COCO WholeBody dataset <ref type="bibr" target="#b11">[12]</ref> with 133 keypoints for a single human pose presents new challenges for existing methods. The authors, Jin et al., established baseline numbers of existing methods on their dataset and proposed ZoomNet, a new neural network architecture that refines regions with fine-grained annotations with dedicated networks.</p><p>In general, state-of-the-art methods for human pose estimation are based on Convolutional Neural Networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>. There are two major approaches for pose estimation. Bottom-up methods estimate each body joint first and then group them into poses. Top-down methods first run a person detector to estimate person bounding boxes before estimating body joint locations within each bounding box.</p><p>The first bottom-up methods were introduced, e.g., by Pishchulin et al. with DeepCut <ref type="bibr" target="#b29">[30]</ref>. They solve the keypoint association problem with an integer linear program. In these early methods, the processing time for a single image was of the order of hours. Newer methods introduced additional concepts to reduce prediction time, e.g., in Part Affinity Fields <ref type="bibr" target="#b5">[6]</ref>, Associative Embedding <ref type="bibr" target="#b26">[27]</ref>, PersonLab <ref type="bibr" target="#b28">[29]</ref> and multi-resolution networks with associate embedding <ref type="bibr" target="#b6">[7]</ref>. Composite Fields as introduced in PifPaf [17] predict more precise associations than Open-Pose's Part Affinity Fields <ref type="bibr" target="#b5">[6]</ref> and PersonLab's mid-range fields <ref type="bibr" target="#b28">[29]</ref> which allows for particularly fast and greedy decoding with high precision.</p><p>Car Pose Estimation. The new ApolloCar3D dataset <ref type="bibr" target="#b34">[35]</ref> with its 66 keypoints for car pose estimators presents similar challenges than the WholeBody dataset <ref type="bibr" target="#b11">[12]</ref>. The authors <ref type="bibr" target="#b34">[35]</ref> presented baseline performance numbers using Convolutional Pose Machines (CPM) <ref type="bibr" target="#b37">[38]</ref> and also quantified the performance of human labelers on their dataset.</p><p>It is only recently that methods that were developed for human pose estimation have been applied to other classes. Car poses provide finer detail for a car than a 2D or 3D detection bounding box. While human pose estimation focuses on the location of body joints within the human body, car poses annotate points on the surface of the car.</p><p>One of the earlier works by Reddy et al. proposes Occlusion-Net <ref type="bibr" target="#b31">[32]</ref> that highlights the issue of selfocclusion for these keypoints on the surface of an object. As the car is viewed from different sides, the set of visible keypoints changes drastically due to self-occlusion. Their work includes extensive modeling with a 3D graph network and self-supervised training with the CarFusion dataset <ref type="bibr" target="#b7">[8]</ref> to predict 2D and 3D keypoints. In OpenPose <ref type="bibr" target="#b4">[5]</ref>, Cao et al. show qualitative results for car pose estimation. Simple Baseline <ref type="bibr" target="#b32">[33]</ref> trains a top-down pose estimator on car annotations of the Pascal3D+ dataset <ref type="bibr" target="#b38">[39]</ref>. Other works choose different representations for finer details beyond bounding boxes. In GSNet <ref type="bibr" target="#b13">[14]</ref>, a car orientation in 3D space is predicted along with a 3D shape estimate.</p><p>Keypoint weighting In general, previous methods used uniform distributions to weigh the keypoints in training for single networks or used separate networks for the different fine-grained regions. We show that with our keypoint weighting method the performance of single neural networks for predicting poses that contain fine-grained and coarse features can be improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We need to devise a training procedure for poses that combine coarse keypoints that localize large body parts (hips, shoulders, etc.) and fine-grained keypoints like the outline of a hand. Individual fine-grained keypoints are highly predictable from neighboring keypoints whereas coarse keypoints are more independent. However, the importance of a keypoint is not only based on its individual predictability, but also on how it contributes to a local group of keypoints. In other words, every individual keypoint in a group of five keypoints might have negligible importance, the group of five keypoints together is still important.  Overview of our method. We obtain the average euclidean distance for every connection in the pose graph from the training dataset. We then create ego graphs of radius three for every vertex and compute the local centrality for the ego vertex. The centrality is directly related to the training weight of a joint. The training weight of a connection is obtained by taking the max of the weights from the vertices of this connection. For the joints, also the circle radius is proportional to the joint weight. In (a), all shortest paths are taken into account. In (b), only shortest paths within a radius of three are taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose Estimation Architecture</head><p>pose estimation algorithm. We will use OpenPifPaf <ref type="bibr" target="#b16">[17]</ref>, which is a bottom-up pose detector based on Composite Fields. A backbone in form of a ResNet <ref type="bibr" target="#b10">[11]</ref> or Shuf-fleNetV2 <ref type="bibr" target="#b20">[21]</ref> processes single images to create a common representation for the head networks. The head networks are the Composite Intensity Fields (CIFs) and Composite Association Fields (CAFs) that are 1 ? 1 convolutions followed by subpixel convolutions <ref type="bibr" target="#b33">[34]</ref>. The heads are trained to detect keypoints and to associate keypoints respectively. Per keypoint type k and for every location i, j in the output field, the CIF head predicts an intensity component c i,j k to indicate a keypoint is nearby, a two-dimensional vector component v i,j k to precisely regress to the keypoint location, the uncertainty of the location b i,j k and a scale component s i,j k to estimate the size of a keypoint. The learnt scale of the keypoint s i,j k depends on the size of that specific joint in the image and is used in the decoding step as the width of an unnormalized Gaussian convolution to create high-resolution confidence fields. Similarly, the CAF head also has an intensity component to indicate the vicinity of an association between two keypoints, two vector components that regress to the two keypoints instances to associate, and two scale components to estimate the two keypoint sizes. The CIF loss with an extension to weigh all loss components by keypoint type k with w k is:</p><formula xml:id="formula_0">L CIF = k w k m k,c BCE(c,?) (1) + m k,v Laplace(v,v,b) (2) + m k,s Laplace 1,? s , b s<label>(3)</label></formula><p>where c, v, b and s are components of the composite field with suppressed indices (k, i, j) for the keypoint type and feature map location and where symbols with a hat indicate predicted quantities. With m k,c , m k,v and m k,s , we indicate keypoint specific masks over the feature map. BCE is a binary cross entropy loss with Focal loss extension <ref type="bibr" target="#b18">[19]</ref> and Laplace is a linear regression loss for vector components that is attenuated by a predictedb or a fixed b s <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. The probabilistic interpretation of the loss function as the negative logarithm of a joint likelihood function requires that the three components of the loss are equally weighted with respect to each other. The CAF head is trained with an equivalent loss with two vector components (2) and two (a) (b) <ref type="figure">Figure 4</ref>: Visualization of proposed weighting for the skeleton of an instance from the ApolloCar3D dataset <ref type="bibr" target="#b34">[35]</ref>. In (a), all shortest paths are taken into account. In (b), only shortest paths within a radius of three are taken into account. The dotted lines indicate connections on the left side of the car for a clearer visualization.</p><p>scale components <ref type="formula" target="#formula_0">(3)</ref>. We now focus our attention on the blue branch of <ref type="figure" target="#fig_1">Figure 2</ref> that determines the training weights w k for the CIF and CAF heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Centrality</head><p>We represent a pose as a graph {V, E} with vertices V representing each keypoint, and edges E representing the Euclidean distance in the image plane between the keypoints. That Euclidean distance is estimated with an average over all training annotations. The importance of a keypoint does not only depend on its direct neighbors, but on its connectedness in the neighborhood. Therefore, we consider graph centrality measures.</p><p>There exists an enormous number of graph centrality measures, for example closeness centrality <ref type="bibr" target="#b0">[1]</ref>, eigenvector centrality <ref type="bibr" target="#b2">[3]</ref>, Katz centrality <ref type="bibr" target="#b12">[13]</ref>, betweenness centrality <ref type="bibr" target="#b8">[9]</ref> and harmonic centrality <ref type="bibr" target="#b21">[22]</ref> among others.</p><p>Traditionally, centrality measures are used to determine the centrality or importance of persons in social networks or more generally the importance of nodes in complex graphs. Highly central nodes get assigned a high centrality value. For our application highly central nodes are part of a community and thus we use the inverse of the centrality.</p><p>We want to use our understanding of the particular problem do derive the best metric for our use case. To train complex poses, we aim to make every keypoint equally well connected to the rest of the pose. Most centrality measures are based on shortest paths from a vertex to all other vertices. The "connectedness" of a vertex is represented in the average length of the shortest paths that originate at the vertex. For example, the ankle keypoint is not very well connected and the average length of all shortest paths that originate at the ankle is high as all the paths to the face and hand keypoints are long. To rebalance our training such that all vertices are connected equally well, we want to assign an equal weight to an average unit of distance in the shortest paths.</p><p>In practice, we restrict the centrality computation to a neighborhood by extracting an ego graph of radius three (the subgraph with all the vertices around a particular vertex that can be reached in three steps) for every vertex and computing the centrality for that vertex only within that subgraph.</p><p>The weighted length (weighted by Euclidean distance) of the shortest path between two vertices v 1 and v 2 is d(v 1 , v 2 ). As we are interested in equally weighting a unit length, the harmonic average is appropriate. For every vertex v i , we compute the harmonic average h of all the shortest paths originating at v i :</p><formula xml:id="formula_1">h(v i ) = ? ? vj ?V \{vi} 1 d(v i , v j ) ? ? ?1<label>(4)</label></formula><p>where we can identify the harmonic centrality H <ref type="bibr" target="#b21">[22]</ref> in the square brackets, leading to h = H ?1 .</p><p>Numerically, the closeness centrality <ref type="bibr" target="#b0">[1]</ref> and harmonic centrality are similar and it might be helpful to interpret this weighting in terms of assigning a high weight to keypoints with low "closeness".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Weights</head><p>We use the graph centrality measure to derive training weights for keypoints and their connections. The keypoint weights are obtained directly from the centrality by normalizing i h(v i ) to the number of keypoints. This normalization facilitates easier comparisons between the different weighting methods.</p><p>We also need to obtain training weights for keypoint connections. Again, the weight of the connection should not depend just on its own length, but also take into account the structure of the local cluster of keypoints this edge is a part of. Given we already have a principled method for the vertices, we derive the weight w ij for the edge that connects vertices v i and v j from h:</p><formula xml:id="formula_2">w ij ? max h(v i ), h(v j ) .<label>(5)</label></formula><p>We normalize the sum of edge weights to the total number of edges. The resulting weights for the human WholeBody pose <ref type="bibr" target="#b11">[12]</ref> and the car pose <ref type="bibr" target="#b34">[35]</ref> are shown in <ref type="figure" target="#fig_3">Figures 3 and 4</ref>. We show two configurations of our method. One where we use the entire pose to compute our graph centrality measure and one where we use an ego graph of radius three. The WholeBody skeleton has clear hierarchical clusters of keypoints in the hands and face and one level down in the eyes and fingers. The ApolloCar3D skeleton is more uniformly distributed. There are keypoint agglomerations in the area of the lights and number plates both in the front and in the rear of the car. The keypoints in the roof are the most separate from the other keypoints. In contrast to the COCO WholeBody skeleton the communities are not as strongly separated. For the WholeBody pose, the computed weights range from 0.21 to 5.15 and for ApolloCar3D from 0.57 to 1.63. Our method automatically determines the keypoint communities for the WholeBody pose and produces highly varied training weights. Groups of keypoints that are highly predictable from each other receive a lower weight. Our method also successfully determines the more uniform distribution of keypoints in the car pose and produces less varied training weights.</p><p>For any generic pose and training dataset, this method automatically creates training weights for keypoints and associations in a principled way and we show its effectiveness on challenging experiments in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments on complex poses to demonstrate the effectiveness and efficiency of our method. We investigate a human pose with 17 COCO keypoints as the main skeleton that was then extended with an additional 116 keypoints for fine-grained details in the face, the hands and the feet. We also demonstrate that our method generalizes to a fine-grained car pose with 66 keypoints.</p><p>Datasets. For human pose estimation, we conduct experiments on the COCO WholeBody <ref type="bibr" target="#b11">[12]</ref> dataset. This dataset contains extra annotations on the 64,000 training and 5,000 validation images of COCO <ref type="bibr" target="#b19">[20]</ref> for face, hands and feet. The full pose contains 133 keypoints with 152 connections. There are about 130,000 instances with annotations for the left hand, the right hand and the face. The body annotations are taken from COCO <ref type="bibr" target="#b19">[20]</ref> that contains about 250,000 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>WB body foot face hand HRNet <ref type="bibr" target="#b35">[36]</ref> 43.2 65. <ref type="bibr" target="#b8">9</ref>   <ref type="bibr" target="#b11">[12]</ref>.</p><p>For car pose estimation, we use the ApolloCar3D dataset <ref type="bibr" target="#b34">[35]</ref>. It provides car annotations with 66 keypoints and we assigned them 108 connections. The dataset consists of 4283 training and 200 validation images with 52942 and 2674 annotated instances respectively. As cars are only visible from one side and often partially occlude each other, only an average of 16.2 keypoints are annotated per instance.</p><p>Evaluation. We follow the evaluation method proposed in the COCO WholeBody <ref type="bibr" target="#b11">[12]</ref> dataset paper. It is based on keypoint-based average precision (AP) that was popularized with the COCO keypoint task <ref type="bibr" target="#b19">[20]</ref>. The evaluation weighs every one of the 133 keypoints equally.</p><p>For the ApolloCar3D dataset, we report the detection rate which was proposed by Song et al. <ref type="bibr" target="#b34">[35]</ref>. A keypoint is counted as detected if the distance from the prediction to the ground truth is less than 10 pixels. Additionally, we also report the keypoint-based AP as it is already common for human pose detection <ref type="bibr" target="#b19">[20]</ref>. We compute AP based on object keypoint similarity with sigmas of 0.05 for all car keypoints.</p><p>Implementation Details. We extend OpenPifPaf <ref type="bibr" target="#b17">[18]</ref> with an option to weigh the training of keypoint and connection types. We populate the weights for the given pose in the generic fashion described in Section 3.</p><p>We train models with ShuffleNetV2 <ref type="bibr" target="#b20">[21]</ref> backbones that were pretrained without weighting the MS COCO keypoint task. The head networks CIF (Composite Intensity Field) and CAF (Composite Association Field) are single 1 ? 1 convolutions followed by a subpixel convolution <ref type="bibr" target="#b33">[34]</ref>. The total stride after the backbone is 16 and decreased to 8 in the head networks. We train for 100 epochs with a learning rate of 0.0001 with an SGD <ref type="bibr" target="#b3">[4]</ref> optimizer with Nesterov momentum <ref type="bibr" target="#b25">[26]</ref> of 0.95 and a batch size of 16.</p><p>Results on the COCO WholeBody dataset. Quantitative results on the COCO WholeBody dataset <ref type="bibr" target="#b11">[12]</ref> are shown in   <ref type="table">Table 1</ref>. Our result is based on a single model that is evaluated for all (WB) or a subset of the predicted keypoints. Our method outperforms previous methods and achieves especially high precision on fine-grained regions such as the face or hands. The "body" task is equivalent to the COCO keypoint task on the val set <ref type="bibr" target="#b19">[20]</ref>. Our method is based on OpenPifPaf <ref type="bibr" target="#b17">[18]</ref> which only achieves 71.6% on the COCO val set with a model trained on that specific task and we therefore did not expect it to outperform ZoomNet <ref type="bibr" target="#b11">[12]</ref>. Our method makes up for its lower body AP with excellent results for face and hand AP and is nearly twice as precise as any other bottom-up method.</p><p>Qualitative results are shown in <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="figure" target="#fig_4">Figure 5</ref>. The additional keypoints in the face can serve as a powerful representation from which human emotions such as happiness and surprise but also attention and intent can be derived. The fine and coarse-grained WholeBody pose can be used to predict actions from images. For the example image on the bottom right of <ref type="figure" target="#fig_4">Figure 5</ref>, it can be predicted that the person is eating while holding a cell phone and for the person on the middle picture in the bottom row it can be predicted that the person is surprised that a cat jumped on the table. Through adding fine-grained hand keypoints the interaction between humans and objects can be detected. In the transportation domain, the fine grained keypoints on the hand region help to understand the intentions of pedestrians. For example in the bottom left image of <ref type="figure" target="#fig_5">Figure 6</ref> it is possible to detect that the pedestrian wants to hail a cab and is not intending to cross the road even though she is standing at the side of the road.</p><p>Results on the ApolloCar3D dataset. Our method is not specific to human poses and can be applied to any pose. To demonstrate that our method generalizes, we apply it to the ApolloCar3D dataset <ref type="bibr" target="#b34">[35]</ref>, where every car instance is annotated with up to 66 keypoints. Our method achieves an average precision (AP) of 72.0% with all sigmas for the computation of the object keypoint similarity set to 0.05. The previous work <ref type="bibr" target="#b34">[35]</ref> evaluates detection rate instead of AP and a comparison of our method with their Convolutional Pose Machines <ref type="bibr" target="#b37">[38]</ref> evaluation and human annotators is shown in <ref type="table" target="#tab_1">Table 2</ref>. We achieve a detection rate of 91.9% and thus outperform the previous state-of-the-art from <ref type="bibr" target="#b34">[35]</ref> which achieved a detection rate of 75.4%. They also report the detection rate for human annotators at 92.4%. Our proposed method reduces the gap to human level performance from 17.0% to just 0.5%.</p><p>We share qualitative results on the validation set of Apol-loCar3D <ref type="bibr" target="#b34">[35]</ref> in <ref type="figure">Figure 7</ref>. We predict fine-grained keypoints for close-by and far-away cars. Cameras for selfdriving technology cover a wide angle and therefore have to perceive small car instances even for cars at moderate distances. It is safety relevant to determine the locations of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detection rate[%] Human annotators 92.4 CPM <ref type="bibr" target="#b37">[38]</ref> 75.4 Ours 91.9  <ref type="table">Table 3</ref>: Ablation studies. Average precision (AP) results in percent on the COCO WholeBody val dataset <ref type="bibr" target="#b11">[12]</ref>. We report the percentage gain of other weighting methods in comparison to our baseline method. In addition to our main method, we compare with applying no weighting (Equal), applying our method to the global graph instead of ego graphs (Global) and using hand-crafted weights (Crafted).</p><p>All results are produced with a ShuffleNetV2k16 backbone. WB indicates an evaluation on all 133 keypoints.</p><p>break and indicator lights for downstream tasks which we achieve with high precision.</p><p>Ablation Studies. We study the effects of different loss weightings on the average precision (AP). In <ref type="table">Table 3</ref> the results from training with different weighting schemes are shown. For this ablation study we train for 50 epochs starting from a model that was pretrained on the 133 keypoints without weighting. "Crafted" means that the body and foot keypoints are weighted three times higher than the rest of the keypoints. Hand-crafting the weights can be seen as choosing 133 additional hyperparameters which is generally infeasible and the motivation for our method. The weighting based on the local harmonic centrality, which is our baseline method, achieves an AP of 55.3%, which is a significant improvement over the unweighted training, which results in an AP of 53.6%. The weighting based on the local harmonic centrality also achieves a higher AP than the weighting based on the vanilla harmonic centrality and the hand-crafted weights. This shows a) that the local influence between the keypoints is more important than the global one and b) that using our proposed centrality measure is more optimal than hand-crafting the weights.</p><p>We study the effect of different parameter choices on the precision and prediction time trade-off of our method. The runtime of our method is influenced by the decoding algorithm. Since the WholeBody pose has 133 joints and 152 <ref type="figure">Figure 7</ref>: Qualitative results from the ApolloCar3D validation set <ref type="bibr" target="#b34">[35]</ref>. We demonstrate excellent detection rates and spatial localizations of all the visible car keypoints even at far distances.  <ref type="table">Table 4</ref>: Ablation studies. Average precision (AP) results in percent on the COCO WholeBody val dataset <ref type="bibr" target="#b11">[12]</ref> and their associated prediction time for different decoding methods. We show ZoomNet's average precision and runtime for better comparison with our method. Our neural network runs in 93ms on a NVIDIA GTX 1080 Ti. The decoding starts with the joints where the confidence exceeds the seed threshold. The vector and scale from a cell of a CAF field is only used if the confidence is above the CAF threshold.</p><p>associations the duration of the decoding is more prevalent than for poses with a lower number of joints and associations. The first step in the decoding process is to determine seed joints from which the decoding starts and from which connections to the other joints will be created with the help of the CAF fields. All joints that have a confidence that is higher than a certain seed threshold will be used as seed joints. Increasing the seed threshold will reduce the number of seeds and thus cause a faster decoding process. However, with a higher seed threshold, some humans may not be detected which can result in a lower average precision. Using a higher CAF threshold generally increases the decoding speed in exchange for a lower accuracy as fewer associations are considered for decoding. The quantitative results that the variation of these parameters yield can be seen in <ref type="table">Table 4</ref>. With our standard decoder setting, we already achieve a higher AP than ZoomNet <ref type="bibr" target="#b11">[12]</ref> whilst being 22 ms faster. With a seed threshold of 0.5 and a CAF threshold of 0.01 our model achieves an AP of 54.5 with a prediction time of 112 ms, which is an excellent trade-off between inference speed and precision. Our ShuffleNetV2K16 backbone achieves an AP of 50.9 at a frame rate of 15.2 frames per second on a NVIDIA GTX 1080 Ti, which makes at suitable for most real-time applications that require finegrained pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a generic and principled method to train complex poses with fine and coarse-grained details. Our experiments demonstrate our ability to perceive detailed facial expressions and hand gestures and produce state-of-the-art results on standard pose benchmarks for human and car poses. We have shown that our method operates at state-of-the-art prediction speeds and we have studied the trade-offs between accuracy and prediction speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed keypoint weighting method for complex poses yields state-of-the-art results for whole body human pose estimation and for complex car poses, whilst running at high frame rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>provides an overview of our architecture. Our method is independent of the particular choice of pose estimator, and could be used for any top-down or bottom up</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our method. We obtain the average euclidean distance for every connection in the pose graph from the training dataset. We then create ego graphs of radius three for every vertex and compute the local centrality for the ego vertex. The centrality is directly related to the training weight of a joint. The training weight of a connection is obtained by taking the max of the weights from the vertices of this connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of proposed weighting for joints and connections. The colors indicate the training weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results from the COCO WholeBody validation set<ref type="bibr" target="#b11">[12]</ref>. Our method resolves multiple persons per image and captures their facial expressions and gestures like hailing a cab. The bottom-left image is processed with human and car pose estimators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results with images from the transport domain that we obtained from flickr. The two images in the bottom row were processed with our human and car pose estimation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>31.4 52.3 30.0 ZoomNet [12] 54.1 74.3 79.8 62.3 40.1 AE [27] 27.4 40.5 7.7 47.7 34.1 OpenPose [6] 33.8 56.3 53.2 48.2 19.8</figDesc><table><row><cell>Ours</cell><cell>60.4 69.6 63.4 85.0 52.9</cell></row><row><cell cols="2">Table 1: Average precision (AP) results in percent on the</cell></row><row><cell cols="2">COCO WholeBody dataset [12]. WB indicates evaluation</cell></row><row><cell cols="2">on all 133 keypoints. The first two methods are top-down</cell></row><row><cell cols="2">methods and the lower three are bottom-up methods. Ref-</cell></row><row><cell cols="2">erence numbers from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detection rate on the ApolloCar3D dataset<ref type="bibr" target="#b34">[35]</ref>. The metrics for the Convolutional Pose Machines and the human annotators are from<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table><row><cell>Method</cell><cell>WB</cell><cell>body</cell><cell>foot</cell><cell>face</cell><cell>hand</cell></row><row><cell>Baseline</cell><cell>55.3</cell><cell>60.8</cell><cell>54.2</cell><cell>86.2</cell><cell>50.6</cell></row><row><cell>Equal</cell><cell cols="5">-3.1% -4.8% -8.1% +1.5% +0.8%</cell></row><row><cell>Global</cell><cell cols="5">-1.2% -2.5% -1.1% +0.8% +2.6%</cell></row><row><cell>Crafted</cell><cell cols="5">-1.2% -1.6% -1.7% +1.2% -1.0%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/DuncanZauss/Keypoint_ Communities</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This project has received funding from the Initiative for Media Innovation based at Media Center, EPFL, Lausanne, Switzerland.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Communication patterns in task-oriented groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bavelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="725" to="730" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Perceiving humans: from monocular 3d localization to social distancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Power and centrality: A family of measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Bonacich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of sociology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1182" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser A</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Carfusion: Combining point tracking and part detection for dynamic 3d reconstruction of vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>N Dinesh Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1906" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A set of measures of centrality based on betweenness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociometry</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Katz</surname></persName>
		</author>
		<idno>1953. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gsnet: Joint vehicle pose and shape reconstruction with geometrical and scene-aware supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="515" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open-PifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Harmony in the smallworld</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Marchiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vito</forename><surname>Latora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="539" to="546" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Mamidanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Cury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiga</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mackenzie</forename><forename type="middle">Weygandt</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional relational machine for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mokhtarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Nickabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting 32 pedestrian attributes for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems -under review</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o(1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurrii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pedestrian intention prediction: A convolutional bottom-up approach. Transportation Research Part C</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haziq</forename><surname>Razali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occlusion-net: 2d/3d occluded keypoint localization using graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>N Dinesh Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7326" to="7335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple baseline for vehicle pose estimation: Experimental validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H?ctor Corrales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Hern?ndez</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rub?n</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noelia</forename><forename type="middle">Hern?ndez</forename><surname>Izquierdo Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Parra</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez-Llorca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132539" to="132550" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5452" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>eeding of the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
