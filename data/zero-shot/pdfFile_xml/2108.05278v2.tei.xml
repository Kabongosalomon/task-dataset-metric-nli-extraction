<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Negative Samples Necessary in Entity Alignment? An Approach with High Performance, Scalability and Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
							<email>xmao@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<email>wenting.wang@lazada.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country>China, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<email>mlan@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country>China, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Are Negative Samples Necessary in Entity Alignment? An Approach with High Performance, Scalability and Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Neural networks</term>
					<term>Natural lan- guage processing</term>
					<term>Knowledge representation and reasoning KEYWORDS Knowledge Graph, Graph Neural Networks, Entity Alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity alignment (EA) aims to find the equivalent entities in different KGs, which is a crucial step in integrating multiple KGs. However, most existing EA methods have poor scalability and are unable to cope with large-scale datasets. We summarize three issues leading to such high time-space complexity in existing EA methods:</p><p>(1) Inefficient graph encoders, (2) Dilemma of negative sampling, and (3) "Catastrophic forgetting" in semi-supervised learning. To address these challenges, we propose a novel EA method with three new components to enable high Performance, high Scalability, and high Robustness (PSR): (1) Simplified graph encoder with relational graph sampling, (2) Symmetric negative-free alignment loss, and (3) Incremental semi-supervised learning. Furthermore, we conduct detailed experiments on several public datasets to examine the effectiveness and efficiency of our proposed method. The experimental results show that PSR not only surpasses the previous SOTA in performance but also has impressive scalability and robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An example of cross-lingual entity alignment.</p><p>but also share an overlapping part in between. Clearly, integrating them offers a broader view, which has been proven effective in some applications such as search engines and cross-border E-commerce. For example, integrating cross-lingual KGs provides an opportunity to benefit the minority language users who usually suffer from lacking language resources. Therefore, how to fuse knowledge from various KGs has attracted increasing attention.</p><p>As shown in <ref type="figure">Figure 1</ref>, entity alignment (EA) aims to find the equivalent entities in different KGs, which is a crucial step in integrating multiple KGs. With the incorporation of advanced techniques, the performances of EA methods are significantly improved over the past years. But on the other hand, the computation costs are also rapidly growing in terms of time and space complexity. Zhao et al. <ref type="bibr" target="#b45">[45]</ref> conduct an efficiency comparison of existing EA methods on a public dataset (DWY100K) that contains 100, 000 entity pairs and near one million triples. In terms of time complexity, most advanced EA methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b42">42]</ref> need to take more than 6 hours for training and prediction, and several <ref type="bibr" target="#b41">[41]</ref> even take days. In terms of space complexity, several methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b46">46]</ref> require large memory consumption, so they are unable to run on a GPU with 12 GB memory, even if setting with a minimum batch size. Such poor time-space scalability hinders the application of existing EA methods to real-world datasets, which usually contain millions of entities and billions of triples (e.g., the full DBpedia contains 6.6+ million entities, 23+ billion triples). We observe three issues below in existing EA methods and believe these are the causes of the above mentioned high time-space complexity bottleneck:</p><p>(1) Inefficient graph encoders: Graph Neural Networks (GNNs) have become increasingly popular in addressing graph-based applications, including EA. The core of GNNs is that each node receives information from its adjacent nodes to update the structure-based embedding. But with the expansion of receptive region (i.e., graph depth), the number of support nodes (and thus the time-space complexity) increases exponentially, a.k.a, "Neighbor Explosion. " Many graph sampling methods have been proposed to tackle this problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">43]</ref>. However, these sampling methods only work on homogeneous graphs and ignore the types of edges, which usually capture important meta information.</p><p>In addition, many complicated techniques are adopted to improve performance, e.g., Graph Matching Networks <ref type="bibr" target="#b23">[23]</ref> and Joint Learning <ref type="bibr" target="#b2">[3]</ref>. The overall architectures of EA methods become more and more complex, while the time complexity is also dramatically increased. For example, the running time of complex encoders (e.g., MuGNN <ref type="bibr" target="#b2">[3]</ref>) is ten times more than that of the vanilla GCN <ref type="bibr" target="#b38">[38]</ref>.</p><p>(2) Dilemma of negative sampling: As a representation learning task, EA relies on margin-based pairwise loss functions (e.g., Triplet loss <ref type="bibr" target="#b30">[30]</ref>, Contrastive loss <ref type="bibr" target="#b15">[16]</ref>, or TransE <ref type="bibr" target="#b0">[1]</ref>). In early studies <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b38">38]</ref>, negative samples are usually generated by uniform sampling and thus highly redundant. During training stage, the model could be hampered by such low-quality negative samples, resulting in slow convergence and performance degradation. Therefore, many previous studies focus on generating high-quality samples (i.e., hard samples), such as Top-loss <ref type="bibr" target="#b11">[12]</ref> and Focal loss <ref type="bibr" target="#b24">[24]</ref>. In the EA task, BootEA <ref type="bibr" target="#b33">[33]</ref> presents Truncated Uniform Negative Sampling Loss to choose -nearest neighbors as the hard negative samples. Many subsequent studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">46]</ref> adopt this simple but effective strategy. However, ranking all entities to find -nearest neighbors in every epoch is extremely resource-consuming. For instance, the sampling stage of BootEA takes up more than 25% of the total time cost. Moreover, a large number of negative samples implicitly link to a massive GPU memory requirement. With the data scale further expanding from experimental datasets to real KGs, how to balance between performance and time-space consumption would become a dilemma.</p><p>(3) "Catastrophic forgetting" in semi-supervised learning: "Catastrophic forgetting" refers to the phenomenon that the networks forget the previously learned samples when learning new samples. In practice, it is expensive to label aligned entity pairs manually. Therefore, existing EA studies <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref> usually reserve 30% of the dataset or even less as the training data to simulate this situation. Several EA methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">33]</ref> introduce iterative strategies to produce semi-supervised data, which significantly boosts up the performance. However, due to the "catastrophic forgetting", existing semi-supervised EA methods have to mix up all the previously learned data with newly generated semi-supervised data and then re-train them altogether in the next iteration. Thus, the running time of such iterative semi-supervised methods is usually several times more than that of a non-iterative method. With the expansion of the data scale, time consumption would continue to increase.</p><p>The above three challenges lead to the poor scalability of existing EA methods, rendering them infeasible for large-scale KGs. In this paper, to address these challenges, we design the following three proposals: (1) Remove inefficient components from existing EA methods to greatly simplify the graph encoder architecture and adopt a new graph sampling strategy based on relational attention mechanism. (2) Inspired by recent contrastive image representation learning (CIRL) studies on the necessity of negative samples (BYOL <ref type="bibr" target="#b13">[14]</ref> and SimSiam <ref type="bibr" target="#b8">[9]</ref>), we prove from another angle that the essence of GNNs-based EA methods is to solve a permutation matrix approximately and the negative samples are unnecessary in EA. Based on this important finding, we design a symmetric negative-free alignment loss to improve the space and time efficiency significantly. (3) To address the "catastrophic forgetting" phenomenon and further speed up the training speed, we demonstrate a new incremental semi-supervised learning strategy. When learning newly generated semi-supervised samples, the model only needs to review a tiny amount of the previous samples but still learns effectively. By incorporating the above three new components, we present a novel EA method with high Performance, high Scalability, and high Robustness (PSR).</p><p>To fully validate our proposed method, we conduct comprehensive experiments on several public datasets. In terms of performance, the proposed method beats all state-of-the-art competitors across all datasets. In ablation experiments, the performance of PSR only fluctuates by 3%, showing strong robustness on various hyper-parameter settings. Most importantly, PSR demonstrates impressive scalability. Its space complexity is only proportional to the batch size and graph density, while totally independent of the graph scale. Meanwhile, its time complexity only increases linearly in accordance with the graph scale. Under the same hardware environment condition, the speed of PSR is several times or even tens of times faster than other EA methods. With a small batch size (128), our proposed method could run with only 3 GB memory, while the performance is still comparable to SOTA (i.e., only degrading by less than 3%). Our main contributions are summarized as follows:</p><p>? To our best knowledge, this is the first work to prove that the essence of GNNs-based EA methods is to solve a permutation matrix approximately and explain why negative samples are unnecessary from a new angle of view. ? We propose a novel EA method with high Performance, high Scalability, and high Robustness (PSR), incorporating three components: (1) Simplified graph encoder with relational graph sampling. (2) Symmetric negative-free alignment loss.</p><p>(3) Incremental semi-supervised learning. ? We design detailed experiments to examine the proposed method from multiple perspectives. The experimental results show that PSR not only surpasses the previous SOTA in performance but also has impressive scalability and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK DEFINITION</head><p>KGs store the real-world knowledge in the form of triples (?, , ).</p><p>A KG could be defined as = ( , , ), where and represent the entity set and relation set respectively, ? ? ? denotes the triple set. Defining 1 and 2 to be two KGs, is the set of pre-aligned entity pairs between 1 and 2 . The task of EA aims to find new aligned pairs set ? based on the pre-aligned seeds .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK 3.1 Entity Alignment</head><p>Most of existing EA methods can be summarized into two steps:</p><p>(1) Using KG embedding methods (e.g., TransE <ref type="bibr" target="#b0">[1]</ref>, GCN <ref type="bibr" target="#b20">[20]</ref>, and GAT <ref type="bibr" target="#b36">[36]</ref>) to generate low-dimensional embeddings for entities and relations in each KG. (2) Mapping these embeddings into a unified vector space through pre-aligned entities and pairing each entity by distance metrics (e.g., Cosine and Manhattan). In addition, several methods apply iterative strategies <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">33]</ref> to generate semisupervised data or introduce extra literal information <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> (e.g., entity name) to enhance the model. In <ref type="table">Table 1</ref>, EA methods are categorized based on what design is chosen for encoder and mapper and whether introducing the enhancement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Encoder</head><p>Mapper Enhancement GCN-Align <ref type="bibr" target="#b38">[38]</ref> GNN Margin None MuGNN <ref type="bibr" target="#b2">[3]</ref> Hybrid Margin None RSNs <ref type="bibr" target="#b32">[32]</ref> Trans Corpus fusion None HyperKA <ref type="bibr" target="#b31">[31]</ref> GNN Margin None</p><p>BootEA <ref type="bibr" target="#b33">[33]</ref> Trans Corpus fusion Semi NAEA <ref type="bibr" target="#b46">[46]</ref> Hybrid Corpus fusion Semi TransEdge <ref type="bibr" target="#b34">[34]</ref> Trans Corpus fusion Semi MRAEA <ref type="bibr" target="#b26">[26]</ref> GNN Margin Semi GM-Align <ref type="bibr" target="#b41">[41]</ref> GNN Margin Entity Name RDGCN <ref type="bibr" target="#b39">[39]</ref> GNN Margin Entity Name HMAN <ref type="bibr" target="#b42">[42]</ref> GNN Margin Attribute DGMC <ref type="bibr" target="#b12">[13]</ref> GNN Margin Entity Name <ref type="table">Table 1</ref>: Categorization of some popular EA methods.</p><p>Graph Encoders. Trans represents TransE <ref type="bibr" target="#b0">[1]</ref> and its derivative algorithms. These methods interpret relations as a translation from head entities to tail entities and assume that the embeddings of entities and relations follow the assumption ? + ? . Due to its easy implementation and high efficiency, the Trans family is widely used in early EA methods. In recent studies, GNNs gradually become the mainstream encoder because of their powerful graph modeling capability. Furthermore, several EA methods adopt the Hybrid strategy, i.e., using GNNs to model KGs and optimizing the Trans loss at the same time. However, Hybrid methods usually require careful tuning on excessive hyper-parameters (e.g., NAEA has more than ten hyper-parameters), which leads to poor scalability and weak robustness.</p><p>Mappers. Margin indicates a series of margin-based pairwise losses, such as Triplet loss <ref type="bibr" target="#b30">[30]</ref> and Contrastive loss <ref type="bibr" target="#b15">[16]</ref>, which are often used in Siamese networks <ref type="bibr" target="#b1">[2]</ref>. Almost all GNNs-based EA methods adopt Siamese architectures with margin-based losses, which is quite similar to CIRL methods. Corpus fusion uses the pre-aligned set to swap the entities in existing triples and generates new triples to anchor the entities into a unified vector space. For example, there are two triples ( 1 , 1 , 2 ) ? 1 and ( 3 , 2 , 4 ) ? 2 . If ( 1 , 3 ) ? holds, Corpus fusion will add two extra triples ( 3 , 1 , 2 ) and ( 1 , 2 , 4 ). Trans methods usually apply such kind of mapper.</p><p>Enhancement. Due to the lack of labeled data, several methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b46">46]</ref> adopt iterative strategies to construct semi-supervised data. Despite significant performance improvements, the time consumption of these methods increases several times more. Besides, some methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> include literal information (e.g., entity name) to provide a multi-aspect EA view. However, literal information is not always available in real applications. For example, there will be privacy risks when using user-generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Image Representation Learning</head><p>The core idea of contrastive learning is to model samples through Siamese networks <ref type="bibr" target="#b1">[2]</ref>, attract positive sample pairs, and repulse negative sample pairs. This methodology has been widely utilized in self-supervised image representation learning. Recent CIRL <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> methods define the inputs as two augmentations of one image and <ref type="bibr" target="#b0">1</ref> His name is BianBian. What a lovely puppy, isn't it? maximize the similarity subject to different conditions (as shown in <ref type="figure" target="#fig_0">Figure 2</ref>). The projection head is to reduce the loss of information induced by the contrastive losses. In practice, contrastive learning methods could benefit from a large number of negative samples, which require massive memory. For example, SimCLR <ref type="bibr" target="#b7">[8]</ref> regards all the other samples in the current batch as the negative samples, and the batch size is set to 8, 192 for best performance. Obviously, most researchers can not afford the cost of such large-scale hardware resources. MoCo <ref type="bibr" target="#b17">[18]</ref> presents the "momentum encoder" to alleviate this problem, but negative sampling remains to be the bottleneck.</p><p>More recently, BYOL <ref type="bibr" target="#b13">[14]</ref> and SimSiam <ref type="bibr" target="#b8">[9]</ref> successfully prove that negative sampling is unnecessary for image contrastive learning by blocking half of the back-propagation stream <ref type="figure" target="#fig_0">(Figure 2b</ref>). GNNs-based EA methods also rely on Siamese architecture with contrastive losses, so we believe that EA may also benefit from removing negative sampling, especially considering the space and time spent to find hard negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RETHINKING OF ENTITY ALIGNMENT</head><p>Recently, GNNs-based methods have become the mainstream of EA approaches. Taking the very first and also the simplest GNNs-based EA method, GCN-align <ref type="bibr" target="#b38">[38]</ref> as an example, it embeds each KG into a low-dimensional space via the following equation:</p><formula xml:id="formula_0">( +1) = ( ( ) ( ) )<label>(1)</label></formula><p>where ? R | |?| | is the symmetric normalized Laplacian matrix, is an activation function, ( ) ? R | |? is the entity embedding matrix from the last layer, and ( ) is a transformation matrix. Note that (0) is randomly initialized. Then, these entity embeddings from two KGs are mapped into a unified space via Triplet loss:</p><formula xml:id="formula_1">= ?? ( , ) ? + ? ? ? ? ? ? ? ? ? +<label>(2)</label></formula><p>where ( , ) is a pre-aligned pair, ( ? , ? ) represents a negative pair generated by replacing one of ( , ), is the margin hyperparameter, and [ ] + indicates ( , 0). Of course, the actual implementation is more complicated, but the core idea can be summarized as the above. Naturally, there is a question raised: What is implicitly optimized in GNNs-based EA methods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Another Perspective of EA</head><p>Our Hypothesis: GNNs-based EA methods are implementations to solve a permutation matrix approximately.</p><p>Unlike the previous studies that regard EA as a representation learning task, we view EA from the perspective of permutation matrix solving. Assuming there is an ideal graph containing all nodes and edges (i.e., the ground-truth graph), 1 is just one particular graph instance constructed from by using information extraction algorithm or manual labor. We use ? R | |?| | and 1 ? R | |? | | to denote the symmetric normalized Laplacian matrix of and 1 , respectively. Then, the construction process of 1 from could be described into two steps: reorder the entities in and introduce certain noise. Formally, this construction process is formulated as below:</p><formula xml:id="formula_2">?1 + = 1<label>(3)</label></formula><p>where ? R | |? | | is a permutation matrix representing the equivalent relation of entities between and 1 . Note that there is exactly one entry of 1 in each row and each column in while 0s elsewhere. Thus, is also an orthogonal matrix, which complies with the fundamental assumption in EA that one entity has and only has one aligned entity. When = 1, it represents that ? and ? 1 are an aligned pair. ? R | |? | | is a noise matrix denoting the subtle noise added during the construction process of <ref type="bibr" target="#b0">1</ref> . Thus, in essence, different graph instances could be regarded as the results of the same ideal graph by changing with different orders and introducing different noises (as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>). Therefore, for two graph instances 1 and 2 constructed from the same ideal graph , we have the following equation:</p><formula xml:id="formula_3">1 ?1 ( 1 ? 1 ) 1 = = 2 ?1 ( 2 ? 2 ) 2<label>(4)</label></formula><p>Let be the permutation matrix equaling to 1 2 ?1 . The above equation is transformed into:</p><formula xml:id="formula_4">( 1 ? 1 ) = ( 2 ? 2 )<label>(5)</label></formula><p>Meanwhile, according to the definition of the permutation matrix, the pre-aligned entity set is equivalent to a set of constraints:</p><formula xml:id="formula_5">[( 1 ? 1 ) ] = [( 2 ? 2 )] , ?( , ) ?<label>(6)</label></formula><p>where [ ] represents the -th row of matrix . Obviously, the aim of EA now turns to solve the permutation matrix under these constraints (i.e., given pre-aligned entity pairs ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate Matrix Solving</head><p>Both the permutation matrix and the two noise matrices (i.e., <ref type="bibr" target="#b0">1</ref> and 2 ) are unknown high-dimensional matrices, which is hard to solve directly. Without loss of generality, the EA task follows the two premises as below: Premise 1: Since the introduced noise is subtle, the structures of 1 and 2 are still isomorphic to . Thus, the noise matrices 1 and 2 in Equation (6) could be dropped, approximately.</p><p>Premise 2: Approximately, the matrix could be further decomposed into the product of two low-rank dense matrices 1 ? R | |? and 2 ? R ? | | (i.e., ? 1 2 ). Let 2 ? R | |? be the right inverse matrix of 2 (i.e., <ref type="bibr">2 2</ref> = ), Equation <ref type="formula" target="#formula_5">(6)</ref> could be transformed into: here 1 and 2 must be full column rank. Obviously, when we modify the above equation by introducing an activation function and adding further decompositions as 1 = 1 1 and 2 = 2 2 , Equation <ref type="formula" target="#formula_6">(7)</ref> still holds:</p><formula xml:id="formula_6">[ 1 1 ] = [ 2 2 ] , ?( , ) ?<label>(7)</label></formula><formula xml:id="formula_7">[ ( 1 1 1 )] = [ ( 2 2 2 )] , ?( , ) ?<label>(8)</label></formula><p>Compared to Equation <ref type="formula" target="#formula_0">(1)</ref>, we find that either side of Equation <ref type="formula" target="#formula_7">(8)</ref> is in the same form as the right-hand side of Equation <ref type="formula" target="#formula_0">(1)</ref>, which suggests that the approximate matrix solving is equivalent to the Siamese networks with GCN encoders. In general, Equation <ref type="formula" target="#formula_7">(8)</ref> could be solved by the least-squares method:</p><formula xml:id="formula_8">1,2 , 1,2 ?? ( , ) ? ([ 1 1 1 ] ) ? ([ 2 2 2 ] ) 2 2<label>(9)</label></formula><p>Simply optimizing Equation <ref type="formula" target="#formula_8">(9)</ref> by gradient descent inclines to a trivial solution <ref type="bibr" target="#b8">[9]</ref> (i.e., all outputs collapse to a constant). Therefore, it is essential to ensure that 1 1 and 2 2 are both full column rank to prevent the model from collapsing. Generally, the involving of negative samples is equivalent to introducing a penalty term to trivial solutions, which can preclude constant outputs from the solution space. Blocking half of the backpropagation stream (i.e., <ref type="figure" target="#fig_0">Figure 2b</ref>) also avoids constant outputs. Putting this trick into our approximate solving framework, it is analogous to approximately solving by only updating 1 1 with a fixed <ref type="bibr">2 2</ref> . Intuitively, in high dimensions, any randomly drawn vectors are always nearly orthogonal. Thus, 1 1 and 2 2 are invariably full column rank. In a nutshell, blocking back-propagation would guarantee that the EA models do not collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Analysis of GNNs-based EA</head><p>In our opinion, GNNs-based EA has a close correlation with CIRL. Once we view the ideal KG as the input image, the graph instance construction process is equivalent to the random image argumentation trick. However, there are still several fundamental differences between these two tasks. For example, there is only one ideal graph in EA, but thousands of images in CIRL. These differences might be the reason that several key techniques (e.g., momentum encoder) are proved to be effective in CIRL but not working in EA.</p><p>Note that although the above inferences depend on the premise that is invariant, it is easily extended into attention-based GNNs <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39]</ref> with variant . Thereby a more general inference method is derived and proved in our subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE PROPOSED METHOD</head><p>This section will describe our proposed method in detail, including the encoder architecture and training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Encoder Architecture</head><p>To effectively deal with large-scale KGs, we propose a minimalism relation-aware graph encoder. The proposed encoder removes all high-complexity components (e.g., hybrid strategy and graph matching) and even abandons the normal linear transformation matrix. The inputs of our model are two matrices:</p><p>(0) ? R | |? represents the input entity features and ? R | |? represents the input relation features. Both of them are initialized by the random uniform initializer. Our proposed minimalism relation-aware graph encoder consists of three major components:</p><p>Relation-specific embeddings. KG usually contains complicated relationships, such as reflexive, one-to-many, many-to-one, and many-to-many. Some KG embedding methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b29">29]</ref> construct relation-specific entity embeddings in order to better describe these complex relations. Inspired by RREA <ref type="bibr" target="#b27">[27]</ref>, we generate reflections of entities along the relational hyperplanes to enforce relationspecificity:</p><formula xml:id="formula_9">( , ) = ? 2<label>(10)</label></formula><p>where and represent the embeddings of entity and relation , and ? ? 2 = 1. Compared with RGCN <ref type="bibr" target="#b25">[25]</ref> and TransR <ref type="bibr" target="#b29">[29]</ref> which set up a transformation matrix for each relation, the above operation only requires much fewer parameters and has higher computational efficiency.</p><p>Relational attention aggregator. When aggregating neighborhood information, the vanilla GCN only considers the degree of nodes and ignores the type of edges, which we believe also embeds hidden semantics. Therefore, we bring in the relational attention mechanism to distinguish the importance between edges and aggregate neighboring information anisotropically. Besides, we remove the normal linear transformation matrix and replace it with the relation-specific embeddings. The output feature of from the? layer is described as following:</p><formula xml:id="formula_10">( +1) = ?? ?N ?? ? ( ) ( ( ) , )<label>(11)</label></formula><p>where N is the neighboring entity set of , and is the set of relations between and . In this work, we adopt ELU <ref type="bibr" target="#b10">[11]</ref> as the activation function.</p><p>( ) represents the relational attention coefficient which is obtained as below:</p><formula xml:id="formula_11">( ) = [ ( ( ) , ) ? ? ( ( ) , )]<label>(12)</label></formula><formula xml:id="formula_12">( ) = ( ( ) ) ? ?N ? ? ? ( ( ) ? ? )<label>(13)</label></formula><p>where ? R 3 is a trainable vector for calculating the attention coefficient. ? means the concatenate operation.</p><p>Multi-hop representation. To enable a global-aware representation, we concatenate the output embeddings from multiple layers, capturing multi-hop neighboring information. The final output embedding of entity is in the following form:</p><formula xml:id="formula_13">= [ (0) ? (1) ?...? ( ) ]<label>(14)</label></formula><p>where (0) represents the input embedding of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Strategy</head><p>In addition to simplifying the encoder's architecture, the proposed method also boosts the training efficiency from three aspects:</p><p>Relational attention graph sampling. Previous GNNs-based EA methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b38">38]</ref> usually train on full graph, with the consequence of demanding large memory and slow speed. Following GraphSAGE <ref type="bibr" target="#b16">[17]</ref>, we only sample a small batch of positive pairs from training data and construct a multi-hop sub-graph. But instead of the equal probability sampling, we propose an unequal probability sampling strategy since we believe not all triples have equal importance. Given the center node , we sample times from its neighboring triples with replacement and directly use the normalized attention coefficient as the probability of triple ( , , ):</p><formula xml:id="formula_14">(( , , )| ) =<label>(15)</label></formula><p>With this setting, more important triples have a higher probability of being sampled. In accordance with this, for different center nodes, the expectations of attention coefficient from one sampling (also means information capacity) are different:</p><formula xml:id="formula_15">E[ ] = ?? ?N ?? ? (( , , )| ) (16) = ?? ?N ?? ? 2<label>(17)</label></formula><p>If we follow the previous studies to use a fixed or set inversely proportional to the degree, it will lead to redundancy or deficiency of information. In fact, we expect that for any center node , the sum of attention coefficient expectations should maintain certain stability after round of sampling:</p><formula xml:id="formula_16">= ? E[ ] ?<label>(18)</label></formula><p>where is a hyper-parameter controlling the sampling ratio. ? ? denotes the ceil operation.   Symmetric negative-free alignment loss. As mentioned in Section 4.2, one side of the Siamese networks' back-propagation should be blocked to prevent the model from collapsing in the case of no negative samples. Specifically, we first initialize the graph encoder randomly, make a forward propagation on each KG, and store the initial output embedding as for each entity. Next, taking as the target, the model is optimized by the following loss function (as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>):</p><formula xml:id="formula_17">= ?? ( , ) ? ? ( , ) ? ( , )<label>(19)</label></formula><p>The back-propagation streams of and are cut off. In this paper, we use the cosine similarity as the metric ( , ).</p><formula xml:id="formula_18">( , ) = ? , ? ? ? 2 ? ? 2<label>(20)</label></formula><p>In fact, and are analogous to the target network in BYOL <ref type="bibr" target="#b13">[14]</ref>. However, due to the fundamental differences between EA and CIRL, adopting the momentum encoder of MoCo and BYOL will cause the model to degrade. Thus, the momentum encoder is not incorporated in our model. Once the training process is completed, the final similarity score between and is calculated as below:</p><formula xml:id="formula_19">( , ) = ( , ) + ( , )<label>(21)</label></formula><p>Incremental semi-supervised learning. "Catastrophic forgetting" refers to the phenomenon that the networks forget the previously learned samples after learning new samples. Specific in the EA task, ( , ) ? is a pre-aligned pair, which has already been mapped to an adjacent space in the early training batch. However, once or is chosen to be negative samples, the model "forgets" their learned relative positions (as shown in <ref type="figure" target="#fig_3">Figure 5</ref>). Therefore, existing EA methods need to mix the newly generated semi-supervised data with the early training data and then retrain the model, resulting in low efficiency. With the complete absence of negative samples, the proposed method is born with the power to do incremental learning. When learning new batches, the learned entity embeddings are only indirectly influenced by their neighboring entities. Thus, the "forgetting" phenomenon is largely weakened. To further alleviate "catastrophic forgetting," we store all the embeddings of the training set after each iteration (embeddings of the previous iteration will be overwritten). In the next iteration,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Incremental Semi-supervised Learning Strategy</head><p>Input: </p><formula xml:id="formula_20">Graphs 1 = ( 1 , 1 , 1 ), 2 = ( 2 , 2 , 2 ),</formula><formula xml:id="formula_21">? = ( , ? 2 ) /* ( , ) is the entity nearest to in .*/ 16: if ( ? , ? 1 ) = then 17: ? { ( , ? ) } ? ; ? 1 ? ? 1 ? { }; ? 2 ? ? 2 ? { ? }; 18: end if 19:</formula><p>end for 20: /*Update the embeddings set*/ 21:</p><formula xml:id="formula_22">for ( , ) ? do, ? ? { ( , ) }; 22:</formula><p>end for 23:</p><p>? ; /*Update the training set*/ 24: until no more aligned entities are added to we compare them with the currently learned embeddings. If the similarity difference of an entity pair exceeds , such pair will be added into the next iteration again (a.k.a, reviewing). According to our experiments, only very few pairs will be added multiple times.</p><p>In addition, we adopt the Bi-directional Iterative Strategy proposed by MRAEA <ref type="bibr" target="#b26">[26]</ref> to construct high-quality semi-supervised aligned pairs. If and only if the entities and are mutually nearest neighbors to each other, then the pair ( , ) would be considered as new semi-supervised aligned entities. The detailed incremental semi-supervised procedure is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>All the experiments are conducted on a PC with a GeForce GTX TI-TAN X GPU (12GB) and 128GB memory. The code is now available (https://github.com/MaoXinn/PSR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>To fairly and comprehensively verify the performance, scalability, and robustness of our model, we experiment with three widely used public datasets: (1) DBP15K <ref type="bibr" target="#b32">[32]</ref>: This dataset consists of three cross-lingual subsets from multi-lingual DBpedia: DBP EN?FR , DBP EN?ZH , and DBP EN?JA . Each subset contains 15, 000 pre-aligned entity pairs. As an early dataset, DBP15K is the most popular one but has two defects: small scale and dense links. (2) SRPRS: Guo et al. <ref type="bibr" target="#b14">[15]</ref> propose this sparse dataset, including two cross-lingual subsets: SRPRS FR?EN and SRPRS DE?EN . Each subset also contains 15, 000 pre-aligned pairs but with much fewer triples compared to  : This dataset comprises two monolingual subsets, each containing 100, 000 pre-aligned entity pairs and nearly one million triples. As the largest dataset, DWY100K raises challenges to the scalability of EA models. The statistics of these datasets are summarized in <ref type="table" target="#tab_1">Table 2</ref>. To be consistent with previous studies <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref>, we randomly split 30% of the pre-aligned entity pairs for training and development while using the remaining 70% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>To fully evaluate our proposed method, we compare it against the following three groups of advanced EA methods: (1) Structure: These methods only use the original structure information (i.e., triples): MTransE <ref type="bibr" target="#b6">[7]</ref>, GCN-Align <ref type="bibr" target="#b38">[38]</ref>, RSNs <ref type="bibr" target="#b14">[15]</ref>, MuGNN <ref type="bibr" target="#b2">[3]</ref>, Hy-perKA <ref type="bibr" target="#b31">[31]</ref>. (2) Semi-supervised: These methods adopt iterative strategy to generate semi-supervised data: BootEA <ref type="bibr" target="#b33">[33]</ref>, NAEA <ref type="bibr" target="#b46">[46]</ref>, TransEdge <ref type="bibr" target="#b34">[34]</ref>, MRAEA <ref type="bibr" target="#b26">[26]</ref>, JEANS <ref type="bibr" target="#b5">[6]</ref>. (3) Literal: To obtain a multi-aspect view, these methods use the literal information (e.g., entity name) of entities as the input features: GM-Align <ref type="bibr" target="#b41">[41]</ref>, RDGCN <ref type="bibr" target="#b39">[39]</ref>, HMAN <ref type="bibr" target="#b42">[42]</ref>, HGCN <ref type="bibr" target="#b40">[40]</ref>, DGMC <ref type="bibr" target="#b12">[13]</ref>.</p><p>To make a fair comparison against the above three groups of methods, PSR also has three corresponding versions: (1) PSR (basic) is the basic version without incremental learning. (2) PSR (semi) introduces incremental learning to generate semi-supervised data.</p><p>(3) PSR (lit) adopts a simple strategy to incorporate the literal information. Specifically, for and , we first use PSR (Semi) to obtain the structural similarity . Then, using the cross-lingual word embeddings (same with GM-Align <ref type="bibr" target="#b21">[21]</ref>) to calculate the literal similarity . Finally, the entities are ranked according to + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Settings</head><p>Metrics. Following convention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41]</ref>, we use @ and Mean Reciprocal Rank ( ) as the evaluation metrics. We report the average of five independent runs as the results.</p><p>Hyper-parameters. For all datasets, the same default configuration is set: embedding dimension = 300; depth of GNN = 2; sampling ratio = 1; reviewing threshold = 0.05; batch size is 512; dropout rate is set to 30%. RMSprop is adopted to optimize the model with a learning rate set to 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Main Experiments</head><p>In <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, we report the performance of all methods on all three datasets. Our method consistently achieves the best performance across all datasets and metrics. Results on DBP15K. On this dataset, PSR (basic) outperforms the previous SOTA in the Basic group by more than 13% on Hit@1 and 10% on MRR. This indicates that the architecture of PSR effectively captures the rich relational structure information existing in this dataset. Benefiting from the extra semi-supervised data, the performance of PSR (semi) is significantly improved on every metric compared to its basic version. Moreover, compared against other advanced semi-supervised EA methods, PSR (semi) still maintains a significant performance advantage of 4% on @1. Through further incorporating of cross-lingual word embeddings, PSR (lit) could model entities from both structure and semantics perspectives. Therefore, its performance further increases and consistently surpasses the previous SOTA in the Literal group. Besides, we observe that the performances of the Literal group vary significantly across language pairs, which is entirely different from the structureonly groups. The French dataset has the most apparent gain, while the Chinese dataset has the least. Results on SRPRS. To simulate the degree distribution in realworld data, SRPRS greatly cuts down the number of triples, which challenges EA methods' embedding capability on sparse KGs. Although the performance of PSR drops compared to DBP15K, it still outperforms all the other advanced EA methods. The semisupervised strategy still has some benefits, but the improvement   <ref type="table">Table 5</ref>: Ablation experiments of model architecture.</p><p>is reduced to 4-5%. We believe the reason for this smaller improvement is that the sparse nature of SRPRS makes the generation of high-quality semi-supervised data very hard. Therefore, the literal information becomes critical on SRPRS and improves @1 by at least 28% if compared to PSR (semi). Results on DWY100K. As the largest dataset, DWY100K raises challenges to the space-time complexity of EA methods. MuGNN and NAEA exceed the GPU memory limit setup in this experiment, so they have to run on the CPU, which substantially slows down the training speed. On the contrary, the high scalability enables our model easy to cope with this large dataset and surpasses the previous SOTA by at least 7% on @1 (shown in <ref type="table" target="#tab_4">Table 4</ref>). In addition, since this dataset shares similar dense degree distribution with DBP15K, the semi-supervised strategy also works well here. Time Efficiency. The trump card of PSR is superior efficiency and scalability. So we specifically evaluate the overall time costs of existing EA methods and report all results in <ref type="table">Table 6</ref>. It is obvious that the efficiency of PSR far exceeds all advanced competitors. The speed of our proposed method is tens or even hundreds of times faster than that of existing complex EA methods. Even compared against the fastest baseline (i.e., GCN-Align), the speed of PSR (basic) is 5? faster on DWY100K, while the @1 is 25% higher. Among the PSR group itself, the semi-supervised strategy consumes 2? more time cost than the basic version, while PSR (lit) has no clear additional time cost than PSR (semi).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Size Dimension</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBPZH-EN</head><p>DWYDBP-WD <ref type="figure">Figure 6</ref>: Ablation experiments on batch size and dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Experiment</head><p>Model Architecture. The architecture of PSR consists of the following five components: (1) Relation-specific embeddings (RSE);</p><p>(2) Relational attention aggregator (RAA); (3) Multi-hop representation (MHR); (4) Relational attention graph sampling (RAGS); (5) Symmetric negative-free alignment loss (SNAL). We replace these components from PSR(semi) individually to demonstrate their effectiveness. Specifically, RSE is replaced by normal linear transformation matrices. RAA and RAGS become isotropic, i.e., all entities and relations have equal importance. MHR is replaced by the residual connection <ref type="bibr" target="#b19">[19]</ref> and SNAL is replaced by the Truncated Uniform Negative Sampling loss <ref type="bibr" target="#b33">[33]</ref>. The experimental results are listed in <ref type="table">Table 5</ref>. Among all these components, MHR has the greatest impact on performance. Without MHR, the performances are degraded by at least 4% on @1. For SNAL, the results are almost identical to the Truncated Uniform Negative Sampling loss, indicating that SNAL could reduce the time-space consumption without losing any accuracy. Besides, the remaining three components also show the necessity as our expectation. On average, adopting them improves performance by 1% to 3% on @1. <ref type="bibr" target="#b2">3</ref> All results are obtained by directly running the source code with default settings. RDGCN requires extremely high memory space and we are unable to obtain its result on DWY100K. Batch Size. To explore the impact of batch size, we report the performances of PSR (semi) with batch size from 128 to 1, 024 on DBP ZH?EN and DWY DBP?WD . In this experiment, only the batch size changes, all the other hyper-parameters remain fixed. As shown in <ref type="figure">Figure 6</ref>, our method constantly works well over this wide range of batch sizes on both datasets. Even a small batch size of 128 performs decently, with a drop of less than 2% on @1. We observe that the batch size greatly impacts the GPU memory occupation almost linearly. Since PSR adopts a mini-batch training strategy, its space complexity is expected to be only related to batch size and graph density, not to the scale of graphs. The small memory consumption gap between DBP15K and DWY100K indeed verify this expectation. Embedding Dimension. <ref type="figure">Figure 6</ref> also reports the performances with embedding dimension from 100 to 400. Similar to batch size, PSR also works well over this wide range of dimensions. Even a dimension size of 100 performs decently, with a drop of only 3% on @1. It is very obvious that both the time and space costs decrease linearly with the shrink of embedding dimension. Overall, PSR is capable of dealing with large-scale datasets and run on devices with limited memory but still output comparable performance. Incremental Learning. In Section 5.2, we mention that PSR is born with incremental learning ability and propose a simple reviewing strategy to further alleviate "catastrophic forgetting". To validate the effectiveness of our design, we set the reviewing threshold from 0.01 to 0.1. Furthermore, there are two baselines: (a) Normal represents existing EA methods' semi-supervised strategy, i.e., mixing all the newly generated data and early data into the next iteration. (b) w/o rev. means not reviewing any early samples at all (i.e., = ?). In this way, each pair will be added to the training set only once.</p><p>As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, the performance gap between these two baselines is only 1%, which proves that PSR is capable of learning incrementally. By further incorporating the complete reviewing mechanism, PSR could significantly reduce the time cost while keeping the performance intact. Such an incremental learning strategy could ensure that the training complexity increases linearly with the dataset scale, making massive-scale EA feasible. Pre-aligned Ratio. In practice, manually annotating pre-aligned entity pairs consumes a lot of resources, especially for large-scale KGs. We expect that the proposed model could keep decent performance with limited pre-aligned entity pairs. To investigate the performance with different pre-aligned ratios, we set the ratios from 10% to 50%. <ref type="figure" target="#fig_7">Figure 8</ref>  Basic Semi Literal HITS@1 Degree <ref type="figure">Figure 9</ref>: @1 of the entities with different degrees.</p><p>SRPRS DE?EN . Benefiting from the rich structure information in DBP FR?EN , the model could iteratively generate a large number of high-quality entity pairs for training. Therefore, the semi-supervised strategy performs pretty well and greatly improves the performance. On the other hand, because SRPRS DE?EN is much more sparse, the impact of the semi-supervised strategy is very limited. In both datasets, introducing literal information could significantly improve the performance. Especially in SRPRS DE?EN , literal information could complement the insufficiency of structural information. Degree Analysis. The above experiments show that the performances of all EA methods on sparse datasets is much worse than that of standard datasets. To further explore the correlation between performance and density, we design an experiment on DBP FR?EN . <ref type="figure">Figure 9</ref> shows the @1 of the three variants on different levels of entity degrees. We observe that there is a strong correlation between performance and degree. The semi-supervised strategy does improve the overall performances but has a limited effect on entities whose local structures are extremely sparse. On the other hand, PSR (lit) has much better performances. Unfortunately, literal information is not always available in real-life applications. Therefore, how to better represent these sparse entities without extra information could be one key point for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we prove that the essence of GNNs-based EA methods is to solve a permutation matrix approximately and explain why negative samples are unnecessary from a new angle of view. Furthermore, we propose a novel EA method with three major modifications: (1) Simplified graph encoder with relational graph sampling. (2) Symmetric alignment loss without negative sampling.</p><p>(3) Incremental semi-supervised learning strategy. Thus, the proposed method could simultaneously possess high performance, high scalability, and high robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Examples of contrastive learning methods.<ref type="bibr" target="#b0">1</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Construction process of 1 and 2 from .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The illustration of symmetric negative-free loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>An example of "forgetting". Green lines represent positive pairs and red lines represent negative pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Method DBP ZH?EN DBP JA?EN DBP FR?EN SRPRS FR?EN SRPRS DE?EN H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR Basic MTransE 0.209 0.512 0.310 0.250 0.572 0.360 0.247 0.577 0.360 0.213 0.447 0.290 0.107 0.248 0.160 GCN-Align 0.434 0.762 0.550 0.427 0.762 0.540 0.411 0.772 0.530 0.243 0.522 0.340 0.385 0.600 0.460 MuGNN 0.494 0.844 0.611 0.501 0.857 0.621 0.495 0.870 0.621 0.131 0.342 0.208 0.245 0.431 0.310 RSNs 0.508 0.745 0.591 0.507 0.737 0.590 0.516 0.768 0.605 0.350 0.636 0.440 0.484 0.729 0) 0.702 0.924 0.781 0.698 0.930 0.782 0.731 0.941 0.807 0.441 0.741 0.542 0.553 0.799 0.638 ?stds 0.003 0.002 0.002 0.004 0.001 0.002 0.003 0.003 0.002 0.004 0.002 0.002 0.003 0.002 0.003 -value 2e-16 8e-15 5e-17 2e-15 6e-18 3e-17 2e-16 1e-12 5e-17 8e-14 4e-17 6e-17 7e-14 2e-15 8e-14 Semi BootEA 0.629 0.847 0.703 0.622 0.853 0.701 0.653 0.874 0.731 0.365 0.649 0.460 0.503 0.732 0.580 NAEA 0.650 0.867 0.720 0.641 0.872 0.718 0.673 0.894 0.752 0.177 0.416 0.260 0.307 0.535 0.390 TransEdge 0.735 0.919 0.801 0.719 0.932 0.795 0.710 0.941 0.796 0.400 0.675 0.490 0.556 0.753 00.930 0.827 0.758 0.934 0.826 0.781 0.948 0.849 0.460 0.768 0.559 0.594 0.818 0.666 PSR(semi) 0.802 0.935 0.851 0.803 0.938 0.852 0.828 0.952 0.874 0.486 0.781 0.577 0.606 0.831 0.680 ?stds 0.004 0.001 0.004 0.003 0.002 0.003 0.003 0.002 0.002 0.003 0.002 0.003 0.004 0.003 0.002 -value 4e-11 6e-08 1e-08 3e-12 1e-04 4e-10 2e-12 1e-04 2e-11 4e-10 6e-09 1e-08 4e-06 2e-07 3e-0.842 0.750 0.763 0.897 0.810 0.873 0.950 0.901 0.672 0.767 0.710 0.779 0.886 0.820 HMAN 0.561 0.859 0.670 0.557 0.860 0.670 0.550 0.876 0.660 0.401 0.705 0.500 0.528 0.778 0.620 HGCN 0.720 0.857 0.760 0.766 0.897 0.810 0.892 0.961 0.910 0.670 0.770 0.710 0.763 0.863 00.982 0.928 0.908 0.987 0.939 0.958 0.997 0.975 0.808 0.933 0.853 0.881 0.970 0.914 ?stds 0.004 0.002 0.003 0.005 0.003 0.002 0.003 0.001 0.001 0.003 0.004 0.002 0.003 0.002 0.003 -value 2e-13 4e-17 2e-17 2e-11 7e-15 7e-18 6e-10 1e-15 6e-18 2e-16 4e-16 3e-18 2e-15 3e-16 4e-15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0.918 0.817 0.779 0.913 0.821 TransEdge 0.788 0.938 0.824 0.792 0.936 0.832 MRAEA 0.794 0.930 0.856 0.819 0.951 0.875 PSR(semi) 0.881 0.967 0.912 0.892 0.978 0.923 ?stds 0.002 0.001 0.002 0.003 0.001 0.002 -value 2e-16 1e-15 1e-14 4e-14 2e-14 5e-14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>@1 and time cost of PSR (semi) with different reviewing threshold on DWY DBP?WD .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>shows the results on DBP FR?EN and @1 of PSR with different pre-aligned ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Datasets DBP ZH DBP EN DBP JA DBP EN DBP FR DBP EN SRP EN SRP FR SRP EN SRP DE DWY DBP DWY YG DWY DBP DWY WD</figDesc><table><row><cell>| |</cell><cell>19,388</cell><cell cols="3">19,572 19,814 19,780</cell><cell>19,661</cell><cell cols="5">19,993 15,000 15,000 15,000 15,000</cell><cell>100,000</cell><cell>100,000</cell><cell>100,000</cell><cell>100,000</cell></row><row><cell>| |</cell><cell>2,830</cell><cell>2,317</cell><cell>2,043</cell><cell>2,096</cell><cell>1,379</cell><cell>2,209</cell><cell>221</cell><cell>177</cell><cell>222</cell><cell>120</cell><cell>302</cell><cell>31</cell><cell>330</cell><cell>220</cell></row><row><cell>| |</cell><cell>70,414</cell><cell cols="9">95,142 77,214 93,484 105,998 115,722 36,508 33,532 38,363 37,377</cell><cell>428,952</cell><cell>502,563</cell><cell>463,294</cell><cell>448,774</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the Datasets. | |, | | and | | are the size of entities, relations and triples respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results (Means ?stds ) on DBP15K and SRPRS. Besides the performances, we further conduct the onesample T-test between PSR and the best baselines. All the -value &lt; 0.01 indicates that PSR significantly outperforms all baselines.</figDesc><table /><note>DBP15K . (3) DWY100K [33]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on DWY100K 2 .</figDesc><table><row><cell>Method</cell><cell cols="6">DBP ZH?EN Hits@1 MRR Hits@1 MRR Hits@1 MRR DBP JA?EN DBP FR?EN</cell></row><row><cell>PSR(semi)</cell><cell>0.802</cell><cell>0.851</cell><cell>0.803</cell><cell>0.852</cell><cell>0.828</cell><cell>0.874</cell></row><row><cell>-RSE.</cell><cell>0.790</cell><cell>0.841</cell><cell>0.774</cell><cell>0.828</cell><cell>0.806</cell><cell>0.854</cell></row><row><cell>-RAA.</cell><cell>0.789</cell><cell>0.842</cell><cell>0.785</cell><cell>0.841</cell><cell>0.809</cell><cell>0.862</cell></row><row><cell>-MHR.</cell><cell>0.751</cell><cell>0.817</cell><cell>0.750</cell><cell>0.815</cell><cell>0.789</cell><cell>0.847</cell></row><row><cell>-RAGS.</cell><cell>0.789</cell><cell>0.843</cell><cell>0.788</cell><cell>0.844</cell><cell>0.816</cell><cell>0.868</cell></row><row><cell>-SNAL.</cell><cell>0.799</cell><cell>0.850</cell><cell>0.798</cell><cell>0.847</cell><cell>0.830</cell><cell>0.876</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">According to Zhao et al.<ref type="bibr" target="#b45">[45]</ref>, entity names between mono-lingual KGs for this dataset are almost identical and the edit distance algorithm could achieve the ground-truth performance. Therefore, the experimental results of literal methods are not listed.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, L?on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Signature Verification Using a Siamese Time Delay Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 6, [7th NIPS Conference</title>
		<editor>Jack D. Cowan, Gerald Tesauro, and Joshua Alspector</editor>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Channel Graph Neural Network for Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1140</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313705</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313705" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-13" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rytstxWAW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-lingual Entity Alignment with Incidental Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.eacl-main.53/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021</title>
		<editor>J?rg Tiedemann, and Reut Tsarfaty</editor>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021<address><addrLine>Online; Paola Merlo</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04-19" />
			<biblScope unit="page" from="645" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/209</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/209" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="1511" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<ptr target="https://arxiv.org/abs/2011.10566" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330925</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330925" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-04" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.07289" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning with Average Top-k Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Gang</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6653-learning-with-average-top-k-loss" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="497" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Graph Matching Consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyeJf1HKvS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Bernardo ?vila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>virtual, Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2020-12-06" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/guo19c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="2505" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.100</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2006.100" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Ulrike von Luxburg, Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00975" />
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<ptr target="http://arxiv.org/abs/1609.02907" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H196sainb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1274</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1274" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="2723" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Matching Networks for Learning the Similarity of Graph Structured Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/li19d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.324</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.324" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MRAEA: An Efficient and Robust Entity Alignment Approach for Cross-lingual Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371804</idno>
		<ptr target="https://doi.org/10.1145/3336191.3371804" />
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-03" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relational Reflection Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412001</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412001" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<editor>Mathieu d&apos;Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe Cudr?-Mauroux</editor>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19" />
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stepwise Reasoning for Multi-Relation Question Answering over Knowledge Graph with Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371812</idno>
		<ptr target="https://doi.org/10.1145/3336191.3371812" />
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-03" />
			<biblScope unit="page" from="474" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-4_38" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference, ESWC 2018</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298682" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge Association with Hyperbolic Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.460</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.460" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Trevor Cohn, Yulan He, and Yang Liu</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online; Bonnie Webber</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5704" to="5716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68288-4_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68288-4_37" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2017 -16th International Semantic Web Conference</title>
		<editor>Claudia d&apos;Amato, Miriam Fern?ndez, Valentina A. M. Tamma, Freddy L?cu?, Philippe Cudr?-Mauroux, Juan F. Sequeda, Christoph Lange, and Jeff Heflin</editor>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-10-21" />
			<biblScope unit="volume">10587</biblScope>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bootstrapping Entity Alignment with Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/611</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/611" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="4396" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">TransEdge: Translating Relation-contextualized Embeddings for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13579</idno>
		<ptr target="https://arxiv.org/abs/2004.13579" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Knowledge Graph Alignment Network with Gated Multi-Hop Neighborhood Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/5354" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313411</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313411" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1032</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/733</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/733" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="5278" to="5284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jointly Learning Entity and Relation Representations for Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1023</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="240" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1304</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1304" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3156" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aligning Cross-Lingual Entities with Multi-Aspect Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1451</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1451" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="4430" to="4440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Complex Factoid Question Answering with a Free-Text Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380197</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380197" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20" />
			<biblScope unit="page" from="1205" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An Experimental Study of State-of-the-Art Entity Alignment Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neighborhood-Aware Attentional Representation for Multilingual Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/269</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/269" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1943" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
