<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ignas Budvytis University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of 3D human body shape and pose estimation from an RGB image. This is often an ill-posed problem, since multiple plausible 3D bodies may match the visual evidence present in the input -particularly when the subject is occluded. Thus, it is desirable to estimate a distribution over 3D body shape and pose conditioned on the input image instead of a single 3D reconstruction. We train a deep neural network to estimate a hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices (i.e. body pose), which exploits the human body's kinematic tree structure, as well as a Gaussian distribution over SMPL body shape parameters. To further ensure that the predicted shape and pose distributions match the visual evidence in the input image, we implement a differentiable rejection sampler to impose a reprojection loss between ground-truth 2D joint coordinates and samples from the predicted distributions, projected onto the image plane. We show that our method is competitive with the state-of-the-art in terms of 3D shape and pose metrics on the SSP-3D and 3DPW datasets, while also yielding a structured probability distribution over 3D body shape and pose, with which we can meaningfully quantify prediction uncertainty and sample multiple plausible 3D reconstructions to explain a given input image. Code is available at https://github.com/akashsengupta1997/ HierarchicalProbabilistic3DHuman.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human body shape and pose estimation from an RGB image is a challenging computer vision problem, partly due to its under-constrained nature wherein multiple 3D human bodies may explain a given 2D image, especially when the subject is significantly occluded, as is common for in-the-wild images. Several recent works <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53]</ref> use deep neural networks to regress a single body shape and pose solution, which can result in impressive 3D body reconstructions given sufficient visual evidence in the input image. However, when visual evidence of the subject's shape and pose is obscured, e.g. due to occluding objects or self-occlusions, a single solution does not fully describe the space of plausible 3D reconstructions. In contrast, we aim to estimate a structured probability distribution over 3D body shape and pose, conditioned on the input image, thereby allowing us to sample any number of plausible 3D reconstructions and quantify prediction uncertainty over the 3D body surface, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We use the SMPL body model <ref type="bibr" target="#b32">[33]</ref> to represent human shape and pose. Identity-dependent body shape is parameterised by coefficients of a PCA basis -hence, a simple multivariate Gaussian distribution over the shape parameters is suitable. Body pose is parameterised by relative 3D joint rotations along the SMPL kinematic tree, which may be represented using rotation matrices. Regressing rotation matrices using neural networks is non-trivial, since they lie in SO(3), a non-linear 3D manifold with a different topology to R 3?3 or R 9 , the space in which unconstrained neural network outputs lie. However, one can define probability density functions over the Lie group SO <ref type="bibr" target="#b2">(3)</ref>, such as the matrix-Fisher distribution <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>, the parameter of which is an element of R 3?3 and may be easily regressed with a neural network <ref type="bibr" target="#b34">[35]</ref>. We propose a hierarchical probability distribution over relative 3D joint rotations along the SMPL kinematic tree, wherein the probability density func-tion of each joint's relative rotation matrix is a matrix-Fisher distribution conditioned on the parents of that joint in the kinematic tree. We train a deep neural network to predict the parameters of such a distribution over body pose, alongside a Gaussian distribution over SMPL shape.</p><p>Moreover, to ensure that 3D bodies sampled from the predicted distributions match the 2D input image, we implement a reprojection loss between predicted samples and ground-truth visible 2D joint annotations. To allow for the backpropagation of gradients through the sampling operation, we present a differentiable rejection sampler for matrix-Fisher distributions over relative 3D joint rotations.</p><p>Finally, a key obstacle for SMPL body shape regression from in-the-wild images is the lack of training datasets with accurate and diverse body shape labels <ref type="bibr" target="#b46">[47]</ref>. To overcome this, we follow <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref> and utilise synthetic data, randomly generated on-the-fly during training. Inspired by <ref type="bibr" target="#b6">[7]</ref>, we use convolutional edge filters to close the large synthetic-to-real gap and show that using edge-based inputs yields better performance than commonly-used silhouettebased inputs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b40">41]</ref>, due to improved robustness and capacity to retain visual shape information.</p><p>In summary, our main contributions are as follows:</p><p>? Given an input image, we predict a novel hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices, whose structure is explicitly informed by the SMPL kinematic tree, alongside a Gaussian distribution over SMPL shape parameters.</p><p>? We present a differentiable rejection sampler to sample any number of plausible 3D reconstructions and quantify prediction uncertainty over the body surface. This enables a reprojection loss between predicted samples and ground-truth coordinates of visible 2D joints, further ensuring that the predicted distributions are consistent with the input image.</p><p>? We use simple convolutional edge filters to improve the random synthetic training framework used by <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. Edge filtering is a computationally-cheap and robust method for closing the domain gap between synthetic RGB training data and real RGB test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews approaches to monocular 3D human body shape and pose estimation, as well as deep-learningbased methods for probabilistic rotation estimation. Monocular 3D shape and pose estimation methods can be classified as optimisation-based or learning-based. Optimisation-based approaches fit a parametric 3D body model <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18]</ref> to 2D observations, such as 2D keypoints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, silhouettes <ref type="bibr" target="#b28">[29]</ref> or body part segmentations <ref type="bibr" target="#b62">[63]</ref>, by optimising a suitable cost function. These methods do not require expensive 3D-labelled training data, but are sensitive to poor intialisations and noisy observations.</p><p>Learning-based approaches can be further split into model-free or model-based. Model-free methods use deep networks to directly output human body vertex meshes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b7">8]</ref>, voxel grids <ref type="bibr" target="#b55">[56]</ref> or implicit surfaces <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> from an input image. In contrast, model-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b60">61]</ref> regress 3D body model parameters <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1]</ref>, which give a lowdimensional representation of a 3D human body. To overcome the lack of in-the-wild 3D-labelled training data, several methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> use diverse 2D-labelled data as a source of weak supervision. <ref type="bibr" target="#b24">[25]</ref> extends this approach by incorporating optimisation into their model training loop, lifting 2D labels to self-improving 3D labels. These approaches often result in impressive 3D pose predictions, but struggle to accurately predict a diverse range of body shapes, since 2D keypoint supervision only provides a sparse shape signal. Shape prediction accuracy may be improved using synthetic training data <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref> consisting of synthetic input proxy representations (PRs) paired with ground-truth body shape and pose. PRs commonly consist of silhouettes and 2D joint heatmaps <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref>, necessitating accurate silhouette segmentations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref> at test-time, which is not guaranteed for challenging in-thewild inputs. Other methods <ref type="bibr" target="#b55">[56]</ref> pre-train on synthetic RGB inputs <ref type="bibr" target="#b56">[57]</ref> and then fine-tune on the scarce and limitedshape-diversity real 3D training data available <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b57">58]</ref>, to avoid over-fitting to artefacts in low-fidelity synthetic data. In contrast, we utilise edge-based PRs, hence dropping the reliance on accurate segmentation networks without requiring fine-tuning on real data or high-fidelity synthetic data.</p><p>3D human shape and pose distribution estimation. Early optimisation-based 3D pose estimators <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> specified a cost function corresponding to the posterior probability of 3D pose given 2D observations and analysed its multi-modal structure due to ill-posedness. Strategies to sample multiple 3D poses with high posterior probability included cost-covariance-scaled <ref type="bibr" target="#b49">[50]</ref> and inversekinematics-based <ref type="bibr" target="#b51">[52]</ref> global search and local refinement, as well as cost-function-modifying MCMC <ref type="bibr" target="#b50">[51]</ref>. Recently, several learning-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b36">37]</ref> predict multi-modal distributions over 3D joint locations conditioned on 2D inputs, using Bayesian mixture of experts <ref type="bibr" target="#b48">[49]</ref>, mixture density networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37]</ref> or normalising flows <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b43">44]</ref>. Our method extends beyond 3D joints and predicts distributions over human pose and shape. This has been addressed by Biggs et al. <ref type="bibr" target="#b2">[3]</ref>, who predict a categorical distribution over a set of SMPL <ref type="bibr" target="#b32">[33]</ref> parameter hypotheses. Sengupta et al. <ref type="bibr" target="#b47">[48]</ref> estimate an independent Gaussian distribution over both SMPL shape and joint rotation vectors. In contrast, we note that 3D rotations lie in SO(3), motivating our hierarchical matrix-Fisher distribution. <ref type="figure">Figure 2</ref>. Network architecture of our hierarchical SMPL <ref type="bibr" target="#b32">[33]</ref> shape and pose distribution predictor. The input image is converted into an edge-and-joint-heatmap proxy representation, which is passed through the prediction network to produce distributions over shape parameters and relative 3D joint rotation matrices. Rejection sampling is used to sample 3D reconstructions from the predicted distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation distribution estimation via deep learning.</head><p>Prokudin et al. <ref type="bibr" target="#b41">[42]</ref> use biternion networks to predict a mixture-of-von-Mises distribution over object pose angle. Gilitschenski et al. <ref type="bibr" target="#b12">[13]</ref> use a Bingham distribution over unit quaternions to represent orientation uncertainty. However, these works have to enforce constraints on the parameters of their predicted distributions (e.g. positive semi-definiteness). To overcome this, Mohlin et al. <ref type="bibr" target="#b34">[35]</ref> train a deep network to regress a matrix-Fisher distribution <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref> over 3D rotation matrices. We adapt this approach to define our hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section provides an overview of SMPL <ref type="bibr" target="#b32">[33]</ref> and the matrix-Fisher distribution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>, presents our structured, hierarchical pose and shape distribution estimation architecture and discusses the loss functions used to train it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SMPL model</head><p>SMPL <ref type="bibr" target="#b32">[33]</ref> is a parametric 3D human body model. Identity-dependent body shape is represented by shape parameters ? ? R 10 , which are coefficients of a PCA body shape basis. Body pose is defined by the relative 3D rotations of the bones formed by the 23 body (i.e. non-root) joints in the SMPL kinematic tree. The rotations may be represented using rotation matrices</p><formula xml:id="formula_0">{R i } 23 i=1 , where R i ? SO(3)</formula><p>. We parameterise the global rotation (i.e. rotation of the root joint) in axis-angle form by ? ? R 3 . A differentiable function S({R i } 23 i=1 , ?, ?) maps the input pose and shape parameters to an output vertex mesh V ? R 6890?3 . 3D joint locations, for L joints of interest, are obtained as J 3D = J V where J ? R L?6890 is a linear vertex-to-joint regression matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matrix-Fisher distribution over SO(3)</head><p>The 3D special orthogonal group may be defined as</p><formula xml:id="formula_1">SO(3) = {R ? R 3?3 |R T R = I, det(R) = 1}.</formula><p>The matrix-Fisher distribution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref> defines a probability density function over SO(3), given by</p><formula xml:id="formula_2">p(R|F ) = 1 c(F ) exp(tr(F T R)) = M(R; F ) (1)</formula><p>where F ? R 3?3 is the matrix parameter of the distribution, c(F ) is the normalising constant and R ? SO(3). We present some key properties of the matrix-Fisher distribution below, but refer the reader to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> for further details, visualisations and a method for approximating the intractable normalising constant and its gradient w.r.t. F . The properties of M(R; F ) can be described in terms of the singular value decomposition (SVD) of F , denoted by</p><formula xml:id="formula_3">F = U ? S ? V ?T , with S ? = diag(s ? 1 , s ? 2 , s ? 3 )</formula><p>. U ? and V ? are orthonormal matrices, but they may have a determinant of -1 and thus are not necessarily elements of SO(3). There-fore, a proper SVD <ref type="bibr" target="#b29">[30]</ref> </p><formula xml:id="formula_4">F = U SV T is used, where U = U ? diag(1, 1, det(U ? )) V = V ? diag(1, 1, det(V ? )) S = diag(s 1 , s 2 , s 3 ) = diag(s ? 1 , s ? 2 , det(U ? V ? )s ? 3 )<label>(2)</label></formula><p>which ensures that U , V ? SO(3). Then, the mode of the distribution is given by <ref type="bibr" target="#b29">[30]</ref> R mode = arg max</p><formula xml:id="formula_5">R?SO(3) p(R|F ) = U V T .<label>(3)</label></formula><p>The columns of U define the distribution's principal axes of rotation (analogous to the principal axes of a multivariate Gaussian distribution), while the proper singular values in S give the concentration of the distribution for rotations about the principal axes <ref type="bibr" target="#b29">[30]</ref>. Specifically, the concentration along rotations of R mode about the i-th principal axis (i-th column of U ) is given by</p><formula xml:id="formula_6">s j + s k for (i, j, k) ? {(1, 2, 3), (2, 3, 1), (3, 1, 2)}.</formula><p>The concentration of the distribution may be different about each principal axis, allowing for axis-dependent rotation uncertainty modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proxy representation computation</head><p>Given an input RGB image I, we first compute a proxy representation X (see <ref type="figure">Figure 2</ref>), consisting of an edgeimage concatenated with joint heatmaps. Comparisons with silhouette-and RGB-based representations are given in Section 5.1. Edge-images are obtained with Canny edge detection <ref type="bibr" target="#b5">[6]</ref>. 2D joint heatmaps are computed using HRNet-W48 <ref type="bibr" target="#b53">[54]</ref>, and joint predictions with low confidence scores (&lt; 0.6) are thresholded out. The edge-image and joint heatmaps are stacked along the channel dimension to produce X ? R H?W ?(L+1) . Proxy representations <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b40">41]</ref> are used to close the domain gap between synthetic training images and real test-time RGB images, since synthetic proxy representations are more similar to their real counterparts than synthetic RGB images are to real RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Body shape and pose distribution prediction</head><p>Our goal is to predict a probability distribution over relative 3D joint rotations {R i } 23 i=1 and SMPL shape parameters ? conditioned upon a given input proxy representation X. We also predict deterministic estimates of the global body rotation ? and weak-perspective camera parameters c = [s, t x , t y ], representing scale and xy translation.</p><p>Since ? represents the linear coefficients of a PCA shape-space, a Gaussian distribution with a diagonal covariance matrix is suitable <ref type="bibr" target="#b47">[48]</ref>,</p><formula xml:id="formula_7">p(?|X) = N (?; ? ? (X), diag(? 2 ? (X))<label>(4)</label></formula><p>where the mean ? ? and variances ? 2 ? are functions of X. The matrix-Fisher distribution (Equation 1) may be naively used to define a distribution over 3D joint rotations</p><formula xml:id="formula_8">p(R i |X) = M(R i ; F i (X))<label>(5)</label></formula><p>for i ? {1, 2, ..., 23}. Here, each joint is modelled independently of all the other joints. Thus, the matrix parameter of the i-th joint, F i , is a function of the input X only.</p><p>To predict the parameters of this naive, independent distribution over 3D joint rotations, in addition to the shape distribution parameters, global body rotation and weak-perspective camera, we learn a function f indep mapping the input X to the set of desired outputs Y =</p><formula xml:id="formula_9">{{F i } 23 i=1 , ? ? , ? 2 ? , ?, c}, where f indep is represented by a deep neural network with weights W indep .</formula><p>However, the independent matrix-Fisher distribution in Equation 5 does not model SMPL 3D joint rotations faithfully, since the rotation of each part/bone is defined relative to its parent joint in the SMPL kinematic tree. Hence, a distribution over the i-th rotation matrix R i conditioned on the input X should be informed by the distributions over all its parent joints P (i), as well as the global body rotation ?, to enable the distribution to match the 2D visual pose evidence present in X. Furthermore, 3D joints in the SMPL rest-pose skeleton are dependent upon the shape parameters ?, while the mapping from 3D to the 2D image plane is given by the camera model. Hence, a distribution over R i given X should also consider the predicted shape mean ? ? and variance ? 2 ? , as well as the predicted camera c. This is similar to the rationale behind the deterministic iterative/hierarchical predictors in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref>, except we model these relationships in a probabilistic sense, by defining</p><formula xml:id="formula_10">p R i |X, {F j } j?P (i) , ?, ? ? , ? 2 ? , c = M(R i ; F i ) F i = f i X, {(U j , S j , R modej )} j?P (i) , ?, ? ? , ? 2 ? , c<label>(6)</label></formula><p>for i ? {1, 2, ..., 23}. Now, the matrix parameter of the i-th joint is a function of all its parent distributions, represented by the principal axes U j , singular values S j and modes R modej = U j V T j for j ? P (i), as well as the shape distribution {? ? , ? 2 ? }, global rotation ?, camera parameters c and the input X. Note that the parent distributions are themselves functions of their respective parent joints, while ?, ? ? , ? 2 ? and c are all functions of X. To predict the parameters of the hierarchical matrix-Fisher distribution in Equation 6, we propose a hierarchical neural network architecture f hier , with weights W hier <ref type="figure">(Figure 2</ref>). When considered as a black-box, f hier yields the same set of outputs Y as f indep . However, f hier utilises the iterative hierarchical architecture presented in <ref type="figure">Figure 2</ref>, which amounts to multiple streams of fully-connected layers, each following one "limb" of the kinematic tree. In contrast, f indep predicts pose similarly to shape, camera and global rotation parameters, using a single stream of fullyconnected layers. We compare the naive independent formulation with the hierarchical formulation in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss functions</head><p>Distribution prediction networks are trained with a syn-</p><formula xml:id="formula_11">thetic dataset {X n , ({R n i } 23 i=1 , ? n , ? n )} N n=1 (Section 4)</formula><p>. Negative log-likelihood (NLL) loss on distribution parameters. The NLL corresponding to the Gaussian body shape distribution (Equation 4) is given by:</p><formula xml:id="formula_12">L ?-NLL = ? N n=1 log N ? n ; ? ? (X n ), diag(? 2 ? (X n )) .<label>(7)</label></formula><p>The NLL corresponding to the matrix-Fisher distribution over relative 3D joint rotations is defined as <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_13">L R-NLL = ? N n=1 log M(R n i ; F n i ) = N n=1 log c(F n i ) ? tr(F nT i R n i )<label>(8)</label></formula><p>for i ? {1, 2, ..., 23}, where F n i may be obtained via the independent or hierarchical matrix-Fisher models presented above. Intuitively, the trace term pushes the predicted distribution mode R n modei (Equation 3) towards the target R n i , while the log normalising constant acts as a regulariser, preventing the singular values of F n i from getting too large <ref type="bibr" target="#b34">[35]</ref>. All predicted distribution parameters are dependent on the model weights, W indep or W hier , which are learnt in a maximum likelihood framework aiming to minimise the joint shape and pose NLL:</p><formula xml:id="formula_14">L NLL = L ?-NLL + L R-NLL .</formula><p>Loss on global body rotation. We predict deterministic estimates of the global body rotation vectors? n , which are supervised using ground-truth global rotations ? n , with loss</p><formula xml:id="formula_15">L global = N n=1 ?R(? n ) ? R(? n )? 2 F . R(?) ? SO(3)</formula><p>is the rotation matrix corresponding to ?. 2D joints loss on samples. Applying L NLL alone results in overly uncertain predicted 3D shape and pose distributions (see Section 5.1). To ensure that the predicted distributions match the visual evidence in the input X n , we impose a reprojection loss between ground-truth 2D joint coordinates (in the image plane) and predicted 2D joint samples, which are obtained by differentiably sampling 3D bodies from the predicted distributions and projecting to 2D using the predicted camera c n = [s n , t n x , t n y ]. Ground-truth 2D joints J n 2D are computed from {{R n i } 23 i=1 , ? n , ? n } during synthetic training data generation (see <ref type="bibr">Section 4)</ref>.</p><p>We adapt the rejection sampler presented in <ref type="bibr" target="#b19">[20]</ref> to sample from a matrix-Fisher distribution M(R; F ), modifying it to allow for backpropagation of gradients through the proposal sampling step (lines 5-7 in Algorithm 1). We refer the reader to <ref type="bibr" target="#b19">[20]</ref> for further details about the rejection sampler. In short, to simulate a matrix-Fisher distribution with parameter F = U SV T , we sample unit quaternions from a Bingham distribution <ref type="bibr" target="#b33">[34]</ref> over the unit 3sphere S 3 , with Bingham parameter A computed from S,</p><formula xml:id="formula_16">Algorithm 1: Differentiable Rejection Sampler Input: U , S = diag(s 1 , s 2 , s 3 ), V , b Output:R ? SO(3) s.t.R ? M(R; U SV T ) 1 A = diag(0, 2(s 2 + s 3 ), 2(s 1 + s 3 ), 2(s 1 + s 2 )) 2 ? = I 4 + 2 b A 3 M = exp b?4 2 4 b 2 4 repeat 5 Sample ? ? N (0 4 , I 4 ) 6 y = (? ?1 ) 1 2 ? 7 Propose x = y ?y? s.t. x ? S 3 8 Sample w ? Unif[0, 1] 9 until w &lt; exp(?x T Ax) M (x T ?x) ?2 ; 10Q = quaternion to matrix(x) s.t.Q ? SO(3) 11 returnR = UQV T</formula><p>and then convert the sampled quaternions into rotation matrices <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> with the desired matrix-Fisher distribution. Rejection sampling is used to sample from the Bingham distribution, which has pdf p Bing (x) ? exp(?x T Ax) for x ? S 3 . The proposal distribution for the rejection sampler is an angular central Gaussian (ACG) distribution, with pdf p ACG (x) ? (x T ?x) ?2 . The ACG distribution is easily simulated <ref type="bibr" target="#b19">[20]</ref> by sampling from a zero-mean Gaussian distribution with covariance matrix ? ?1 and normalising to unit-length (lines 5-7 in Algorithm 1). The reparameterisation trick <ref type="bibr" target="#b22">[23]</ref> is used to differentiably sample from this zero-mean Gaussian, thus allowing for backpropagation of gradients through the rejection sampler.</p><p>Algorithm 1 samples K sets of relative 3D joint rotation</p><formula xml:id="formula_17">matrices {{R n i,k } 23 i=1 } K k=1 from the corresponding distribu- tions {M(R n i ; F n i )} 23 i=1</formula><p>. Furthermore, we differentiably sample K SMPL shape vectors from the predicted Gaussian distribution {? n k ? N (?; ? ? (X n ), diag(? 2 ? (X n )))} K k=1 , again using the re-parameterisation trick <ref type="bibr" target="#b22">[23]</ref>.</p><p>The body shape and 3D joint rotation samples are converted into 2D joint samples using the SMPL model and weak-perspective camera parameter?</p><formula xml:id="formula_18">J n 2D k = s n ?(J S({R n i,k } 23 i=1 ,? n k ,? n )) + [t n x , t n y ]<label>(9)</label></formula><p>where ?() is an orthographic projection. The reprojection loss applied between the predicted 2D joint samples and the visible target 2D joint coordinates is given by </p><formula xml:id="formula_19">L 2D Samples = N n=1 K k=1 ?? n (J n 2D ?? n 2D k )? 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>Synthetic training data. To train our 3D body shape and pose distribution prediction networks, we require a training dataset {X n , ({R n i } 23 i=1 , ? n , ? n )} N n=1 . We extend the synthetic training frameworks presented in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, which involve generating inputs and corresponding SMPL body shape and pose (i.e. 3D joint rotation) labels randomly and on-the-fly during training. In brief, for every training iteration, SMPL shapes ? n are randomly sampled from a prior Gaussian distribution while relative 3D joint rotations {R n i } 23 i=1 and global rotation ? n are chosen from the training sets of UP-3D <ref type="bibr" target="#b28">[29]</ref>, 3DPW <ref type="bibr" target="#b57">[58]</ref> or Human3.6M <ref type="bibr" target="#b15">[16]</ref>. These are converted into training inputs X n and groundtruth 2D joint coordinates J n using the SMPL model and a light-weight renderer <ref type="bibr" target="#b42">[43]</ref>. Cropping, occlusion and noise augmentations are then applied to the synthetic inputs.</p><p>Previous synthetic training frameworks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53]</ref> often use silhouette-based training inputs. This necessitates accurate human silhouette segmentation at test-time, which may be challenging to do robustly. In contrast, our input representations consist of edge-images concatenated with 2D joint heatmaps. To generate edge-images, we first create synthetic RGB images by rendering textured SMPL meshes. For each training mesh, clothing textures are randomly chosen from <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b1">2]</ref>. The textured SMPL mesh is rendered onto a background image (randomly chosen from LSUN <ref type="bibr" target="#b61">[62]</ref>), using randomly-sampled lighting and camera parameters. Canny edge detection <ref type="bibr" target="#b5">[6]</ref> is used to compute edge-images from the synthetic RGB images. We show in Section 5.1 that, despite the lack of photorealism in the synthetic RGB images, edge-filtering bridges the syntheticto-real domain gap at test-time -and performs better than either silhouette-based or synthetic-RGB-based training inputs in our experiments. Examples of synthetic training samples are given in the supplementary material.</p><p>Training details. We use Adam <ref type="bibr" target="#b21">[22]</ref> with a learning rate of 0.0001, batch size of 80 and train for 150 epochs. For stability, the 2D joints reprojection loss is only applied on the mode pose and shape (projected to 2D) in the first 50 epochs and not on the samples, which are supervised in the next 100 epochs. To boost 3D pose metrics, an MSE loss on  <ref type="table">Table 1</ref>. Experiments investigating different input representations, hierarchical versus independent distribution prediction networks and the 2D samples reprojection loss, evaluated in terms of shape and pose prediction metrics on synthetic data, SSP-3D <ref type="bibr" target="#b46">[47]</ref> and 3DPW <ref type="bibr" target="#b57">[58]</ref>.</p><p>the mode 3D joint locations is applied in the final 50 epochs. Evaluation datasets. 3DPW <ref type="bibr" target="#b57">[58]</ref> is used to evaluate 3D pose prediction accuracy. We report mean-per-jointposition-error after scale correction (MPJPE-SC) <ref type="bibr" target="#b46">[47]</ref> and after Procrustes analysis (MPJPE-PA), both in mm. Both metrics are computed using the mode 3D joint coordinates of the predicted shape and pose distributions. SSP-3D is primarily used to evaluate 3D body shape prediction accuracy, using per-vertex Euclidean error in a Tpose after scale-correction (PVE-T-SC) <ref type="bibr" target="#b46">[47]</ref> in mm, computed with the mode 3D body shape from the predicted shape distribution. We also evaluate 2D joint prediction error (2D Joint Err. Mode/Samples) in pixels, computed using both the mode 3D body and 10 3D bodies randomly sampled from the predicted shape and pose distributions, projected onto the image plane using the camera prediction. 2D joint error is evaluated on visible target 2D joints only.</p><p>Finally, we use a synthetic test dataset for our ablation studies investigating different input representations. It consists of 1000 synthetic input-label pairs, generated in the same way as the synthetic training data, with poses sampled from the test set of Human3.6M. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>This section investigates different input representations and the benefits of the 2D joints samples loss, compares independent and hierarchical distribution predictors and benchmarks our method against the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation studies</head><p>Input proxy representation. Rows 1-3 in <ref type="table">Table 1</ref> compare different choices of input proxy representation: binary silhouettes, RGB images and edge-filtered images (each additionally concatenated with 2D joint heatmaps). The independent network architecture is used for all three input types. To investigate the synthetic-to-real domain gap, metrics are presented for synthetic test data, as well as real test images from SSP-3D and 3DPW. For the latter, silhouette segmentation is carried out with DensePose <ref type="bibr" target="#b14">[15]</ref>. Using RGB-based input representations (row 2) results in the best 3D shape and pose metrics on synthetic data, which is reasonable since RGB contains more information than both silhouettes and edge-filtered images. However, metrics are significantly worse on real datasets, suggesting that the network has over-fitted to unrealistic artefacts present in lowfidelity (i.e. computationally cheap) synthetic RGB images. Silhouette-based input representations (row 1) also demonstrate a deterioration of 3D metrics on real test data compared to synthetic data, since they are heavily reliant upon accurate silhouettes, which are difficult to robustly segment in test images containing challenging poses or severe occlusions. Inaccurate silhouette segmentations critically impair the network's ability to predict 3D body pose and shape. In contrast, edge-filtering is a simpler and more robust operation than segmentation, but is still able to retain important shape information from the RGB image. Thus, edge-images (concatenated with 2D joint heatmaps) can better bridge the synthetic-to-real domain gap, resulting in improved metrics on real test inputs (row 3). Hierarchical architecture and reprojection loss on 2D joints samples. <ref type="figure" target="#fig_1">Figure 3</ref> and rows 3-6 in <ref type="table">Table 1</ref> compare the independent and hierarchical distribution prediction architectures (f indep and f hier ) presented in Section 3.4, both with and without the reprojection loss on sampled 2D joints (L 2D Samples ) from Section 3.5. When L 2D Samples is not applied, the shape and pose distributions predicted by both the independent and hierarchical network architectures do not consistently match the the input image, as evidenced by the significant gap between the visible 2D joint error computed using the distributions' modes versus samples drawn from the distributions (in rows 3 and 5 of <ref type="table">Table 1</ref>) on both synthetic test data and SSP-3D <ref type="bibr" target="#b46">[47]</ref>. This implies that the predicted distributions are overly uncertain about parts of the subject's body that are visible and unambiguous in the input image. The visualisations corresponding to the hierarchical architecture trained without L 2D Samples in <ref type="figure" target="#fig_1">Figure 3</ref> (centre) further demonstrate that the predicted samples often do not match the input image, particularly at the extreme ends of the body. This results in significant undesirable per-vertex uncertainty over unambiguous body parts.</p><p>Applying L 2D Samples to the independent network f indep partially alleviates the mismatch between inputs and predicted samples, as shown by <ref type="figure" target="#fig_1">Figure 3 (right)</ref>   <ref type="bibr" target="#b59">[60]</ref> and HRNet <ref type="bibr" target="#b53">[54]</ref> as 2D joint detectors for proxy representation computation, to enable a fair comparison with past methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> that used Detectron2. <ref type="table">Table 1</ref>, where the mode versus sample 2D joint error gap has reduced. However, training with L 2D Samples deteriorates the independent architecture's mode pose prediction metrics (MPJPE-SC and 2D Joint Err. Mode in row 3 vs 4 of <ref type="table">Table 1</ref>) on both synthetic and real test data. This is because f indep naively models each joint's relative rotation independently of its parents' rotations (Equation 5); however, to predict realistic human pose samples that match the visible input, each joint's rotation distribution must be informed by its parents. L 2D Samples attempts to force predicted samples to match the input despite this logical inconsistency, which causes a trade-off between mode and sample pose prediction metrics, particularly worsening MPJPE-SC.</p><p>In contrast, applying L 2D Samples to the hierarchical network f hier improves metrics corresponding to both mode and sample predictions, as shown by row 6 in <ref type="table">Table 1</ref>. Now, each SMPL joint's relative rotation distribution is conditioned on all its parents' distributions (Equation 6). Thus, L 2D Samples and L NLL work in conjunction in enabling predicted hierarchical distributions (and samples) to match the visible input, while yielding improved 3D metrics. <ref type="figure" target="#fig_1">Figure 3</ref> (left) exhibits such visually-consistent samples and demonstrates greater prediction uncertainty for ambiguous parts. Note that uncertainty can arise even without occlusion in a monocular setting, e.g. due to depth ambiguities <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref> as shown by the left arm samples in the last row of <ref type="figure" target="#fig_1">Figure 3</ref>. Further visual results are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max. input set size</head><p>Method SSP-3D PVE-T-SC HMR <ref type="bibr" target="#b18">[19]</ref> 22.9 GraphCMR <ref type="bibr" target="#b25">[26]</ref> 19.5  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the state-of-the-art</head><p>Shape prediction. <ref type="table">Table 3</ref> evaluates 3D body shape metrics on SSP-3D <ref type="bibr" target="#b46">[47]</ref> for single image inputs and multi-image input sets, which we evaluate using both mean and probabilistic combination methods from <ref type="bibr" target="#b47">[48]</ref>. Our network surpasses the state-of-the-art <ref type="bibr" target="#b47">[48]</ref>, mainly due to our use of an edge-based proxy representation, instead of the silhouettebased representations used in <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b47">[48]</ref>. These methods rely on accurate human silhouettes, which may be difficult to compute at test-time, as discussed in Section 5.1, while our method does not have such dependencies. However, our method may result in erroneous shape predictions when the subject is wearing loose clothing which obscures body shape, in which case the shape prediction over-estimates the subject's true proportions (see rows 1-2 in <ref type="figure" target="#fig_1">Figure 3</ref>). Pose prediction. <ref type="table">Table 2</ref> evaluates 3D pose metrics on 3DPW <ref type="bibr" target="#b57">[58]</ref>. Our method is competitive with the state-ofthe-art and surpasses other methods that do not require 3Dlabelled training images <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="figure" target="#fig_2">Figure 4(a)</ref> shows that our method performs well for most test examples in 3DPW, even matching pose-focused approaches that do not attempt to accurately predict diverse body shapes <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25]</ref>. However, some images in 3DPW contain significant occlusion, which can lead to noisy 2D joint heatmaps in the proxy representations, resulting in poor 3D pose metrics as shown by the right end of the curve in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. Further quantitative comparison with other shape and pose distribution/multi-hypothesis prediction approaches is given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a probabilistic approach to the ill-posed problem of monocular 3D human shape and pose estimation, motivated by the fact that multiple 3D bodies may explain a given 2D image. Our method predicts a novel hierarchical matrix-Fisher distribution over relative 3D joint rotations and a Gaussian distribution over SMPL <ref type="bibr" target="#b32">[33]</ref> shape parameters, from which we can sample any number of plausible 3D reconstructions. To ensure that the predicted distributions match the input image, we have implemented a differentiable rejection sampler to impose a loss between predicted 2D joint samples and ground-truth 2D joint coordinates. Our method is competitive with the stateof-the-art in terms of pose metrics on 3DPW, while surpassing the state-of-the-art for shape accuracy on SSP-3D. <ref type="figure">Figure 5</ref>. Examples of synthetic training and validation data rendered on-the-fly during model training. Synthetic RGB images are converted into edge-filtered images and 2D joint heatmaps, which act as the input to the distribution prediction network presented in the main manuscript. The synthetic RGB images are computationally-cheap and far from photorealistic. However, edge detection <ref type="bibr" target="#b5">[6]</ref> is able to significantly bridge the synthetic-to-real domain gap, as can be seen by comparing the synthetic edge-images with real edge-images in <ref type="figure" target="#fig_3">Figures 6 and 7</ref> of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild</head><p>Section 7 in this supplementary material contains implementation details, particularly regarding synthetic training data generation and per-vertex uncertainty visualisation. Section 8 discusses qualitative results on the SSP-3D <ref type="bibr" target="#b46">[47]</ref> and 3DPW <ref type="bibr" target="#b57">[58]</ref> datasets, and compares distribution predictions on images with versus without artificial occlusions. <ref type="table">Table 5</ref> compares several recent multi-hypothesis 3D human shape and pose estimation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Synthetic Training Data</head><p>Our shape and pose distribution prediction neural networks are trained using synthetic training data, consisting of edge-and-joint-heatmap inputs paired with ground truth SMPL <ref type="bibr" target="#b32">[33]</ref> shape and pose parameters. Inputs are rendered on-the-fly during model training using randomly sampled camera extrinsics, lighting, backgrounds and clothing textures. Examples of synthetic training and validation data are given in <ref type="figure">Figure 5</ref>. Note how each body pose may be paired with a different body shape, clothing, camera and background, as well as occlusion and noise augmentations. Thus, we are able to render highly diverse training data on-the-fly during training, enabling the network to see a new pose/shape/clothing/camera/background combination in each training iteration.</p><p>Our synthetic RGB images ( <ref type="figure">Figure 5</ref>) are computationally cheap but clearly far from photorealistic, resulting in a large synthetic-to-real domain gap. However, simple edge detection <ref type="bibr" target="#b5">[6]</ref> is able to significantly reduce this gap <ref type="bibr" target="#b6">[7]</ref>, motivating the use of edge-filtered images as part of our input proxy representation. We found that noisy edge detections (as seen in <ref type="figure">Figure 5</ref>) retained sufficient visual shape and pose information, and efforts to produce clean edge-images (e.g. hysteresis-based edge tracking or further hyperparameter tuning) did not improve performance.</p><p>The required body shape, pose, clothing and backgrounds are obtained as follows. For training, groundtruth SMPL 3D joint rotation matrices are sampled from the training splits of 3DPW <ref type="bibr" target="#b57">[58]</ref> and UP-3D <ref type="bibr" target="#b28">[29]</ref>, as well as Human3.6M <ref type="bibr" target="#b15">[16]</ref> subjects 1, 5, 6, 7 and 8, giving a total of 91106 training poses. Validation poses are sampled from the 3DPW/UP-3D validation splits and Human3.6M subjects 9 and 11, resulting in 33347 validation poses. SMPL body shape parameters are randomly sampled from N (? i ; 0, 1.25 2 ) for i = 1, ..., <ref type="bibr">10 [47]</ref>. RGB clothing textures for the SMPL body mesh are selected from SURREAL <ref type="bibr" target="#b56">[57]</ref> and MultiGarmentNet <ref type="bibr" target="#b1">[2]</ref>, resulting in 917 training textures and 108 validation textures. Backgrounds are obtained from LSUN <ref type="bibr" target="#b61">[62]</ref>, which contains a collection of diverse indoor and outdoor scenes. We sample from 397582 different training backgrounds and 3000 different validation backgrounds. Note that background training images may contain other humans, which is intentional and essential for robustness against test images with multiple people. The network learns to focus on the person corresponding to the input joint heatmaps and ignore persons in the background. Textured SMPL meshes are rendered with Pytorch3D <ref type="bibr" target="#b42">[43]</ref>, using a perspective camera model and Phong shading. Camera and lighting parameters are randomly sampled, with sampling hyperparameters given in <ref type="table">Table 4</ref>. Generated images are cropped around the rendered body using a square bounding box, where the bounding box size is randomly scaled by a factor in range (0.8, 1.2).</p><p>To further bridge the gap synthetic-to-real gap, we implement random occlusion, body part removal, 2D joint removal and 2D joint noise augmentations during training. Hyperparameters associated with data augmentations are given in <ref type="table" target="#tab_5">Table 6</ref>. to the predicted shape and 3D joint rotation distributions. These are computed by i) sampling 100 shape parameter vectors and relative 3D joint rotations (for the entire kinematic tree) from the predicted distributions, ii) passing each of these samples through the SMPL function <ref type="bibr" target="#b32">[33]</ref> to get the corresponding vertex meshes, iii) computing the mean location of each vertex over all the samples and iv) determining the average Euclidean distance from the sample mean for each vertex over all the samples, which is ultimately visualised in the vertex scatter plots as a measure of per-vertex 3D location uncertainty.  <ref type="table">Table 5</ref>. Comparison with other 3D human shape and pose distribution/multi-hypothesis estimation methods. Following Biggs et al. <ref type="bibr" target="#b2">[3]</ref>, we report body shape (PVE-T-SC) and pose (MPJPE and MPJPE-PA) metrics computed using the minimum error sample out of a set of n predicted samples for each test image, for n ? {1, 5, 10, 25}. This is motivated by the fact that the single ground-truth 3D body only represents one plausible 3D solution out of many (for ambiguous images), which may not be the same as the mode of our predicted shape and pose distribution. The improvement in 3D shape and pose metrics with increasing number of samples shows that our predicted distribution is able to model the 3D ground-truth as a possible sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Visualisation of Per-Vertex Uncertainty</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation</head><p>Hyperparameter Value  <ref type="figure">Figure 7</ref> presents results on artificially occluded images from SSP-3D <ref type="bibr" target="#b46">[47]</ref>. In particular, note that i) occluded/invisible body parts result in increased 3D location uncertainty for corresponding vertices and ii) 3D body samples from the predicted distributions match the visible body parts in the 2D image, while invisible body part samples are more diverse. However, occluded sample diversity is still somewhat limited and samples tend to be clustered around the mode predictions, which is a weakness of our method. This may be alleviated by predicting multi-modal distributions over 3D shape and pose in future work. <ref type="figure">Figure 7</ref> also illustrates our method's ability to predict a range of body shapes, owing to the synthetic training framework used. <ref type="figure" target="#fig_3">Figure 6</ref> presents results on the test split of 3DPW <ref type="bibr" target="#b57">[58]</ref>. Again, note the increased uncertainty and sample diversity for occluded and out-of-frame body parts, and the reprojection consistency between predicted samples and the visible bodies in the images. Results on 3DPW highlight another key challenge for future work: when faced with baggy/loose clothing, our method tends to over-estimate the subject's body proportions. This is because our synthetic training data does not model the shape of clothing on the human body surface, but only its texture. Future work could focus on using synthetic clothed humans for training. <ref type="figure">Figure 8</ref> compares shape and pose distribution predictions on images from SSP-3D with versus without artificial occlusions, further corroborating that ambiguous parts result in greater uncertainty and more diverse 3D samples. However, it is again apparent that sample diversity for highly ambiguous parts is more limited than expected, as samples tend to be closely clustered around the mode prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Qualitative Results</head><p>Note that uncertainty does not only arise from occlusion -depth ambiguities are prevalent when estimating 3D pose from a monocular 2D image <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref>. This is demonstrated in the non-occluded images in <ref type="figure">Figure 8</ref> (left), by the left arm samples in rows 1 and 5 and the right arm in row 4. <ref type="figure" target="#fig_3">Figure 6</ref>. 3D reconstruction samples and per-vertex uncertainties corresponding to shape and relative 3D joint rotation distributions predicted from 3DPW images <ref type="bibr" target="#b57">[58]</ref>. The selected images exhibit self-occlusion and out-of-frame body parts, which result in greater 3D location uncertainty for vertices belonging to ambiguous parts. <ref type="figure">Figure 7</ref>. 3D reconstruction samples and per-vertex uncertainties corresponding to shape and relative 3D joint rotation distributions predicted from SSP-3D images <ref type="bibr" target="#b46">[47]</ref>. The images are artificially occluded, resulting in greater 3D location uncertainty for vertices belonging to ambiguous parts. <ref type="figure">Figure 8</ref>. Comparison between 3D samples and per-vertex uncertainties obtained using artificially occluded versus non-occluded input images from SSP-3D <ref type="bibr" target="#b46">[47]</ref>. Ambiguous parts have greater prediction uncertainty.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>3D reconstruction samples and per-vertex uncertainty corresponding to the predicted hierarchical shape and pose distributions computed from the given input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>3D reconstruction samples and per-vertex uncertainty corresponding to shape and pose distributions predicted using the hierarchical architecture with 2D samples loss (left), hierarchical architecture without 2D samples loss (centre) and independent architecture with 2D samples loss (right). Per-vertex uncertainty (in cm) is estimated by sampling 100 SMPL meshes from the predicted distributions and determining the average Euclidean distance from the sample mean for each vertex. Both the hierarchical architecture and the sample reprojection loss are required for predicted distributions to match the inputs, while demonstrating greater uncertainty for ambiguous parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparison with SOTA using sorted per-sample distributions of a) MPJPE-SC on 3DPW and b) PVE-T-SC on SSP-3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figures 6 ,</head><label>6</label><figDesc>7 and 8 in this supplementary material, as well as several figures in the main manuscript, visualise per-vertex 3D location uncertainties corresponding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Sengupta et al. [48] 97.1 95.8 93.1 89.7 61.1 59.4 58.2 56.5 15.2 14.8 13.6 11.9 Independent) w. HRNet [54] 88.3 85.0 82.6 78.5 56.6 54.5 52.8 50.2 13.9 12.9 12.0 10.3 Ours (Hierarchical) w. HRNet [54] 84.9 81.6 79.0 75.1 53.6 51.4 49.6 47.0 13.6 12.3 11.3 9.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3DPW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SSP-3D</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MPJPE</cell><cell></cell><cell></cell><cell cols="2">MPJPE-PA</cell><cell></cell><cell></cell><cell cols="2">PVE-T-SC</cell><cell></cell></row><row><cell>Number of Samples:</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>25</cell></row><row><cell>Biggs et al. [3]</cell><cell cols="8">93.8 82.2 79.4 75.8 59.9 57.1 56.6 55.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ProHMR [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">59.8 56.5 54.6 52.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>List of synthetic training data augmentations and their associated hyperparameter values. Body part occlusion uses the 24 DensePose [15] parts. Joint L/R swap is done for shoulders, elbows, wrists, hips, knees, ankles.</figDesc><table><row><cell>Body part occlusion</cell><cell>Occlusion probability</cell><cell>0.1</cell></row><row><cell>2D joints L/R swap</cell><cell>Swap probability</cell><cell>0.1</cell></row><row><cell cols="2">Half-image occlusion Occlusion probability</cell><cell>0.05</cell></row><row><cell>2D joints removal</cell><cell>Removal probability</cell><cell>0.1</cell></row><row><cell>2D joints noise</cell><cell>Noise range</cell><cell>[-8, 8] pixels</cell></row><row><cell>Occlusion box</cell><cell>Probability, Size</cell><cell>0.5, 48 pixels</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b9">(10)</ref> where the visibilities of the target joints are denoted by ? n ? {0, 1} L (1 if visible, 0 otherwise).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SCAPE: Shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-garment net: Learning to dress 3D people from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D multibodies: Fitting sets of plausible 3D models to ambiguous image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Erhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time screen reading: reducing domain shift for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">People tracking using hybrid monte carlo filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiam</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Articulated body motion capture by annealed particle filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deutscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orientation statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">D</forename><surname>Downs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV), 2020. 1</title>
		<meeting>the European Conference on Computer Vision (ECCV), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep orientation uncertainty learning based on a bingham loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshni</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Schwarting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3D pose consistent with 2D joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshops (PeopleCap)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Total capture: A 3D deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A new method to simulate the Bingham and related distributions in directional data analysis with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaad</forename><forename type="middle">M</forename><surname>Ganeiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanti</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Von Mises-Fisher matrix distribution in orientation statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic modeling for human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Appearance consensus driven self-supervised human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian attitude estimation with the matrix fisher distribution on so(3)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3D human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH Asia</title>
		<meeting>ACM SIGGRAPH Asia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Jupp</surname></persName>
		</author>
		<title level="m">Directional statistics</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic orientation estimation with matrix fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mohlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rald</forename><surname>Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">GraphMDN: Leveraging graph structure and deep learning to solve inverse problems. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tuomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Oikarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohrob</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazerounian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep directional statistics: Pose estimation with uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Prokudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501</idno>
		<title level="m">Accelerating 3D deep learning with PyTorch3D</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3D human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Probabilistic 3D human shape and pose estimation from multiple unconstrained images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Discriminative density propagation for 3D human motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="390" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Covariance scaled sampling for monocular 3D body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Trigg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hyperdynamics importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Trigg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3D human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Trigg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards accurate 3D human body reconstruction from silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Vince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC</title>
		<meeting>the British Machine Vision Conference (BMVC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Probabilistic monocular 3D human pose estimation with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wehrbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Danet: Decompose-and-aggregate network for 3D human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
