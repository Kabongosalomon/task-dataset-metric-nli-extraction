<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
						</author>
						<title level="a" type="main">Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in diffusion models bring stateof-the-art performance on image generation tasks. However, empirical results from previous research in diffusion models imply an inverse correlation between density estimation and sample generation performances. This paper investigates with sufficient empirical evidence that such inverse correlation happens because density estimation is significantly contributed by small diffusion time, whereas sample generation mainly depends on large diffusion time. However, training a score network well across the entire diffusion time is demanding because the loss scale is significantly imbalanced at each diffusion time. For successful training, therefore, we introduce Soft Truncation, a universally applicable training technique for diffusion models, that softens the fixed and static truncation hyperparameter into a random variable. In experiments, Soft Truncation achieves stateof-the-art performance on CIFAR-10, CelebA, CelebA-HQ 256 ? 256, and STL-10 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in generative models enable the creation of highly realistic images. One direction of such modeling is likelihood-free models <ref type="bibr">(Karras et al., 2019)</ref> based on minimax training. The other direction is likelihood-based models, including VAE <ref type="bibr">(Vahdat &amp; Kautz, 2020)</ref>, autoregressive models <ref type="bibr">(Parmar et al., 2018)</ref>, and flow models <ref type="bibr">(Grci? et al., 2021)</ref>. Diffusion models <ref type="bibr" target="#b13">(Ho et al., 2020)</ref> are one of the most successful likelihood-based models, where the reverse diffusion models the generative process. The success of diffusion models achieves state-of-the-art performance in image generation <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref>. Previously, a model with the emphasis on Fr?chet Inception Distance (FID), such as DDPM <ref type="bibr" target="#b13">(Ho et al., 2020)</ref> and ADM <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref>, trains the score network with the variance weighting; whereas a model with the emphasis on Negative Log-Likelihood (NLL), such as ScoreFlow <ref type="bibr">(Song et al., 2021a)</ref> and <ref type="bibr">VDM (Kingma et al., 2021)</ref>, trains the score network with the likelihood weighting. Such models, however, have the trade-off between NLL and FID: models with the emphasis on FID perform poorly on NLL, and vice versa. Instead of widely investigating the trade-off, they limit their work by separately training the score network on FID-favorable and NLL-favorable settings. This paper introduces Soft Truncation that significantly resolves the trade-off, with the NLL-favorable setting as the default training configuration. Soft Truncation reports a comparable FID against FID-favorable diffusion models while keeping NLL at the equivalent level of NLL-favorable models.</p><p>For that, we observe that the truncation hyperparameter is a significant hyperparameter that determines the overall scale of NLL and FID. This hyperparameter, , is the smallest diffusion time to estimate the score function, and the score function beneath is not estimated. A model with small enough favors NLL at the sacrifice on FID, and a model with relatively large is preferable to FID but has poor NLL. Therefore, we introduce Soft Truncation, which softens the fixed and static truncation hyperparameter ( ) into a random variable (? ) that randomly selects its smallest diffusion time at every optimization step. In every mini-batch update, we sample a new smallest diffusion time, ? , randomly, and the batch optimization endeavors to estimate the score function only on <ref type="bibr">[?, T ]</ref>, rather than [ , T ], by ignoring beneath ? . As ? varies by mini-batch updates, the score network successfully estimates the score function on the entire range of diffusion time on [ , T ], which brings an improved FID.</p><p>There are two interesting properties of Soft Truncation. First, though Soft Truncation is nothing to do with the weighting function in its algorithmic design, surprisingly, Soft Truncation turns out to be equivalent to a diffusion model with a general weight in the expectation sense (Eq. (10)). The random variable of ? determines the weight function (Theorem 1), and this gives a partial reason why Soft Truncation is successful in FID as much as the FID-favorable train-arXiv:2106.05527v5 <ref type="bibr">[cs.</ref>LG] 11 Jun 2022 ing <ref type="table" target="#tab_4">(Table 4)</ref>, even though Soft Truncation only considers the truncation threshold in its implementation <ref type="bibr">(Section 4.2)</ref>. Second, once ? is sampled in a mini-batch optimization, Soft Truncation optimizes the log-likelihood perturbed by ? (Lemma 1). Thus, Soft Truncation could be framed by Maximum Perturbed Likelihood Estimation (MPLE), a generalized concept of MLE that is specifically defined only in diffusion models (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head><p>Throughout this paper, we focus on continuous-time diffusion models <ref type="bibr">(Song et al., 2021b)</ref>. A continuous diffusion model slowly and systematically perturbs a data random variable, x 0 , into a noise variable, x T , as time flows. The diffusion mechanism is represented as a Stochastic Differential Equation (SDE), written by</p><formula xml:id="formula_0">dx t = f (x t , t) dt + g(t) dw t ,<label>(1)</label></formula><p>where w t is a standard Wiener process. The drift (f ) and the diffusion (g) terms are fixed, so the data variable is diffused in a fixed manner. We denote {x t } T t=0 as the solution of the given SDE of Eq. (1), and we omit the subscript and superscript to denote {x t }, if no confusion is arised.</p><p>The theory of stochastic calculus indicates that there exists a corresponding reverse SDE given by dx t = f (x t , t) ? g 2 (t)? log p t (x t ) dt + g(t) dw t , <ref type="formula">(2)</ref> where the solution of this reverse SDE exactly coincides to the solution of the forward SDE of Eq. (1). Here, dt is the backward time differential; dw t is a standard Wiener process flowing backward in time <ref type="bibr" target="#b0">(Anderson, 1982)</ref>; and p t (x t ) is the probability distribution of x t . Henceforth, we represent {x t } as the solution of SDEs of Eqs. (1) and (2).</p><p>The diffusion model's objective is to learn the stochastic process, {x t }, as a parametrized stochastic process, {x ? t }. A diffusion model builds the parametrized stochastic process as a solution of a generative SDE,</p><formula xml:id="formula_1">dx ? t = f (x ? t , t) ? g 2 (t)s ? (x ? t , t) dt + g(t) dw t . (3)</formula><p>We construct the parametrized stochastic process by solving the generative SDE of Eq. (3) backward in time with a starting variable of x ? T ? ?, where ? is an noise distribution. Throughout the paper, we denote p ? t as the probability distribution of x ? t . A diffusion model learns the generative stochastic process by minimizing the score loss <ref type="bibr">(Song et al., 2021a)</ref> of</p><formula xml:id="formula_2">L(?; ?) = 1 2 T 0 ?(t)E xt s ? (x t , t) ? ? log p t (x t ) 2 2 dt,</formula><p>where ?(t) is a weighting function that counts the contribution of each diffusion time on the loss function. This score loss is infeasible to optimize because the data score, ? log p t (x t ), is intractable in general. Fortunately, L(?; ?) is known to be equivalent to the (continuous) denoising NCSN loss <ref type="bibr">(Song et al., 2021b;</ref><ref type="bibr">Song &amp; Ermon, 2019)</ref>,</p><formula xml:id="formula_3">L N CSN (?; ?) = 1 2 T 0 ?(t)E x0,xt s ? (x t , t) ? ? log p 0t (x t |x 0 ) 2 2 dt,</formula><p>up to a constant that is irrelevant to ?-optimization.</p><p>Two important SDEs are known to attain analytic transition probabilities, log p 0t (x t |x 0 ): Variance Exploding SDE (VESDE) and Variance Preserving SDE (VPSDE) <ref type="bibr">(Song et al., 2021b)</ref>. First, VESDE assumes f (x t , t) = 0 and g(t) = ? min ( ?max ?min ) t 2 log ?max ?min . With such specific forms of f and g, the transition probability of VESDE turns out to follow a Gaussian distribution of</p><formula xml:id="formula_4">p 0t (x t |x 0 ) = N (x t ; ? V E (t)x 0 , ? 2 V E (t)I) with ? V E (t) ? 1 and ? 2 V E (t) = ? 2 min [( ?max ?min ) 2t ? 1]. Similarly, VPSDE takes f (x t , t) = ? 1 2 ?(t)x t and g(t) = ?(t), where ?(t) = ? min + t(? max ? ? min ); and its transition prob- ability falls into a Gaussian distribution of p 0t (x t |x 0 ) = N (x t ; ? V P (t)x 0 , ? 2 V P I) with ? V P (t) = e ? 1 2 t 0 ?(s) ds and ? 2 V P (t) = 1 ? e ? t 0 ?(s) ds .</formula><p>Recently, <ref type="bibr">Kim et al. (2022)</ref> categorize VESDE and VPSDE as a family of linear diffusions that has the SDE of</p><formula xml:id="formula_5">dx t = ? 1 2 ?(t)x t dt + g(t) dw t ,<label>(4)</label></formula><p>where ?(t) and g(t) are generic t-functions. Under the linear diffusions, we derive the transition probability to follow a Gaussian distribution p 0t (x t |x 0 ) = N (x t ; ?(t)x 0 , ? 2 (t)I) for certain ?(t) and ?(t) depending on ?(t) and g(t), respectively (see Eq. (16) of Appendix A.1). We emphasize that the suggested Soft Truncation is applicable for any SDE of Eq. (1), but we limit our focus to the family of linear SDEs of Eq. (4), particularly VESDE and VPSDE among linear SDEs, to maintain the simplicity. With such a Gaussian transtion probability, the denoising NCSN loss with a linear SDE is equivalent to</p><formula xml:id="formula_6">1 2 T 0 ?(t) ? 2 (t) E x0, ? (?(t)x 0 + ?(t) , t) ? 2 2 dt, if ? (?(t)x 0 + ?(t) , t) = ??(t)s ? (?(t)x 0 + ?(t) , t),</formula><p>where ? N (0, I) is a random perturbation, and ? is the neural network that predicts . This is the (continuous) DDPM loss <ref type="bibr">(Song et al., 2021b)</ref>, and the equivalence of the two losses provides a unified view of NCSN and DDPM. Hence, NCSN and DDPM are exchangeable for each other, and we take the NCSN loss as a default form of a diffusion loss throughout the paper.  </p><formula xml:id="formula_7">E x0 [? log p ? 0 (x 0 )] ? L N CSN (?; g 2 ),<label>(5)</label></formula><p>when the weighting function is the square of the diffusion term as ?(t) = g 2 (t), called the likelihood weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training and Evaluation of Diffusion Models in Practice</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Need of Truncation</head><p>In the family of linear SDEs, the gradient of the log transition probability satisfies ? log p 0t (x t |x 0 ) = ? xt??(t)x0</p><formula xml:id="formula_8">? 2 (t) = ? z ?(t)</formula><p>, where x t is given to ?(t)x 0 + ?(t)z with z ? N (0, I). The denominator of ?(t) converges to zero as t ? 0, which leads s ? (x t , t) ? ? log p 0t (x t |x 0 ) 2 to diverge as t ? 0, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>-(a), see Appendix A.2 for details. Therefore, the Monte-Carlo estimation of the NCSN loss is under high variance, which prevents stable training of the score network. In practice, therefore, previous research truncates the diffusion time range to [?, T ], with a positive truncation hyperparameter, ? = &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variational Bound With Positive Truncation</head><p>For the analysis for density estimation in Section 3.3, this section derives the variational bound of the log-likelihood when a diffusion model has a positive truncation because Inequality (5) holds only with zero truncation (? = 0). Lemma 1 provides a generalization of Inequality (5), proved by applying the data processing inequality <ref type="bibr" target="#b7">(Gerchinovitz et al., 2020)</ref> and the Girsanov theorem <ref type="bibr">(Pavon &amp; Wakolbinger, 1991;</ref><ref type="bibr">Vargas et al., 2021;</ref><ref type="bibr">Song et al., 2021a)</ref>. Lemma 1. For any ? ? [0, T ],</p><formula xml:id="formula_9">E x? ? log p ? ? (x ? ) ? L(?; g 2 , ? )<label>(6)</label></formula><p>holds, where L(?; g 2 , ? ) = 1 2 T ? g 2 (t)E x0,xt s ? (x t , t) ? ? log p 0t (x t |x 0 ) 2 2 dt, up to a constant, see Eq. (17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>The truncation time is key to enhance the microscopic sample quality.</p><p>Lemma 1 is a generalization of Inequality (5) in that Inequality (6) collapses to Inequality (5) under the zero truncation:</p><formula xml:id="formula_10">L N CSN (?; ?) = L(?; ?, ? = 0). If the time range is trun- cated to [?, T ] for ? ? [0, T ]</formula><p>, then from the variational inference, the log-likelihood becomes</p><formula xml:id="formula_11">E x0 ? log p ? 0 (x 0 ) ? E x? ? log p ? ? (x ? ) + R ? (?) (7) where R ? (?) = E x0 p 0? (x ? |x 0 ) log p 0? (x ? |x 0 ) p ? (x 0 |x ? ) dx ? ,</formula><p>with p ? (x 0 |x ? ) being the probability distribution of x 0 given x ? and the score estimation with s ? at ? . For any ? , we apply Lemma 1 to the right-hand-side of Inequality (7) to obtain the variational bound of the log-likelihood as</p><formula xml:id="formula_12">E x0 ? log p ? 0 (x 0 ) ? L(?; g 2 , ? ) + R ? (?).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Universal Phenomenon in Diffusion Training: Extremely Imbalanced Loss</head><p>To avoid the diverging issue introduced in Section 3.1, previous works in VPSDE <ref type="bibr">(Song et al., 2021a;</ref><ref type="bibr">Vahdat et al., 2021)</ref> modify the loss by truncating the integration on [?, T ] with a fixed hyperparameter ? = &gt; 0 so that the score network does not estimate the score function on [0, ). Analogously, previous works in VESDE <ref type="bibr">(Song et al., 2021b;</ref><ref type="bibr" target="#b2">Chen et al., 2022)</ref> approximate ? 2 V E (t) ? ? 2 min ( ?max ?min ) 2t to truncate the minimum variance of the transition probability to be ? 2 min . Truncating diffusion time at in VPSDE is equivalent to Monte-Carlo Norm truncating diffusion variance (? 2 min ) in VESDE, so these two truncations on VE/VP SDEs have the identical effect on bounding the diffusion loss. Henceforth, this paper discusses the argument of truncating diffusion time (VPSDE) and diffusion variance (VESDE) exchangeably. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the significance of truncation in the training of diffusion models. With the truncation of strictly positive = 10 ?5 , <ref type="figure" target="#fig_0">Figure 1</ref>-(a) shows that the integrand of L(?; g 2 , ? ) in the Bits-Per-Dimension (BPD) scale is still extremely imbalanced. It turns out that such extreme imbalance appears to be a universal phenomenon in training a diffusion model, and this phenomenon lasts from the beginning to the end of training. <ref type="figure" target="#fig_0">Figure 1</ref>-(b) with the green line presents the variational bound of the log-likelihood (right-hand-side of Inequality (8)) on the y-axis, and it indicates that the variational bound is sharply decreasing near the small diffusion time. Therefore, if is insufficiently small, the variational bound is not tight to the log-likelihood, and a diffusion model fails at MLE training. In addition, <ref type="figure">Figure 2</ref> indicates that insufficiently small (or ? min ) would also harm the microscopic sample quality. From these observations, becomes a significant hyperparameter that needs to be selected carefully. by lowering because NELBO is largely contributed by small diffusion time at test time as well as training time. Therefore, it could be a common strategy to reduce as much as possible to reduce test NELBO/NLL.  <ref type="bibr">, 2021b)</ref>, presents that FID is worsened as we take smaller hyperparameter ? min for the training. It is the range of small diffusion time that significantly contributes to the variational bound in the blue line of <ref type="figure" target="#fig_0">Figure 1</ref>-(b), so the score network with a small truncation hyperparameter, ? min or , remains unoptimized on large diffusion time. In the lens of <ref type="figure">Figure 2</ref>, therefore, the inconsistent result of <ref type="table" target="#tab_1">Table 1</ref> is attributed to the inaccurate score on large diffusion time. We design an experiment to validate the above argument in Table 2. This experiment utilizes two types of score networks: 1) three alternative networks (As) with diverse ? min ? {10 ?3 , 10 ?4 , 10 ?5 } trained in <ref type="table" target="#tab_1">Table 1</ref> experiment; 2) a network (B) with ? min = 10 ?5 (the last row of <ref type="table" target="#tab_1">Table 1</ref>). With these score networks, we denoise the noises by either one of the first-typed As from ? max to a common and fixed ? tr (= 1), and we use B to further denoise from ? tr to ? min = 10 ?5 . This further denoising step with model B enables us to compare the score accuracy on large diffusion time for models with diverse truncation hyperparameters in a fair resolution setting. <ref type="table" target="#tab_2">Table 2</ref> presents that the model with ? min = 10 ?3 has the best FID, implying that the training with too small truncation would harm the sample fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Effect of Truncation on Model Evaluation</head><p>Specifically, <ref type="figure" target="#fig_2">Figure 4</ref> shows the Euclidean norm of g 2 (t)s ? (x t , t), where each dot represents for a Monte-Carlo sample from p t (x t ). Here, g 2 (t)s ? (x t , t) is in the reverse drift term of the generative process, <ref type="figure" target="#fig_2">Figure 4</ref> illustrates that it is Importance Distribution </p><formula xml:id="formula_13">dx ? t = [f (x ? t , t) ? g 2 (t)s ? (x ? t , t)] dt + g(t) dw t .</formula><formula xml:id="formula_14">Likelihood Weighting g 2 (t)/? 2 (t) SofTrunc ( T t P5(? )d? )g 2 (t)/? 2 (t) SofTrunc ( T t P2(? )d? )g 2 (t)/? 2 (t) SofTrunc ( T t P1(? )d? )g 2 (t)/? 2 (t) Variance Weighting ? 2 (t)/? 2 (t) (c) Importance Distribution</formula><formula xml:id="formula_15">(t) s ? (x t , t) ? ? log p 0t (x t |x 0 ) 2 2 . (b)</formula><p>The Monte-Carlo loss for each diffusion time on variaous truncation time. (c) The importance distribution for various truncation distributions. the large diffusion time that dominates the sampling process. Therefore, a precise score network on large diffusion time is particularly important in sample generation.</p><p>The imprecise score mainly affects the global sample context, as the denoising on small diffusion time only crafts the image in its microscopic details, illustrated in Figures 3 and 5. <ref type="figure" target="#fig_1">Figure 3</ref> shows how the global fidelity is damaged: a man synthesized in the second row has unrealistic curly hair on his forehead, constructed on the large diffusion time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Soft Truncation: A Training Technique for a Diffusion Model</head><p>As in Section 3, the choice of is crucial for training and evaluation, but it is computationally infeasible to search for the optimal . Therefore, we introduce a training technique that predominantly mediates the need for -search by softening the fixed truncation hyperparameter into a truncation random variable so that the truncation time varies in every optimization step. Our approach successfully trains the score network on large diffusion time without sacrificing NLL. We explain the Monte-Carlo estimation of the variational bound in Section 4.1, which is the common practice of previous research but explained to emphasize how simple (though effective) Soft Truncation is, and we subsequently introduce Soft Truncation in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Monte-Carlo Estimation of Truncated Variational Bound with Importance Sampling</head><p>In this section, we fix a truncation hyperparameter to be ? = . For every batch {x</p><formula xml:id="formula_16">(b) 0 } B b=1 , the Monte-Carlo estimation of the variational bound in Inequality (6) is L(?; g 2 , ) ? L(?; g 2 , ) = 1 2B B b=1 g 2 (t (b) ) s ? (x t (b) , t (b) ) ? ? log p 0t (b) (x t (b) |x 0 ) 2 2 , up to a constant irrelevant to ?, where x t (b) = ?(t (b) )x 0 + ?(t (b) ) (b) with {t (b) } B b=1 and { (b) } B b=1 be the corresponding Monte-Carlo samples from t (b) ? [ , T ] and (b) ? N (0, I),</formula><p>respectively. Note that this Monte-Carlo estimation is tractably computed from the analytic form of the transition probability as</p><formula xml:id="formula_17">? log p 0t (b) (x t (b) |x 0 ) = (b) ?(t (b) ) under linear SDEs.</formula><p>Previous works <ref type="bibr">(Song et al., 2021a;</ref><ref type="bibr">Huang et al., 2021)</ref> apply the importance sampling with the importance dis-</p><formula xml:id="formula_18">tribution of p iw (t) = g 2 (t)/? 2 (t) Z 1 [ ,T ] (t), where Z = T g 2 (t) ? 2 (t) dt. It is well known (Goodfellow et al., 2016) that the Monte-Carlo variance ofL is minimum if the im- portance distribution is p * iw (t) ? g 2 (t)L(t) with L(t) = E x0,xt [ s ? (x t , t) ? ? log p 0t (x t |x 0 ) 2 2 ]</formula><p>, but sampling of Monte-Carlo diffusion time from p * iw (t) at every training iteration would incur 2? slower training speed, at least, because the importance sampling requires the score evaluation. Therefore, previous research approximates</p><formula xml:id="formula_19">L(t) byL(t) = E x0,xt [ ? log p 0t (x t |x 0 ) 2 2 ] ? 1/? 2 (t),</formula><p>and p iw (t) becomes the approximate importance weight. This approximation, at the expense of bias, is cheap because the closed-form of the inverse Cumulative Distribution Function (CDF) is known. Unless we train the variance directly as in <ref type="bibr">Kingma et al. (2021)</ref>, we believe p iw (t) is the maximally efficient sampler as long as the training speed matters. The importance weighted Monte-Carlo estimation becomes</p><formula xml:id="formula_20">L(?; g 2 , ) = Z 2 T p iw (t)? 2 (t)E s ? (x t , t) ? ? log p 0t (x t |x 0 ) 2 2 dt ? Z 2B B b=1 ? 2 (t (b) iw ) s ? x t (b) iw , t (b) iw ? (b) ?(t (b) iw ) 2 2 :=L iw (?; g 2 , ),<label>(9)</label></formula><p>where {t</p><formula xml:id="formula_21">(b) iw } B b=1 is the Monte-Carlo sample from the impor- tance distribution, i.e., t (b) iw ? p iw (t) ? g 2 (t) ? 2 (t) . = 0 = 0.1 = 0.2 = 0.3 = 0.4 = 0.5 = 0.6 = 0.7 = 0.8 = 0.9 = 1 Q1 (25%) Q2 (50%) Q3 (75%) Q4 (100%) = = 0.1 Q0 (0%) Q1 (25%) Q2 (50%) Q3 (75%) Q4 (100%) = 0</formula><p>Figure 7: Quartile of importance weighted Monte-Carlo time of VPSDE. Red dots represent Q1/Q2/Q3/Q4 quantiles when truncated at ? = = 10 ?5 . About 25% and 50% of Monte-Carlo time are located in [ , 5 ? 10 ?3 ] and [ , 0.106], respectively. Green dots represent Q0-Q5 quantiles when truncated at ? = 0.1. Importance weighted Monte-Carlo time with ? = 0.1 is distributed much more balanced compared to the truncation at ? = .</p><p>The importance sampling is advantageous in both NLL and FID (Song et al., 2021a) over the uniform sampling, as the importance sampling significantly reduces the estimation variance. <ref type="figure" target="#fig_5">Figure 6</ref>-(a) illustrates the sample-by-sample loss, and the importance sampling significantly mitigates the loss scale by diffusion time compared to the scale in <ref type="figure" target="#fig_0">Figure 1</ref>-(a). However, the importance distribution satisfies p iw (t) ? ? as t ? 0 in <ref type="figure" target="#fig_5">Figure 6</ref>-(c) blue line, and most of the importance weighted Monte-Carlo time is concentrated at t ? in <ref type="figure">Figure 7</ref>. Hence, the use of the importance sampling has a trade-off between the reduced variance ( <ref type="figure" target="#fig_5">Figure 6-(a)</ref>) versus the over-sampled diffusion time near t ? <ref type="figure">(Figure 7)</ref>. Regardless of whether to use the importance sampling or not, therefore, the inaccurate score estimation on large diffusion time appears sampling-strategic-independently, and solving this pre-matured score estimation becomes a nontrivial task.</p><p>Instead of the likelihood weighting, previous works <ref type="bibr" target="#b13">(Ho et al., 2020;</ref><ref type="bibr">Nichol &amp; Dhariwal, 2021;</ref><ref type="bibr">Dhariwal &amp; Nichol, 2021)</ref> train the denoising score loss with the variance weighting, ?(t) = ? 2 (t). With this weighting, the importance distribution becomes the uniform distribution, p iw (t) = ?(t) ? 2 (t) ? 1, so it significantly alleviates the tradeoff of using the likelihood weighting. However, the variance weighting favors FID at the sacrifice in NLL because the loss is no longer the variational bound of the log-likelihood. In contrast, the training with the likelihood weighting is leaning towards NLL than FID, so Soft Truncation is for the balanced NLL and FID, using the likelihood weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Soft Truncation</head><p>Soft Truncation releases the truncation hyperparameter from a static variable to a random variable with a probability distribution of P(? ). In every mini-batch update, Soft Truncation optimizes the diffusion model withL iw (?; g 2 , ? ) in Eq. (9) for a sampled ? ? P(? ). In other words, for every batch {x</p><formula xml:id="formula_22">(b) 0 } B b=1 , Soft Truncation optimizes the Monte-Carlo los? L iw (?; ?, ? ) = Z ? 2B B b=1 ? 2 (t (b) iw ) s ? x t (b) iw , t (b) iw ? (b) ?(t (b) iw ) 2 2 with {t (b) iw } B b=1 sampled from the importance distribution of p iw,? (t) = g 2 (t)/? 2 (t) Z? 1 [?,T ] (t), where Z ? := T ? g 2 (t) ? 2 (t) dt.</formula><p>Soft Truncation resolves the oversampling issue of diffusion time near t ? , meaning that Monte-Carlo time is not concentrated on anymore. <ref type="figure">Figure 7</ref> illustrates the quantiles of importance weighted Monte-Carlo time with Soft Truncation under ? = and ? = 0.1. The score network is trained more equally on diffusion time when ? = 0.1, and as a consequence, the loss imbalance issue in each training step is also alleviated as in <ref type="figure" target="#fig_5">Figure 6</ref>-(b) with purple dots. This limited range of [?, T ] provides a chance to learn a score network more balanced on diffusion time. As ? is softened, such truncation level will vary by mini-batch updates: see the loss scales change by blue, green, red, and purple dots according to various ? s in <ref type="figure" target="#fig_5">Figure 6</ref>-(b). Eventually, the softened ? will provide a fair chance to learn the score network from small as well as large diffusion time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Soft Truncation Equals to A Diffusion Model With A General Weight</head><p>In the original diffusion model, the loss estimation, L(?; g 2 , ), is just a batch-wise approximation of a population loss, L(?; g 2 , ). However, the target population loss of Soft Truncation, L(?; g 2 , ? ), is depending on a random variable ? , so the target population loss itself becomes a random variable. Therefore, we derive the expected Soft Truncation loss to reveal the connection to the original diffusion model:</p><formula xml:id="formula_23">L ST (?; g 2 , P) := E P(? ) L(?; g 2 , ? ) = 1 2 T P(? ) T ? g 2 (t)E s ? ? ? log p 0t 2 2 dt d? = 1 2 T g 2 P (t)E s ? ? ? log p 0t 2 2 dt,</formula><p>up to a constant, where g 2 P (t) = t 0 P(? ) d? g 2 (t), by exchanging the orders of the integrations. Therefore, we conclude that Soft Truncation reduces to a diffusion model with a general weight of g 2 P (t), see Appendix A.3: L ST (?; g 2 , P) = L(?; g 2 P , ). </p><formula xml:id="formula_25">P ? ([a, b]) = b max(a, ) ?(s) g 2 (s) ds + ?( ) g 2 ( ) 1 [a,b] ( ) Z,</formula><p>where Z = ?(T ) g 2 (T ) ; up to a constant, the variational bound of the general weighted diffusion loss becomes Old wisdom is to minimize the loss variance if available for stable training. However, some optimization methods in the deep learning era (e.g., stochastic gradient descent) deliberately add noises to a loss function that eventually helps escape from a local optimum. Soft Truncation is categorized in such optimization methods that inflate the loss variance by intentionally imposing auxiliary randomness on loss estimation. This randomness is represented by the outmost expectation of E P ? (? ) , which controls the diffusion time range batch-wisely. Additionally, the loss with a sampled ? is the proxy of the perturbed KL divergence by ? , so the auxiliary randomness on loss estimation is theoretically tamed, meaning that it is not a random perturbation. <ref type="bibr">1</ref> If ?(t) = cg 2 (t), the probability satisfies P([a, b]) = 1 <ref type="bibr">[a,b]</ref> ( ), which is a probability distribution of one mass at .</p><formula xml:id="formula_26">E P ? (? ) D KL (p ? p ? ? ) ? 1 2Z T ?(t)E xt s ? (x t , t) ? ? log p t (x t ) 2 2 dt = 1 Z L(?; ?, ) = E P ? (? ) L(?; g 2 , ? ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Choice of Truncation Probability Distribution</head><p>We parametrize the probability distribution of ? by</p><formula xml:id="formula_27">P k (? ) = 1/? k Z k 1 [ ,T ] (? ) ? 1 ? k ,<label>(11)</label></formula><p>where Z k = T 1 ? k d? with sufficiently small enough truncation hyperparameter. Note that it is still beneficial to remain strictly positive because a batch update with ? ? 0 &lt; would drift the score network away from the optimal point. <ref type="figure" target="#fig_5">Figure 6-(c)</ref> illustrates the importance distribution of ? P k for varying k. From the definition of Eq. (11), P k (? ) ? ? (? ) as k ? ?, and this limiting delta distribution corresponds to the original diffusion model with the likelihood weighting. <ref type="figure" target="#fig_5">Figure 6-(c)</ref> shows that the importance distribution of P k with finite k interpolates the likelihood weighting and the variance weighting.</p><p>With the current simple form, we experimentally find that the sweet spot is k ? 1.0 in VPSDE and k = 2.0 in VESDE with the emphasis on the sample quality. For VPSDE, the importance distribution in <ref type="figure" target="#fig_5">Figure 6</ref>-(c) is nearly equal to that of the variance weighting if k ? 1.0, so Soft Truncation with k ? 1.0 improves the sample fidelity, while maintaining low NLL. On the other hand, if k is too small, no ? will be sampled near , so it hurts both sample generation and density estimation. We leave further study on searching for the optimal distribution of ? as future work. Soft Truncation is a universal training technique indepedent to model architectures and diffusion strategies. In the experiments, we test Soft Truncation on various architectures, including vanilla NCSN++, DDPM++, Unbounded NCSN++ (UNCSN++), and Unbounded DDPM++ (UD-DPM++). Also, Soft Truncation is applied to various diffusion SDEs, such as VESDE, VPSDE, and Reverse VESDE (RVESDE). Although we use continuous SDEs for the diffusion strategies, Soft Truncation with the discrete model, such as DDPM <ref type="bibr" target="#b13">(Ho et al., 2020)</ref>, is a straightforward application of continuous models. Appendix D enumerates the specifications of score architectures and SDEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>From <ref type="figure" target="#fig_0">Figure 1-(c)</ref>, a sweet spot of the hard threshold is = 10 ?5 , in which NLL/NELBO are no longer improved under this threshold. As the diffusion model has no informa-    <ref type="bibr">, 2021b)</ref>. We denote L(?; ?, ) as the vanilla training with ?-weighting, and L ST (?; g 2 , P) as the training by Soft Truncation with the truncation probability of   P. We additionally denote L ST (?; ? 2 , P) for updating the network by the variance weighted loss per batch-wise update. We release our code at https://github.com/ Kim-Dongjun/Soft-Truncation. <ref type="figure" target="#fig_9">Figure 8</ref> illustrates the FID score <ref type="bibr" target="#b11">(Heusel et al., 2017)</ref> in y-axis by training steps in x-axis. <ref type="figure" target="#fig_9">Figure 8</ref> shows that Soft Truncation beats the vanilla training after 150k of training iterations. <ref type="table" target="#tab_4">Tables 4, 5</ref>, 6, and 7 show ablation studies on various weighting functions, model architectures, SDEs, s, and probability distributions of ? , respectively. See Appendix E.2. <ref type="table" target="#tab_4">Table 4</ref> shows that Soft Truncation beats or equals to the vanilla training in all performances. We highlight that Soft Truncation with P 0.9 outperforms the FID-favorable model with the variance weighting with respect to FID on both CIFAR-10 and ImageNet32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FID by Iteration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Not only comparing with the pre-existing weighting functions, such as ? = g 2 or ? = ? 2 , <ref type="table" target="#tab_4">Table 4</ref> additionally reports the experimental result of a general weighting function of ? = g 2 P1 . From Eq. (10), Soft Truncation with P 1 and the vanilla training with ? = g 2 P1 coincide in their loss functions in average, i.e., L(?; g 2 P1 , ) = L ST (?; g 2 , P 1 ). Thus, when  <ref type="table" target="#tab_4">Table 4</ref> implies that Soft Truncation gives better optimization than the vanilla method. This strongly implies that Soft Truncation could be a default training method for a general weighted denoising diffusion loss. <ref type="table" target="#tab_5">Table 5</ref> provides two implications. First, Soft Truncation particularly boosts FID while maintaining density estimation performances under the variation of score networks and diffusion strategies. Second, <ref type="table" target="#tab_5">Table 5</ref> shows that Soft Truncation is effective on CelebA even when we apply Soft Truncation on the variance weighting, i.e., L ST (?; ? 2 , P), but we find that this does not hold on CIFAR-10 and Ima-geNet32. We leave it as a future work on this extent. Quantitative Comparison to SOTA <ref type="table" target="#tab_9">Table 9</ref> compares Soft Truncation (ST) against the current best generative models. It shows that Soft Truncation achieves the state-of-the-art sample generation performances on CIFAR-10, CelebA, CelebA-HQ, and STL-10, while keeping NLL intact. In particular, we have experimented thoroughly on the CelebA dataset, and we find that Soft Truncation largely exceeds the previous best FID scores by far. In FID, Soft Truncation with DDPM++ performs 1.90, which exceeds the previous best FID of 2.92 by DDGM. Also, Soft Truncation significantly improves FID on STL-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes a generally applicable training method for diffusion models. The suggested training method, Soft Truncation, is motivated from the observation that the density estimation is mostly counted on small diffusion time, while the sample generation is mostly constructed on large diffusion time. However, small diffusion time dominates the Monte-Carlo estimation of the loss function, so this imbalance contribution prevents accurate score learning on large diffusion time. Soft Truncation softens the truncation level at each mini-batch update, and this simple modification is connected to the general weighted diffusion loss and the concept of Maximum Perturbed Likelihood Estimation. </p><formula xml:id="formula_28">dx t = ? 1 2 ?(t)x t dt + g(t) dw t ,<label>(12)</label></formula><p>where ? : R ? R ?0 and g : R ? R ?0 are real-valued functions. VESDE has ?(t) ? 0 and g(t) = d? 2 (t)/ dt = ? min ( ?max ?min ) t 2 log ?max ?min , where ? min and ? max are the minimum/maximum perturbation variances, respectively. It has the transition probability of</p><formula xml:id="formula_29">p 0t (x t |x 0 ) = N (x t ; ? V E (t)x 0 , ? 2 V E (t)I), where ? V E (t) ? 1 and ? 2 V E (t) := ? 2 min [( ?max ?min ) 2t ? 1]</formula><p>. VPSDE has ?(t) = ? min + (? max ? ? min )t and g(t) = ?(t) with the transition probability of</p><formula xml:id="formula_30">p 0t (x t |x 0 ) = N (x t ; ? V P (t)x 0 , ? 2 V P (t)I),</formula><p>where ? V P (t) = e ? 1 2 t 0 ?(s) ds and ? 2 (t) = 1 ? e ? t 0 ?(s) ds .</p><p>Analogous to VE/VP SDEs, the transition probability of the generic linear SDE of Eq. (12) is a Gaussian distribution of p 0t (x t |x 0 ) = N (x t |?(t)x 0 , ? 2 (t)I), where its mean and covariance functions are characterized as a system of ODEs of</p><formula xml:id="formula_31">d?(t) dt = ? 1 2 ?(t)?(t),<label>(13)</label></formula><formula xml:id="formula_32">d? 2 (t) dt = ??(t)? 2 (t) + g 2 (t),<label>(14)</label></formula><p>with initial conditions to be ?(0) = 1 and ? 2 (0) = 0.</p><p>Eq. (13) has its solution by</p><formula xml:id="formula_33">?(t) = e ? 1 2 t 0 ?(s) ds .</formula><p>If we multiply e </p><p>If we impose ? 2 (0) = 0 to Eq. (15), then the constant C satisfies C = 0, and the variance formula becomes To sum up, the family of linear SDEs of dx t = ? 1 2 ?(t)x t dt + g(t) dw t gets the transition probability to be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Diverging Denoising Loss</head><p>The gradient of the log transition probability, ? log p 0t (x t |x 0 ) = ? xt??(t)x0</p><formula xml:id="formula_36">? 2 (t) = ? z ?(t)</formula><p>, is diverging at ?(t)x 0 , where x t = ?(t)x 0 + ?(t)z. Below Lemma 2 indicates that s(x t , t) ? ? log p 0t (x t |x 0 ) 2 ? ? for any continuous score function, s. This leads that the denoising score loss diverges as t ? 0 as illustrated in <ref type="figure" target="#fig_0">Figure 1-(a)</ref>. </p><formula xml:id="formula_37">Lemma 2. Let H [0,T ] = {s : R d ? [0, T ] ? R d ,</formula><formula xml:id="formula_38">, x 1 ? x 2 ? diam(M ) for all x 1 , x 2 ? U . Also, if t 1 , t 2 ? [0, T ], |t 1 ? t 2 | is bounded.</formula><p>Hence, the local Lipschitzness of s implies that there exists a positive K &gt; 0 such that</p><formula xml:id="formula_39">s(x 1 , t 1 ) ? s(x 2 , t 2 ) ? K( x 1 ? x 2 + |t 1 ? t 2 |)</formula><p>for any x 1 , x 2 ? U and t 1 , t 2 ? [0, T ]. Therefore, for any s ? H [0,T ] , there exists C &gt; 0 such that s(x, t) &lt; C for all x ? U and t ? [0, T ], which leads no s that satisfies s(x, t) ? v(x) a.e. on U as t ? 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. General Weighted Diffusion Loss</head><p>The denoising score loss is</p><formula xml:id="formula_40">L(?; g 2 , ? ) = 1 2 T ? g 2 (t)E x0,xt s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) 2 2 ? log p 0t (x t |x 0 ) 2 2 dt ? T ? E xt div(f (x t , t)) dt ? E x T log ?(x T ) ,<label>(17)</label></formula><p>for any ? ? [0, T ]. For an appropriate class of function A(t),</p><formula xml:id="formula_41">T 0 P(? ) T ? A(t) dt d? = T 0 T 0 P(? )A(t)1 [?,T ] (t) dt d? = T 0 T 0 P(? )A(t)1 [?,T ] (t) d? dt = T 0 t 0 P(? )A(t) d? dt = T 0 t 0 P(? ) d? A(t) dt</formula><p>holds by changing the order of integration. Therefore, we get L ST (?; g 2 , P) := E P(? ) L(?; g 2 , ? )</p><formula xml:id="formula_42">= T 0 P(? ) 1 2 T ? g 2 (t)E x0,xt s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) 2 2 ? log p 0t (x t |x 0 ) 2 2 dt ? T ? E xt div(f (x t , t)) dt ? E x T log ?(x T ) d? = T 0 t 0 P(? ) d? 1 2 g 2 (t)E x0,xt s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) 2 2 ? log p 0t (x t |x 0 ) 2 2 ?E xt div(f (x t , t)) dt ? E x T log ?(x T ) = 1 2 T 0 g 2 P (t)E x0,xt s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) 2 2 dt + C, where C = ? 1 2 T 0 g 2 P (t)E x0,xt log p 0t (x t |x 0 ) 2 2 dt ? T 0 t 0 P(? ) d? E xt div(f (x t , t)) dt ? E x T log ?(x T ) .</formula><p>If f (x t , t) = ? 1 2 ?(t)x t , then we have</p><formula xml:id="formula_43">C = ? d 2 T 0 t 0 P(? ) d? g 2 (t) ? 2 (t) dt + d 2 T 0 t 0 P(? ) d? ?(t) dt ? E x T log ?(x T ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Theorems and Proofs</head><p>Lemma 1. For any ? ? [0, T ],</p><formula xml:id="formula_44">E x? ? log p ? ? (x ? ) ?L(?; g 2 , ? ) = 1 2 T ? g 2 (t)E x0,xt s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) 2 2 ? ? xt log p 0t (x t |x 0 ) 2 2 dt ? T ? E xt div(f (x t , t)) dt ? E x T log ?(x T ) .</formula><p>Proof. Suppose ? is the path measure of the forward SDE, and ? ? is the path measure of the generative SDE. The restricted measure is defined by ?| <ref type="bibr">[?,T ]</ref> </p><formula xml:id="formula_45">({F t } T t=? ) := ?({F t } T t=0 ), where F t = R d if t ? [0, ? ) and F t is a measurable set in R d otherwise.</formula><p>The restricted measure of ? ? is defined analogously. Then, by the data processing inequality, we get</p><formula xml:id="formula_46">D KL (p ? p ? ? ) ? D KL (?| [?,T ] ? ? | [?,T ] ).<label>(18)</label></formula><p>Now, from the chain rule of KL divergences, we have</p><formula xml:id="formula_47">D KL (?| [?,T ] ? ? | [?,T ] ) = D KL (p T ?) + E z?p T D KL ?| [?,T ] (?|x T = z) ? ? | [?,T ] (?|x T = z) .<label>(19)</label></formula><p>From the Girsanov theorem and the Martingale property, we get</p><formula xml:id="formula_48">T ?(? ) g 2 (? ) L(?; g 2 , ? ) d? + ?( ) g 2 ( ) L(?; g 2 , ) + T ?(t) g 2 (t) ? 1 E xt div(f (x t , t)) dt + ?(T ) g 2 (T ) ? 1 E x T log ?(x T ) .<label>(29)</label></formula><p>Then, applying Lemma 1 to Eq. (29) yields the desired result.</p><p>Corollary 1. Suppose ?(t) is a weighting function of the NCSN loss. If ?(t) g 2 (t) is a nondecreasing and nonnegative continuous function on [ , T ] and zero on [0, ), then</p><formula xml:id="formula_49">1 2 T ?(t)E xt s ? (x t , t) ? ? xt log p t (x t ) 2 2 dt + ?(T ) g 2 (T ) D KL (p T ?) ? T ?(? ) g 2 (? ) D KL (p ? p ? ? ) d? + ?( ) g 2 ( ) D KL (p p ? ).</formula><p>Remark 1. A direct extension of the proof indicates that Theorem 1 still holds when ?(t) g 2 (t) has finite jump on [0, T ]. Remark 2. The weight of ?(T ) g 2 (T ) is the normalizing constant of the unnormalized truncation probability, P.</p><p>Proof. By plugging A(t) = 1 2 E xt s ? (x t , t) ? ? xt log p t (x t ) 2 2 in Eq. (26) and using Lemma 1, we have</p><formula xml:id="formula_50">1 2 T ?(t)E xt s ? (x t , t) ? ? xt log p t (x t ) 2 2 dt + ?(T ) g 2 (T ) D KL (p T ?) = T ?(? ) g 2 (? ) 1 2 T ? g 2 (t)E xt s ? (x t , t) ? ? xt log p t (x t ) 2 2 dt d? + ?( ) g 2 ( ) 1 2 T g 2 (t)E xt s ? (x t , t) ? ? xt log p t (x t ) 2 2 dt + ?(T ) g 2 (T ) D KL (p T ?) ? T ?(? ) g 2 (? ) D KL (p ? p ? ? ) ? D KL (p T ?) d? + ?( ) g 2 ( ) D KL (p p ? ) ? D KL (p T ?) + ?(T ) g 2 (T ) D KL (p T ?) = T ?(? ) g 2 (? ) D KL (p ? p ? ? ) d? + ?( ) g 2 ( ) D KL (p p ? ).</formula><p>C. Additional Score Architectures and SDEs C.1. Additional Score Architectures: Unbounded Parametrization</p><p>From the released code of <ref type="bibr">Song et al. (2021b)</ref>, the NCSN++ network is modeled by s ? (x t , log ?(t)), where the second argument is log ?(t) instead of t. Experiments with s ? (x t , t) or s ? (x t , ?(t)) were not as good as the parametrization of s ? (x t , log ?(t)), and we analyze this experimental results from Lemma 2 and Proposition 1.</p><formula xml:id="formula_51">10 ?3 10 ?2 10 ?1 10 0 10 1 Diffusion Level ? (log) 0?10 6 1?10 6 2?10 6 3?10 6 4?10 6 5?10 6 6?10 6</formula><p>Appr. of Data Score ? xt log p 0t (x t |x 0 ) 2 2 (a) Approximate data score diverges.  <ref type="figure">Figure 9</ref>: (a) The approximate data score, The gradient of the log transition probability diverges at t ? 0 theoretically (Section A.2) and empirically <ref type="figure">(Figure 9-(a)</ref>).</p><formula xml:id="formula_52">? xt log p t (x t ) 2 2 = ? xt log p r (x 0 )p 0t (x t |x 0 ) dx 0 2 2 ? ? xt log p 0t (x t |x 0 ) 2 2 ,</formula><p>Here, in high-dimensional space, p 0t (x t |x 0 )/p 0t (x t |x 0 ) with x 0 = x 0 is either zero or infinity. Thus, the data score is nearly identical to the gradient of the log transition probability,</p><formula xml:id="formula_53">? xt log p t (x t ) 2 2 = ? xt log p r (x 0 )p 0t (x t |x 0 ) dx 0 2 2 ? ? xt log p 0t (x t |x 0 ) 2</formula><p>2 , and the observation of <ref type="figure">Figure 9-(a)</ref> is valid for the exact data score, as well. Although Lemma 2 is based on s ? (x t , t), the identical result also holds for the parametrization of s ? (x t , ?(t)), so it indicates that both s ? (x t , t) and s ? (x t , ?(t)) cannot estimate the data score as t ? 0. On the other hand, Proposition 1 implies that there exists a score function that estimates the unbounded data score asymptotically, and Proposition 1 explains the reason why the parametrization of <ref type="bibr">Song et al. (2021b)</ref>, i.e., s ? (x t , log ?(t)), is successful on score estimation.</p><p>On top of that, we introduce another parametrization that particularly focuses on the score estimation near t ? 0. We name Unbounded NCSN++ (UNCSN++) as the network of s ? (x t , ?(t)) with</p><formula xml:id="formula_54">?(t) = log ?(t) if ?(t) ? ? 0 ? c1 ?(t) + c 2 if ?(t) &lt; ? 0</formula><p>and Unbounded DDPM++ (UDDPM++) as the network of s ? (x t , ?(t)) with ?(t) := g 2 (t) ? 2 (t) dt. In UNCSN++, c 1 , c 2 and ? 0 are the hyperparameters. By acknowledging the parametrization of log ?(t), we choose ? 0 as 0.01. Also, to satisfy the continuously differentiability of ?(t), two hyperparameters c 1 and c 2 satisfy a system of equations with degree 2, so c 1 and c 2 are fully determined with this system of equations.</p><p>The choice of such ?(t) for UDDPM++ is expected to enhance the score estimation near t ? 0 because the input of ?(t) is distributed uniformly when we draw samples from the importance weight. Concretely, when the sampling distribution on the diffusion time is given by p iw (t) ? g 2 (t) ? 2 (t) , the ?-distribution from the importance sampling becomes p(?) ? 1, which is depicted in <ref type="figure">Figure 9</ref>-(b).</p><p>Proof of Proposition 1. Let h be a standard mollifier function. If h t (x) = t ?n h(x/t), then v t := h t * v converges to v a.e. on U as t ? 0 (Theorem 7-(ii) of Appendix C in <ref type="bibr" target="#b6">(Evans, 1998)</ref>). Therefore, if we define s(x, ?) := v 1/? (x) on the domain of v 1/? (x) and s(x, ?) := 0 elsewhere, then s(x,</p><formula xml:id="formula_55">?) = v 1/? (x) ? v(x) a.e. on U as ? ? ?. Now, to show that s(x, ?) is locally Lipschitz, letM ? [?, ?] be a compact subset of R n ? [1, ?). From s(x 1 , ? 1 ) ? s(x 2 , ? 2 ) = v 1/?1 (x 1 ) ? v 1/?2 (x 2 ) ? v 1/?1 (x 1 ) ? v 1/?1 (x 2 ) + v 1/?1 (x 2 ) ? v 1/?2 (x 2 ) , if there exists K 1 , K 2 &gt; 0 such that v 1/?1 (x 1 ) ? v 1/?1 (x 2 ) ? K 1 x 1 ? x 2 and v 1/?1 (x 1 ) ? v 1/?2 (x 1 ) ? K 2 |? 1 ? ? 2 | for all x 1 , x 2 ?M and ? 1 , ? 2 ? [?, ?], then s(x, ?) = v 1/? (x) is Lipschitz onM ? [?, ?].</formula><p>First, since v 1/? is infinitely differentiable on its domain (Theorem 7-(i) of Appendix C in <ref type="bibr" target="#b6">(Evans, 1998)</ref>) and ? ? [?, ?], there exists K 1 &gt; 0 such that v 1/? (x 1 ) ? v 1/? (x 2 ) ? K 1 x 1 ? x 2 . Second, the mollifier satisfies the uniform convergence on any compact subset of U (Theorem 7-(iii) of Appendix C in <ref type="bibr" target="#b6">(Evans, 1998)</ref>), which leads that v 1/?1 (</p><formula xml:id="formula_56">x) ? v 1/?2 (x) ? K 2 | 1 ?1 ? 1 ?2 | = K 2 |?1??2|</formula><p>?1?2 ? K 3 |? 1 ? ? 2 | for some K 2 , K 3 &gt; 0. Therefore, s becomes an element of H [1,?) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Additional SDE: Reciprocal VESDE</head><p>VESDE assumes g(t) = ? min ( ?max ?min ) t 2 log ?max ?min . Then, the variance of the transition probability</p><formula xml:id="formula_57">p 0t (x t |? V E (t)x 0 , ? 2 V E (t)) becomes ? 2 V E (t) = t 0 g 2 (s) ds = ? 2 min [( ?max ?min ) 2t ? 1]</formula><p>if the diffusion starts from t = 0 with the initial condition of x 0 ? p r . VESDE was originally introduced in Song &amp; Ermon (2020) in order to satisfy the geometric property for its smooth transition of the distributional shift. Mathematically, the variance is geometric if d dt log ? 2 V E (t) is a constant, but VESDE losses the geometric property as illustrated in <ref type="figure">Figure 9</ref>-(c). To attain the geometric property in VESDE, VESDE approximates the variance to be? 2 V E (t) = ? 2 min ( ?max ?min ) 2t by omitting 1 from ? 2 V E (t). However, this approximation leads that x t is not converging to x 0 in distribution because ? 2 min ( ?max ?min ) 2t ? ? 2 min = 0 as t ? 0. Indeed, a bit stronger claim is possible: Proposition 2. There is no SDE that has the stochastic process {x t } t?[0,T ] , defined by a transition probability p 0t (x t |x 0 ) = N (x t ; x 0 , ? 2 min ( ?max ?min ) 2t I), as the solution.</p><p>Proposition 2 indicates that if we approximate the variance by ? 2 V E (t), then the reverse diffusion process cannot be modeled by a generative process.</p><p>Rigorously, however, if the diffusion process starts from t = ??, rather than t = 0, then the variance of the transition probability becomes ? 2 V E,?? (t) = t ?? g 2 (s) ds = ? 2 min ( ?max ?min ) 2t , which is exactly the variance? 2 V E (t). Therefore, VESDE can be considered as a diffusion process starting from t = ??.</p><p>From this point of view, we introduce a SDE that satisfies the geometric progression property starting from t = 0. We name a new SDE as the Reciprocal VE SDE (RVESDE). RVESDE has the identical form of SDE, dx t = g RV E (t) dw t , with</p><formula xml:id="formula_58">g RV E (t) := ? max ?min ?max ) t 2 log ( ?max ? min ) t if t &gt; 0, 0 if t = 0.</formula><p>Then, the transition probability of RVESDE becomes</p><formula xml:id="formula_59">p 0t (x t |x 0 ) = N x t ; x 0 , ? 2 max ? min ? max 2 t I .</formula><p>As illustrated in <ref type="figure">Figure 9</ref>-(c), RVESDE attains the geometric property at the expense of having reciprocated time, 1/t. Also, RVESDE satisfies ? 2 RV E ( ) = ? 2 min and ? 2 RV E (T ) ? ? 2 max . The existence and uniqueness of solution for RVESDE is guaranteed by Theorem 5.2.1 in (Oksendal, 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Experimental Details</head><p>Training Throughout the experiments, we train our model with a learning rate of 0.0002, warmup of 5000 iterations, and gradient clipping by 1. For UNCSN++, we take ? min = 10 ?3 , and for NCSN++, we take ? min = 10 ?2 . On ImageNet32 training of the likelihood weighting and the variance weighting without Soft Truncation, we take = 5 ? 10 ?5 , following the setting of <ref type="bibr">Song et al. (2021a)</ref>. Otherwise, we take = 10 ?5 . For other hyperparameters, we run our experiments according to <ref type="bibr">Song et al. (2021b;</ref><ref type="bibr">a)</ref>.</p><p>On datasets of resolution 32 ? 32, we use the batch size of 128, which consumes about 48Gb GPU memory. On STL-10 with resolution 48 ? 48, we use the batch size of 192, and on datasets of resolution 64 ? 64, we experiment with 128 batch size. The batch size for the datasets of resolution 256 ? 256 is 40, which takes nearly 120Gb of GPU memory. On the dataset of 1024 ? 1024 resolution, we use the batch size of 16, which takes around 120Gb of GPU memory. We use five NVIDIA RTX-3090 GPU machines to train the model exceeding 48Gb, and we use a pair of NVIDIA RTX-3090 GPU machines to train the model that consumes less than 48Gb.</p><p>Evaluation We apply the EMA with rate of 0.999 on NCSN++/UNCSN++ and 0.9999 on DDPM++/UDDPM++. For the density estimation, we obtain the NLL performance by the Instantaneous Change of Variable <ref type="bibr">(Song et al., 2021b;</ref><ref type="bibr" target="#b1">Chen et al., 2018)</ref>. We choose [ = 10 ?5 , T = 1] to integrate the instantaneous change-of-variable of the probability flow as default, even for the ImageNet32 dataset. In spite that Song et al. <ref type="figure" target="#fig_0">(2021b;a)</ref> integrates the change-of-variable formula with the We dequantize the data variable by the uniform dequantization <ref type="bibr" target="#b12">(Ho et al., 2019)</ref> for both after-and-before corrections. In the main paper, we only report the after correction performances.</p><p>For the sampling, we apply the Predictor-Corrector (PC) algorithm introduced in Song et al. <ref type="bibr">(2021b)</ref>. We set the signal-tonoise ratio as 0.16 on 32 ? 32 datasets, 0.17 on 48 ? 48 and 64 ? 64 datasets, 0.075 on 256?256 sized datasets, and 0.15 on 1024 ? 1024. On datasets less than 256 ? 256 resolution, we iterate 1,000 steps for the PC sampler, while we apply 2,000 steps on the other high-dimensional datasets. Throughout the experiments for VESDE, we use the reverse diffusion (Song et al., 2021b) for the predictor algorithm and the annealed Langevin dynamics (Welling &amp; Teh, 2011) for the corrector algorithm. For VPSDE, we use the Euler-Maruyama for the predictor algorithm, and we do not use any corrector algorithm.</p><p>We compute the FID score (Song et al., 2021b) based on the modified Inception V1 network 3 using the tensorflow-gan package for CIFAR-10 dataset, and we use the clean-FID (Parmar et al., 2022) based on the Inception V3 network <ref type="bibr">(Szegedy et al., 2016)</ref> for the remaining datasets. We note that FID computed by (Parmar et al., 2022) reports a higher FID score compared to the original FID calculation 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Ablation Study on Reconstruction Term</head><p>Table 10 presents that the training with the reconstruction term outperforms the training without the reconstruction term on NLL/NELBO with the sacrifice on sample generation. If ? is fixed as , then the bound E x0 ? log p ? 0 (x 0 ) ? L(?; g 2 , ? ) + E x0,x? ? log p(x 0 |x ? )</p><p>is tight enough to estimate the negative log-likelihood. However, if ? is a subject of random variable, then the bound is not tight to the negative log-likelihood, as evidenced in <ref type="figure" target="#fig_0">Figure 1-(b)</ref>. On the other hand, if we do not count the reconstruction, then the bound becomes E x0 ? log p ? ? (x ? ) ? L(?; g 2 , ? ), up to a constant, and this bound becomes tight regardless of ? , which is evidenced in <ref type="figure" target="#fig_0">Figure 1-(c)</ref>. This is why we call Soft Truncation as Maximum Perturbed Likelihood Estimation (MPLE). <ref type="table" target="#tab_1">Tables   Tables 11, 12</ref>, 13, 14, and 15 present the full list of performances for Soft Truncation.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Full</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The contribution of diffusion time on the variational bound experimented on CIFAR-10 with DDPM++ (VP, NLL) (Song et al., 2021a). (a) The integrand of the variational bound is extremely imbalanced on [ , T ]. (b) The truncated variational bound only changes near ? ? 0. (c) The truncation hyperparameter ( ) is a significant factor for performances. The NCSN loss training is connected to the likelihood training in Song et al. (2021a) by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the generative process trained on CelebA-HQ 256 ? 256 with NCSN++ (VE)(Song et al.,  2021b). The score precision on large diffusion time is key to construct the realistic overall sample quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Norm of reverse drift of generative process, trained on CIFAR-10 with DDPM++ (VP, FID) (Song et al., 2021b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 -Figure 5 :</head><label>15</label><figDesc>(c) reports test performances on density estimation. Figure 1-(c) illustrates that both Negative Evidence Lower Bound (NELBO) and NLL monotonically decrease Regenerated samples synthesized by solving the probability flow ODE on [ , ? ] backwards with the initial point of x ? = ?(? )x 0 + ?(? )z for z ? N (0, I), trained on CelebA with DDPM++ (VP, FID) (Song et al., 2021b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The experimental result trained on CIFAR-10 with DDPM++ (VP, NLL) (Song et al., 2021a). (a) The Monte-Carlo loss for each diffusion time, ? 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>deepens the importance of learning a good score estimation on large diffusion time. It shows the regenerated samples by solving the generative process time reversely, starting from x ? (Meng et al., 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The meaning of Soft Truncation becomes clearer in view of Theorem 1. Instead of training the general weighted diffusion loss, L(?; ?, ), we optimize the truncated variational bound, L(?; g 2 , ? ). This truncated loss upper bounds the perturbed KL divergence, D KL (p ? p ? ? ) by Lemma 1, andFigure 1-(c) indicates that the Inequality (6) is nearly tight. Therefore, Soft Truncation could be interpreted as the Maximum Perturbed Likelihood Estimation (MPLE), where the perturbation level is a random variable. Soft Truncation is not MLE training because the Inequality 8 is not tight as demonstrated inFigure 1-(b) unless ? is sufficiently small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>This section empirically studies our suggestions on benchmark datasets, including CIFAR-10 (Krizhevsky et al., 2009), ImageNet 32 ? 32 (Van Oord et al., 2016), STL-10 (Coates et al., 2011) 2 CelebA (Liu et al., 2015) 64 ? 64 and CelebA-HQ (Karras et al., 2018) 256 ? 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>LFigure 8 :</head><label>8</label><figDesc>ST (?; g 2 , P 2 ) Soft Truncation improves FID on CelebA trained with UNCSN++ (RVE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>t</head><label></label><figDesc>0 ?(s) ds to Eq. (14), then Eq. (14) equals to e t 0 ?(s) ds d? 2 (t) dt + e t 0 ?(s) ds ?(t)? 2 (t) = e t 0 ?(s) ds g 2 (t) s) ds g 2 (? ) d? + Ce ? t 0 ?(s) ds .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>s) ds g 2 (? ) d?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>p</head><label></label><figDesc>0t (x t |x 0 ) = N x t e ? 1 2 t 0 ?(s) ds x 0 , e ? t 0 ?(s) ds t 0 e ? 0 ?(s) ds g 2 (? ) d? I .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>s is locally Lipschitz}. Suppose a continuous vector field v defined on a subset U of a compact manifold M (i.e., v : U ? M ? R d ) is unbounded, then there exists no s ? H [0,T ] such that lim t?0 s(x, t) = v(x) a.e. on U . Proof of Lemma 2. Since U is an open subset of a compact manifold M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>? 2 (t) of VESDE d dt log ? 2 (1/t) of RVESDE (c) VESDE violates geometric progression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>diverges as t ? 0. (b) Comparison of DDPM++ and UDDPM++ in terms of the cumulative density function of the second input. (c) Comparison of VESDE and RVESDE in terms of d dt log ? 2 .Proposition 1. Let H [1,?) = {s : R d ? [1, ?) ? R d ,s is locally Lipschitz}. Suppose a continuous vector field v defined on a d-dimensional open subset U of a compact manifold M is unbounded, and the projection of v on each axis is locally integrable. Then, there exists s ? H [1,?) such that lim ??? s(x, ?) = v(x) a.e. on U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10</head><label>10</label><figDesc>shows how images are created from the trained model, and Figures from 11 to 16 present non-cherry picked generated samples of the trained model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation on ? min .</figDesc><table><row><cell>? min</cell><cell cols="2">CIFAR-10 NLL (?) FID-10k (?)</cell></row><row><cell>10 ?2</cell><cell>4.95</cell><cell>6.95</cell></row><row><cell>10 ?3</cell><cell>3.04</cell><cell>7.04</cell></row><row><cell>10 ?4</cell><cell>2.99</cell><cell>8.17</cell></row><row><cell>10 ?5</cell><cell>2.97</cell><cell>8.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID-10k scores.</figDesc><table><row><cell>? min</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>10 ?5</cell></row><row><cell>? tr = 1</cell><cell>6.84</cell><cell>8.04</cell><cell>8.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>) 4.4. Soft Truncation is Maximum Perturbed Likelihood Estimation As explained in Section 4.3, Soft Truncation is a diffusion model with a general weight, in the expected sense. Reversely, this section analyzes a diffusion model with a general weight in view of Soft Truncation. Suppose we have a general weight ?. Theorem 1 implies that this general weighted diffusion loss,L(?; ?, ), is the variational bound of the perturbed KL divergence expected by P ? (? ). Theorem 1 collapses to Lemma 1 if ?(t) = cg 2 (t) for any c &gt; 0 1 . See Appendix B for the detailed statement and proof.</figDesc><table><row><cell>Theorem 1. Suppose ?(t) g 2 (t) is a nondecreasing and nonneg-</cell></row><row><cell>ative absolutely continuous function on [ , T ] and zero on</cell></row><row><cell>[0, ). For the probability defined by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of Soft Truncation for various weightings on CIFAR-10 and ImageNet32 with DDPM++ (VP).</figDesc><table><row><cell></cell><cell>Loss</cell><cell>Soft Truncation</cell><cell>NLL</cell><cell>NELBO</cell><cell>FID ODE</cell></row><row><cell></cell><cell>L(?; g 2 , )</cell><cell></cell><cell>3.03</cell><cell>3.13</cell><cell>6.70</cell></row><row><cell>CIFAR-10</cell><cell>L(?; ? 2 , ) L(?; g 2 , ) P 1 L ST (?; g 2 , P 1 ) L ST (?; g 2 , P 0.9 )</cell><cell></cell><cell>3.21 3.06 3.01 3.03</cell><cell>3.34 3.18 3.08 3.13</cell><cell>3.90 6.11 3.96 3.45</cell></row><row><cell></cell><cell>L(?; g 2 , )</cell><cell></cell><cell>3.92</cell><cell>3.94</cell><cell>12.68</cell></row><row><cell>ImageNet32</cell><cell>L(?; ? 2 , ) L(?; g 2 , ) P 1 L ST (?; g 2 , P 0.9 )</cell><cell></cell><cell>3.95 3.93 3.90</cell><cell>4.00 3.97 3.91</cell><cell>9.22 11.89 8.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of Soft Truncation for various model architectures and diffusion SDEs on CelebA. It is the common practice of continuous diffusion models (Song et al., 2021b;a; Dockhorn et al., 2022) to report their performances with log p ? (x 0 ), but Kim et al. (2022) show that log p ? (x ) differs to log p ? (x 0 ) by 0.05 in BPD scale when = 10 ?5 , which is quite significant. We use the uniform dequantization (Theis et al., 2016) as default, otherwise noted.</figDesc><table><row><cell>SDE</cell><cell>Model</cell><cell>Loss</cell><cell>NLL</cell><cell>NELBO</cell><cell>PC</cell><cell>FID</cell><cell>ODE</cell></row><row><cell>VE</cell><cell>NCSN++</cell><cell>L(?; ? 2 , ) L ST (?; ? 2 , P 2 )</cell><cell>3.41 3.44</cell><cell>3.42 3.44</cell><cell>3.95 2.68</cell><cell></cell><cell>--</cell></row><row><cell>RVE</cell><cell>UNCSN++</cell><cell>L(?; g 2 , ) L ST (?; g 2 , P 2 )</cell><cell>2.01 1.97</cell><cell>2.01 2.02</cell><cell>3.36 1.92</cell><cell></cell><cell>--</cell></row><row><cell></cell><cell>DDPM++</cell><cell>L(?; ? 2 , ) L ST (?; ? 2 , P 1 )</cell><cell>2.14 2.17</cell><cell>2.21 2.29</cell><cell>3.03 2.88</cell><cell></cell><cell>2.32 1.90</cell></row><row><cell>VP</cell><cell>UDDPM++</cell><cell>L(?; ? 2 , ) L ST (?; ? 2 , P 1 )</cell><cell>2.11 2.16</cell><cell>2.20 2.28</cell><cell>3.23 2.22</cell><cell></cell><cell>4.72 1.94</cell></row><row><cell></cell><cell>DDPM++</cell><cell>L(?; g 2 , ) L ST (?; g 2 , P 1 )</cell><cell>2.00 2.00</cell><cell>2.09 2.11</cell><cell>5.31 4.50</cell><cell></cell><cell>3.95 2.90</cell></row><row><cell></cell><cell>UDDPM++</cell><cell>L(?; g 2 , ) L ST (?; g 2 , P 1 )</cell><cell>1.98 2.00</cell><cell>2.12 2.10</cell><cell>4.65 4.45</cell><cell></cell><cell>3.98 2.97</cell></row></table><note>tion on [0, ), we comply Kim et al. (2022) to use Inequality (7) for NLL computation and Inequality (8) for NELBO computation. Following Kim et al. (2022), we compute log p ? (x ), rather than log p ? (x 0 ).For sample generation, we use either of Predictor-Corrector (PC) sampler or Ordinary Differential Equation (ODE) sampler (Song et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of Soft Truncation for various on CIFAR-10 with DDPM++ (VP).</figDesc><table><row><cell>Loss</cell><cell></cell><cell>NLL</cell><cell>NELBO</cell><cell>FID (ODE)</cell></row><row><cell></cell><cell>10 ?2</cell><cell>4.64</cell><cell>4.69</cell><cell>38.82</cell></row><row><cell>L(?; g 2 , )</cell><cell>10 ?3 10 ?4</cell><cell>3.51 3.05</cell><cell>3.52 3.08</cell><cell>6.21 6.33</cell></row><row><cell></cell><cell>10 ?5</cell><cell>3.03</cell><cell>3.13</cell><cell>6.70</cell></row><row><cell></cell><cell>10 ?2</cell><cell>4.65</cell><cell>4.69</cell><cell>39.83</cell></row><row><cell>L ST (?; g 2 , P 1 )</cell><cell>10 ?3 10 ?4</cell><cell>3.51 3.05</cell><cell>3.52 3.08</cell><cell>5.14 4.16</cell></row><row><cell></cell><cell>10 ?5</cell><cell>3.01</cell><cell>3.08</cell><cell>3.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of Soft Truncation for various P k on CIFAR-10 trained with DDPM++ (VP).</figDesc><table><row><cell>Loss</cell><cell>NLL</cell><cell>NELBO</cell><cell>FID (ODE)</cell></row><row><cell>L ST (?; g 2 , P 0 ) L ST (?; g 2 , P 0.8 ) L ST (?; g 2 , P 0.9 ) L ST (?; g 2 , P 1 ) L ST (?; g 2 , P 1.1 ) L ST (?; g 2 , P 1.2 ) L ST (?; g 2 , P 2 ) L ST (?; g 2 , P 3 )</cell><cell>3.24 3.03 3.03 3.01 3.02 3.03 3.01 3.02</cell><cell>3.39 3.05 3.13 3.08 3.09 3.09 3.10 3.09</cell><cell>6.27 3.61 3.45 3.96 3.98 3.98 6.31 6.54</cell></row><row><cell>L ST (?; g 2 , P?) = L(?; g 2 , )</cell><cell>3.01</cell><cell>3.09</cell><cell>6.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of Soft Truncation for CIFAR-10 trained with DDPM++ when a diffusion is combined with a normalizing flow inINDM (Kim et al., 2022).</figDesc><table><row><cell>Loss</cell><cell>NLL</cell><cell>NELBO</cell><cell>FID (ODE)</cell></row><row><cell>INDM (VP, NLL)</cell><cell>2.98</cell><cell>2.98</cell><cell>6.01</cell></row><row><cell>INDM (VP, FID)</cell><cell>3.17</cell><cell>3.23</cell><cell>3.61</cell></row><row><cell>INDM (VP, NLL) + ST</cell><cell>3.01</cell><cell>3.02</cell><cell>3.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance comparisons on benchmark datasets. The boldfaced numbers present the best performance, and the underlined numbers present the second-best performance. We report NLL of DDPM++ on CIFAR-10, ImageNet32, and CelebA with the variational dequantization(Song et al., 2021a)  to compare with the baselines in a fair setting.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell>ImageNet32</cell><cell></cell><cell cols="2">CelebA</cell><cell>CelebA-HQ</cell><cell>STL-10</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">32 ? 32 NLL (?) FID (?) IS (?) NLL</cell><cell>32 ? 32 FID</cell><cell>IS</cell><cell cols="2">64 ? 64 NLL FID</cell><cell>256 ? 256 FID</cell><cell cols="2">48 ? 48 FID IS</cell></row><row><cell>Likelihood-free Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StyleGAN2-ADA+Tuning (Karras et al., 2020)</cell><cell>-</cell><cell>2.92</cell><cell>10.02</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Styleformer (Park &amp; Kim, 2022)</cell><cell>-</cell><cell>2.82</cell><cell>9.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.66</cell><cell>-</cell><cell cols="2">15.17 11.01</cell></row><row><cell>Likelihood-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ARDM-Upscale 4 (Hoogeboom et al., 2021)</cell><cell>2.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VDM (Kingma et al., 2021)</cell><cell>2.65</cell><cell>7.41</cell><cell>-</cell><cell>3.72</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LSGM (FID) (Vahdat et al., 2021)</cell><cell>3.43</cell><cell>2.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NCSN++ cont. (deep, VE) (Song et al., 2021b)</cell><cell>3.45</cell><cell>2.20</cell><cell>9.89</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">2.39 3.95</cell><cell>7.23</cell><cell>-</cell><cell>-</cell></row><row><cell>DDPM++ cont. (deep, sub-VP) (Song et al., 2021b)</cell><cell>2.99</cell><cell>2.41</cell><cell>9.57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DenseFlow-74-10 (Grci? et al., 2021)</cell><cell>2.98</cell><cell>34.90</cell><cell>-</cell><cell>3.63</cell><cell>-</cell><cell>-</cell><cell>1.99</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ScoreFlow (VP, FID) (Song et al., 2021a)</cell><cell>3.04</cell><cell>3.98</cell><cell>-</cell><cell>3.84</cell><cell>8.34</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Efficient-VDVAE (Hazami et al., 2022)</cell><cell>2.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.83</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PNDM (Liu et al., 2022)</cell><cell>-</cell><cell>3.26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.71</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ScoreFlow (deep, sub-VP, NLL) (Song et al., 2021a)</cell><cell>2.81</cell><cell>5.40</cell><cell>-</cell><cell cols="2">3.76 10.18</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Improved DDPM (L simple ) (Nichol &amp; Dhariwal, 2021)</cell><cell>3.37</cell><cell>2.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UNCSN++ (RVE) + ST</cell><cell>3.04</cell><cell>2.33</cell><cell>10.11</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">1.97 1.92</cell><cell>7.16</cell><cell cols="2">7.71 13.43</cell></row><row><cell>DDPM++ (VP, FID) + ST</cell><cell>2.91</cell><cell>2.47</cell><cell>9.78</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">2.10 1.90</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DDPM++ (VP, NLL) + ST</cell><cell>2.88</cell><cell>3.45</cell><cell>9.19</cell><cell>3.85</cell><cell cols="4">8.42 11.82 1.96 2.90</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">comparing the paired experiments, Soft Truncation could</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">be considered as an alternative way of estimating the same</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>loss, and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows a contrastive trend of the vanilla training and</cell></row><row><cell>Soft Truncation. The inverse correlation appears between</cell></row><row><cell>NLL and FID in the vanilla training, but Soft Truncation</cell></row><row><cell>monotonically reduces both NLL and FID by . This im-</cell></row><row><cell>plies that Soft Truncation significantly reduces the effort of</cell></row><row><cell>the search. Table 7 studies the effect of the probability dis-</cell></row><row><cell>tribution of ? in VPSDE. It shows that Soft Truncation sig-</cell></row><row><cell>nificantly improves FID upon the experiment of L(?; g 2 , ) on the range of 0.8 ? k ? 1.2. Finally, Table 8 shows that Soft Truncation also works with a nonlinear forward SDE</cell></row><row><cell>(Kim et al., 2022), so the scope of Soft Truncation is not</cell></row><row><cell>limited to a family of linear SDEs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021. Oksendal, B. Stochastic differential equations: an introduction with applications. Vahdat, A., Kreis, K., and Kautz, J. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34, 2021. Van Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. In International Conference on Machine Learning, pp. 1747-1756. PMLR, 2016. .1. Transition Probability for Linear SDEs Kim et al. (2022) has classified linear SDEs as</figDesc><table><row><cell>A. Derivation</cell><cell></cell></row><row><cell>Springer Science &amp; Business</cell><cell></cell></row><row><cell>Media, 2013.</cell><cell></cell></row><row><cell>Park, J. and Kim, Y. Styleformer: Transformer based gener-</cell><cell>Huang, C.-W., Lim, J. H., and Courville, A. C. A varia-Vargas, F., Thodoroff, P., Lamacraft, A., and Lawrence,</cell></row><row><cell>ative adversarial networks with style vector. Proceedings</cell><cell>tional perspective on diffusion-based generative models N. Solving schr?dinger bridges via maximum likelihood.</cell></row><row><cell>of the IEEE/CVF International Conference on Computer</cell><cell>and score matching. Advances in Neural Information Entropy, 23(9):1134, 2021.</cell></row><row><cell>Vision, 2022.</cell><cell>Processing Systems, 34, 2021. Welling, M. and Teh, Y. W. Bayesian learning via stochastic</cell></row><row><cell>Parmar, G., Zhang, R., and Zhu, J.-Y. On buggy resizing</cell><cell>Jiang, Y., Chang, S., and Wang, Z. Transgan: Two pure gradient langevin dynamics. In Proceedings of the 28th</cell></row><row><cell>libraries and surprising subtleties in fid calculation. Pro-</cell><cell>transformers can make one strong gan, and that can scale international conference on machine learning (ICML-11),</cell></row><row><cell>ceedings of the IEEE/CVF International Conference on</cell><cell>up. Advances in Neural Information Processing Systems, pp. 681-688. Citeseer, 2011.</cell></row><row><cell>Computer Vision, 2022.</cell><cell>34, 2021.</cell></row><row><cell>Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer. In Interna-tional Conference on Machine Learning, pp. 4055-4064. PMLR, 2018.</cell><cell>Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progres-sive growing of gans for improved quality, stability, and variation. In International Conference on Learning Rep-resentations, 2018.</cell></row><row><cell>Pavon, M. and Wakolbinger, A. On free energy, stochastic control, and schr?dinger processes. In Modeling, Es-timation and Control of Systems with Uncertainty, pp. 334-348. Springer, 1991.</cell><cell>Karras, T., Laine, S., and Aila, T. A style-based genera-tor architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4401-4410, 2019.</cell></row><row><cell>Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.</cell><cell>Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T. Training generative adversarial networks with limited data. Advances in Neural Information Pro-</cell></row><row><cell>Song, Y. and Ermon, S. Improved techniques for train-</cell><cell>cessing Systems, 33:12104-12114, 2020.</cell></row><row><cell>ing score-based generative models. Advances in neural information processing systems, 33:12438-12448, 2020.</cell><cell>Kim, D., Na, B., Kwon, S. J., Lee, D., Kang, W., and Moon, I.-C. Maximum likelihood training of implicit nonlin-</cell></row><row><cell>Song, Y., Durkan, C., Murray, I., and Ermon, S. Maximum</cell><cell>ear diffusion models. arXiv preprint arXiv:2205.13699,</cell></row><row><cell>likelihood training of score-based diffusion models. Ad-</cell><cell>2022.</cell></row><row><cell>vances in Neural Information Processing Systems, 34, 2021a.</cell><cell>Kingma, D. P., Salimans, T., Poole, B., and Ho, J. Varia-tional diffusion models. In Advances in Neural Informa-</cell></row><row><cell>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-</cell><cell>tion Processing Systems, 2021.</cell></row><row><cell>mon, S., and Poole, B. Score-based generative modeling</cell><cell></cell></row><row><cell>through stochastic differential equations. In International</cell><cell>Krizhevsky, A., Hinton, G., et al. Learning multiple layers</cell></row><row><cell>Conference on Learning Representations, 2021b.</cell><cell>of features from tiny images. 2009.</cell></row><row><cell>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,</cell><cell>Liu, L., Ren, Y., Lin, Z., and Zhao, Z. Pseudo numeri-</cell></row><row><cell>Z. Rethinking the inception architecture for computer vi-</cell><cell>cal methods for diffusion models on manifolds. arXiv</cell></row><row><cell>sion. In Proceedings of the IEEE conference on computer</cell><cell>preprint arXiv:2202.09778, 2022.</cell></row><row><cell>vision and pattern recognition, pp. 2818-2826, 2016.</cell><cell>Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning</cell></row><row><cell>Theis, L., van den Oord, A., and Bethge, M. A note on</cell><cell>face attributes in the wild. In Proceedings of the IEEE</cell></row><row><cell>the evaluation of generative models. In International</cell><cell>international conference on computer vision, pp. 3730-</cell></row><row><cell>Conference on Learning Representations (ICLR 2016),</cell><cell>3738, 2015.</cell></row><row><cell>pp. 1-10, 2016.</cell><cell></cell></row><row><cell>Vahdat, A. and Kautz, J. Nvae: A deep hierarchical vari-</cell><cell></cell></row><row><cell>ational autoencoder. Advances in Neural Information</cell><cell></cell></row><row><cell>Processing Systems, 33:19667-19679, 2020.</cell><cell></cell></row></table><note>Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021.A</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Ablation study of Soft Truncation with/without the reconstruction term when training on CIFAR-10 trained with DDPM++ (VP). ,Table 5ofKim et al. (2022)  analyzes that there are significant difference between starting fromx and x 0 , if is not small enough. Therefore, we followKim et al. (2022)  to compute E x ? log p ? (x ) . However, to compare with the baseline models, we also evaluate the way Song et al.(2021b;a) and Vahdat et al. (2021) compute NLL. We denote the way of Kim et al. (2022) as after correction and Song et al. (2021a) as before correction, throughout the appendix.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NLL</cell><cell cols="2">NELBO</cell><cell>FID</cell></row><row><cell>Loss</cell><cell>Soft Truncation</cell><cell>Reconstruction Term for Training</cell><cell>Ex 0 [? log p ? (x 0 )] (before correction)</cell><cell>Ex [? log p ? (x )] + R (?) (after correction)</cell><cell>L(?; g 2 , ) (without residual)</cell><cell>L(?; g 2 , ) residual) +R (?) (with</cell><cell>ODE</cell></row><row><cell>L(?; g 2 , )</cell><cell></cell><cell></cell><cell>2.97</cell><cell>3.03</cell><cell>3.11</cell><cell>3.13</cell><cell>6.70</cell></row><row><cell>L(?; g 2 , ) + Ex 0 ,x ? log p(x 0 |x )</cell><cell></cell><cell></cell><cell>3.01</cell><cell>2.99</cell><cell>3.07</cell><cell>3.09</cell><cell>6.93</cell></row><row><cell>L ST (?; g 2 , P 1 ) = E P 1 (? ) L(?; g 2 , ? ) = E P 1 (? ) L(?; g 2 , ? )</cell><cell></cell><cell></cell><cell>2.98</cell><cell>3.01</cell><cell>3.08</cell><cell>3.08</cell><cell>3.96</cell></row><row><cell>E P 1 (? ) L(?; g 2 , ? ) + R? (?)</cell><cell></cell><cell></cell><cell>2.95</cell><cell>2.98</cell><cell>3.04</cell><cell>3.04</cell><cell>4.23</cell></row><row><cell>starting variable to be x 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Ablation study of Soft Truncation for various weightings on CIFAR-10 and ImageNet32 with DDPM++ (VP).</figDesc><table><row><cell>Dataset</cell><cell>Loss</cell><cell>Soft Truncation</cell><cell>after correction</cell><cell>NLL</cell><cell>before correction</cell><cell cols="2">NELBO residual with without residual</cell><cell>FID ODE</cell></row><row><cell></cell><cell>L(?; g 2 , )</cell><cell></cell><cell>3.03</cell><cell></cell><cell>2.97</cell><cell>3.13</cell><cell>3.11</cell><cell>6.70</cell></row><row><cell>CIFAR-10</cell><cell>L(?; ? 2 , ) L(?; g 2 , ) P 1 L ST (?; g 2 , P 1 )</cell><cell></cell><cell>3.21 3.06 3.01</cell><cell></cell><cell>3.16 3.02 2.98</cell><cell>3.34 3.18 3.08</cell><cell>3.32 3.14 3.08</cell><cell>3.90 6.11 3.96</cell></row><row><cell></cell><cell>L(?; g 2 , )</cell><cell></cell><cell>3.92</cell><cell></cell><cell>3.90</cell><cell>3.94</cell><cell>3.95</cell><cell>12.68</cell></row><row><cell></cell><cell>L(?; ? 2 , )</cell><cell></cell><cell>3.95</cell><cell></cell><cell>3.96</cell><cell>4.00</cell><cell>4.01</cell><cell>9.22</cell></row><row><cell>ImageNet32</cell><cell>L(?; g 2 P 1 L ST (?; g 2 , P 1 ) , ) L ST (?; g 2 , P 0.9 )</cell><cell></cell><cell>3.93 3.90 3.90</cell><cell></cell><cell>3.92 3.87 3.88</cell><cell>3.97 3.92 3.91</cell><cell>3.98 3.92 3.91</cell><cell>11.89 9.52 8.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Ablation study of Soft Truncation for various model architectures and diffusion SDEs on CelebA.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>NLL</cell><cell></cell><cell cols="2">NELBO</cell><cell>FID</cell><cell></cell></row><row><cell>SDE</cell><cell>Model</cell><cell>Loss</cell><cell>after correction</cell><cell>before correction</cell><cell>with residual</cell><cell>without residual</cell><cell>PC</cell><cell>ODE</cell></row><row><cell>VE</cell><cell>NCSN++</cell><cell>L(?; ? 2 , ) L ST (?; ? 2 , P 2 )</cell><cell>3.41 3.44</cell><cell>2.37 2.42</cell><cell>3.42 3.44</cell><cell>3.96 3.97</cell><cell>3.95 2.68</cell><cell>--</cell></row><row><cell>RVE</cell><cell>UNCSN++</cell><cell>L(?; g 2 , ) L ST (?; g 2 , P 2 )</cell><cell>2.01 1.97</cell><cell>1.96 1.91</cell><cell>2.01 2.02</cell><cell>2.17 2.18</cell><cell>3.36 1.92</cell><cell>--</cell></row><row><cell></cell><cell>DDPM++</cell><cell>L(?; ? 2 , ) L ST (?; ? 2 , P 1 )</cell><cell>2.14 2.17</cell><cell>2.07 2.08</cell><cell>2.21 2.29</cell><cell>2.22 2.26</cell><cell>3.03 2.88</cell><cell>2.32 1.90</cell></row><row><cell>VP</cell><cell>UDDPM++</cell><cell>L(?; ? 2 , ) L ST (?; ? 2 , P 1 )</cell><cell>2.11 2.16</cell><cell>2.07 2.08</cell><cell>2.20 2.28</cell><cell>2.21 2.25</cell><cell>3.23 2.22</cell><cell>4.72 1.94</cell></row><row><cell></cell><cell>DDPM++</cell><cell>L(?; g 2 , ) L ST (?; g 2 , P 1 )</cell><cell>2.00 2.00</cell><cell>1.93 1.94</cell><cell>2.09 2.11</cell><cell>2.09 2.11</cell><cell>5.31 4.50</cell><cell>3.95 2.90</cell></row><row><cell></cell><cell>UDDPM++</cell><cell>L(?; g 2 , ) L ST (?; g 2 , P 1 )</cell><cell>1.98 2.00</cell><cell>1.95 1.94</cell><cell>2.12 2.10</cell><cell>2.15 2.10</cell><cell>4.65 4.45</cell><cell>3.98 2.97</cell></row><row><cell>E.3. Generated Samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Ablation study of Soft Truncation for various ? min (equivalently, ) on CIFAR-10 with UNCSN++ (RVE).</figDesc><table><row><cell></cell><cell></cell><cell>NLL</cell><cell></cell><cell cols="2">NELBO</cell><cell>FID</cell></row><row><cell>Loss</cell><cell></cell><cell>after correction</cell><cell>before correction</cell><cell>with residual</cell><cell>without residual</cell><cell>ODE</cell></row><row><cell></cell><cell>10 ?2</cell><cell>4.64</cell><cell>4.02</cell><cell>4.69</cell><cell>5.20</cell><cell>38.82</cell></row><row><cell>L(?; g 2 , )</cell><cell>10 ?3 10 ?4</cell><cell>3.51 3.05</cell><cell>3.20 2.98</cell><cell>3.52 3.08</cell><cell>3.90 3.24</cell><cell>6.21 6.33</cell></row><row><cell></cell><cell>10 ?5</cell><cell>3.03</cell><cell>2.97</cell><cell>3.13</cell><cell>3.11</cell><cell>6.70</cell></row><row><cell></cell><cell>10 ?2</cell><cell>4.65</cell><cell>4.03</cell><cell>4.69</cell><cell>5.20</cell><cell>39.83</cell></row><row><cell>L ST (?; g 2 , P 1 )</cell><cell>10 ?3 10 ?4</cell><cell>3.51 3.05</cell><cell>3.21 2.98</cell><cell>3.52 3.08</cell><cell>3.88 3.24</cell><cell>5.14 4.16</cell></row><row><cell></cell><cell>10 ?5</cell><cell>3.01</cell><cell>2.98</cell><cell>3.08</cell><cell>3.08</cell><cell>3.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Ablation study of Soft Truncation for various P k on CIFAR-10 trained with DDPM++ (VP).</figDesc><table><row><cell></cell><cell>NLL</cell><cell></cell><cell cols="2">NELBO</cell><cell>FID</cell></row><row><cell>Loss</cell><cell>after correction</cell><cell>before correction</cell><cell>with residual</cell><cell>without residual</cell><cell>ODE</cell></row><row><cell>L ST (?; g 2 , P 0 ) L ST (?; g 2 , P 0.8 ) L ST (?; g 2 , P 0.9 ) L ST (?; g 2 , P 1 ) L ST (?; g 2 , P 1.1 ) L ST (?; g 2 , P 1.2 ) L ST (?; g 2 , P 2 ) L ST (?; g 2 , P 3 )</cell><cell>3.24 3.03 3.03 3.01 3.02 3.03 3.01 3.02</cell><cell>3.16 3.00 2.99 2.98 2.99 2.99 2.97 2.96</cell><cell>3.39 3.05 3.13 3.08 3.09 3.09 3.10 3.09</cell><cell>3.34 3.05 3.13 3.08 3.10 3.09 3.09 3.09</cell><cell>6.27 3.61 3.45 3.96 3.98 3.98 6.31 6.54</cell></row><row><cell>L ST (?; g 2 , P?) = L(?; g 2 , )</cell><cell>3.01</cell><cell>2.95</cell><cell>3.09</cell><cell>3.07</cell><cell>6.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Ablation study of Soft Truncation for CIFAR-10 trained with DDPM++ when a diffusion is combined with a normalizing flow(Kim et al., 2022). We use P([a, b]) = 1 2 1 [a,b] ( ) + 1 2 P 0.9 ([a, b]).</figDesc><table><row><cell></cell><cell>NLL</cell><cell></cell><cell cols="2">NELBO</cell><cell>FID</cell></row><row><cell>Loss</cell><cell>after correction</cell><cell>before correction</cell><cell>with residual</cell><cell>without residual</cell><cell>ODE</cell></row><row><cell>L(?; g 2 , )</cell><cell>2.97</cell><cell>2.94</cell><cell>2.97</cell><cell>2.96</cell><cell>6.06</cell></row><row><cell>L(?; ? 2 , )</cell><cell>3.17</cell><cell>3.11</cell><cell>3.23</cell><cell>3.18</cell><cell>3.61</cell></row><row><cell>L(?; g 2 , P)</cell><cell>3.01</cell><cell>2.98</cell><cell>3.02</cell><cell>3.01</cell><cell>3.89</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">KAIST, South Korea 2 University of Seoul, South Korea 3 Summary.AI. Correspondence to: Dongjun Kim &lt;dongjoun57@kaist.ac.kr&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We downsize the dataset from 96 ? 96 to 48 ? 48 following Jiang et al. (2021); Park &amp; Kim (2022).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://tfhub.dev/tensorflow/tfgan/eval/inception/1 4 See https://github.com/GaParmar/clean-fid for the detailed experimental results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and ICT(2022-0-00077). We thank Jaeyoung Byeon and Daehan Park for their fruitful mathematical advice, and Byeonghu Na for his support of the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Likelihood training of schr?dinger bridge using forward-backward SDEs theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Score-based generative modeling with critically-damped langevin diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Partial differential equations. Graduate studies in mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fano&apos;s inequality for random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerchinovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?nard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stoltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="201" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grubi?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thurairatnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientvdvae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13751</idno>
		<title level="m">Less is more</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02037</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Autoregressive diffusion models. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
