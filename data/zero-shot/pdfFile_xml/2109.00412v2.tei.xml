<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<email>huichen@mymail.sutd.edu.sgsporia@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain taskrelated information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach. The implementation of this work is publicly available at https://github.com/ declare-lab/Multimodal-Infomax.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the unprecedented advances in social media in recent years and the availability of smartphones with high-quality cameras, we witness an explosive boost of multimodal data, such as movies, short-form videos, etc. In real life, multimodal data usually consists of three channels: visual (image), acoustic (voice), and transcribed text. Many of them often express sort of sentiment, which is a long-term disposition evoked when a person encounters a specific topic, person or entity <ref type="bibr" target="#b10">(Deonna and Teroni, 2012;</ref>. Mining and understanding these emotional elements from multimodal data, namely multimodal sentiment analysis (MSA), has become a hot research topic because of numerous appealing applications, such as obtaining overall product feedback from customers or gauging polling intentions from potential voters <ref type="bibr" target="#b23">(Melville et al., 2009)</ref>. Generally, different modalities in the same data segment are often complementary to each other, providing extra cues for semantic and emotional disambiguation <ref type="bibr" target="#b25">(Ngiam et al., 2011)</ref>. The crucial part for MSA is multimodal fusion, in which a model aims to extract and integrate information from all input modalities to understand the sentiment behind the seen data. Existing methods to learn unified representations are grouped in two categories: through loss back-propagation or geometric manipulation in the feature spaces. The former only tunes the parameters based on back-propagated gradients from the task loss <ref type="bibr" target="#b34">Tsai et al., 2019a;</ref>, reconstruction loss <ref type="bibr" target="#b22">(Mai et al., 2020)</ref>, or auxiliary task loss <ref type="bibr" target="#b40">Yu et al., 2021)</ref>. The latter additionally rectifies the spatial orientation of unimodal or multimodal representations by matrix decomposition <ref type="bibr" target="#b21">(Liu et al., 2018)</ref> or Euclidean measure optimization <ref type="bibr" target="#b32">(Sun et al., 2020;</ref>. Although having gained excellent results in MSA tasks, these methods are limited to the lack of control in the information flow that starts from raw inputs till the fusion embeddings, which may risk losing practical information and introducing unexpected noise carried by each modality <ref type="bibr" target="#b36">(Tsai et al., 2020)</ref>. To alleviate this issue, different from previous work, we leverage the functionality of mutual information (MI), a concept from the subject of information theory. MI measures the dependencies between paired multi-dimensional variables. Maximizing MI has been demonstrated ef-ficacious in removing redundant information irrelevant to the downstream task and capturing invariant trends or messages across time or different domains <ref type="bibr" target="#b29">(Poole et al., 2019)</ref>, and has been shown remarkable success in the field of representation learning <ref type="bibr" target="#b37">Veli?kovi? et al., 2018)</ref>. Based on these experience, we propose MultiModal InfoMax (MMIM), a framework that hierarchically maximizes the mutual information in multimodal fusion. Specifically, we enhance two types of mutual information in representation pairs: between unimodal representations and between fusion results and their low-level unimodal representations. Due to the intractability of mutual information <ref type="bibr" target="#b6">(Belghazi et al., 2018)</ref>, researchers always boost MI lower bound instead for this purpose. However, we find it is still difficult to figure out some terms in the expressions of these lower bounds in our formulation. Hence for convenient and accurate estimation of these terms, we propose a hybrid approach composed of parametric and non-parametric parts based on data and model characteristics. The parametric part refers to neural network-based methods, and in the non-parametric part we exploit a Gaussian Mixture Model (GMM) with learning-free parameter estimation. Our contributions can be summarized as follows:</p><p>1. We propose a hierarchical MI maximization framework for multimodal sentiment analysis. MI maximization occurs at the input level and fusion level to reduce the loss of valuable task-related information. To our best knowledge, this is the first attempt to bridge MI and MSA.</p><p>2. We formulate the computation details in our framework to solve the intractability problem. The formulation includes parametric learning and non-parametric GMM with stable and smooth parameter estimation.</p><p>3. We conduct comprehensive experiments on two publicly available datasets and gain superior or comparable results to the state-ofthe-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly overview some related work in multimodal sentiment analysis and mutual information estimation and application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multimodal Sentiment Analysis (MSA)</head><p>MSA is an NLP task that collects and tackles data from multiple resources such as acoustic, visual, and textual information to comprehend varied human emotions <ref type="bibr" target="#b24">(Morency et al., 2011)</ref>. Early fusion models adopted simple network architectures, such as RNN based models <ref type="bibr" target="#b39">(W?llmer et al., 2013;</ref> that capture temporal dependencies from low-level multimodal inputs, SAL-CNN  which designed a selectadditive learning procedure to improve the generalizability of trained neural networks, etc. Meanwhile, there were many trials to combine geometric measures as accessory learning goals into deep learning frameworks. For instance, <ref type="bibr" target="#b14">Hazarika et al. (2018)</ref>; <ref type="bibr" target="#b32">Sun et al. (2020)</ref> optimized the deep canonical correlation between modality representations for fusion and then passed the fusion result to downstream tasks. More recently, formulations influenced by novel machine learning topics have emerged constantly: Akhtar et al. <ref type="formula" target="#formula_0">(2019)</ref> presented a deep multi-task learning framework to jointly learn sentiment polarity and emotional intensity in a multimodal background. <ref type="bibr" target="#b28">Pham et al. (2019)</ref> proposed a method that cyclically translates between modalities to learn robust joint representations for sentiment analysis. <ref type="bibr" target="#b36">Tsai et al. (2020)</ref> proposed a routing procedure that dynamically adjusts weights among modalities to provide interpretability for multimodal fusion. Motivated by advances in the field of domain separation,  projected modality features into private and common feature spaces to capture exclusive and shared characteristics across different modalities. <ref type="bibr" target="#b40">Yu et al. (2021)</ref> designed a multi-label training scheme that generates extra unimodal labels for each modality and concurrently trained with the main task.</p><p>In this work, we build up a hierarchical MImaximization guided model to improve the fusion outcome as well as the performance in the downstream MSA task, where MI maximization is realized not only between unimodal representations but also between fusion embeddings and unimodal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mutual Information in Deep Learning</head><p>Mutual information (MI) is a concept from information theory that estimates the relationship between pairs of variables. It is a reparameterizationinvariant measure of dependency <ref type="bibr">(Tishby and Za-slavsky, 2015)</ref> defined as:</p><formula xml:id="formula_0">I(X; Y ) = E p(x,y) log p(x, y) p(x)p(y)<label>(1)</label></formula><p>Alemi et al. <ref type="bibr">(2016)</ref> first combined MI-related optimization into deep learning models. From then on, numerous works studied and demonstrated the benefit of the MI-maximization principle <ref type="bibr" target="#b4">(Bachman et al., 2019;</ref><ref type="bibr" target="#b16">He et al., 2020;</ref><ref type="bibr" target="#b2">Amjad and Geiger, 2019)</ref>. However, since direct MI estimation in high-dimensional spaces is nearly impossible, many works attempted to approximate the true value with variational bounds <ref type="bibr" target="#b6">(Belghazi et al., 2018;</ref><ref type="bibr" target="#b8">Cheng et al., 2020;</ref><ref type="bibr" target="#b29">Poole et al., 2019)</ref>.</p><p>In our work, we apply MI lower bounds at both the input level and fusion level and formulate or reformulate estimation methods for these bounds based on data characteristics and mathematical properties of the terms to be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>In MSA tasks, the input to a model is unimodal raw sequences X m ? R lm?dm drawn from the same video fragment, where l m is the sequence length and d m is the representation vector dimension of modality m, respectively. Particularly, in this paper we have m ? {t, v, a}, where t, v, a denote the three types of modalities-text, visual and acoustic that we obtained from the datasets. The goal for the designed model is to extract and integrate task-related information from these input vectors to form a unified representation and then utilize that to make accurate predictions about a truth value y that reflects the sentiment strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model firstly processes raw input into numerical sequential vectors with feature extractor (firmware for visual and acoustic with no parameters to train) and tokenizer (for text). Then we encode them into individual unitlength representations. The model then works in two collaborative parts-fusion and MI maximization, marked by solid and dash lines in <ref type="figure" target="#fig_0">Figure  1</ref> respectively. In the fusion part, a fusion network F of stacked linear-activation layers transforms the unimodal representations into the fusion result Z, which is then passed through a regression multilayer perceptron (MLP) for final predictions. In the MI part, the MI lower bounds at two levels-input level and fusion level are estimated and boosted. The two parts work concurrently to produce task and MI-related losses for back-propagation, through which the model learns to infuse the task-related information into fusion results as well as improve the accuracy of predictions in the main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modality Encoding</head><p>We firstly encode the multimodal sequential input X m into unit-length representations h m . Specifically, we use BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> to encode an input sentence and extract the head embedding from the last layer's output as h t . For visual and acoustic, following previous works <ref type="bibr" target="#b40">Yu et al., 2021)</ref>, we employ two modality-specific unidirectional LSTMs (Hochreiter and Schmidhuber, 1997) to capture the temporal features of these modalities:</p><formula xml:id="formula_1">h t = BERT X t ; ? BERT t h m = sLSTM X m ; ? LST M m m ? {v, a}<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inter-modality MI Maximization</head><p>For a modality representation pair X, Y that comes from a single video clip, although they seem to be independent sequences, there is a certain correlation between them <ref type="bibr" target="#b3">(Arandjelovic and Zisserman, 2017)</ref>. Formally, suppose we have a collection of videos V and assume that their prior distributions are known. Then the prior distribution of X and Y can be decomposed by the sampling process in V as</p><formula xml:id="formula_2">P (X) = V P (X|V )P (V ) and P (Y ) = V P (Y |V )P (V ), as well as their joint distribu- tion P (X, Y ) = V P (X, Y |V )P (V ). Unless P (X, Y |V ) = P (X|V )P (Y |V ),</formula><p>i.e., X and Y are conditionally independent from V , the MI is never trivially 0.</p><p>Since the analysis above, we hope that through prompting MI between multimodal input we can filter out modality-specific random noise that is irrelevant to our task and keep modality-invariant contents that span all modalities as much as possible. As stated before, we boost a tractable lower bound instead of computing MI directly for this purpose. We exploit an accurate and straightforward MI lower bound introduced in <ref type="bibr" target="#b5">Barber and Agakov (2004)</ref>. It approximates the truth conditional distribution p(y|x) with a variational coun-  terpart q(y|x):</p><formula xml:id="formula_3">I(X; Y ) =E p(x,y) log q(y|x) p(y) + E p(y) [KL(p(y|x) q(y|x))] ?E p(x,y) [log q(y|x)] + H(Y ) I BA (3)</formula><p>where H(Y ) is the differential entropy of Y . This lower bound is tight, i.e., there is no gap between the bound and truth value, when q(y|x) = p(y|x). In our implementation, we optimize the bounds for two modality pairs-(text, visual) and (text, acoustic). In each pair, we treat text as X and the other modality as Y in <ref type="formula">(3)</ref>. We do so because 1) Since we have to train a predictor q(y|x) to approximate p(y|x), prediction from higher-</p><formula xml:id="formula_4">dimensional vectors h t ? R dt (d t =768) to lower ones h v ? R dv and h a ? R da (d v , d a<label>?</label></formula><p>50) converges faster with higher accuracy; 2) many previous works <ref type="bibr" target="#b34">(Tsai et al., 2019a;</ref> pointed out that from empirical study the text modality is predominate, which can integrate more task-related features than other modalities in this step. Additionally, we examine the efficacy of the design choice in the ablation study part. Following <ref type="bibr" target="#b8">Cheng et al. (2020)</ref>, we formulate q(y|x) as a multivariate Gaussian distributions q ? (y|x) = N (y|? ? 1 (x), ? 2 ? 2 (x)I), with two neural networks parameterized by ? 1 and ? 2 to predict the mean and variance, respectively. The loss function for likelihood maximization is:</p><formula xml:id="formula_5">L lld = ? 1 N tv,ta N i=1 log q(y i |x i )<label>(4)</label></formula><p>where N is the batch size in training, tv, ta means summing the likelihood of two predictors. For the entropy term H(Y ), we solve its computation with the Gaussian Mixture Model (GMM), a commonly utilized approach for unknown distribution approximation that can facilitate distribution-based estimation <ref type="bibr" target="#b26">(Nilsson et al., 2002;</ref><ref type="bibr">Kerroum et al., 2010)</ref>. GMM builds up multiple Gaussian distributions for different property classes. We choose the sentiment polarity (non-negative/negative), which is a natural property in the datasets, as the classification criterion, which can also balance the trade-off between estimation accuracy (requires more classes) and computational cost (requires fewer classses). We build up two normal distributions N pos (? 1 , ? 1 ) and N neg (? 2 , ? 2 ) for each class, where ? is the mean vector and ? is the covariance matrix. The parameters are estimated via the maximum likelihood method on a sufficiently large sampling batch D s ? D train :</p><formula xml:id="formula_6">? c = 1 N c Nc i=1 h i ? ? c = 1 N c Nc i=1 h i c h i c ?? T c? c<label>(5)</label></formula><p>where c ? {pos, neg} represents the polarity class that the sample belongs to, N c is the number of samples in class c and is component-wise multiplication. The entropy of a multivariate normal distribution is given by:</p><formula xml:id="formula_7">H = 1 2 log (2?e) k det(?) = log(det(2?e?)) 2 (6)</formula><p>where k is the dimensionality of the vectors in GMM and det(?) is the determinant of ?. Based on the nearly equal frequencies of the two polarity classes in the dataset, we assume the prior probability that one data point x = (x 1 , ..., x k ) belongs to each is equal, i.e., </p><formula xml:id="formula_8">c w c h c ? H(Y ) ? c w c (? log w c + h c ) (7)</formula><p>where h c is the entropy of the sub-distribution for class c. Taking the lower bound as an approximation, we obtain the entropy term for the MI lower bound:</p><formula xml:id="formula_9">H(Y ) = 1 4 [log((det(? 1 ) det(? 2 ))] (8)</formula><p>In this formulation, we implicitly assume that the prior probabilities of the two classes are equal. We further notice that H(Y ) changes every time during each training epoch but at a very slow pace in several continuous steps due to the small gradients and consequently slight fluctuation in parameters. This fact demands us to update parameters timely to ensure estimation accuracy. Besides, according to statistical theory, we should increase the batch size (N * ) to reduce estimation error, but the maximum batch size is restricted to the GPU's capacity. Considering the situation above, we indirectly enlarge D s by encompassing the data from the nearest history. In implmentation, we store such data in a history data memory. The loss function for MI lower bound maximization in this level is given by:</p><formula xml:id="formula_10">L BA = ?I t,v BA ? I t,a BA<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">MI Maximization in the Fusion Level</head><p>To enforce the intermediate fusion results to capture modality-invariant cues among modalities, we repeat MI maximization between fusion results and input modalities. The optimization target is the fusion network F that produces fusion results Z = F (X t , X v , X a ). Since we already have a generation path from X m to Z, we expect an opposite path, i.e. to constructs X m , m ? {t, v, a} from Z. Inspired by but different from Oord et al.</p><p>(2018), we use a score function that acts on the normalized prediction and truth vectors to gauge their correlation:</p><formula xml:id="formula_11">G ? (Z) = G ? (Z) G ? (Z) 2 , h m = h m h m 2 s(h m , Z) = exp h m G ? (Z) T<label>(10)</label></formula><p>where G ? is a neural network with parameters ? that generates a prediction of h m from Z, ? 2 is the Euclidean norm, by dividing which we obtain unit-length vectors. Because we find the model intends to stretch both vectors to maximize the score in <ref type="formula" target="#formula_0">(10)</ref>  </p><formula xml:id="formula_12">L N (Z, H m ) = ?E H log s(Z, h i m ) h j m ?Hm s(Z, h j m )<label>(11)</label></formula><p>Here is a short explanation of the rationality of such formulation. Contrastive Predictive Coding (CPC) scores the MI between context and future elements "across the time horizon" to keep the portion of "slow features" that span many time steps <ref type="bibr" target="#b27">(Oord et al., 2018)</ref>. Similarly, in our model, we ask the fusion result Z to reversely predict representations "across modalities" so that more modality-invariant information can be passed to Z. Besides, by aligning the prediction to each modality we enable the model to decide how much information it should receive from each modality. This insight will be further discussed with experimental evidence in Section 5.2. The loss function for this level is given by:</p><formula xml:id="formula_13">L CP C = L z,v N + L z,a N + L z,t N<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training</head><p>The training process consists of two stages in each iteration: In the first stage, we approximate </p><formula xml:id="formula_14">B = {(X i t , X i v , X i a )} N i=1 sampled from D sub ? D do Encode X i m to h i m as (2) Compute L lld as (4) Update parameters of predictor q: ?q ? ?q ? ? lld ? ? L lld end Stage 2: MI-maximization Joint Training: for minibatch B = {(X i t , X i v , X i a )} N i=1</formula><p>sampled from D do Encode X i m to h i m as (2) Estimate the mean vectors and co-variance matrices in GMM model with M as (5) Update history M : <ref type="formula" target="#formula_10">(3), (8), (9)</ref> Produce fusion results Zi = F (X i t , X i v , X i a ) and predictions? Compute LN , LCP C as (10), (11), (12) Compute Lmain as <ref type="formula" target="#formula_0">(14)</ref> Update all parameters in the model except q: ? k ? ? k ? ? k ? ? Lmain end end p(y|x) with q(y|x) by minimizing the negative log-likelihood for inter-modality predictors with the loss in (4). In the second stage, hierarchical MI lower bounds in previous subsections are added to the main loss as auxiliary losses. After obtaining the final prediction?, along with the truth value y, we have the task loss:</p><formula xml:id="formula_15">M ? M \ {Oldest Hidden Batch} M ? M ? {h i m } |B| i=1 , m ? {v, a} Compute LBA as</formula><formula xml:id="formula_16">L task = MAE(?, y)<label>(13)</label></formula><p>where MAE stands for mean absolute error loss, which is a common practice in regression tasks. Finally, we calculate the weighted sum of all these losses to obtain the main loss for this stage:</p><formula xml:id="formula_17">L main = L task + ?L CP C + ?L BA<label>(14)</label></formula><p>where ?, ? are hyper-parameters that control the impact of MI maximization. We summarize the training algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present some experimental details, including datasets, baselines, feature extraction tool kits, and results. <ref type="table" target="#tab_4">Train  1284  16326  Validation  229  1871  Test  686  4659  All  2199  22856   Table 1</ref>: Dataset split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU-MOSI CMU-MOSEI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We conduct experiments on two publicly available academic datasets in MSA research: CMU-MOSI <ref type="bibr" target="#b43">(Zadeh et al., 2016)</ref> and CMU-MOSEI . CMU-MOSI contains 2199 utterance video segments sliced from 93 videos in which 89 distinct narrators are sharing opinions on interesting topics. Each segment is manually annotated with a sentiment value ranged from -3 to +3, indicating the polarity (by positive/negative) and relative strength (by absolute value) of expressed sentiment. CMU-MOSEI dataset upgrades CMU-MOSI by expanding the size of the dataset. It consists of 23,454 movie review video clips from YouTube. Its labeling style is the same as CMU-MOSI. We provide the split specifications of the two datasets in <ref type="table">Table 1</ref>.</p><p>We use the same metric set that has been consistently presented and compared before: mean absolute error (MAE), which is the average mean absolute difference value between predicted values and truth values, Pearson correlation (Corr) that measures the degree of prediction skew, seven-class classification accuracy (Acc-7) indicating the proportion of predictions that correctly fall into the same interval of seven intervals between -3 and +3 as the corresponding truths, binary classification accuracy (Acc-2) and F1 score computed for positive/negative and non-negative/negative classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>To inspect the relative performance of MMIM, we compare our model with many baselines. We consider pure learning based models, such as TFN , LMF <ref type="bibr" target="#b21">(Liu et al., 2018)</ref>, MFM <ref type="bibr">(Tsai et al., 2019b)</ref> and MulT <ref type="bibr" target="#b34">(Tsai et al., 2019a)</ref>, as well as approaches involving feature space manipulation like ICCN <ref type="bibr" target="#b32">(Sun et al., 2020)</ref> and MISA . We also compare our model with more recent and competitive baselines, in-  cluding BERT-based model-MAG-BERT (Rahman et al., 2020) and Self-MM <ref type="bibr" target="#b40">(Yu et al., 2021)</ref>, which works with multi-task learning and is the SOTA method. Some of the baselines are available at https://github.com/declare-lab/ multimodal-deep-learning.</p><p>The baselines are listed below: TFN : Tensor Fusion Network disentangles unimodal into tensors by threefold Cartesian product. Then it computes the outer product of these tensors as fusion results.</p><p>LMF <ref type="bibr" target="#b21">(Liu et al., 2018)</ref>: Low-rank Multimodal Fusion decomposes stacked high-order tensors into many low rank factors then performs efficient fusion based on these factors.</p><p>MFM <ref type="bibr">(Tsai et al., 2019b)</ref>: Multimodal Factorization Model concatenates a inference network and a generative network with intermediate modality-specific factors, to facilitate the fusion process with reconstruction and discrimination losses.</p><p>MulT <ref type="bibr" target="#b34">(Tsai et al., 2019a)</ref>: Multimodal Transformer constructs an architecture unimodal and crossmodal transformer networks and complete fusion process by attention.</p><p>ICCN <ref type="bibr" target="#b32">(Sun et al., 2020)</ref>: Interaction Canonical Correlation Network minimizes canonical loss between modality representation pairs to ameliorate fusion outcome.</p><p>MISA : Modality-Invariant and -Specific Representations projects features into separate two spaces with special limitations. Fusion is then accomplished on these features. <ref type="bibr" target="#b31">(Rahman et al., 2020)</ref>: Multimodal Adaptation Gate for BERT designs an alignment gate and insert that into vanilla BERT model to refine the fusion process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAG-BERT</head><p>SELF-MM <ref type="bibr" target="#b40">(Yu et al., 2021)</ref>: Self-supervised Multi-Task Learning assigns each modality a unimodal training task with automatically generated labels, which aims to adjust the gradient backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Basic Settings and Results</head><p>Experimental Settings. We use unaligned raw data in all experiments as in <ref type="bibr" target="#b40">Yu et al. (2021)</ref>. For visual and acoustic, we use COVAREP <ref type="bibr" target="#b9">(Degottex et al., 2014)</ref> and P2FA <ref type="bibr" target="#b41">(Yuan and Liberman, 2008)</ref>, which both are prevalent tool kits for feature extraction and have been regularly employed before. We trained our model on a single RTX 2080Ti GPU and ran a grid search for the best set of hyper-parameters. The details are provided in the supplementary file.</p><p>Hyperparameter Setting We perform a gridsearch for the best set of hyper-parameters: batch size in {32, 64}, ? lld in {1e-3,5e-3}, ? main in {5e-4, 1e-3, 5e-3}, ?, ? in {0.05, 0.1, 0.3}, hidden dim in {32, 64}, memory size in {1, 2, 3} batches, gradient clipping value is fixed at 5.0, learning rate for BERT fine-tuning is 5e-5, BERT embedding size is 768 and fusion vector size is 128. The hyperparameters are given in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>Summary of the Results. In accord with previous work, we ran our model five times under the same hyper-parameter settings and report  the average performance in <ref type="table" target="#tab_4">Table 2</ref>. We find that MMIM yields better or comparable results to many baseline methods. To elaborate, our model significantly outperforms SOTA in all metrics on CMU-MOSI and in (non-0) Acc-7, (non-0) Acc-2, F1 score on CMU-MOSEI. For other metrics, MMIM achieves very closed performance (?0.5%) to SOTA. These outcomes preliminarily demonstrate the efficacy of our method in MSA tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To show the benefits from the proposed loss functions and the corresponding estimation methods in MMIM, we carried out a series of ablation experiments on CMU-MOSEI. The results under different ablation settings are categorized and listed in <ref type="table" target="#tab_8">Table 4</ref>. First, we eliminate one or several MI loss terms, for both the inter-modality MI lower bound (I BA ) and CPC loss <ref type="bibr">(L z,m N</ref> where m ? {v, a, t}), from the total loss. We note the manifest performance degradation after removing part of the MI loss, and the results are even worse when removing all terms in one loss than only removing single term, which shows the efficacy of our MI maximization framework. Besides, by replacing current optimization target pairs in inter-modality MI with single pair or other pair combinations we can not gain better results, which provides experimental evidence for the candidate pair choice in that level. Then we test the components for entropy estimation. We deactivate the history memory and evaluate ? and ? in (5) using only the current batch. It is surprising to observe that the training process broke down due to the gradient's "NaN" value. Therefore, the history-based estimation has another advantage of guaranteeing the training stability. Finally, we substitute the GMM with a unified Gaussian where ? and ? are estimated on all samples regardless of their polarity classes. We spot a clear drop in all metrics, which implies the GMM built on natural class leads to a more accurate estimation for entropy terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Further Analysis</head><p>In this section, we dive into our models to explore how it functions in the MSA task. We first visualize all types of losses in the training process, then we analyze some representative cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tracing the Losses</head><p>To better understand how MI losses work, we visualize the variation of all losses during training in <ref type="figure" target="#fig_2">Figure 2</ref>. The values for plotting are the average losses in a constant interval of every 20 steps. From the figure, we can see throughout the training process, L task and L CP C keep decreasing nearly all the time, while L BA goes down in an epoch except the beginning of that. We also mark the time that the best epoch ends, i.e., the task loss on the validation set reaches the mini-  mum. It is notable that L BA and L CP C reach a relatively lower level at this time while the task loss on the training set does not. This scenario reveals the crucial role that L BA and L CP C play in the training process-they offer supplemental unsupervised gradient rectification to the parameters in their respective back-propagation path and fix up the over-fitting of the task loss. Besides, because in the experiment settings ? and ? are in the same order and at the end of best epoch L BA reaches the lowest value, which is synchronized as the validation loss, but L CP C fails to, we can conclude that L BA , or MI maximization in the input (lower) level, has a more significant impact on model's performance than L CP C , or MI maximization in the fusion (higher) level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study</head><p>We display some predictions and truth values, as well as corresponding input raw data (for visual and acoustic we only illustrate literally) and three CPC scores in <ref type="table" target="#tab_10">Table 5</ref>. As described in Section 3.5, these scores imply how much the fusion results depend on each modality. It is noted that the scores are all beyond 0.35 in all cases, which demonstrates the fusion results seize a certain amount of domain-invariant features. We also observe the different extents that the fusion results depend on each modality. In case (A), visual provides the only clue of the truth sentiment, and correspondingly s zv is higher than the other two scores. In case (B), the word "only" is a piece of additional evidence apart from what visual modality exposes, and we find s zt achieves a higher level than in (A). For (C), acoustic and visual help infer a neutral sentiment and thus s zv and s za are large than s zt . Therefore, we conclude that the model can intelligently adjust the information that flows from unimodal input into the fusion results consistently with their individual contribution to the final predictions. However, this mechanism may malfunction in cases like (D). The remark "I'm sorry" bewilders the model and meanwhile visual and acoustic remind none. In this circumstance, the model casts attention on text and is misled to a wrong prediction in the opposite direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present MMIM, which hierarchically maximizes the mutual information (MI) in a multimodal fusion pipeline. The model applies two MI lower bounds for unimodal inputs and the fusion stage, respectively. To address the intractability of some terms in these lower bounds, we specifically design precise, fast and robust estimation methods to ensure the training can go on normally as well as improve the test outcome. Then we conduct comprehensive experiments on two datasets followed by the ablation study, the results of which verify the efficacy of our model and the necessity of the MI maximization framework. We further visualize the losses and display some representative examples to provide a deeper insight into our model. We believe this work can inspire the creativity in representation learning and multimodal sentiment analysis in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of the MMIM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>w pos = p(x ? pos) = w neg = p(x ? neg) = 1 2 . Under the assumption that the two sub-distributions are disjoint, from Huber et al. (2008) the lower and upper bound of a GMM's entropy are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of loss changing as training proceeds on CMU-MOSEI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Update [CLS] And I just love it [SEP]</head><label></label><figDesc></figDesc><table><row><cell>Modality Type</cell><cell cols="2">Acoustic</cell><cell>A-sLSTM</cell><cell></cell><cell>History Memory</cell><cell>Entropy Estimator</cell><cell>( )</cell></row><row><cell>Acoustic</cell><cell cols="2">Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Extractor</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLP for Regression</cell></row><row><cell></cell><cell></cell><cell cols="2">BERT Encoder</cell><cell>Predictor</cell><cell>lld</cell><cell></cell></row><row><cell>Text</cell><cell>Layer 1</cell><cell>2 Layer</cell><cell>12 Layer</cell><cell></cell><cell>Fusion Network</cell><cell>Contrastive</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Predictor</cell><cell>lld</cell><cell>Coding Predictive</cell><cell>CPC score</cell></row><row><cell></cell><cell cols="2">Visual</cell><cell>V-sLSTM</cell><cell></cell><cell></cell><cell></cell><cell>Fusion MI</cell></row><row><cell>Visual</cell><cell cols="2">Feature Extractor</cell><cell></cell><cell></cell><cell>History Memory Update</cell><cell>Estimator Entropy</cell><cell>( )</cell><cell>Estimation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on CMU-MOSI and CMU-MOSEI; ?: all models use BERT as the text encoder; ?: from Hazarika</figDesc><table><row><cell>et al. (2020);  ?: from Yu et al. (2021);  * : reproduced from open-source code with hyper-parameters provided in</cell></row><row><cell>original papers. For Acc-2 and F1, we have two sets of non-negative/negative (left) and positive/negative (right)</cell></row><row><cell>evaluation results. Best results are marked in bold and means the corresponding result is significantly better than</cell></row><row><cell>SOTA with p-value ? 0.05 based on paired t-test.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters for best performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of MMIM on CMU-MOSEI.</figDesc><table><row><cell>t, v, a, z represent text, visual, acoustic and fusion re-</cell></row><row><cell>sults.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Representative examples with their predictions and fusion-modality scores in the case study. High scores (? 0.8) are highlighted in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">grant titled: "CSK-NLP: Leveraging Commonsense Knowledge for NLP", and the SRG grant id: T1SRIS19149 titled "An Affective Multimodal Dialogue System".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project is supported by the AcRF MoE Tier-</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Implementation details of history memory</p><p>The workflow of the history embedding memory comprises two stages, as shown in <ref type="figure">Figure 3</ref>. In the estimation stage, parameters of the GMM model is estimated using both history embeddings readout from history memory and current batch input, as shown in (a). Then in the update stage, the oldest batch of data is driven out of the memory to leave space for new data, as described in (b). The memory is implemented as a FIFO queue.  Proof. For a GMM model, the marginal probability density function of x can be written as</p><p>where z is an indicator that reflects which class x falls in and C i is the i th class. Since i P (x ? C i ) = 1 by Jensen's inequality we have (note H(X) = (?p(x) log p(x))dx = g(x)dx and g(x) is a convex function)</p><p>In our case we have p(x ? C 1 ) = p(x ? C 2 ) = 1 2 , then</p><p>Hence we get a lower bound of H(X) as the right side of the inequality. On the other hand, an upper bound as proposed in Huber et al. <ref type="formula">(2008)</ref> is</p><p>To summarize K L (X) ? H(X) ? K U (X) = 0.693 + K L (X) (19) Then through maximizing the lower bound K L (X) we can maximize H(X).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task learning for multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Md Shad Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
	<note>Asif Ekbal, and Pushpak Bhattacharyya</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<title level="m">Deep variational information bottleneck</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning representations for neural network-based classification using the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Ali Amjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">C</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2225" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The IM algorithm: A variational approach to information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/3136755.3136801</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Club: A contrastive log-ratio upper bound of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1779" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Covarep-a collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 ieee international conference on acoustics, speech and signal processing (icassp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The emotions: A philosophical introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Deonna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Teroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DialogueGCN: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CASCADE: Contextual sarcasm detection in online discussion forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sruthi</forename><surname>Gorantla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1837" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MISA: Modality-invariant andspecific representations for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/3394171.3413678</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On entropy approximation for gaussian mixture random vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe D</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Textural feature selection by joint mutual information based on gaussian mixture model for multispectral image classification. Pattern Recognition Letters</title>
		<editor>Mounir Ait Kerroum, Ahmed Hammouch, and Driss Aboutajdine</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1168" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient lowrank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlong</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentiment analysis of blogs by combining lexical knowledge with text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Gryc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/1557019.1557156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1275" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/2070481.2070509</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaussian mixture model based mutual information estimation between frequency bands in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Gustaftson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Vang Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2002 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">525</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
	<note>Louis-Philippe Morency, and Barnab?s P?czos</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2020.3038167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwu</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfeng</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoque</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.214</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2359" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathusha</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8992" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal Transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning factorized multimodal representations</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning</title>
		<editor>Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal routing: Improving local and global interpretability of multimodal language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muqiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1823" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Select-additive learning: Improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>W?llmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiele</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04830</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speaker identification on the scotus corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3878" to="3878" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
