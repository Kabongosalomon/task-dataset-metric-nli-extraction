<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Liang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
							<email>lslee@gate.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Various spoken language processing tasks, such as translation <ref type="bibr" target="#b0">[1]</ref>, retrieval <ref type="bibr" target="#b1">[2]</ref>, summarization <ref type="bibr" target="#b2">[3]</ref> and understanding <ref type="bibr" target="#b3">[4]</ref> have been very successful with a standard cascade architecture: an ASR front-end module transforming the speech signals into text form, followed by the downstream task module (such as translation) trained on text taking the ASR output as normal text input. However, the end-to-end approach trying to consider the two modules as a whole is always attractive for the following reasons. The two modules in the cascade architecture locally optimize the two tasks with different criteria, while the end-toend approach may obtain globally optimized performance for the overall task. The ASR module minimizes the WER, which is not necessarily directly proportional to the performance measure of the overall task. Much information is inevitably lost when the speech signals are transformed into text with errors, and the errors can't be recovered in the following module. The end-to-end approach allows the possibility of capturing information directly from the speech signals not shown in the ASR output and offering overall performance less limited by the ASR accuracy.</p><p>Some spoken language tasks such as translation, retrieval, and understanding (intent classification and slot filling) have been achieved with end-to-end approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>, although it remains difficult to obtain significantly better results than the standard cascade architecture. These tasks are primarily sentence-level, in which extracting some local information in a short utterance may be adequate. Spoken question answering (SQA) considered here is known as a much more difficult problem. The inputs to the SQA task are much longer spoken paragraphs. In addition to understanding the literal meaning, the global information in the paragraphs needs to be organized, and sophisticated reasoning is usually required to find the an-swers. Fine-grained information is also important to predict the exact position of the answer span from a very long context. This paper is probably the first known attempt to try to perform such a difficult SQA task with an end-to-end approach.</p><p>Substantial improvements in text question answering (TQA) tasks trained and tested on text data have been observed after the large-scale self-supervised pre-trained language models appeared, such as BERT <ref type="bibr" target="#b10">[10]</ref> and GPT <ref type="bibr" target="#b11">[11]</ref>. Instead of learning the TQA tasks from scratch, these models were first pre-trained on a large unannotated text corpus to learn selfsupervised representations for the general purpose and then fine-tuned on the downstream TQA dataset. Results comparable to human performance on SQuAD datasets <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref> were obtained in this way. However, previous work <ref type="bibr" target="#b14">[14]</ref> indicated that such an approach may not be easily extendable to the SQA task on audio data by directly cascading an ASR module in the front. Not only the ASR caused serious problems, but very often the true answer spans included name entities or OOV words which cannot be correctly recognized at all, thus cannot be identified by the following TQA module trained on text. That is why all questions with recognition errors in answer spans were discarded in the previous work <ref type="bibr" target="#b14">[14]</ref>. This led to the end-to-end approach to SQA proposed here.</p><p>The BERT model <ref type="bibr" target="#b10">[10]</ref> useful in TQA tasks was able to transform word tokens into contextualized embeddings carrying plenty of information. For SQA task, it is certainly desirable to transform audio words (audio signals for word tokens) also into such embeddings, but much more challenging. The same word token can have millions of different audio signal realizations in different utterances. The boundaries for audio words in utterances are not available. The next problem is even much more challenging. BERT can learn semantic information of a word token based on its context in text form. Audio words in audio data have context only in audio form, which is much more noisy, confusing, unpredictable, and difficult to handle. Learning semantics from audio context is really hard.</p><p>Audio Word2Vec <ref type="bibr" target="#b15">[15]</ref> was the first effort to transform audio words with known boundaries into embeddings carrying phonetic information only, no semantics at all. Speech2Vec <ref type="bibr" target="#b16">[16]</ref> then tried to imitate the training process of skip-gram or CBOW in Word2Vec <ref type="bibr" target="#b17">[17]</ref> to extract some semantic features. It was proposed <ref type="bibr" target="#b18">[18]</ref> to learn to jointly segment the audio words out of utterance and extract the embeddings. Other efforts then tried to align the audio word embeddings with text word embeddings <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>. Some more recent works <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> even tried to obtain embeddings for audio signals using approaches very similar to BERT, but primarily extracting acoustic information with very limited semantics. These approaches may be able to extract some semantics with audio word embeddings, but the level of semantics obtained was still very far from that required for the SQA tasks.</p><p>In this paper, we propose an audio-and-text jointly learned  SpeechBERT model for the end-to-end SQA task. Speech-BERT is a pre-trained model learned from audio and text datasets, so as to be able to produce embeddings for audio words, and these embeddings can be properly aligned with the embeddings for the corresponding text words offered by a typical text BERT model trained with text datasets. Standard pretraining and fine-tuning as text BERT are also performed. When used in the SQA task, performance comparable to or better than the cascade architecture was obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SpeechBERT for End-to-end SQA</head><p>Here we assume the training audio datasets include the ground truth transcripts, and an off-the-shelf ASR engine is available. So we can segment the audio training data into audio words (audio signals for the underlying word tokens) by forced alignment.</p><p>For audio testing data we can use the ASR engine to obtain audio words including their boundaries too, although with errors. The overall training process for the SpeechBERT for end-toend SQA proposed here is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We first use the text dataset to pre-train a Text BERT (upper right block, Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text BERT Pre-training</head><p>Here we follow the standard Text BERT pre-training procedure <ref type="bibr" target="#b10">[10]</ref>. For a sentence in the training text set with n tokens T = {t1, t2, ..., tn}, we represent them as vectors Etext = {e1, e2, ..., en} and sum them with corresponding positional and sentence segment embeddings to get E text to be fed into the multi-layer Transformer. Masked language model (MLM) task is performed at the output layer of the Text BERT by randomly replacing 15% of vectors in Etext with a special mask token vector and predicting the masked tokens at the same position of the output layers. Next sentence prediction (NSP) usually performed in BERT training is not used here because some recent studies indicated it may not be helpful <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Initial Phonetic-Semantic Joint Embedding</head><p>For an utterance in the training audio dataset with n audio words X = {x (1) , x <ref type="bibr" target="#b1">(2)</ref> , ..., x (n) }, the goal here is to encode these n audio words into n embeddings Eaudio = {?1,?2, ...,?n}.</p><p>Let one of the audio words include a total of T speech feature vectors, x = {x1, x2, ..., xT }. The complete process of initial phonetic-semantic joint embedding is in <ref type="figure" target="#fig_2">Fig. 2</ref>. With the speech features for each audio word x = {x1, x2, ..., xT }, we use an RNN sequence-to-sequence autoencoder <ref type="bibr" target="#b15">[15]</ref> as in <ref type="figure" target="#fig_2">Fig. 2</ref>. This includes an audio encoder (low left corner of the figure) transforming the input x into a vector z (in red in the middle), and an audio decoder reconstructing the output y = (y1, y2, ..., yT ) from z. The autoencoder is trained to minimize the reconstruction error:  where k is the index for training audio words. This process enables the vector z to capture the phonetic structure information of the audio words but not semantics at all, which is not adequate for the goal here. So we make the vector z be constrained by a L1-distance loss (lower right) further:</p><formula xml:id="formula_0">Lrecons = k T t=1 xt ? yt 2 2 ,<label>(1)</label></formula><formula xml:id="formula_1">LL 1 = k z ? Emb(t) 1 ,<label>(2)</label></formula><p>where t is the token label for the audio word x and Emb is the embedding layer of the Text BERT trained in Sec. 2.1. In this way, we use the token label of the audio word to access the semantic information about the considered audio word extracted by the Text BERT as long as it is within the vocabulary of the Text BERT. So, the autoencoder model learns to keep the phonetic structure of the audio word so as to reconstruct the original speech features x as much as possible, but at the same time it learns to fit to the embedding distribution of the Text BERT which carries plenty of semantic information for the word tokens. This makes it possible for the model to learn a joint embedding space integrating both phonetic and semantic information extracted from both audio and text datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">MLM Pre-training for SpeechBERT with Both Text and Audio Data</head><p>This is the lower left block of <ref type="figure" target="#fig_0">Fig. 1</ref> with details in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>.</p><p>Here we pre-train the SpeechBERT with the MLM task using both text and audio datasets, before fine-tuning it on the downstream QA task. As in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>, for the SpeechBERT learning to take embeddings for both discrete text words and continuous spoken words, we jointly optimize the MLM loss with the mixture of both audio and text data. The same training target for text MLM as described in Sec. 2.1 is used here. For audio data input, after obtaining the phonetic-semantic joint embeddings Eaudio = {?1,?2, ...,?n} for each utterance as in Sec. 2.2, we also randomly replace 15% of those embeddings with mask token vectors as we do in Sec. 2.1. With the supervised setting, we can similarly predict the corresponding tokens behind the masked spoken words. During training, we freeze the audio encoder in <ref type="figure" target="#fig_2">Fig. 2</ref> to speed up the process, while have the text word embedding unfrozen to keep the joint audio-and-text embeddings flexible, considering the earlier experiences reported for end-to-end SLU <ref type="bibr" target="#b31">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fine-tuning on Question Answering</head><p>This is the last block in <ref type="figure" target="#fig_0">Fig. 1</ref>, with details in <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>. Here the downstream QA task is fine-tuned to minimize the loss for predicting the correct start/end positions of the answer span, as proposed in BERT <ref type="bibr" target="#b10">[10]</ref>. By introducing a start and an end vector S, E ? R H , we compute a dot product for S or E with each final hidden vector Ti ? R H for audio word i from the    <ref type="figure">Figure 4</ref>: Illustration of the way to evaluate frame-level F1score and AOS on SQuAD-lost. If the predicted span overlaps well with the ground truth answer span, high F1 and AOS will be obtained even with recognition errors.</p><formula xml:id="formula_2">= | | | ? | ! = 2? ? + = | | | | = | | | |</formula><p>SpeechBERT, as in <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>. The dot product value is softmaxnormalized over all audio words in the utterance to compute the probability of audio word i being the start or end position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data and Evaluation</head><p>The SpeechBERT model was trained on the Spoken SQuAD dataset <ref type="bibr" target="#b14">[14]</ref>, in which the text for all audio paragraphs and all questions are from original SQuAD <ref type="bibr" target="#b12">[12]</ref> dataset. It also includes SQuAD format ASR transcripts plus 37k questionanswer pairs as the training set and 5.4k as the testing set. This is smaller than the official SQuAD dataset (with 10.6k questions in its development set) since 5.2k of questions in SQuAD for which the answers couldn't be found in the ASR transcripts were removed and not shown in the Spoken SQuAD testing set. These removed questions in SQuAD were collected to form another testing set referred to as SQuAD-lost (lost by ASR errors). This is helpful below in evaluating whether the end-toend approach can better handle the questions with incorrectly recognized answer spans. All questions in SQuAD-lost share the same audio files as the Spoken SQuAD testing set. The evaluation metrics we used are the Exact Matched (EM) percentage and F1 score for the word tokens as in the normal QA tasks, but for SQuAD-lost frame-level F1 and Audio Overlapping Score (AOS) <ref type="bibr" target="#b14">[14]</ref> based on the boundaries of the answer spans as illustrated in <ref type="figure">Figure 4</ref> were used instead. For the latter case, the boundaries for the audio words were found by forced alignment with Kaldi <ref type="bibr" target="#b32">[32]</ref>, based on which the start/end points of the answer for both the ground truth and predicted results in training and testing sets were obtained. The ground truth text of the testing set was used only in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Setting and Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Initial Phonetic-semantic Joint Embedding</head><p>For the autoencoder in <ref type="figure" target="#fig_2">Fig. 2</ref>, we used a bidirectional LSTM as the encoder and a single-directional LSTM as the decoder, both with input size 39 (MFCC-dim) and hidden size 768 (BERT embedding-dim). Two layers of the fully-connected network are added at the encoder output to transform the encoded vectors to fit the BERT embedding space. We directly used the audio from Spoken SQuAD training set to train this autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Text BERT and SpeechBERT</head><p>We used the PyTorch implementation 1 of BERT to build the BERT model with 12 layers of bert-base-uncased setting. We randomly initialized a new embedding layer with our vocabulary set counted in the dataset rather than using the WordPiece tokenizer <ref type="bibr" target="#b33">[33]</ref> in processing the text because it is inconsistent with the audio word units. The official pre-trained weights were loaded as the weights of the BERT model. We first trained the MLM task with the text of Spoken SQuAD training set for three epochs. Next, we used the Spoken SQuAD training set by directly feeding text and audio into the BERT model. After pre-training, we fine-tuned the model with the Spoken SQuAD training set for another two epochs. The other hyper-parameters used were identical to the original PyTorch implementation 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spoken SQuAD (no ASR errors in answer spans)</head><p>In <ref type="table" target="#tab_4">Table 1</ref> Section (I) is for models trained on text data, with rows (a)(b)(c) for different QA models trained on the same SQuAD (in text), while column (A)(B) respectively for testing sets of SQuAD (text) and Spoken SQuAD (ASR). Rows (a)(b)(c) showed the prior arts and the superiority of BERT (rows (c) vs (a)(b)), and the serious performance degradation for these models when tested on ASR output directly (column (B) vs (A)). Row (d) is for BERT trained on ASR transcriptions of Spoken SQuAD, or the "cascade" of ASR and BERT, where we see BERT performed much better for ASR output if trained on ASR output (rows (d) vs (c) for column (B)).</p><p>Section (II) is for the end-to-end QA model proposed here, with row (e) for the SpeechBERT trained on Spoken SQuAD. We see the end-to-end model is still 4-5% lower than the "cascade" architecture (row (e) vs (d)), although much better or  comparable to prior models directly used on ASR output (rows (e) vs (a)(b)(c)). Considering the high level of the difficulty for the end-to-end SQA model to learn the sophisticated semantic knowledge out of the relatively long audio signals directly in one model without using the word tokens from ASR, the results here showed the promising potential for end-to-end approaches for SQA when better data, better model and better training become possible.</p><p>When we further ensembled the end-to-end model in row (e) with the cascade architecture in row (d) as shown in row (h) of Section (III), we see significantly improved performance compared to the two component models (rows (h) vs (d) or (e)), achieving the state-of-the-art result on Spoken SQuAD. Note that we can also ensemble two separately trained cascade models [(d) plus (d)] as shown in row (i), but the achievable improvement was much less (row (i) vs (h)). These results showed that the end-to-end SpeechBERT can learn extra knowledge complementary to that learned by the cascade architecture. Furthermore, the results in row (h) and column (B) trained and tested on audio are already higher than those in row (a) and column (A) on ground truth text and comparable to those in row (b) and column (A), although still much lower than row (c) and column (A). This implies that the results obtained here are substantial and significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SQuAD-lost (with ASR errors in answer spans)</head><p>We wish to find out further with the end-to-end model if we can better handle the questions with incorrectly recognized answer spans. Here we used the frame-level F1 and AOS as illustrated in <ref type="figure">Fig. 4</ref> for evaluation and the results for cascade (row (d) in <ref type="table" target="#tab_4">Table 1</ref>) and end-to-end (row (e) in <ref type="table" target="#tab_4">Table 1</ref>) are in <ref type="table" target="#tab_5">Table 2</ref>  SpeechBERT (End-to-end) (e) <ref type="figure">Figure 5</ref>: Frame-level F1 scores evaluated for small groups of Total (Spoken SQuAD/SQuAD-lost) at different levels of WER.</p><p>Spoken SQuAD (same as in <ref type="table" target="#tab_4">Table 1</ref>, all with correctly recognized answer spans), and the total of the two. The results show the end-to-end models did significantly better on SQuAD-lost (middle), although worse by a gap on Spoken SQuAD (left), but offered some improvements on the total (right). This verified the end-to-end model could learn some phonetic-semantic knowledge directly from audio signals before errors occurred in ASR, and explained indirectly why the ensembled model ([(e) plus (d)] in row (h)) of <ref type="table" target="#tab_4">Table 1</ref> can do better than cascade alone (row (d)). The scenario on the total on the right of <ref type="table" target="#tab_5">Table 2</ref> was closer to the real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Ablation MLM pre-training and Better Word Boundaries</head><p>The results in row (f) of Section (II) in <ref type="table" target="#tab_4">Table 1</ref> are for the endto-end model trained directly with SQA fine-tuning without the MLM pre-training, and significant performance drop can be observed (rows (f) vs (e)). Also, the results in row (e) of Table 1 are for audio word boundaries obtained by forced alignment using ASR transcripts with WER of 22.73% in Spoken SQuAD <ref type="bibr" target="#b14">[14]</ref>. We further tested the model on better boundaries obtained with forced alignment using ground truth text. By doing so, we can see the extent to which performance is limited by segmentation quality. The results in row (g) showed some improvements could still be achieved with better boundaries (rows (g) vs (e)), which can be a direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Analysis on WER of ASR</head><p>To investigate how the cascade and end-to-end models (rows (d) and (e) in <ref type="table" target="#tab_4">Table 1</ref>) worked with audio at different WER, we split the questions in the total dataset including both SQuAD-lost and Spoken SQuAD into smaller groups with different WER. The frame-level F1 results for these groups were plotted in <ref type="figure">Figure 5</ref>. Obviously, at lower WER both models offered higher F1, and the cascade architecture performed better. Both models suffered from performance degradation at higher WER, and the end-to-end model outperformed cascade when WER exceeded 40% since it never relied on ASR output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>Audio signals are in form of phonetic structures, while carrying semantics. ASR has long been used to transform the phonetic structures into word-level tokens carrying semantics, but with inevitable errors causing troubles to the downstream application tasks. With reasonable performance of the end-to-end SQA task, the proposed SpeechBERT was shown to be able to do similar transformation, but the semantics were somehow directly tuned to the downstream task, or question answering here, bypassing the intermediate problem of ASR errors. This concept (and the SpeechBERT) is definitely useful to many other spoken language processing tasks to be studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We are grateful to the National Center for High-performance Computing for computer time and facilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall training process for SpeechBERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2.1), based on which we train the initial phonetic-semantic joint embedding with the audio words in the training audio dataset (upper left block, Sec. 2.2). The training of SpeechBERT (or a shared BERT model for both text and audio) is then in the bottom of the figure, including pre-training (Sec 2.3) and finetuning on SQA (Sec. 2.4). The details are given below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Training procedure for the Initial Phonetic-Semantic Joint Embedding. After training, the the encoded vector (z in red) obtained here is used to train the SpeechBERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Two training stages for the SpeechBERT model: (a) Pre-training and (b) Fine-tuning. The two stages use identical model architecture except for the output layers. The special tokens [CLS] and [SEP] are added following the original Text BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Alignment with ASR Transcriptions Forced Alignment with Ground Truth Text Overlapping Span</head><label></label><figDesc></figDesc><table><row><cell>Forced</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted Span</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?. by the</cell><cell>brevity</cell><cell>rock</cell><cell>group</cell><cell>called</cell><cell>play</cell><cell>with</cell><cell>special</cell><cell>?.</cell></row><row><cell>?. by the</cell><cell>British</cell><cell>rock</cell><cell>group</cell><cell>Coldplay</cell><cell></cell><cell>with</cell><cell>special</cell><cell>?.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Answer Span</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on Spoken SQuAD. Sections (I)(II)(III) are respectively for models trained on text, end-toend models trained on audio, and ensembled models; while column (A)(B) are respectively for text and ASR testing sets.</figDesc><table><row><cell>Models and Training set</cell><cell cols="4">Testing Set (A) Text (B) ASR</cell></row><row><cell>(I) trained on text</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>(a) BiDAF on Text [34]</cell><cell cols="4">58.40 69.90 37.02 50.90</cell></row><row><cell>(b) Dr.QA on Text [35]</cell><cell cols="4">62.84 73.74 41.16 54.51</cell></row><row><cell>(c) BERT on Text [10]</cell><cell cols="4">76.90 85.71 53.30 66.17</cell></row><row><cell>(d) BERT on ASR [10](cascade)</cell><cell>-</cell><cell>-</cell><cell cols="2">56.28 68.22</cell></row><row><cell cols="2">(II) End-to-end trained on Audio</cell><cell></cell><cell>EM</cell><cell>F1</cell></row><row><cell>(e) SpeechBERT (proposed)</cell><cell></cell><cell></cell><cell cols="2">51.19 64.08</cell></row><row><cell>(f) SpeechBERT w/o MLM</cell><cell></cell><cell></cell><cell cols="2">46.02 59.62</cell></row><row><cell cols="3">(g) SpeechBERT tested on better boundaries</cell><cell cols="2">53.42 66.27</cell></row><row><cell cols="2">(III) Ensembled models</cell><cell></cell><cell>EM</cell><cell>F1</cell></row><row><cell>(h) ensembled [(e) plus (d)]</cell><cell></cell><cell></cell><cell cols="2">60.37 71.75</cell></row><row><cell>(i) ensembled [(d) plus (d)]</cell><cell></cell><cell></cell><cell cols="2">57.88 69.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on SQuAD-lost, Spoken SQuAD and Total for the cascade and end-to-end models. 66.56 63.87 30.78 27.58 48.74 45.84 End-to-end (e) 62.76 59.70 37.31 33.57 50.12 46.72</figDesc><table><row><cell>Model</cell><cell>Spoken SQuAD SQuAD-lost F1 AOS F1 AOS F1 AOS Total</cell></row><row><cell>Cascade (d)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, SQuAD-lost (incorrectly recognized answer spans),</figDesc><table><row><cell>71.06</cell><cell></cell><cell></cell></row><row><cell>70.07</cell><cell></cell><cell></cell></row><row><cell>62.07</cell><cell></cell><cell></cell></row><row><cell>61.06</cell><cell>57.99</cell><cell></cell></row><row><cell></cell><cell>56.24</cell><cell>51.83</cell></row><row><cell></cell><cell></cell><cell>50.63</cell><cell>45.96</cell><cell>44.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell>45.63</cell><cell>40.49</cell><cell>39.64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>38.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell>36.54</cell><cell>36.68</cell></row><row><cell></cell><cell cols="3">Document Word Error Rate (%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>B?rard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS End-to-end Learning for Speech and Audio Processing Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spoken content retrievalbeyond cascading speech recognition with text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1389" to="1420" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Orderpreserving abstractive summarization for spoken content based on connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2899" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5754" to="5758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From audio to semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6189" to="6193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end architectures for asr-free spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Palogiannidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gkinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mastrapas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mizera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="7974" to="7978" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end spoken language understanding without matched language speech model pretraining data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7979" to="7983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging unpaired text data for training end-to-end speech-to-intent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7984" to="7988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Know what you dont know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spoken squad: A study of mitigating the impact of speech recognition errors on listening comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="3459" to="3463" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="765" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2018</title>
		<meeting>Interspeech 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="811" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segmental audio word2vec: Representing utterances as sequences of vectors with applications in spoken term detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6269" to="6273" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Almost-unsupervised speech recognition with close-to-zero resource based on phonetic structures learned from very small unpaired speech and text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12566</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised cross-modal alignment of speech and text embedding spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7354" to="7364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving transformer-based speech recognition using unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09932</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Speech-xlnet: Unsupervised acoustic model pretraining for selfattention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10387</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised pre-traing for sequence to sequence speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12418</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep contextualized acoustic representations for semi-supervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01679</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="814" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
