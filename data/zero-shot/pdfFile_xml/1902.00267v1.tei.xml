<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ColorNet: Investigating the importance of color spaces for image classification ?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-01">1 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyank</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>10084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Yuan</surname></persName>
							<email>yuanc@sz.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ColorNet: Investigating the importance of color spaces for image classification ?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-01">1 Feb 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Color spaces ? Densenet ? Fusion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image classification is a fundamental application in computer vision. Recently, deeper networks and highly connected networks have shown state of the art performance for image classification tasks. Most datasets these days consist of a finite number of color images. These color images are taken as input in the form of RGB images and classification is done without modifying them. We explore the importance of color spaces and show that color spaces (essentially transformations of original RGB images) can significantly affect classification accuracy. Further, we show that certain classes of images are better represented in particular color spaces and for a dataset with a highly varying number of classes such as CIFAR and Imagenet, using a model that considers multiple color spaces within the same model gives excellent levels of accuracy. Also, we show that such a model, where the input is preprocessed into multiple color spaces simultaneously, needs far fewer parameters to obtain high accuracy for classification. For example, our model with 1.75M parameters significantly outperforms DenseNet 100-12 that has 12M parameters and gives results comparable to Densenet-BC-190-40 that has 25.6M parameters for classification of four competitive image classification datasets namely: CIFAR-10, CIFAR-100, SVHN and Imagenet. Our model essentially takes an RGB image as input, simultaneously converts the image into 7 different color spaces and uses these as inputs to individual densenets. We use small and wide densenets to reduce computation overhead and number of hyperparameters required. We obtain significant improvement on current state of the art results on these datasets as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image classification is one of the most fundamental applications in the field of computer vision. Most of the datasets used for image classification tend to consist of color images. These color images are represented in RGB format. To a computer, these images are just numbers and do not contain any inherent meaning. Most recent models developed for classification do not perform a color space transformation to the image and instead use the RGB image directly for classification.</p><p>In this paper, we propose the use of different color spaces. The main color spaces will be discussed briefly in this section.</p><p>A color space is essentially an organization of colors. Along with physical device profiling, color spaces help us to reproduce analog and digital representations of color. Color spaces can also be thought of as an abstract mathematical model that helps us to describe colors as numbers.</p><p>RGB color space, often the most popular color space, is a system dependent color space. Commonly, it is represented using a 24-bit implementation where each channel R, G and B are given 8 bits each. This results in each channel having a range of values from 0 to 255. This color space models on the basis that all colors can be represented using different shades of red, green and blue.</p><p>Images from popular datasets such as CIFAR <ref type="bibr" target="#b1">[2]</ref> have images present in the sRGB format. The first step is in converting the sRGB images to RGB by linearizing it by a power-law of 2.2.</p><p>Some of the other popular color spaces we shall discuss briefly are HSV, LAB, YUV, YCbCr, YPbPr, YIQ, XYZ, HED, LCH and CMYK.</p><p>HSV stands for hue, saturation and value. HSV was developed taking into consideration how humans view color. It describes color (hue) in terms of the saturation (shade) and value (brightness). H has a range from 0 to 360, S and V have range 0 to 255. The transformation from RGB to HSV can be seen in (1)- <ref type="bibr" target="#b5">(6)</ref>. R, G and B are the values of the red channel, green channel and blue channel respectively. H obtained represents the hue channel. Similarly, S represents the saturation channel and V the value channel.</p><formula xml:id="formula_0">R ? = R/255, G ? = G/255, B ? = B/255 (1) Cmax = max(R ? , G ? , B ? ), Cmin = min(R ? , G ? , B ? ) (2) ? = Cmax ? Cmin (3) H = ? ? ? ? ? ? ? ? ? ? ? ? ? 0 ? , ? = 0 60 ? G ? ?B ? ? mod6 , Cmax = R ? 60 ? B ? ?R ? ? + 2 , Cmax = G ? 60 ? R ? ?G ? ? + 4 , Cmax = B ? (4) S = 0 , Cmax = 0 ? Cmax , Cmax = 0 (5) V = Cmax<label>(6)</label></formula><p>To define quantitative links between distributions of wavelengths in the EM visible spectrum (Electromagnetic) along with the physiological perceived colors in human eye sight, the CIE (Commission internationale de lclairage) 1931 color spaces were introduced. The mathematical relationships between these color spaces form fundamental tools to deal with color inks, color management, illuminated displays, cameras and printers.</p><p>CIE XYZ (from now on referred to as XYZ) was formed on the mathematical limit of human vision as far as color is concerned. X, Y and Z are channels extrapolated from the R, G and B channels to prevent the occurrence of negative values. Y represents luminance, Z represents a channel close to blue channel and X represents a mix of cone response curves chosen to be orthogonal to luminance and non-negative. XYZ image can be derived from RGB using <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_1">? ? X Y Z ? ? = ? ? 0.489989 0.310008 0.2 0.176962 0.81240 0.010 0 0.01 0.99 ? ? ? ? R G B ? ?<label>(7)</label></formula><p>LAB is another very popular color space. This color space is often used as an interchange format when dealing with different devices. This is done because it is device independent. Here, L stands for lightness, 'a' stands for color component green-red and 'b' for blue-yellow. An image in RGB can be transformed to LAB by first converting the RGB image to XYZ image.</p><p>YIQ, YUV, YPbPr, YCbCr are all color spaces that are used for television transmission. Hence, they are often called transmission primaries. YUV and YIQ are analog spaces for PAL and NTSC systems. YCbCr is used for encoding of digital color information used in video and still-image transmission and compression techniques such as JPEG and MPEG.</p><p>YUV, YCbCr, YPbPr, and YIq belong to opponent spaces. They have one channel for luminance and 2 channels for chrominace, represented in an opponency way (usually red versus green, and blue versus yellow).</p><p>The RGB color space is an additive color model, where to obtain the final image we add the individual channel values. A subtractive color model exists, called CMYK where C stands for Cyan, M stands for magenta, Y stands for yellow and K stands for key (black). The CMYK model works by masking colors on a light background. RGB to CMYK conversion can be seen in (8)- <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_2">R ? = R/255, G ? = G/255, B ? = B/255 (8) K = 1 ? max(R ? , G ? , B ? )<label>(9)</label></formula><formula xml:id="formula_3">C = (1 ? R ? ? K) (1 ? K)<label>(10)</label></formula><formula xml:id="formula_4">M = (1 ? G ? ? K) (1 ? K) (11) Y = (1 ? B ? ? K) (1 ? K)<label>(12)</label></formula><p>LCH is another color space. It is similar to LAB. It is in the form of a sphere that has three axes: L, c and h. L stands for lightness, c stands for chroma (saturation) and h stands for hue. It is a device-independent color space and is used for retouching images in a color managed workflow that utilizes high-end editing applications. To convert from RGB to LCH, we first convert from RGB to LAB and then from LAB to LCH.</p><p>There are other color spaces that have not been explored. Only the most popular ones have been referenced. The idea behind the paper is that an image is nothing but numbers to a computer, hence, transformations of these images should be viewed as a completely new image to a computer. Essentially transforming an image into different color spaces should yield us new images in the view of the computer.</p><p>We exploit our idea by using small networks to classify images in different color spaces and combine the final layer of each network to obtain an accuracy that takes into account all the color spaces involved. To obtain a high accuracy, we need each output to be less correlated to each other. This is also something we will explain in detail in the proposed approach section.</p><p>We have proposed the following novel contributions, that have not been performed before to the best of our knowledge.</p><p>1) We show that certain classes of images are better represented in certain color spaces.</p><p>2) We show that combining the outputs for each color space will give us a much higher accuracy in comparison to individually using each color space.</p><p>3) We show that a relatively small model can obtain a similar level of accuracy to recent state of the art approaches (See experimental analysis section, our model with 1.75M parameters can provide a similar accuracy to that of a densenet model with 25.6M parameters for CIFAR datasets) 4) We also obtain new state of the art results on CIFAR-10, CIFAR-100, SVHN and imagenet, to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Image classification has been a fundamental task in the field of computer vision. This task has gained huge importance in recent times with the development of datasets such as Imagenet [10, CIFAR <ref type="bibr" target="#b1">[2]</ref>, SVHN <ref type="bibr" target="#b2">[3]</ref>, MNIST <ref type="bibr" target="#b3">[4]</ref>, CALTECH-101 <ref type="bibr" target="#b4">[5]</ref>, CALTECH-256 <ref type="bibr" target="#b5">[6]</ref> among others.</p><p>Deep convolutional neural networks <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> have been developed which have drastically affected the accuracy of the image classification algorithms. These have in turn resulted in breakthroughs in many image classification tasks <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>.</p><p>Recent research <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> has shown that going deeper will result in higher accuracy. In fact, current state-of-the-art approaches on the challenging Imagenet <ref type="bibr" target="#b0">[1]</ref> dataset has been obtained by very deep networks <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Many complex computer vision tasks have also been shown to obtain great results on using deeper networks <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>.</p><p>Clearly, depth has been giving us good results. But learning better networks is not as simple as adding more layers. A big deterrent for this is the vanishing gradient problem which hampers convergence of layers <ref type="bibr" target="#b20">[21]</ref>.</p><p>A solution to the vanishing gradient problem has been normalized initialization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Another solution has been the introduction of intermediate normalization layers <ref type="bibr" target="#b14">[15]</ref>. These enabled very deep networks to start converging for SGD (stochastic gradient descent) with back propagation <ref type="bibr" target="#b23">[24]</ref>.</p><p>Networks with an extremely high number of layers were giving high accuracy results on Imagenet as mentioned before. For example, <ref type="bibr" target="#b24">[25]</ref> had 19 layers, <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> surpassed 100 layers. Eventually, it was seen in <ref type="bibr" target="#b27">[28]</ref> that not going deeper, but going wider provided a higher accuracy.</p><p>The problem with deeper networks as mentioned above is the vanishing gradient problem. To solve this we could bypass signal between layers using identity connections as seen in the popular networks Resnets <ref type="bibr" target="#b25">[26]</ref> and Highway networks <ref type="bibr" target="#b26">[27]</ref>.</p><p>Repeatedly combining multiple parallel layer sequences with a varying number of convolutional blocks was done in order to get a large nominal depth in FractalNets <ref type="bibr" target="#b28">[29]</ref>. This was done along with maintaining multiple short paths in the network. The main similarity between <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> was that they all created short paths from early layers to the later layers.</p><p>Densenet <ref type="bibr" target="#b29">[30]</ref> proposed connecting all layers to ensure information from each layer is passed on to every other layer. They also showed state-of-the-art results on popular image classification tasks.</p><p>All these recent approaches gave excellent results, however, the number of parameters has been very high. We look at the possibility of reducing the number of parameters needed for performing the same task, whilst, ensuring the accuracy of classification remains high.</p><p>Also, all these recent approaches used images from the dataset directly as it is for the task of classification. We propose transformations of these images using color space conversions as the medium to do so.</p><p>Performing image classification tasks by preprocessing input with color conversion has been explored before. In <ref type="bibr" target="#b30">[31]</ref> for instance, YCbCr was the color space used for skin detection. Color pixel classification was done in <ref type="bibr" target="#b31">[32]</ref> using a hybrid color space. Soccer image analysis was done using a hybrid color space in <ref type="bibr" target="#b32">[33]</ref>.</p><p>To see if color space conversion actually makes a difference, an analysis was done on skin detection <ref type="bibr" target="#b33">[34]</ref>. They found that RGB color space was the model that gave the best results. Skin pixel classification was studied using a Bayesian model in <ref type="bibr" target="#b34">[35]</ref> with different color spaces. In this case, the authors found LAB color space gave best results for accuracy.</p><p>Based on the works in <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> we can say that the authors found conflicting results. But importantly we take from these works the fact that using different color spaces gave authors different results, which means we could experiment on the same. We exploit this idea for our approach. The main modification we do, which will be explained in the proposed approach section, is that we combine the color spaces model to obtain a higher accuracy. This is due to the fact, as we shall see in the next section, that the color spaces are not completely correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>The idea was thought of while trying the effects of color space conversion on the CIFAR-10 dataset <ref type="bibr" target="#b1">[2]</ref>. We started with a simple convolutional network. The network consisted of two convolutional layers followed by a max pooling layer. This was followed by a dropout layer, 2 more convolutional layers, one more max pooling, one more dropout layer and finished with a dense layer.</p><p>We started the classification on CIFAR-10 using the input data as it is i.e in RGB format. We obtained an accuracy of 78.89 percent and time needed was 26 seconds. Next, we performed the classification by introducing color space conversion. We did the same with HSV, LAB, YIQ, YPbPr, YCbCr, YUV, LCH, HED, LCH and XYZ. The reason we chose the selected color spaces was due to the ease to perform the conversion using Scikit library <ref type="bibr" target="#b35">[36]</ref>. The results of the classification can be seen in <ref type="table" target="#tab_0">Table 1</ref>. From the table, we can see that the results obtained were highest for LAB color space, whilst the time of execution remained constant. However, like previous papers <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> showed, the accuracy levels are not too distant showing that the color space conversion results in more or less the same results while adding the time needed for the conversion.</p><p>We decided to have a closer look at the results by observing the confusion matrix for each case. The confusion matrices are shown in <ref type="figure">Figure 1</ref> for RGB, HSV, YUV, LAB, HED, LCH, XYZ and YPbPr.</p><p>On closer inspection, we see that each color space gives different accuracy for different classes. There are some classes that have equal classification rate in some classes, however, the majority differ in some way. For example, class 4 in CIFAR-10 is detected with 82 percent accuracy using YPbPr, however, the same class is detected only with 72 percent in case of HED. Such differences can be noticed in multiple classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Confusion matrices for various color spaces</head><p>Based on these findings, we can conclude two important things. Firstly, there isn't 100 percent correlation between color spaces, which infers they can be used in combination to obtain better results. Secondly, certain classes of objects are being better represented by different color spaces.</p><p>We use these findings as the basis of our proposed model. We use small and wide Densenets as the base of our combination model. The proposed model is explored in the next section. Another important finding was that some color spaces are causing a loss of accuracy. We show a sample of such losses in <ref type="table" target="#tab_1">Table  2</ref>. Here, we individually combined multiple color spaces to obtain higher classification rates using the simple CNN proposed earlier. This table was created only to show the reason we exclude certain color spaces in the next section. For this, we use RGB, HSV, XYZ, HED, LAB and YUV.</p><p>We also tried a late fusion approach. This would decrease the number of parameters in the model.</p><p>The combination being talked about above is using the output of the simple CNN with a particular color space and combining the outputs to obtain an average output of the different color spaces. The idea will be better understood in the next subsection. However, the important finding was that we can use the combination of color spaces to obtain a high accuracy of classification.</p><p>As can be seen, an early fusion approach did not significantly alter the accuracy in comparison to a model with a single color space itself. Although, the number of parameters significantly reduced in comparison to the model with late fusion, the accuracy was also lower in case of early fusion. Hence, we have chosen a late fusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture of model used</head><p>The base of the model is using a Densenet for each color space. Essentially, the model consists of 7 Densenets, one for each color space being used, along with a final dense layer to give weighted predictions to classes. Densenet <ref type="bibr" target="#b29">[30]</ref> proposed the use of feed-forward connection between every layer in a network. They proposed the idea to overcome problems such as the vanishing gradient problem, but also believed that such a network could obtain remarkably high results with fewer parameters in comparison to other recent models. The Densenet model is represented as DenseNet-L-k where L stands for the depth of the model and k stands for the growth factor. However, the model that obtained highest accuracy in both CIFAR-10 and CIFAR-100 datasets, the DenseNet-190-40 needed 25.6M parameters. We believed that this could be reduced significantly if we could find a way to preprocess the data.</p><p>We notice from our observations in the previous section, that combining the outputs from the 7 CNNs increased the accuracy of the model from 78.89 percent to 86.14 percent. Which meant a rise close to 7 percent. This result provoked the thought that, using a combination of smaller densenets which use far fewer parameters than the ones that obtained state of the art results could help us to reach similar levels of accuracy if not obtain better ones. Along with this, we thought of the idea that a wider densenet could possibly obtain better results than a deeper densenet based on <ref type="bibr" target="#b27">[28]</ref>.</p><p>Using these two thoughts, we decided to implement a model that consisted of small densenets for each color space. We started by using a DenseNet-BC-40-12 which used only 0.25M parameters. The BC in the name refers to the use of bottleneck layers after each block to compress the size of the model.</p><p>The color spaces that were selected based on the results obtained by individually checking if the accuracy affects the overall model are: RGB, LAB, HSV, YUV, YCbCr, HED and YIQ. The overall architecture of the model is seen in <ref type="figure" target="#fig_1">Figure 2</ref>. The input RGB image is simultaneously converted into the 6 other color spaces and all 7 inputs are passed to Densenets. The output scores of each Densenet is then passed to a dense layer which helps to give weighted predictions to each color space. The output of the dense layer is used as the final classification score.</p><p>Many questions may arise for such a model, for example, the time needed for multiple color space conversions will cause an overhead. Also, the Densenets as mentioned in earlier sections has many parameters by itself, therefore, a combination of densenets will have, in this case, 7 times the number of parameters.</p><p>The first problem is something we do for the benefit of a higher accuracy which will be seen in later sections. The second, however, is solved by using smaller and wider densenets which use far fewer parameters than the models that have state of the art results on popular image classification datasets. The experimental results should help satisfy some of the questions regarding the time-accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We perform the experimental evaluation on CIFAR-10, CIFAR-100, SVHN and Imagenet.</p><p>The CIFAR-10 dataset consists of 60000 color images of size 32x32 each. There are 10 classes present. The dataset is divided into 50000 training images and 10000 test images. The CIFAR-100 dataset consists of 100 classes. We use data augmentation for these datasets and represent the results with a '+' sign. For example, C10+ refers to Cifar-10 results with augmentation. The augmentation done is horizontal flip and width/height shift.</p><p>The SVHN dataset (Street View House Numbers) contains 32x32 size images. There are 73,257 training images in the dataset and 26,032 test images in the dataset. Additionally, there are 531,131 images for extra training.</p><p>The Imagenet dataset consists 1.2 million training images and 50000 validation images. There are a total of 1000 classes. We apply single crop or 10 crop with size 224x224 at test time. We follow <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b29">[30]</ref> in showing the imagenet classification score on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>Since we use densenets as the base for building our model, we follow the training procedure followed in <ref type="bibr" target="#b29">[30]</ref>. Each individual densenet is trained using stochastic gradient descent (SGD). We use a batch size of 32 for CIFAR datasets and run the model for 300 epochs. The learning rate is initially 0.1 and is reduced to 0.01 after 75 epochs and 0.001 after 150 epochs. In case of SVHN we run the model for 100 epochs and the learning rate is initially 0.1 and is reduced to 0.01 after 25 epochs and 0.001 after 50 epochs.</p><p>As with the original densenet implementation, we use a weight decay of 0.0001 and apply Nesterov momentum <ref type="bibr" target="#b36">[37]</ref> of 0.9 without dampening. When we do not use data augmentation we add a dropout of 0.2.</p><p>There are 2 parameters that can affect the accuracy and number of parameters of our model. These are the growth factor and depth of network as was the case with original densenet <ref type="bibr" target="#b29">[30]</ref>. With this regard, here onwards, colornet-L-k refers to a colornet that has densenets-L-k as subparts of the colornet model. L here refers to the depth of the network and k the growth factor. We use densenets with bottleneck as the densenet model part of our model. <ref type="table" target="#tab_2">Table 3</ref> refers to the classification accuracies obtained on CIFAR-10 dataset for recent state of the art approaches. We compare the results obtained from our model with these approaches. In the table C10 refers to the accuracy of a particular model on CIFAR-10 and C10+ refers to the accuracy of the same model with data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification results on CIFAR-10</head><p>We compare our model with Network in Network <ref type="bibr" target="#b37">[38]</ref>, the All-CNN <ref type="bibr" target="#b38">[39]</ref>, Highway network <ref type="bibr" target="#b25">[26]</ref>, fractalnet <ref type="bibr" target="#b28">[29]</ref>, Resnet <ref type="bibr" target="#b26">[27]</ref>, Wide-Resnet <ref type="bibr" target="#b27">[28]</ref> and Densenet <ref type="bibr" target="#b29">[30]</ref>. As can be seen, the Colornet-40-48 obtains an error rate of just 1.54 for CIFAR-10 with augmentation, which to the best of our knowledge obtains a new state of the art classification accuracy. Along with this a smaller Colornet model, the Colornet-40-12 with just 1.75M parameters has a better accuracy than Densenet-BC-250-24, with 15.3M parameters and is almost equal to that of Densenet-BC-190-40 which has 25.6M parameters. <ref type="table" target="#tab_3">Table 4</ref> refers to the classification accuracies obtained on CIFAR-100 dataset for recent state of the art approaches. We compare the results obtained from our model with these approaches. In the table C100 refers to the accuracy of a particular model on CIFAR-100 and C100+ refers to the accuracy of the same model with data augmentation. We compare our model with Network in Network <ref type="bibr" target="#b37">[38]</ref>, the All-CNN <ref type="bibr" target="#b38">[39]</ref>, Highway network <ref type="bibr" target="#b25">[26]</ref>, fractalnet <ref type="bibr" target="#b28">[29]</ref>, Resnet <ref type="bibr" target="#b26">[27]</ref>, Wide-Resnet <ref type="bibr" target="#b27">[28]</ref> and Densenet <ref type="bibr" target="#b29">[30]</ref>. As can be seen, the Colornet-40-48 obtains an error rate of 11.68 for CIFAR-10 with augmentation, which to the best of our knowledge obtains a new state of the art classification accuracy. Along with this a smaller Colornet model, the Colornet-40-12 with just 1.75M parameters has a better accuracy than Densenet-BC-250-24, with 15.3M parameters and is almost equal to that of Densenet-BC-190-40 which has 25.6M parameters. <ref type="table" target="#tab_4">Table 5</ref> refers to the classification accuracies obtained on the imagenet dataset for recent state of the art approaches. Top-1 and Top-5 accuracy is compared for each approach. The error rates are represented as x/y where x represents error rate for single-crop testing and y for 10-crop testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Classification results on CIFAR-100</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Classification results on Imagenet</head><p>We compare our model with Densenet as it is the paper that shows state of the art results for imagenet. As can be seen, the Colornet-121, which replaces all the Densenets in the proposed model with Densenets-121 obtains a new state of the art accuracy on imagenet dataset to the best of our knowledge. <ref type="table" target="#tab_5">Table 6</ref> refers to the classification accuracies obtained on SVHN dataset for recent state of the art approaches. We compare the results obtained from our model with these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Classification results on SVHN</head><p>We compare our model with Network in Network <ref type="bibr" target="#b37">[38]</ref>, fractalnet <ref type="bibr" target="#b28">[29]</ref>, Resnet <ref type="bibr" target="#b26">[27]</ref>, Wide-Resnet <ref type="bibr" target="#b27">[28]</ref> and Densenet <ref type="bibr" target="#b29">[30]</ref>. As can be seen, the Colornet-40-48 obtains an error rate of 1.12 for SVHN, which to the best of our knowledge obtains new state of the art classification accuracy. Along with this a smaller Colornet model, the Colornet-40-12 with just 1.75M parameters has a better accuracy than Densenet-BC-250-24, with 15.3M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Further analysis of results</head><p>We further breakdown the reported results into the four error metrics namely true positives (TP), true negatives TN), false positives (FP) and false negatives (FN) for each dataset. <ref type="table" target="#tab_6">Table 7</ref> highlights the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We found that preprocessing images by transforming them into different color spaces yielded different results. Although, the accuracy by itself did not vary too much, on closer inspection with the aid of confusion matrices we could see that there wasn't a 100 percent correlation between the results. We could see that certain classes were being better represented in certain color spaces. Using this as the idea, we dug deeper to see that the LCH, YPbPr and XYZ color spaces reduced the overall accuracy of the model and hence were discarded. We chose 7 color spaces as models that could be combined to obtain high accuracy of classification. These color spaces included RGB, YIQ, LAB, HSV, YUV, YCbCr and HED.</p><p>We used a densenet model as the base of the proposed architecture. We combined 7 densenets with the input to each being a different color space transformation of the original input image. The outputs of each densenet was sent to a dense layer to obtain weighted predictions from each densenet. Using a densenet model helped us to deal with common issues such as the vanishing gradient problem, the problem of overfitting among others.</p><p>Based on the results, we could see that state of the art results was obtained. We could compete against models with 27M parameters using a model of just 1.75M parameters.</p><p>Although the accuracy reached state of the art level, the time needed could still be optimized. For starters, the preprocessing step of converting to each color space needs a certain amount of time by itself. For small images like with the case of CIFAR or SVHN, this preprocessing can be done in real-time. However, for bigger images, like the ones in imagenet the time needed creates a cost-overhead. In addition to this, there is the computation overhead of using several densenet models. Although, we dealt with this using smaller and wider densenets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Supported by NSFC project Grant No. U1833101, Shenzhen Science and Technologies project under Grant No. JCYJ20160428182137473 and the Joint Research Center of Tencent and Tsinghua.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of proposed model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of results for different color spaces on CIFAR-10 with simple CNN</figDesc><table><row><cell cols="3">Color Space Accuracy Time</cell></row><row><cell>RGB</cell><cell>78.89</cell><cell>26 secs</cell></row><row><cell>HSV</cell><cell>78.57</cell><cell>26 secs</cell></row><row><cell>YUV</cell><cell>78.89</cell><cell>26 secs</cell></row><row><cell>LAB</cell><cell>80.43</cell><cell>26 secs</cell></row><row><cell>YIQ</cell><cell>78.79</cell><cell>26 secs</cell></row><row><cell>XYZ</cell><cell>78.72</cell><cell>26 secs</cell></row><row><cell>YPbPr</cell><cell>78.78</cell><cell>26 secs</cell></row><row><cell>YCbCr</cell><cell>78.81</cell><cell>26 secs</cell></row><row><cell>HED</cell><cell>78.98</cell><cell>26 secs</cell></row><row><cell>LCH</cell><cell>78.82</cell><cell>26 secs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of results for combination of different color spaces on CIFAR-10 with simple CNN</figDesc><table><row><cell>Color Space</cell><cell cols="2">Accuracy Number of color spaces used</cell></row><row><cell>RGB+HSV</cell><cell>81.42</cell><cell>2</cell></row><row><cell>RGB+YUV</cell><cell>81.41</cell><cell>2</cell></row><row><cell>HSV+YUV</cell><cell>81.97</cell><cell>2</cell></row><row><cell>RGB+LAB</cell><cell>81.91</cell><cell>2</cell></row><row><cell>LAB+HSV</cell><cell>81.95</cell><cell>2</cell></row><row><cell>YUV+LAB</cell><cell>82.05</cell><cell>2</cell></row><row><cell>RGB+HSV+YUV</cell><cell>82.33</cell><cell>3</cell></row><row><cell>RGB+HSV+LAB</cell><cell>82.49</cell><cell>3</cell></row><row><cell>RGB+LAB+YUV</cell><cell>82.62</cell><cell>3</cell></row><row><cell>LAB+HSV+YUV</cell><cell>82.66</cell><cell>3</cell></row><row><cell>RGB+HSV+YUV+LAB</cell><cell>82.96</cell><cell>4</cell></row><row><cell cols="2">RGB+HSV+YUV+LAB+HED(RHYLH) 83.61</cell><cell>5</cell></row><row><cell>RGB+HSV+YUV+LAB+HED+XYZ</cell><cell>82.81</cell><cell>6</cell></row><row><cell>RHYLH with Conv Layer</cell><cell>84.32</cell><cell>5</cell></row><row><cell>RHYLH with early fusion</cell><cell>81.63</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of error rates for CIFAR-10</figDesc><table><row><cell>Model Name</cell><cell cols="3">No of Parameters C10 C10+</cell></row><row><cell>N-in-N [38]</cell><cell>-</cell><cell cols="2">10.41 8.81</cell></row><row><cell>all-CNN [39]</cell><cell>-</cell><cell cols="2">9.08 7.25</cell></row><row><cell>Highway Network [26]</cell><cell>-</cell><cell>-</cell><cell>7.72</cell></row><row><cell>Fractalnet [29]</cell><cell>38.6M</cell><cell cols="2">10.18 5.22</cell></row><row><cell cols="2">Fractalnet [29] with dropout 38.6M</cell><cell cols="2">7.33 4.60</cell></row><row><cell>Resnet-101 [27]</cell><cell>1.7M</cell><cell cols="2">11.66 5.23</cell></row><row><cell>Resnet-1202 [27]</cell><cell>10.2M</cell><cell>-</cell><cell>4.91</cell></row><row><cell>Wide-Resnet-28 [28]</cell><cell>36.5M</cell><cell>-</cell><cell>4.17</cell></row><row><cell>Densenet-BC-100-12 [30]</cell><cell>0.8M</cell><cell cols="2">5.92 4.51</cell></row><row><cell>Densenet-BC-250-24 [30]</cell><cell>15.3M</cell><cell cols="2">5.19 3.62</cell></row><row><cell>Densenet-BC-190-40 [30]</cell><cell>25.6M</cell><cell>-</cell><cell>3.46</cell></row><row><cell>Colornet-40-12</cell><cell>1.75M</cell><cell cols="2">4.98 3.49</cell></row><row><cell>Colornet-40-48</cell><cell>19.0M</cell><cell cols="2">3.14 1.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of error rates for CIFAR-100</figDesc><table><row><cell>Model Name</cell><cell cols="3">No of Parameters C100 C100+</cell></row><row><cell>N-in-N [38]</cell><cell>-</cell><cell cols="2">35.68 -</cell></row><row><cell>all-CNN [39]</cell><cell>-</cell><cell>-</cell><cell>33.71</cell></row><row><cell>Highway Network [26]</cell><cell>-</cell><cell>-</cell><cell>32.29</cell></row><row><cell>Fractalnet [29]</cell><cell>38.6M</cell><cell cols="2">35.34 23.30</cell></row><row><cell cols="2">Fractalnet [29] with dropout 38.6M</cell><cell cols="2">28.20 23.73</cell></row><row><cell>Resnet-101 [27]</cell><cell>1.7M</cell><cell cols="2">37.80 24.58</cell></row><row><cell>Wide-Resnet-28 [28]</cell><cell>36.5M</cell><cell>-</cell><cell>20.50</cell></row><row><cell>Densenet-BC-100-12 [30]</cell><cell>0.8M</cell><cell cols="2">23.79 20.20</cell></row><row><cell>Densenet-BC-250-24 [30]</cell><cell>15.3M</cell><cell cols="2">19.64 17.60</cell></row><row><cell>Densenet-BC-190-40 [30]</cell><cell>25.6M</cell><cell>-</cell><cell>17.18</cell></row><row><cell>Colornet-40-12</cell><cell>1.75M</cell><cell cols="2">19.86 17.42</cell></row><row><cell>Colornet-40-48</cell><cell>19.0M</cell><cell cols="2">15.62 11.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of error rates for Imagenet</figDesc><table><row><cell>Model Name Top-1</cell><cell>Top-5</cell></row><row><cell>DenseNet-121 25.02 / 23.61</cell><cell>7.71 / 6.66</cell></row><row><cell>DenseNet-201 22.58 / 21.46</cell><cell>6.34 / 5.54</cell></row><row><cell>DenseNet-264 22.15 / 20.80</cell><cell>6.12 / 5.29</cell></row><row><cell cols="2">Colornet-121 17.65 / 15.42 5.22 / 3.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison of error rates for SVHN</figDesc><table><row><cell>Model Name</cell><cell cols="2">No of Parameters SVHN</cell></row><row><cell>N-in-N [38]</cell><cell>-</cell><cell>2.35</cell></row><row><cell>Fractalnet [29]</cell><cell>38.6M</cell><cell>2.01</cell></row><row><cell cols="2">Fractalnet [29] with dropout 38.6M</cell><cell>1.87</cell></row><row><cell>Resnet-101 [27]</cell><cell>1.7M</cell><cell>1.75</cell></row><row><cell>Densenet-BC-100-12 [30]</cell><cell>0.8M</cell><cell>1.76</cell></row><row><cell>Densenet-BC-250-24 [30]</cell><cell>15.3M</cell><cell>1.74</cell></row><row><cell>Colornet-40-12</cell><cell>1.75M</cell><cell>1.59</cell></row><row><cell>Colornet-40-48</cell><cell>19.0M</cell><cell>1.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Further analysis of obtained results for various datasets 10+ 98.68 98.24 1.44 1.72 Colornet-40-48 C-100 84.12 84.64 15.88 15.33 Colornet-40-48 C-100+ 88.65 87.99 11.35 12.01 Colornet-40-48 SVHN 98.90 98.86 1.11 1.13</figDesc><table><row><cell>Model</cell><cell cols="2">Dataset TP</cell><cell>TN</cell><cell>FP</cell><cell>FN</cell></row><row><cell cols="2">Colornet-40-48 C-10</cell><cell cols="4">97.14 96.78 2.86 3.22</cell></row><row><cell cols="2">Colornet-40-48 C-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<title level="m">The CIFAR-10 dataset</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">December. Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer vision and Image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human activity recognition using combinatorial Deep Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1589" to="1594" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face verification across age progression using facial feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal and Information Processing (IConSIP), International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions. CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fiducial points detection of a face using RBF-SVM and Adaboost Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">european conference on computer vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Age Estimation by LS-SVM Regression on Facial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Bayesian approach to skin color classification in YCbCr color space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TENCON 2000. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="421" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Color pixels classification in an hybrid color space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandenbroucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Macaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Postaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP 98. Proceedings. 1998 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="176" to="180" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Color image segmentation by pixel classification in an adapted hybrid color space. Application to soccer image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandenbroucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Macaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Postaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="216" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Does colorspace transformation make any difference on skin detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Tsap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Sixth IEEE Workshop on</title>
		<meeting>Sixth IEEE Workshop on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="275" to="279" />
		</imprint>
	</monogr>
	<note>Applications of Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison of five color models in skin pixel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Zarit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Super</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
	<note>International Workshop on</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>scikit-image: image processing in Python</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Striving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
