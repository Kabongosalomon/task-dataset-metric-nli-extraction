<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Indoor Depth Completion with Boundary Consistency and Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kai</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
							<email>whsu@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Indoor Depth Completion with Boundary Consistency and Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation features are helpful for 3D recognition. Commodity-grade depth cameras are able to capture depth and color image in real-time. However, glossy, transparent or distant surface cannot be scanned properly by the sensor. As a result, enhancement and restoration from sensing depth is an important task. Depth completion aims at filling the holes that sensors fail to detect, which is still a complex task for machine to learn. Traditional hand-tuned methods have reached their limits, while neural network based methods tend to copy and interpolate the output from surrounding depth values. This leads to blurred boundaries, and structures of the depth map are lost. Consequently, our main work is to design an end-to-end network improving completion depth maps while maintaining edge clarity. We utilize self-attention mechanism, previously used in image inpainting fields, to extract more useful information in each layer of convolution so that the complete depth map is enhanced. In addition, we propose boundary consistency concept to enhance the depth map quality and structure. Experimental results validate the effectiveness of our selfattention and boundary consistency schema, which outperforms previous state-of-the-art depth completion work on Matterport3D dataset. Our code is publicly available at https://github.com/tsunghan-wu/Depth-Completion</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this task, we take single input RGB and raw sensing depth to complete depth value of missing parts. Depth sensing is widely used in real-world applications from autonomous driving, robotics, augmented reality to scene reconstruction. Most of the applications are for recognition purpose. However, commercially available RGB-D cameras, such as Microsoft Kinect and Intel RealSense, still fail on sensing depth map without corruption or noise along object edges. Sensors produce large amounts of missing pixels when surface is shiny, transparent, too close or too far. In * equal contribution <ref type="figure">Figure 1</ref>. Our work produces clearer structures and overcomes the hardness that previous works suffer from: interpolation and blurred boundaries. On the first row, our model provides clear complete depth but FCN interpolates depth value (marked in red) at the large missing area of raw depth map. On the second row, our model can assure boundary sharpness, while FCN generates blurred boundaries. indoor dataset Matterport3D <ref type="bibr" target="#b2">[3]</ref>, for example, over 15% of values in raw depth are missing. As a consequence, completing large missing parts of sensor depth is of crutial importance.</p><p>Traditional works reconstruct depth value by combining multiple views of sensor data. Simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> method incrementally builds a consistent depth map of the environment while simultaneously determines location within this map. Previous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> utilizes SLAM to estimate more accurate depth map. In our work, we do not consider multiple views of reconstruction. Rather, we put emphasis on single view for depth prediction, because single view prediction is more efficient when consider inference time and computing power.</p><p>For single view of depth completion, previous works that tried to solve the problem have their own issue. Traditional mathematical methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> contain much hand-tuning hyper-parameters and reach their limits in filling high quality depth holes. Recent deep learning methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref> performed well yet only learned to interpolate or copy-andpaste depth value from neighboring pixels. Blurred boundaries and structure can be easily recognized by visualization of output depth. In <ref type="figure">Figure 1</ref>, the visualization results show the above issue of copy-and-paste, interpolation, blurred boundaries and structure. We conclude the issues of deep learning based methods in two orientations: estimating precise depth value and producing clear structure.</p><p>In this paper, we solve the issues mentioned above. Previous works pass the whole feature maps into the network and perform convolution. However, to complete the missing depths, some regions may be more important and some may be minor details depending on the semantics and geometric meanings of one scene. Inspired by Yu et al. <ref type="bibr" target="#b35">[36]</ref>, we leverage the self-attention network to encourage the model to pay attention on relevant parts, especially the semantics of the scene. Our model benefits from the attention mechanism and outputs depth maps with comprehensive consideration of the attentioned parts. Furthermore, with useful information from depth representation (surface normals and occlusion boundaries), proposed by <ref type="bibr" target="#b38">[39]</ref>, our self-attention model meets the needs of estimating more precise depth value rather than just interpolation.</p><p>In addition, we solve the issue of vague structures with boundary consistency concept. In order to generate clear depth structures, we equip another network to predict occlusion boundaries given the output depth from depth completion network. As a result, the depth completion network is indirectly refined to preserve clear boundaries in the output depth, making the complete depth image to be more structured and conformed to realistic situation. To the best of our knowledge, we are the first to apply self-attention mechanism and regard boundary as the main feature in indoor depth completion task.</p><p>On the whole, our main contributions of our work are:</p><p>? Enhance depth completion task using self-attention mechanism, which has never been used in RGB-D estimation before.</p><p>? Propose a novel idea, boundary consistency, to produce depth map of clear structure.</p><p>? Reach state-of-the-art performance on Matterport3D <ref type="bibr" target="#b2">[3]</ref> on RGB-D depth completion task.</p><p>Extensive ablation study and visualization results validate our proposed idea and concept. The promising results on depth completion task makes contribution to recognition applications such as robotics and autonomous driving. Those applications suffer from various types of degradation of low-quality visual data, including large missing holes of sensor depth map. Our work enhances the robustness for recognition on sensor depth, and, consequently, contributes to real-world recognition from low-quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We introduce order as follows: depth estimation, superresolution, reconstruction from sparse samples, image inpainting, and, finally, our task depth completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Depth Estimation</head><p>Depth estimation predicts depth value from monocular RGB image. This is a long-established problem in computer vision history. However, with little information of a single view image, this is still a hard task even for human beings to answer the exact depth distance <ref type="bibr" target="#b25">[26]</ref>. Classic methods like Shape from Shading <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> and depth from focus <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> elaborated physical and mathematical property about light shading and focal setting at each pixel. Recent works extended classic methods by machine learning, like deep depth from focus <ref type="bibr" target="#b11">[12]</ref> and deep estimation based on fourier domain analysis <ref type="bibr" target="#b18">[19]</ref>. Fully connected convolution (FCN) networks are used to predict depth map <ref type="bibr" target="#b17">[18]</ref> or refine coarse-scale depth value <ref type="bibr" target="#b5">[6]</ref>.</p><p>Previous works give us insight into the methods to solve the task, but different from what our topic focuses on. Specifically, depth estimation generate raw depth based on RGB image, while we utilize RGB to improve raw depth to become complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Depth Super-Resolution</head><p>The focus of depth super-resolution is the enhancement of spatial resolution. Generally, raw depth data are regularly sampled or quantized with low resolution. Yang et al. <ref type="bibr" target="#b34">[35]</ref> iteratively refined depth map in terms of both its spatial resolution and depth precision with bilateral filtering. Mac et al. <ref type="bibr" target="#b22">[23]</ref> utilized Markov random field to select candidate of depth patch. Some other works are shape-from-shading <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>, rigid body self-similarity <ref type="bibr" target="#b14">[15]</ref>, and deep learning based network <ref type="bibr" target="#b27">[28]</ref>.</p><p>Different from depth completion, these works take complete but low-resolution depth map as input. In our work, we aim at recovering the large missing parts of the depth map, which cannot be recovered by super-resolution methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Depth Reconstruction from Sparse Samples</head><p>Depth reconstruction from sparse samples is an interesting topic that tries to reconstruct full depth map from sparse one. Restrictions of camera costs and power consumption make it a necessary research topic. The difficulties of the topic are how to maintain decent performance, while saving costs and energy. Ma's method <ref type="bibr" target="#b23">[24]</ref> can be used as a plug-in module in sparse SLAM and visual inertial odometry algorithms, creating more accurate, dense point cloud. Also, Ma <ref type="bibr" target="#b21">[22]</ref> proposed a self-supervised framework without the need for dense labels, achieving great performance on KITTI <ref type="bibr" target="#b7">[8]</ref> dataset. Some works combined semantic segmentation <ref type="bibr" target="#b15">[16]</ref> to improve the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Image Inpainting</head><p>Image inpainting is related to depth completion in some ways. For example, free-form image inpainting <ref type="bibr" target="#b35">[36]</ref> allows <ref type="figure">Figure 2</ref>. Overview of the network architecture. Main improvement of our method are twofold: self-attention mechanism 3.1 (blue dashed box) and boundary consistency 3.2 (green dashed box). Self-attention network pays attention on useful feature information. On the other hand, boundary consistency loss forces attention network to preserve boundary information, allowing network to reconstruct more structured depth map. users to erase values of a RGB image, and then network inpaints the missing parts of RGB values as real as possible. Compared to our task, the same part is the missing holes of depth sensor are similar to the erased parts of RGB image. The different part is depth completion network is trained to learn precise value of depth map supervised with correct answer, while image inpainting results do not require a correct answer but a real and reasonable one. Self-attention mechanism, which applies attention on each layer of convolution, is useful and powerful for image inpainting task. Yu et al. utilized self-attention mechanism on partial convolution <ref type="bibr" target="#b19">[20]</ref> and gated convolution <ref type="bibr" target="#b35">[36]</ref> and made astonishing performance on the inpainting image. We introduce the concept of self-attention on our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Depth Completion</head><p>Depth completion is to fill in missing holes in the relatively dense depth images. The definition of depth completion concept is proposed by Zhang et al. <ref type="bibr" target="#b38">[39]</ref>. Earlier works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> evaluated performance on pixels captured by commodity RGB-D cameras. Under the circumstances, they can at best output raw depth (captured by the sensor), which contains large missing holes. Zhang extended the term "Depth Completion" to predicting the complete depth map generated from multi-view reconstruction.</p><p>In Zhang's work, he proved surface normals and occlusion boundaries from RGB are two geometric representations best for deep depth completion. Zhang also claimed that deep regression method simply learns to copy and interpolate depth value, so he optimized loss with sparse Cholesky factorization scheme <ref type="bibr" target="#b30">[31]</ref> and reach state-of-theart performance.</p><p>In our work, we re-verify the copy and interpolation issue of standard FCN. We overcome the obstacles that FCN encounters, and our model can generate depth map with clear boundaries and structures through an end-to-end neural network. Our approach outperforms the previous meth-ods. Compared to Zhang's, we have faster inference time by replacing the Cholesky optimization, which is more desirable for real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this paper, we investigate how to complete indoor depth image given a single RGB-D image with effective deep learning method. Our work mainly focuses on the following two questions: "How to overcome the difficulty that network simply learns to interpolate depth value?" and "how to make clear structures of depth image?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Attention Mechanism</head><p>The first issue we address is that previous deep learning methods tend to copy and interpolate nearby depth value as outputs as shown in <ref type="figure">Figure 1</ref>. Since in most cases, predicting average depth values for unobserved area makes the loss drop rapidly, so the network can easily fall into local minima of copy and interpolation instead of predicting precise depth values. To solve this problem, we propose to use self-attention mechanism on each convolution layer. The network is allowed to focus on precise feature values at each convolution stage and forward useful information. With self-attention, the network can be more robust in predicting precise depth values and would not be easily trapped in local minima of copy and interpolation.</p><p>We use gated convolution, proposed by Yu et al. <ref type="bibr" target="#b35">[36]</ref>, as our backbone component. Yu et al. used gated convolution on free-form image inpainting task. We use gated convolution on our task, since, first, depth completion task aims to complete missing depth value, which is similar to free-form masks from users. Additionally, completing precise depth value is similar to inpainting with realistic RGB pixel values. As a result, we gain insight from the strong connection between the two different tasks.</p><p>For an input I of a convolution block, and convolution blocks for feature extraction Conv f and for gating Conv g , the mathematical formulation for self-attention mechanism can be presented as:</p><formula xml:id="formula_0">Gating = ? (SN (Conv g (I))) (1) Feature = ? (SN (Conv f (I))) (2) Output = Feature Gating<label>(3)</label></formula><p>where ? is sigmoid function, SN is spectral normalization, ? is any activation function, and is the pixel-wise multiplication. The self-attention mechanism is in the presence of an additional gating operation that Feature Gating. It allows the model to learn effective dynamic feature selection, and also highlights the feature meanings for each channel map and each spatial location.</p><p>Since Conv g learns to identify region that is useful and important, according to the above equation, the model preserves useful region of Feature in the Output. Therefore, the gated convolution can predict more accurate depth values by paying more attention on local and detailed information extracted by the self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Boundary consistency</head><p>Ordinary FCN methods are likely to generate depth image of copy and interpolation as explained above. This causes the output depth to have blurred boundaries and structures. For instance, line segments may be slightly distorted and object occlusion boundary is vague in most cases. Since depth completion is a complex regression problem, FCN models can hardly learn to generate depth map of sharp boundaries where neighboring pixel values differ greatly. In order to fix the problem, we introduce boundary consistency to encourage models to learn clear boundaries and structures.</p><p>Boundary consistency constrains the output depth map to have boundary information by constructing another network, boundary consistency network. The boundary consistency network, modified from U-Net <ref type="bibr" target="#b28">[29]</ref>, takes complete depth as input and predicts the occlusion boundary. It is trained end-to-end along with self-attention network. The boundary consistency network encourages the self-attention network to generate sharp edges so that boundary consistency network can estimate more precise boundary values. With moderate balance between the weights of losses, our self-attention network successfully completes depth map with sharper and clearer structures. The overview pipeline is shown in <ref type="figure">Figure 1</ref>, we add an additional encode-decode boundary consistency network to the original self-attention network. We name it "boundary consistency" because we use occlusion boundary as depth input feature and require our model to preserve the boundary information in the complete depth map.</p><p>The boundary estimation result is supervised by Sobel <ref type="bibr" target="#b16">[17]</ref> boundary from ground truth depth image. There are two reasons we use Sobel as our target loss. (1) The generated edges are sharp, clear and few variate compared to some other methods. (2) Sobel Algorithm can extract occlusion boundaries without being too sensitive to the noise.</p><p>With the help of occlusion boundary ground truth from Sobel, our generated depth map can learn sharper and clearer structures, preventing outputs like blurred boundaries and interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Representation</head><p>In Zhang et al. <ref type="bibr" target="#b38">[39]</ref> work, he mentioned two important depth representations: surface normals and occlusion boundaries. Surface normals relate strongly to local lighting variations, providing more information about local surface properties. Occlusion boundaries also represent local texture features observed from RGB images. Both of the two representations can be robustly detected by networks, called depth representation network in the following. Because it is proven by <ref type="bibr" target="#b38">[39]</ref> that surface normals and occlusion boundaries are powerful, we simply combine the two representations with RGB and raw depth as the input of self-attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>The following section would introduce each component of our loss in detail. The total loss can be written as: Then, RGB, depth representations and raw depth are combined as input of self-attention network which predicts complete depth D. The ground truth depth D 0 comes from rendering multiple raw depth views of the same scene. The loss for self-attention network is L SA = 1 |p?obs| p?obs D(p) ? D 0 (p) . Note that we only consider observed pixels, which have valid depth values in ground truth D 0 , as mentioned in <ref type="bibr" target="#b38">[39]</ref>. Missing parts in ground truth depth map are ignored. We denote p ? obs to be observed pixels.</p><formula xml:id="formula_1">L = L SA ? ? S L S + ? BC L BC + ? N L N + ? B L B</formula><p>To enhance structural information, we add small fraction of structural related loss L S , Structural Similarity Index (SSIM) <ref type="bibr" target="#b32">[33]</ref>. SSIM measures quality assessment based on the degradation of structural information. In our task, the higher SSIM index is, the more structured the complete depth is. With SSIM, we expect our network to learn higher quality and undistorted depth map with better structure.</p><p>Finally, complete depth D is fed into boundary consistency network and outputs B. The ground truth depth D 0 <ref type="figure">Figure 3</ref>. In the visualization result, we can see that our method learns better on the geometric meaning. For example, on the first row, our model notices the corridors on the right hand side next to the pillar and judge the depth to be deep (mark in red). Other methods tend to interpolate the large vacancy with nearby values, like Zhang fills in light blue depth value.</p><p>generates ground truth B 0 with Sobel <ref type="bibr" target="#b16">[17]</ref>. The boundary consistency loss is</p><formula xml:id="formula_2">L BC = 1 |p| p B(p) ? B 0 (p) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We did not use the widely-known NYUv2 dataset <ref type="bibr" target="#b29">[30]</ref>, since the dataset does not provide complete ground-truth depth map for a single captured RGB-D image. Instead, we ran the experiments on Matterport3D dataset <ref type="bibr" target="#b2">[3]</ref> to evaluate our proposed methods. Matterport3D is an indoor largescale RGB-D dataset with 10.8k real panoramic views and 90 real indoor scenes. The dataset is scalable, diverse and representative of indoor depth completion task. We use the same training and testing lists as Zhang <ref type="bibr" target="#b38">[39]</ref> did. We remove large bias image pairs (about 5%), and, finally, the dataset contains about 1M training data and 474 testing data. The ground truth of Matterport3D is generated from Zhang by multi-view reconstruction; that is, multiple camera views of the same scene can reconstruct a more complete indoor information, including depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We use similar evaluation metrics in <ref type="bibr" target="#b38">[39]</ref>. As mentioned in 3.4, we calculate loss based on observed data; that is, observed pixel values in ground truth depth. Given ground truth depth D 0 and complete depth D, the metrics include:</p><p>? Root Mean Square Error (RMSE):</p><formula xml:id="formula_3">1 |obs| p?obs D(p) ? D 0 (p) 2<label>(5)</label></formula><p>? Mean Error (Mean):</p><formula xml:id="formula_4">1 |obs| p?obs D(p) ? D 0 (p)<label>(6)</label></formula><p>? SSIM <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_5">(2? D0(p) ? D(p) + c 1 )(2? D0(p)D(p) + c 2 ) (? 2 D0(p) + ? 2 D(p) + c 1 )(? 2 D0(p) + ? 2 D(p) + c 2 )<label>(7)</label></formula><p>where c 1 = 0.0001, c 2 = 0.0009.</p><p>? ? t : percentage of pixels within the error range t, the error range is defined by equation 8.</p><formula xml:id="formula_6">max( D(p) D 0 (p) , D 0 (p) D(p) ) &lt; t<label>(8)</label></formula><p>where t ? {1.05, 1.10, 1.25, 1.25 2 , 1.25 3 }, p ? obs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance</head><p>As shown in <ref type="table">Table 1</ref>, our proposed method defeats all other works, including traditional methods and previous state-of-the-art <ref type="bibr" target="#b38">[39]</ref>, and shows significant performance gain on RMSE and Mean error. Not only SSIM metric but also most delta values show improvement with boundary consistency and self-attention. SSIM is an index standing for the depth quality and structure. Delta percentage is a measurement of relative depth difference, which means the pixels closer to depth sensor requires more precise prediction. Our model attains the highest performance on all of the above evaluation metrics. <ref type="figure">Figure 3</ref> shows the visualization results. Previous methods sometimes misunderstands the geometric meaning of one scene once raw depth is missing large coverage of depth value, which cause copy and interpolation. On the contrary, our self-attention network realizes the geometric features from RGB, normal and boundary and produces depth maps with more accurate geometric meaning than others.  <ref type="table">Table 1</ref>. We achieve state-of-the-art performance on Matterport3D dataset. We compare our method with some inpainting methods, including traditional methods, such as anisotropic diffusion, joint bilateral filter, FCN-based methods (ResNet18) and Zhang <ref type="bibr" target="#b38">[39]</ref> et al.</p><p>The result shows our significant improvement on all of the evaluation metrics. circle in <ref type="figure">Figure 3</ref> shows the power of self-attention mechanism on our task. Our network notices detailed geometric properties, like walls and corridors. Besides, with boundary consistency <ref type="figure" target="#fig_0">Figure 4</ref>, our network learns architectural details, like the wall's edge, and produces depth with desirable quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>For better understanding of our works, we investigate the effects of each component of our method. The following ablation study concentrates on the following three things: self-attention, SSIM loss and boundary consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Self-Attention</head><p>The first two rows of <ref type="table">Table 2</ref> exam whether self-attention mechanism has positive effect on the result of complete depth. Indeed, self-attention makes huge gains of performance over traditional FCN models. In this setting, we take ResNet18 <ref type="bibr" target="#b12">[13]</ref>, which has similar parameters, as the classic FCN method.</p><p>As we can see in <ref type="figure" target="#fig_1">Figure 5</ref>, on the right hand side of corridors next to the pillar, model with self-attention(SA) notices those depth value should be distant, while without SA misunderstands the geometric meaning. We believe that the improvement comes from the attention on convolution features to helps the model focus on important regions and features. In this case, self-attention mechanism helps model to learn more geometric meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">SSIM Loss</head><p>By adding small weight of SSIM loss to optimize, the selfattention network learns to balance structural information without degradation much on RMSE and delta percentage. The straightforward results that SSIM score improves 8.6% after adding SSIM loss represent network successfully produce higher quality of depth map values.</p><p>As we can see in <ref type="figure" target="#fig_1">Figure 5</ref>, on the second row of columns w/SA and SA+SSIM, the background surface is smooth and accurate. Also, the edge detected from Sobel shows the enhancement of depth image quality and reduction in noise, which can be seen from the layered lines on the floor and ceiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Boundary Consistency</head><p>The last two rows of <ref type="table">Table 2</ref> show boundary consistency loss helps a lot in generating better depth map. There is extraordinary improvement in SSIM and delta percentage. 65.7% of ?1.05 and 75.8% of ?1.10 require highly sophisticated prediction of depth map for most of depth values. Besides, 3% of SSIM gains implies boundary consistency helps enhance the structure and sharpness of boundary and validates the effectiveness of boundary consistency.</p><p>As we can see in <ref type="figure" target="#fig_1">Figure 5</ref>, the structure of the bottom left sofa is clearer both in depth map and Sobel edge detection. Besides, the floor and ceiling are smoother with boundary consistency shown in the Sobel edge detection. <ref type="figure" target="#fig_0">Figure 4</ref>. With the help of boundary consistency and SSIM loss, our method produces depth map of clearer structures. The upper row is original output depth, and the lower row is the partial amplification of the upper one. In the left two columns, our wall structure is clear. In the right two columns, our depth quality is higher, since boundary detail is not blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison</head><p>In this section, we compare our main advantages over previous state-of-the-art work <ref type="bibr" target="#b38">[39]</ref>. Mentioned in Section 4.3, self-attention mechanism can be easier to learn geometric information than other methods. We find <ref type="bibr">Zhang</ref>  <ref type="table">Table 2</ref>. Effect of each component of our model. We show that our proposed self-attention mechanism (SA), SSIM loss and boundary consistency (BC) enhances completion performance. space, Zhang's method fell short of understanding the geometric meaning and easily fit onto noises in raw depth. The results seems to be simple interpolation but not realization of the whole view via clues from RGB and depth representation features. Running real-time of sensing depth and training time can be an issue for most depth completion models. Our work is end-to-end trainable and training time is faster than optimization-based algorithms. Zhang proposed model that was not end-to-end trainable and used global optimization. Our proposed method overcomes those difficulties and outperforms previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Attention Map</head><p>To show the self-attention mechanism can capture the structure information, we randomly sample the attention output from our network. The result is shown in <ref type="figure">Figure  6</ref>. The attention map is able to distinguish the occlusion boundaries and the geometric shape of the objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose to use self-attention mechanism and boundary consistency to improve performance in depth completion task. Self-attention mechanism allows network to learn more geometric meanings and complete depth with <ref type="figure">Figure 6</ref>. Visualization of some self-attention maps. Some filters pays attention on the missing holes (a), and some on the background and structures (b). More importantly, some filters pay attention on the semantics and object boundaries (c). For decoding phase, (d) shows the geometric meaning that the missing depth value behind the white walls at the right of the image should be relatively distant.</p><p>more precise values. Boundary consistency improves both the depth boundary and image quality, producing clearer and sharper structures. Extensive experiments demonstrate that our proposed method reach state-of-the-art on Matter-port3D dataset. Moreover, ablation study validates the effectiveness of each component we propose.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 4 )</head><label>4</label><figDesc>Given input RGB, depth representation networks predict normals N and boundaries B. The depth representation losses are L N = 1 |p| p N (p) ? N 0 (p) 2 and L B = 1 |p| p B(p) ? B 0 (p) 2 , where N 0 , B 0 are ground truths and p is a pixel in image channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>In the visualization result, we can see our proposed SA+SSIM+BC (self-attention, SSIM loss and boundary consistency) performs the best. The upper row is the Sobel edge detection results from the depth images of bottom row. From left to right images, we can see the depth background surface and bottom left sofa gradually improve both in structure and boundary. Model can learn better geometric meanings and semantics with the help of SA, SSIM and BC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The red Model RMSE? Mean? SSIM? 1.05 ? 1.10 ? 1.25 ? 1.25 2 ? 1.25 3 ?</figDesc><table><row><cell>Bilateral</cell><cell>1.978</cell><cell>0.774</cell><cell>0.507</cell><cell>0.385</cell><cell>0.497</cell><cell>0.613</cell><cell>0.689</cell><cell>0.730</cell></row><row><cell>MRF [11]</cell><cell>1.675</cell><cell>0.618</cell><cell>0.692</cell><cell>0.506</cell><cell>0.556</cell><cell>0.651</cell><cell>0.780</cell><cell>0.856</cell></row><row><cell>AD [21]</cell><cell>1.653</cell><cell>0.610</cell><cell>0.696</cell><cell>0.503</cell><cell>0.560</cell><cell>0.663</cell><cell>0.792</cell><cell>0.861</cell></row><row><cell>FCN</cell><cell>1.262</cell><cell>0.517</cell><cell>0.605</cell><cell>0.397</cell><cell>0.527</cell><cell>0.681</cell><cell>0.808</cell><cell>0.868</cell></row><row><cell>Zhang [39] 1</cell><cell>1.316</cell><cell>0.461</cell><cell>0.762</cell><cell>0.657</cell><cell>0.708</cell><cell>0.781</cell><cell>0.851</cell><cell>0.888</cell></row><row><cell>Ours</cell><cell>1.092</cell><cell>0.342</cell><cell>0.799</cell><cell>0.661</cell><cell>0.750</cell><cell>0.850</cell><cell>0.911</cell><cell>0.936</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>'s method depended heavily on raw depth. Once there are large holes in raw depth, like views of a wide and deep Model RMSE? Mean? SSIM? 1.05 ? 1.10 ? 1.25 ? 1.25 2 ? 1.25 3 ?</figDesc><table><row><cell>W/O SA</cell><cell>1.262</cell><cell>0.517</cell><cell>0.605</cell><cell>0.397</cell><cell>0.527</cell><cell>0.681</cell><cell>0.808</cell><cell>0.868</cell></row><row><cell>SA</cell><cell>1.095</cell><cell>0.400</cell><cell>0.706</cell><cell>0.497</cell><cell>0.629</cell><cell>0.785</cell><cell>0.881</cell><cell>0.923</cell></row><row><cell>SA+SSIM</cell><cell>1.096</cell><cell>0.397</cell><cell>0.767</cell><cell>0.488</cell><cell>0.626</cell><cell>0.787</cell><cell>0.884</cell><cell>0.926</cell></row><row><cell>SA+SSIM+BC</cell><cell>1.092</cell><cell>0.342</cell><cell>0.799</cell><cell>0.661</cell><cell>0.750</cell><cell>0.850</cell><cell>0.911</cell><cell>0.936</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We reproduce Zhang's work on github and find they use root median square error instead of root mean square error. Thus, we show the results reproduced ourselves.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 108-2634-F-002-004, FIH Mobile Limited, and Qualcomm Technologies, Inc., under Grant NAT-410477. We are grateful to the National Center for High-performance Computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous localization and mapping (slam): Part ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE robotics &amp; automation magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="108" to="117" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal hole filling strategy for kinect depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Three-dimensional image processing (3DIP) and applications Ii</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8290</biblScope>
			<biblScope unit="page">82900</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06158</idno>
		<title level="m">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Filling large holes in lidar data by inpainting depth gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simultaneous localization and mapping: part i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE robotics &amp; automation magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="110" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lsd-slam: Large-scale direct monocular slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth from focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grossmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="69" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High quality shape from a single rgb-d image under uncalibrated natural illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1617" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image and sparse laser fusion for dense scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep depth from focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="525" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rgbd mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="647" to="663" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth super resolution by rigid body self-similarity in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hornacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1123" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the accuracy of the sobel edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Guided depth enhancement via anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-supervised sparse-to-dense: self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00275</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Brostow. Patch based synthesis for single depth image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth image enhancement using local tangent plane approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3574" to="3583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perception of solid shape from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="137" to="151" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perception of solid shape from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape from shading</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="409" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Atgv-net: Accurate depth super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?ther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to applied mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aarikka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Wellesley-Cambridge Press</publisher>
			<biblScope unit="volume">16</biblScope>
			<pubPlace>Wellesley, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Depth from focus with your mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3497" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatial-depth super resolution for range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nist?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shading-based shape refinement of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1415" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape-fromshading: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
