<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Contrastive Optimization of Siamese Networks for Place Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a</forename><surname>Leyva-Vallina</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Strisciuglio</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Petkov</surname></persName>
						</author>
						<title level="a" type="main">Generalized Contrastive Optimization of Siamese Networks for Place Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-contrastive learning</term>
					<term>image retrieval</term>
					<term>siamese networks</term>
					<term>visual place recognition !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual place recognition is a challenging task in computer vision and a key component of camera-based localization and navigation systems. Recently, Convolutional Neural Networks (CNNs) achieved high results and good generalization capabilities. They are usually trained using pairs or triplets of images labeled as either similar or dissimilar, in a binary fashion. In practice, the similarity between two images is not binary, but continuous. Furthermore, training these CNNs is computationally complex and involves costly pair and triplet mining strategies. We propose a Generalized Contrastive loss (GCL) function that relies on image similarity as a continuous measure, and use it to train a siamese CNN. Furthermore, we present three techniques for automatic annotation of image pairs with labels indicating their degree of similarity, and deploy them to re-annotate the MSLS, TB-Places, and 7Scenes datasets. We demonstrate that siamese CNNs trained using the GCL function and the improved annotations consistently outperform their binary counterparts. Our models trained on MSLS outperform the state-of-the-art methods, including NetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the Pittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons datasets. Furthermore, training a siamese network using the GCL function does not require complex pair mining. We release the source code at https://github.com/marialeyvallina/generalized contrastive loss.</p><p>Index Terms-contrastive learning, image retrieval, siamese networks, visual place recognition !</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>V Isual place recognition has received large interest from researchers in computer vision, machine learning and information retrieval. It consists of, given a query image, seeking an image depicting a similar scene in a map or database. Possible instances of this problem are the retrieval of an image containing a specific distinctive landmark <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or the recognition of a previously visited place <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Algorithms for image retrieval and place recognition are deployed in and are a key component of many visual localization <ref type="bibr" target="#b4">[5]</ref> and Simultaneous Localization and Mapping (SLAM) systems <ref type="bibr" target="#b5">[6]</ref>. In these applications, effectively retrieving similar images from a map set is of utmost importance to facilitate a reliable estimation of the camera pose in an environment and perform the subsequent 3D reconstruction tasks. Methods for place recognition have been tested on data recorded in different environments, each including different challenges. Indoor scenes can be subject to heavy viewpoint variations <ref type="bibr" target="#b6">[7]</ref>. Outdoor environments include drastic changes in illumination <ref type="bibr" target="#b3">[4]</ref>, weather <ref type="bibr" target="#b5">[6]</ref>, seasonal <ref type="bibr" target="#b7">[8]</ref> as well as long-term variations <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recently, approaches based on Convolutional Neural Networks (CNNs) achieved very good results for place recognition <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Their great generalization capabilities allow to deploy them as feature extractors, yielding acceptable results <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. It is, however, with end-toend trained methods that the best performance results were achieved. These methods usually optimize a triplet loss function, using triplets made of a query image, a positive and a negative match <ref type="bibr" target="#b15">[16]</ref>, or a query image, a set of similar images and a set of dissimilar images <ref type="bibr" target="#b3">[4]</ref>. Other approaches rely on a contrastive loss function <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and are trained using as input similar or dissimilar image pairs. All these methods are built upon considering image similarity as a binary option, i.e. two images either depict the same scene/object or not. In practice, image similarity is not a binary attribute, but rather a continuous one. A pair of images may be 100% similar if they are the same, or not similar at all, if they do not share features. In between, one should consider a range where two images share some features, while not being exactly the same. We illustrate some examples in <ref type="figure" target="#fig_1">Fig. 1</ref>. Several existing methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref> are trained using binary similarity labels, ignoring the range of partial similarity that exists in practice.</p><p>Since deep learning methods rely on the amount and quality of the training data, noise or errors in the labels may cause the models to not be effectively trained, lowering their performance. We more complete and precise label information can contribute to increasing the performance of existing methods. This improvement can be achieved by reannotating a dataset <ref type="bibr" target="#b20">[21]</ref>, by implementing a multi-task architecture <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, or by also using semantic <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, geometric <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, or sequential <ref type="bibr" target="#b5">[6]</ref> information to train the models. In <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the authors involved scene geometry to construct more meaningful labels to learn pose regression models. They considered the 3D camera frustum overlap as a direct measure of camera pose similarity and used it as a regression target to learn image representations in a continuous setting. The formulation of the loss functions and the measure of similarity in these works are, however, strictly linked to the geometry of the arXiv:2103.06638v3 [cs.CV] 9 May 2022 scene and the visual localization task.</p><p>In this work, we build on the concept of partial image similarity for image retrieval, and propose a generalized formulation of the Contrastive Loss (GCL) function, to learn a suitable embedding space for image retrieval. We deploy it as objective function to train siamese network architectures, and apply to a visual place recognition task. The proposed GCL function enforces the representation of similar images to be closer in the latent space, while pushing apart the representations of dissimilar images proportionally to their annotated degree of similarity.</p><p>We implement three strategies to automatically reannotate existing datasets for place recognition with labels that indicate the degree of similarity of image pairs: Weak 2D Field-of-View overlap, Strong 2D Field-of-View overlap and 3D Field-of-View overlap. These strategies rely on different available information recorded in the original data sets. The Weak 2D Field-of-View overlap labeling strategy exploits the GPS position and compass angle information associated to the images to estimate the 2D Field-of-View (FoV) of the camera that has taken the concerned picture. The intersection area between the FoV of two cameras determines the ground truth degree of similarity of the image pair. We use this to re-label the Mapillary Street Level Sequences dataset <ref type="bibr" target="#b8">[9]</ref>, which contains images recorded in urban, suburban and countryside environments in 30 cities all over the world. The Strong 2D Field-of-View overlap also computes the intersection of the FoV of two cameras, but it is calculated based on the 6DOF camera pose information. We use it to re-annotate the TB-Places dataset, where the 6DOF ground truth camera pose was recorded using a laser tracker and an IMU sensor <ref type="bibr" target="#b34">[35]</ref>. The 3D Field-of-View overlap strategy computes the intersection of the 3D FoV of two cameras, based on the 3D reconstruction of the scene. We apply it to re-label the 7Scenes dataset <ref type="bibr" target="#b6">[7]</ref>, which contains images and 3D models of indoor scenes.</p><p>We use the proposed automatic annotations to train a siamese CNN architecture by optimizing the Generalized Contrastive Loss and we compare it to existing approaches that rely on binary image similarity ground truth. The method that we propose consists of a fully convolutional backbone with a simple pooling operation on top (i.e. Global Average or GeM pooling). We do not incorporate complex pair mining strategies into the training process of our method. We only ensure that each batch contains approximately the same amount of positive and negative pairs, each composed of two images: a query and a pos-itive or negative counterpart. We carry out experiments on the MSLS <ref type="bibr" target="#b8">[9]</ref> (for which we report new state-of-the-art results), Pittsburgh30k <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, Tokyo 24/7 <ref type="bibr" target="#b35">[36]</ref>, RobotCar Seasons v2 <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, Extended CMU Seasons <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, TB-Places <ref type="bibr" target="#b34">[35]</ref> and 7Scenes <ref type="bibr" target="#b6">[7]</ref> benchmark data sets.</p><p>The contributions of this work are two-fold:</p><p>1) a novel Generalized Contrastive Loss (GCL) function, which we use to train siamese networks using graded image similarity annotations and a na?ve pair mining strategy that does not involve memoryintensive computations on the GPU; 2) new state-of-the-art results on the MSLS data set.</p><p>The na?ve pair mining strategy uses only the graded similarity associated to image pairs as in <ref type="bibr" target="#b33">[34]</ref>, and ensures that a training batch contains pairs with a roughly uniform distribution of graded similarity. With respect to <ref type="bibr" target="#b33">[34]</ref>, in which three matches are selected for each anchor (easy, moderate and hard sample on the basis of the camera frustum overlap), we simplify the sampling by selecting only one match per query based on the annotated similarity. Our strategy does not select hard-pairs by computing distances in a latent-space like commonly used computationallyintensive mining strategies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Additionally, we introduce three automatic data annotation techniques, based on GPS coordinates and compass angle, 6DOF camera pose and 3D reconstruction information and deploy them to reannotate the MSLS, TB-Places and 7Scenes datasets. We make the new labels available.</p><p>The paper is organized as follows. We discuss related works in Section 2. We introduce the Generalized Contrastive Loss function, the model architecture and mining strategy in Section 3, and provide details about our automatic labeling techniques in Section 4. We explain the experimental framework and evaluation procedure in Section 5. We present and discuss the results that we achieved in Section 6. Finally, we draw conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metric learning for place recognition</head><p>Traditional place recognition algorithms rely on local image features and holistic representations, such as Fisher Vectors <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, Bag of Words <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b42">[43]</ref> and VLAD <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, or on exploiting image sequences <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref> or panoramas <ref type="bibr" target="#b4">[5]</ref>.</p><p>Deep learning methods achieved state-of-the-art results <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, initially using pre-trained CNNs as feature extractors <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, and subsequently as end-to-end trainable models. Several methods optimize a contrastive loss function <ref type="bibr" target="#b16">[17]</ref>, with pairs of positive and negative samples. In <ref type="bibr" target="#b47">[48]</ref>, the authors demonstrate the benefits of using a ground truth based on soft assignments to positive and negative classes, based on the Euclidean distance between the annotated GPS coordinates of images in their dataset, without considering orientation. This leads to some discrepancies in the soft assignment, e.g. two images taken in very close places but facing opposite directions do not share visual cues. It makes the use of a hard mining strategy necessary for convergence. In <ref type="bibr" target="#b48">[49]</ref>, the authors propose a Contrastive Loss function that weights similar pairs of images and dissimilar images on the basis of the distance of their representations in the embedding space. As the weighing depends on the distance in the embedding space, it requires extra regularization during training. In <ref type="bibr" target="#b32">[33]</ref>, the authors proposed to regress the 3D camera frustum overlap as part of an image retrieval pipeline for image localization, learning image retrieval and pose regression simultaneously. The authors of <ref type="bibr" target="#b33">[34]</ref> build further into the idea of frustum overlap based retrieval, learning a relocalization pipeline with two modules for coarse and fine localization. Other methods are trained by optimizing a triplet loss function, with image tuples consisting of a query, a positive match and a negative match <ref type="bibr" target="#b15">[16]</ref>. NetVLAD <ref type="bibr" target="#b3">[4]</ref> is trained using a triplet loss function with a query, a set of potential positive matches and a set of definite negative matches. They optimize a weak triplet loss function that enforces higher distances between the query and all the negative matches than between the query and any potential positive. PointNetVLAD <ref type="bibr" target="#b49">[50]</ref> exploits 3D information by combining PointNet <ref type="bibr" target="#b50">[51]</ref> and NetVLAD for large-scale place recognition. In <ref type="bibr" target="#b19">[20]</ref>, the authors use spatial pyramid pooling to encode spatial and structural information into the NetVLAD descriptors, and a weighted triplet loss to maximize the distances between the negative pairs and maximize that between the positive pairs. In <ref type="bibr" target="#b18">[19]</ref>, the authors train a NetVLAD by optimizing a SARE (Stochastic Attraction-Repulsion Embedding) loss function, which aims to minimize the distance among similar scences while maximizing the distance among dissimilar scenes in a probabilistic framework. The use of the SARE loss function required a reformulation of hard-pair mining, which is also necessary for the training process to converge. In <ref type="bibr" target="#b39">[40]</ref>, a measure of the semantic degree of similarity between object pairs was incorporated into the learning process to optimize a novel log-ratio triplet loss, and applied to human pose, room layout, and caption-aware image retrieval. This approach, however, required the reformulation of the triplet mining strategy to perform dense sampling in the neighborhood of the anchor sample. In <ref type="bibr" target="#b51">[52]</ref>, the global retrieval ranking is optimized, rather than a distance loss function for image retrieval, which was used for visual localization systems <ref type="bibr" target="#b52">[53]</ref>. In <ref type="bibr" target="#b53">[54]</ref>, the authors use the patch-level features of VLAD layer to do local feature matching and re-rank the retrieval predictions, obtaining an improvement in the recall.</p><p>Several challenges of visual place recognition in different environments were studied and several benchmark datasets were publicly released. The KITTI <ref type="bibr" target="#b54">[55]</ref>, Oxford RobotCar <ref type="bibr" target="#b37">[38]</ref>, RobotCar Seasons <ref type="bibr" target="#b36">[37]</ref>, Pittsburgh250K <ref type="bibr" target="#b35">[36]</ref> and TokyoTM <ref type="bibr" target="#b3">[4]</ref> datasets contain images taken in urban environments. The CMU dataset <ref type="bibr" target="#b38">[39]</ref> and the Extended CMU Seasons dataset <ref type="bibr" target="#b36">[37]</ref> contain images taken in urban, suburban and park environments. Tokyo 24/7 <ref type="bibr" target="#b3">[4]</ref> and Aachen <ref type="bibr" target="#b55">[56]</ref> also provide images taken at day and night that depict drastic changes in illumination conditions. The Alderley dataset also includes weather variations <ref type="bibr" target="#b5">[6]</ref>. The Mapillary Street Level Sequences dataset includes images taken with different cameras in 30 cities in six continents <ref type="bibr" target="#b8">[9]</ref>. It contains changes in viewpoint, weather, illumination and long-term variations, as well as urban, suburban and countryside environments. To the best of our knowledge, it is the largest dataset for long-term place recognition. The Norland dataset includes images taken in countryside and contains variations of weather conditions and season <ref type="bibr" target="#b7">[8]</ref>. The TB-Places dataset was designed for place recognition in gardens <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b34">[35]</ref>. It includes challenging long-term variation, and viewpoint changes, as well as very repetitive textures and scenes dominated by objects of green color appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data annotation</head><p>CNNs, which constitute the current state-of-the-art in most computer vision tasks, are trained on large-scale annotated datasets, such as ImageNet <ref type="bibr" target="#b56">[57]</ref> and Places <ref type="bibr" target="#b57">[58]</ref> for object and scene classification, COCO <ref type="bibr" target="#b58">[59]</ref> and Pascal VOC <ref type="bibr" target="#b59">[60]</ref> for object detection, and Pittsburgh250k <ref type="bibr" target="#b35">[36]</ref> and Mapillary Street Level Sequences <ref type="bibr" target="#b8">[9]</ref> for visual place recognition. The quality, diversity, and richness of the ground truth annotations have a direct impact on the performance of the trained models and contribute to the strength of their generalization capabilities <ref type="bibr" target="#b60">[61]</ref>. For instance, in <ref type="bibr" target="#b61">[62]</ref> it was shown that models trained using inaccurate ground truth data can lead to gender and racial biased predictions, as they fail to learn the actual distribution of the real-world problem.</p><p>Thus, a correct, informative and complete definition of the ground truth labels is of utmost importance to learn more accurate and unbiased models. Previous works on data curation include dataset expansion, e.g. Places2 <ref type="bibr" target="#b57">[58]</ref>, and manual re-annotations, e.g. Google Landmarks <ref type="bibr" target="#b20">[21]</ref>. Another trend in the literature is Multi-Task learning, which consists of learning two (or more) related but yet diverse tasks simultaneously, benefiting from their commonalities. This generally results in higher prediction accuracy for the individual tasks, compared to learning them separately <ref type="bibr" target="#b62">[63]</ref>. Recent works include the combination of semantic and geometric tasks for pose regression <ref type="bibr" target="#b21">[22]</ref> or joint learning of semantic segmentation and image intrinsic decomposition <ref type="bibr" target="#b23">[24]</ref>. Although its efficacy, Multi-Task learning is limited by the scarcity of adequate datasets, which are required to provide samples with multiple labels, for each task. Another option is to incorporate available complementary information in the training process, such as for SeqSLAM <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref> where image sequences were used for Simultaneous Localization and Mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we propose the Generalized Contrastive Loss function and show how to use it to train a siamese network architecture. Furthermore, we describe the place recognition pipeline and how we apply it to search and retrieve similar images from a reference map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fully convolutional backbone and pooling</head><p>We deploy a siamese architecture with a fully convolutional backbone. Given an input image x ? R wn?hn?dn , we consider a convolutional network that computes a representation f (x) ? R wm?hm?dm , where w m and h m are the width and height of the last convolutional activation map, and d m corresponds to the number of kernels of the last convolutional layer. The output tensor of the last convolutional layer of the network is fed to a global pooling layer, which computes the image representationf (x) ? R dm . In this work, we experiment with versions of the network with a Global Average Pooling and a GeM pooling layer <ref type="bibr" target="#b16">[17]</ref>. However, one can explore the use of other global pooling strategies, in accordance with the application at hand. Given a pair of images (x i , x j ), their representation is computed as (f (x i ),f (x j )). We illustrate the architecture in <ref type="figure" target="#fig_2">Fig. 2</ref>. Our methodology can be applied using any convolutional architecture as backbone: in this work we use DenseNet <ref type="bibr" target="#b63">[64]</ref>, ResNet <ref type="bibr" target="#b64">[65]</ref>, VGG16 <ref type="bibr" target="#b65">[66]</ref> and ResNeXt <ref type="bibr" target="#b66">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalized Contrastive Loss</head><p>Siamese architectures are used to learn representations that disentangle the (dis-)similarity of pairs of input images. They consist of two identical networks (i.e. they share their weights) that process input image pairs. Applications include signature verification <ref type="bibr" target="#b67">[68]</ref>, face identification <ref type="bibr" target="#b68">[69]</ref>, visual place recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> and image retrieval <ref type="bibr" target="#b16">[17]</ref>. The training of a siamese architecture is generally carried out by optimizing a Contrastive Loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Contrastive Loss function</head><p>Let us consider two input images x i and x j , and their representationsf (x i ) andf (x j ). We define the distance between the representations of the input images x i and x j in the latent space as d(</p><formula xml:id="formula_0">x i , x j ) = f (x i ) ?f (x j ) 2</formula><p>. The Contrastive Loss <ref type="bibr" target="#b69">[70]</ref> function L CL is defined as:</p><formula xml:id="formula_1">L CL (x i , x j ) = 1 2 d(x i , x j ) 2 , if y = 1 1 2 max(? ? d(x i , x j ), 0) 2 , if y = 0 (1)</formula><p>where ? is the margin, i.e. a threshold for the descriptor distance above which a pair of images is not considered as depicting the same place. The margin ? is an hyperparameter defined by the user. The ground truth label y is such that 1 indicates a pair of similar images, and 0 a not-similar pair of images. Similarity, however, is not a binary attribute, and defining it as such may cause the trained models to produce unreliable predictions. Let us consider an example dataset consisting of three images, as depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>. The pairs (A, B) and (B, C) are labeled as similar, while the pair (A, C) is labeled as notsimilar. The aim of a siamese network is to learn a functionf that maps similar images to points in a latent space that are close together, and non-similar image pairs to points with larger distance in the latent space.</p><p>The optimization of the Contrastive Loss function aims at minimizing the distance between the representations of the similar image pairs (A, B) and (B, C), such that:</p><formula xml:id="formula_2">f (A) ?f (B) 2 ? 0 ? f (B) ?f (C) 2 ? 0 that corresponds to: f (A) ?f (B) ?f (B) ?f (C)</formula><p>Therefore, by the transitive property of equality, the Euclidean distance of the representation of A and C is also ensured to be close in the latent space:</p><formula xml:id="formula_3">f (A) ?f (B) ? f (B) ?f (C) ?f (C) ?f (A) (2)</formula><p>In the example we considered, however, (A, C) is labeled as dissimilar. The result of Eq. 2 is in contrast with the optimization ensured by Eq. 1, where the distance of the representation of dissimilar images is maximized. This inconsistency in the training process is due to the fact that the Contrastive Loss function does not take into account the partial similarity between the input pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Generalized Contrastive Loss function</head><p>We propose a generalized formulation of the Contrastive Loss that relies on a definition of continuous, rather than binary, similarity. We define the Generalized Contrastive Loss function L GCL as:</p><formula xml:id="formula_4">L GCL (x i , x j ) = ? i,j ? 1 2 d(x i , x j ) 2 + (1 ? ? i,j ) ? 1 2 max(? ? d(x i , x j ), 0) 2<label>(3)</label></formula><p>where x i and x j denote the two input images,f (x) is the representation of the input x and ? i,j ? [0, 1] is the ground truth degree of similarity of x i and x j . In contrast to Eq. 1, here the similarity ? i,j is a continuous value ranging from 0 (completely dissimilar) to 1 (identical). By minimising a Generalized Contrastive Loss function, the distance of each image pair in the latent space is optimized proportionally to the corresponding degree of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Gradient of the Generalized Contrastive Loss</head><p>In the training phase, the loss function is minimized by gradient descent optimization and the weights of the network are updated by backpropagation. In the case of the Constrastive Loss function, the gradient is:</p><formula xml:id="formula_5">?L CL (x i , x j ) = d(x i , x j ), if y = 1 min(d(x i , x j ) ? ?, 0), if y = 0<label>(4)</label></formula><p>It is worth noting that the min function ensures that the gradient is computed for the negative pairs whose distance in the latent space is lower than the margin ? (see the supplementary materials for the derivation of the gradient).</p><p>The weights of the model are updated in order to minimize the distance between similar images, and to maximize the distance between dissimilar ones. A partial similarity is therefore not considered, and the training process may incur in learning inconsistent representations. In contrast, the proposed Generalized Constrastive loss function takes into account the degree of similarity between the input images during the training process. Its gradient is:</p><formula xml:id="formula_6">?L GCL (x i ,x j ) = d(x i ,x j )+? (? i,j ?1), if d(x i ,x j ) &lt; ? d(x i , x j ) ? ? i,j , if d(x i ,x j ) ? ? (5)</formula><p>It is worth pointing out that the magnitude of the gradient of the Generalized Contrastive Loss function is modulated by the ground truth degree of similarity of the input image pairs, ? i,j . In the case the distance d(x i , x j ) is larger than the margin ? , the weights of the network are updated proportionally to ? i,j . More interestingly, when the distance d(x i , x j ) &lt; ? , the Generalized Contrastive Loss function has an intrinsic regularization effect of the learned latent space. The network weights are indeed updated so that the vector representations of the training images x i and x j are moved closer in the latent space if their ground truth degree of similarity is ? i,j &gt; 1 ? d(xi,xj ) ? , otherwise they are pushed away. For the boundary cases ? i,j = 0 (completely dissimilar input images) and ? i,j = 1 (same exact input images), the gradient is the same as in Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image search and place recognition</head><p>Let us consider a set X of map images, for which the location in the environment of the camera that took them is known. We are presented with a set Y of query images taken  The point (t 0 , t 1 ) is the camera location in the environment, and ? is the camera orientation in the form of a compass angle with respect to the north N . A (b) soft positive match: two cameras (with ? = 90 ? and r = 50m) in the same position but with orientations 40 ? apart. A (c) soft negative example: two cameras (with ? = 90 ? and r = 50m) located 25m apart but with the same orientation. from unknown positions. In order to effectively localize the camera that took the query images in the environment, we are required to retrieve similar images to the query from the map set. We compute a representation of the map imagesf (x) ?x ? X, and of the query imagesf (y) ?y ? Y using our network. For a given query representationf (y), image retrieval is performed by an exhaustive nearest neighbor search among the representations of the map set f (x) ?x ? X, retrieving the map images with the closest representations in the latent space.</p><p>It is worth pointing out that in this paper we do not address the camera localization and pose estimation problems. We focus on training networks that are able to compute an effective representation for better retrieval results, which can further benefit the localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING DATA AND AUTOMATIC LABELLING</head><p>The optimization of the Generalized Contrastive Loss function relies on the ground truth similarity of image pairs defined in the range [0, 1]. In this section we present three approaches to automatically label the degree of similarity of image pairs, which we used depending on the data available together with the images in the concerned datasets. We describe two measures of image similarity based on the 2D Field-of-View overlap: weakly labeled, relying on GPS data, and strongly labeled, relying on the 6DOF camera pose information associated to the images. Additionally, we used the 3D Field-of-View overlap, estimated from 3D reconstruction and 6DOF camera pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">2D Field-of-View overlap</head><p>We estimate the similarity of two images by approximating a measure of the overlap of their two-dimensional Fieldof-View (FoV) in the horizontal plane. Let us consider a camera with a FoV defined by the angle ? and radius r, positioned in an environment according to a 2D translation vector (t 0 , t 1 ) with respect to the origin of the reference system. The camera is oriented at an angle ? with respect to the north direction of the reference system. We define the 2D FoV as the sector of the circle denoted by the center (t 0 , t 1 ) and radius r enclosed in the angle range delimited by [? ? ? 2 , ? + ? 2 ] (see <ref type="figure" target="#fig_5">Fig. 4a</ref>). The 2D Field-of-View overlap between two images corresponds to the intersectionover-union (IoU) of their FoVs. Differently from <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> that compute the overlap of 3D camera frusta for camera localization, we relax this concept by only considering the overlap of the camera field-of-views in the horizontal plane. We consider as positives the image pairs with determined ground truth FoV overlap higher than 50%, and the rest as negatives. More specifically, the negative pairs with a similarity higher than 0% are soft negatives, while the pairs with a similarity degree equal to 0% are hard negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Weak 2D Field-of-View overlap</head><p>We present the weak 2D Field-of-View overlap to estimate the similarity of image pairs for which GPS position labels (UTM format) and compass angle information are available. We consider the UTM data as the translation vector (t 0 , t 1 ) and the compass angle as the orientation ? necessary to estimate the 2D FoV of the cameras.</p><p>We use the weak 2D Field-of-View overlap to re-annotate the Mapillary Street Level Sequences (MSLS) dataset <ref type="bibr" target="#b8">[9]</ref>, a large-scale outdoor visual place recognition dataset. It contains images of 30 cities across 6 continents that depict urban, suburban and countryside environments, and are annotated with the UTM position and compass angle. The FoV angle of the cameras and the intrinsics are not provided. We thus estimate the value of the FoV angle ? according to the following reasoning. The authors of MSLS define a positive match when the retrieved map image falls within 25m and 40 ? from the query. We define a similarity measure that satisfies those constraints. Image pairs taken at locations that are closer than 25m and with orientation differences lower than 40 ? are expected to have a similarity higher than 50%. Moreover, the borderline cases with distances close to 25m and/or orientation difference near to 40 ? should have a similarity close to 50%. Hence, we define r = 25m ? 2 = 50m and estimate a ? that gives approximately a 50% FoV overlap for the borderline cases, i.e. 0m@40 ? , and 25m@0 ? . For the former, the optimal ? corresponds to 80 ? , and for the second latter to 102 ? . We settle for a value in the middle and define ? = 90 ? , which gives 55.63% and 45.01% FoV overlaps, as shown in <ref type="figure" target="#fig_5">Fig. 4b</ref> and 4c. We display examples of the similarity ground truth for MSLS in <ref type="figure" target="#fig_6">Fig. 5</ref>. We compute the similarity ground truth for each possible query-map pair, per city. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Strong 2D Field-of-View overlap</head><p>We present the strong 2D Field-of-View overlap to estimate the similarity of image pairs in the case 6DOF camera pose information is available together with the recorded images in the dataset. The translation vector (t 0 , t 1 ) and orientation angle ? are extracted from the pose vector. This is the case of the TB-Places dataset <ref type="bibr" target="#b34">[35]</ref>, for visual place recognition in garden environments, created for the Trimbot2020 project <ref type="bibr" target="#b70">[71]</ref>. It contains images taken in an experimental garden over three years and includes variations in illumination, season and viewpoint. Each image comes with a 6DOF camera pose, which allows us to estimate a very precise 2D FoV. According to the original paper of the TB-places dataset, we set the FoV angle of the cameras as ? = 90 ? and the radius as r = 3.5m. We thus estimate the 2D Field-of-View overlap and use it to re-label the pairs of images contained in the dataset. We provide the similarity ground truth for all possible pairs within W17, the training set. We show some examples of image pairs and their 2D FoV overlap in <ref type="figure" target="#fig_7">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Field-of-View overlap</head><p>When a 3D reconstruction of a concerned environment is available, we propose to estimate the degree of similarity of image pairs by computing the 3D Field-of-View overlap. We project a given image with an associated 6DOF camera pose onto the reconstructed pointcloud of the environment. We select the subset of 3D points that falls within the boundaries of the image as the image 3D FoV. For an image pair, we compute their 3D FoV overlap as the intersection-overunion (IoU) of the sets of 3D points associated with the two images. This computation is similar to the maximum inliers measure proposed in <ref type="bibr" target="#b16">[17]</ref>, where it was used as part of a pair-mining strategy that also involved the computation of a distance in the latent-space. We consider, instead, the computed 3D FoV overlap as a measure of the degree of similarity of a pair of images, and use it to compute the graded ground truth.</p><p>We use the 3D Field-of-View overlap to re-annotate the 7Scenes dataset <ref type="bibr" target="#b6">[7]</ref>, an indoor localization benchmark, that contains RGBD images taken in seven environments. Each image has an associated 6DOF pose, and a 3D reconstruction of each scene is provided. We provide annotations for each possible pair within the training set per scene. We also compute the test-vs-train similarities for evaluation purposes. We show some examples of the 3D FoV overlap in <ref type="figure" target="#fig_8">Fig. 7</ref>. We display the 3D FoV associated to the query image in red, that of the map image in blue and their overlap in magenta.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Selection of training pairs</head><p>Training a model for visual place recognition is usually formulated as a binary classification problem. The aim is to determine whether two images depict a similar (class 1) or dissimilar (class 2) place. To train binary classifiers, it is generally desirable to have balanced datasets, to ensure that the two classes are equally weighted. This is often not the case for visual place recognition, where the dissimilar image pairs significantly outnumber the positive pairs. Moreover, when training a siamese architecture, it is necessary to form batches with meaningful image pairs or triplets. For instance, if the training batch consists of pairs that are too easy, the learning process might stall, and the weights of the network not be updated. If the pairs are too difficult, the training dynamics can become unstable <ref type="bibr" target="#b71">[72]</ref>. Hence, the selection of image pairs is a crucial element of the training. Existing approaches use complex image pair mining strategies. In the case of NetVLAD <ref type="bibr" target="#b3">[4]</ref> and NetVLAD-based methods like Patch-NetVLAD <ref type="bibr" target="#b53">[54]</ref> or NetVLAD-SARE <ref type="bibr" target="#b18">[19]</ref>, the authors use a Hard Negative Mining strategy. For each query image, they select a set of potential positive map images and a set with its 10 hardest negative map images. The best combination of positive and negative samples is selected by means of their contribution to a Triplet Ranking Loss function. This implies that all the images need to be forwarded to the network to compute the value of the loss function, although only three contribute to the learning. In <ref type="bibr" target="#b8">[9]</ref>, the authors deploy a similar strategy, selecting only the top-5 hardest negatives. For each triplet, many image latent representations are computed. Even with a caching strategy, bigger backbones like VGG cannot be trained in their entirety <ref type="bibr" target="#b3">[4]</ref> and large training batches do not fit in the memory of a regular GPU. In <ref type="bibr" target="#b8">[9]</ref>, indeed, the authors use a batch size of 4 triplets.</p><p>A continuous similarity ground truth can lead to improved learning <ref type="bibr" target="#b47">[48]</ref>, but it does not dispense with the need for hard mining strategies by itself. When relying only on the GPS without taking in consideration the orientation, for instance, the ground truth annotations are likely to contain visual inaccuracies, i.e. two images taken 20cm apart facing in opposite directions will not be visually similar, despite having a high annotated similarity. These contradictions make necessary the use of further curation of the selected pairs or triplets by means of a costly hard mining strategy.</p><p>We argue that a better curated and more informative ground truth can replace these complicate mining techniques. We use our graded similarity annotations, presented in Sections 4.1 and 4.2 to select the image pairs that compose a training batch. For our training process, we ensure that each training batch contains a balanced amount of positive and negative pairs. In the case of the negative pairs, we also ensure that half of them are soft negatives (i.e. their annotated similarity is higher than 0). We explore several batch composition strategies and discuss them in Section 6.4. This means that each selected image pair consist of a query and a match image, and we do not require complicated mining strategies or to compute any latent representations to form the pairs. Hence, we are able to train our models with a batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapillary Street Level Sequences (MSLS)</head><p>. This is a large scale place recognition dataset that contains images taken in 30 different cities across six continents <ref type="bibr" target="#b8">[9]</ref>. It includes challenging variations of camera viewpoint, season, time and illumination. Moreover, the images have been taken with different cameras. The training set contains over 500k query images and 900k map images, taken in 22 cities. The validation set consists of 19k map images and 11k query images, taken in two cities, and the test set has 39k map images and 27k query images, taken in six different cities.</p><p>We evaluate our models on the MSLS validation set and on the MSLS test set. For the former, we use the evaluation script published by the authors of the dataset. For the latter, since the ground truth is not publicly available, we submit our predictions to the official evaluation server. Following the protocol established by the authors of the dataset <ref type="bibr" target="#b8">[9]</ref>, two images are considered as similar if they are taken by cameras located within 25m of distance, and with less than 40 ? of viewpoint variation.</p><p>Pittsburgh. This dataset contains images of urban environments gathered from Google Street View in the city of Pittsburgh, Pennsylvania, USA <ref type="bibr" target="#b4">[5]</ref>, taken in the span of several years. We use the Pitts30k as test set <ref type="bibr" target="#b3">[4]</ref>, a subset of the Pittsburgh dataset which consists of 10k map images and 7k query images, to evaluate the generalization of our models trained on MSLS.</p><p>Tokyo 24/7. It consists of images taken in Tokyo, Japan, containing large variations of illumination, as they are taken during day and night <ref type="bibr" target="#b35">[36]</ref>. The query set consists of 315 images, and the map contains 76k photos.</p><p>RobotCar Seasons v2. This dataset <ref type="bibr" target="#b36">[37]</ref> is a subset of Oxford RobotCar <ref type="bibr" target="#b37">[38]</ref>, a visual localization dataset with images taken in the city of Oxford, UK, within the span of twelve months. Its query set contains 1872 images under different lighting and weather conditions, and its reference set consists of 21k images taken during at daytime, with overcast weather.</p><p>Extended CMU Seasons. This is an expansion of the CMU Seasons dataset <ref type="bibr" target="#b36">[37]</ref>, which in turn is a subset of the CMU Dataset <ref type="bibr" target="#b38">[39]</ref>. It contains images taken in urban, suburban and park environments, in the city of Pittsburgh, over a period of 1 year. Its query set contains 61k images, and its reference set contains 57k images.</p><p>TB-Places. This dataset was designed for place recognition in garden environments <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b34">[35]</ref>. It contains images taken by a rover robot in a garden at the University of Wageningen, the Netherlands, over the course of three years. The dataset was collected for the TrimBot2020 project <ref type="bibr" target="#b70">[71]</ref>. It includes drastic viewpoint variations, as well as illumination changes. The garden is a very challenging small environment with repetitive textures.</p><p>The dataset consists of three subsets, i.e. W16, that contains 41k images taken in 2016; W17, that includes 11k images taken in 2017; and W18, that has 23k images taken in 2018. As in <ref type="bibr" target="#b17">[18]</ref> we use the W17 subset to train our models. We design two experiments to evaluate our models. For the first one we establish W17 as map set (11k images) and W18 as query (23k images). With this configuration we aim to test the robustness of our models w.r.t. changes between the map and the query sets. For the second experiment, we divide W18 into query (17k images) and map (6k images) to test the generalization capabilities of our models in the case both map and query sets were not used for training. In <ref type="figure" target="#fig_9">Fig. 8</ref>, we show a sketch of the trajectory that the robot covered in the TrimBot2020 garden for the recording of the reference map (blue trajectory) and query (orange trajectory) images. It is worth pointing out that the query images were taken from locations not covered by map images, thus including substantial viewpoint variations.</p><p>7Scenes. It is a benchmark dataset for indoor camera localization algorithms <ref type="bibr" target="#b6">[7]</ref>. It includes 26k training images and 17k test images, taken in seven different environments. Each image has an associated ground truth 6DOF camera pose. Additionally, a 3D reconstruction of each scene is available. We use it to test our models for visual place recognition in indoor environments. For evaluation purposes, we define an image pair as a positive match if their annotated degree of similarity is higher than 50%. We use the training set as map, and the test set as query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation details</head><p>We implemented our models using the PyTorch framework 1 and trained them on a NVIDIA V100 GPU card. We optimized the weights of our models with Stochastic Gradient Descent. To train the networks that use the binary Contrastive Loss function, we set an initial learning rate l 0 = 0.01. For those that deploy the Generalized Contrastive Loss function, we used as initial learning rate l 0 = 0.1. In both cases the learning rate is divided by 10 after every 250k pairs for MSLS, or every 15 epochs for TB-Places and 7Scenes. We set the margin value ? = 0.5 and we use a batch size of 64. We trained the two last convolutional blocks of the backbone networks for all experiments. The necessary code to replicate the experiments is publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics</head><p>We followed a widely used place recognition evaluation protocol and considered a query as correctly identified if any of the top-k retrieved images are annotated as a positive match <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b55">[56]</ref>. We computed the following metrics.</p><p>Top-k recall (R@k) is the percentage of queries for which we retrieved at least a correct map image among their k nearest neighbors.</p><p>Percentage of correctly localized queries is the amount of images that are correctly retrieved for a given translation and rotation threshold. We use it for evaluating on the RobotCar Seasons v2 and the Extended CMU datasets.</p><p>Average Precision (AP), an approximation of the area under the precision-recall curve for the classification of all the possible pairs of images. We use this measure for the 7Scenes dataset <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Large scale outdoor place recognition</head><p>We trained several models on the MSLS training set and report results on the MSLS validation and test set. We evaluate the generalization capabilities of our models by 1. http://pytorch.org/ 2. http://github.com/marialeyvallina/generalized contrastive loss also testing them on the Pittsburgh30k, Tokyo 24/7, Robot-Car Seasons v2, and ExtendedCMU Seasons datasets. In the following, we use the naming backbone-pooling-loss for the considered models. For example, ResNet50-GeM-GCL indicates a ResNet50 backbone with GeM pooling trained using the Generalized Contrastive Loss function.</p><p>Mapillary Street Level Sequences. We carried out experiments using four backbones, namely VGG16, ResNet50, ResNet152, and ResNeXt101-32x8d (hereinafter ResNeXt), and the GeM <ref type="bibr" target="#b16">[17]</ref> global pooling layer. Additional results with an average global pooling layer are included in the supplementary material. For each combination of backbone network and pooling layer, we optimize the models with the binary Contrastive Loss and with our Generalized Contrastive Loss. As shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_11">Fig. 9</ref>, the models trained with the GCL function consistently outperform their counterparts trained with the CL. For all models, we also perform a postprocessing step consisting of PCA whitening and observe that it further boosts the performance.</p><p>We compared the results of our models with those obtained by state-of-the-art methods, namely NetVLAD <ref type="bibr" target="#b3">[4]</ref>, NetVLAD-SARE <ref type="bibr" target="#b18">[19]</ref>, AP-GeM <ref type="bibr" target="#b51">[52]</ref> and Patch-NetVLAD <ref type="bibr" target="#b53">[54]</ref>. We achieved better results than existing approaches on the MSLS test set. We demonstrate that a simpler architecture, consisting only of a convolutional backbone and a simple pooling layer arranged as a twoway siamese network, trained using the proposed GCL loss function outperforms more complex architectures, which deploy triplet networks and VLAD layers. For instance, NetVLAD achieved a top-5 recall equal to 44% on the MSLS test set <ref type="bibr" target="#b8">[9]</ref>, while our VGG16-GeM-GCL model, which uses the same backbone as NetVLAD, achieved a top-5 recall of 55.7%. The superiority of our VGG16-GeM-GCL model holds also in the case we post-process the descriptors using PCA whitening. For ResNet50-GeM-GCL, we obtained a top-5 recall equal to 59.2%. Our ResNet152-GeM-GCL and our ResNeXt-GeM-GCL achieved a top-5 recall equal to 62.3% and 70.8%, respectively. When whitening is applied, the performance of our models further improved, reaching a top-5 recall of 60.8% for VGG-GeM-GCL, 65.7% for ResNet50-GeM-GCL 70.7% for ResNet152-GeM-GCL, and 76.2% for ResNeXt-GeM-GCL.</p><p>Our results are higher than those of all the other methods on the MSLS dataset, including NetVLAD (with different configurations) <ref type="bibr" target="#b3">[4]</ref>, NetVLAD-SARE <ref type="bibr" target="#b18">[19]</ref>, AP-GeM <ref type="bibr" target="#b51">[52]</ref> and Patch-NetVLAD <ref type="bibr" target="#b53">[54]</ref>. We used the models and code from the original papers, as also done in <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. It is worth pointing out that our learned representations have a size of 2048 for the ResNet50, ResNet152 and ResNeXt models, and 512 for the VGG16, while the NetVLAD vectors consist of 32768 features when using 64 clusters or 8192 when using 16 clusters before dimensionality reduction. When PCA whitening is applied, the NetVLAD-based representations are reduced to a size of 4096, while ours remain 512 for VGG16, 1024 for ResNet50 and ResNeXt, and 2048 for ResNet152.</p><p>Generalization to Pittsburgh30K, Tokyo 24/7, RobotCar-Seasons v2 and ExtendedCMU Seasons. In <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>, we also report the results that our models achieved on the Pittsburgh30K, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons datasets. Ablation study on the considered datasets: all the models are trained on the MSLS train set. CL stands for Contrastive Loss and while GCL for our Generalized Contrastive Loss. For the cases in which PCA whitening is applied we report the dimensionality that achieves the best results on the MSLS validation set.    We compare our results with those achieved by different configurations of NetVLAD, trained on Pittsburgh30k with 64 clusters and trained on MSLS with 16 and 64 clusters. Additionally, we compare to ResNet101-AP-GeM, NetVLAD-SARE and Patch-NetVLAD. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the models trained with the GCL function on the MSLS dataset generalize well to unseen datasets, in some cases better than existing methods, despite using descriptors of lower dimensionality. In the case of Pittsburgh30k, our models achieve competitive results, up to 91.5% top-5 recall, which is very close to the 91.8% reported by NetVLAD-64 trained on the Pittsburgh30k dataset itself, and slightly lower than NetVLAD-SARE, which reaches a top-5 recall of 94.3%, and Patch-NetVLAD, with a top-5 recall of 94.5%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSLS-Val</head><p>In the case of Tokyo24/7 we record a lower performance, which we attribute to the drastic variations in illumination included in the dataset, with images taken at day and at night. Such conditions are underrepresented in the MSLS training set: only 4k query images are taken at night, out of 500k query images that compose the dataset. However, we observe that applying a whitening transform greatly enhances the generalization capabilities of our models, and it boosts the performance for all datasets. On Tokyo 24/7, we achieved an improvement of 28.2% on the top-5 recall for the ResNet152-GeM-GCL model, up to 81%, which outperforms NetVLAD and AP-GeM. NetVLAD-SARE and Patch-NetVLAD generalize better to this dataset, with top-5 recall of 86.7% and 88.6% respectively. Our models also generalize well to urban localization datasets like RobotCar Seasons V2 and Extended CMU Seasons, achieving up to 21.9% and 19% of correctly localized queries within 0.5m and 5 ? , respectively. For both datasets, the results are comparable to those obtained by NetVLAD trained on MSLS, but are outperformed by NetVLAD trained on Pittsburgh, AP-GeM, NetVLAD-SARE and Patch-NetVLAD. Note that we do not perform 6DOF pose estimation, but estimate the pose of a query image by inheriting that of the best retrieved match. We thus do not aim at comparing with methods that perform refined pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Small scale outdoor place recognition</head><p>We trained our method on the TB-Places dataset and show the results that we achieved in <ref type="figure" target="#fig_1">Fig. 10</ref>. We considered three different backbones, namely ResNet18, ResNet34, and DenseNet161, which learn representations of size 512, 512 and 2208, respectively. We trained them with the binary Contrastive Loss function and with our Generalized Contrastive Loss function. Furthermore, we compare them with the NetVLAD off-the-shelf model. For these experiments we use only a Global Average Pooling, so this information is omitted from the names of the models.</p><p>We report results of two experiments. For the first one, we used the training set (W17) as map, and the W18 set as query. With this experiment we tested the strength of our learned representations against significant variations between the query and the map set. We show the results that we obtained in <ref type="figure" target="#fig_1">Fig. 10a</ref>. For the second experiment, we divided the W18 set into map and query, to test the generalization capabilities when both map and query sets are unknown to the place recognition model (i.e. not seen during training). The results are displayed in <ref type="figure" target="#fig_1">Fig. 10b</ref>. For both experiments the models trained with our GCL function and the proposed similarity ground truth consistently achieve better recall than the ones trained with the binary Contrastive Loss, with the exception of the ResNet18 model. In some cases the models trained with the binary ground truth do not outperform the NetVLAD model, even though this one is not trained on the TB-Places dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Indoor place recognition</head><p>We report the results that we achieved on the seven sets of the 7scenes dataset in <ref type="figure" target="#fig_1">Fig. 11</ref>. We used the ResNet18 and ResNet34 architectures as backbones with a Global Average Pooling layer and we compare them to NetVLAD off-the-shelf. Since we only use one type of pooling for this experiments this information is omitted from the model names. We achieved generally higher Recall@K results for the models trained using the GCL function, for all scenes. The cases of the stairs <ref type="figure" target="#fig_1">(Fig. 11b)</ref>, chess ( <ref type="figure" target="#fig_1">Fig. 11f)</ref> and office <ref type="figure" target="#fig_1">(Fig. 11g</ref>) scenes are particularly interesting, since with the GCL descriptors we are able to retrieve positive matches for nearly all the query images, with a top-5 recall for ResNet34-GCL of 98.1%, 98.2% and 99.7%, respectively. Furthermore, we report the average precision results in <ref type="table" target="#tab_3">Table 3</ref>. This measure is an approximation of the area under the precision-recall curve and measures the number of pairs that can be correctly discriminated into positive and negative on the whole dataset. The models trained with the GCL function achieved higher AP than their corresponding models trained with the binary CL function. We achieved an average AP equal to 0.89 using the ResNet34-GCL model. Following the general expectation that improved retrieval results would contribute to a better localization, we performed a small-scale localization experiment to compare the contribution of the GCL and CL training to the localization performance of an InLoc pipeline <ref type="bibr" target="#b72">[73]</ref>. We report the results in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion</head><p>State-of-the-art visual place recognition pipelines rely on difficult pair and triplet mining strategies for training <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Their goal is to compose batches of training images that substantially contribute to the learning process, evaluating their potential impact on the loss function, at the cost of GPU memory and computing time. As we discussed in Section 4.3, we opted for a na?ve mining strategy, based only on the annotated graded similarity of image pairs and not on their embedded representations.</p><p>Pair selection in na?ve mining. The composition of the training batches, in terms of distribution of image pairs with different ground truth graded similarity, resulting from the na?ve mining strategy plays an important role in the effective convergence of the training process. In order to test the influence that different pair sampling strategies have on the effectiveness of the training process and accuracy of retrieval results, we carried out several experiments (using the four considered backbones), for which we report results in <ref type="table" target="#tab_4">Table 4</ref>. We thus defined four strategies of na?ve mining for the composition of training batches, which we refer to as strategy A, B, C and D. For strategy A, we compose a training batch including 50% of positive pairs (? ? [0.5, 1]), 25% of soft negative pairs (? ? (0, 0.5)) and 25% of hard negative pairs (? = 0). For strategy B, we select 25% hard positive pairs (? ? [0.75, 1]), 25% soft positive pairs (? ? [0.5, 0.75)), 25% of soft negative pairs (? ? (0, 0.5)) and 25% of hard negative pairs (? = 0). For strategy C, we prepare the batches by selecting 33.3% positive pairs (? ? [0.5, 1]), 33.3% of soft negative pairs (? ? (0, 0.5)) and 33.3% of hard negative pairs (? = 0). Finally, for strategy D we randomly sample 50% positive pairs (? ? [0.5, 1]) and 50% negative pairs (? ? [0, 0.5)). In <ref type="figure" target="#fig_1">Fig. 12</ref>, we show histograms of the pair similarity distribution of the composition of training batches resulting from the four considered na?ve mining strategies. The results in <ref type="table" target="#tab_4">Table 4</ref> clearly indicate that the most important factor to consider when composing the training batches is to select an adequate number of soft negative pairs, i.e. at least 25% of the pairs included in a batch have similarity ? ? (0, 0.5), as done for strategy A, B and C. For strategy D, we can observe an improvement with respect to the binary similarity case (i.e. with only pairs with similarity ? = 1 and ? = 0, respectively), due to the use of soft positive samples. However, the use of soft-and hard positive pairs (i.e. pairs with similarity ? ? (0.5, 1]) together with hard negatives only in strategy D does not guarantee optimal performance. Due to the very large amount of hard positive pairs in the dataset, sampling randomly among negatives does ensure the presence of soft negative pairs in the training batches and results in a non-uniform distribution of pair similarity (see <ref type="figure" target="#fig_1">Fig. 12d</ref>). The generalized contrastive optimization that we proposed, instead, largely benefits from a fairly uniform distribution of soft positive and soft negative pairs in the composition of the training batches.</p><p>We have demonstrated that a na?ve mining strategy can be as effective as or better than a more complex one, provided that the ground truth is informative enough. The results that we achieved are attributable to the use of the proposed GCL function, which allows a graded similarity ground truth to be considered. Furthermore, since our na?ve strategy is much less resource demanding, it is possible to use it while training big CNN backbones in their entirety, and with batch size up to 512.  Partial training. We studied the effects that training different layers of the ResNet50 and ResNet152 backbones with the GCL function has on the performance of our models. We report the results that we achieved on the MSLS validation set in <ref type="table" target="#tab_5">Table 5</ref>. The models pre-trained on the Im-ageNet dataset <ref type="bibr" target="#b56">[57]</ref> achieved reasonable results. However, we obtained the highest results when the networks were further trained on the MSLS training set until Layer 3 (i.e. updating the weights of the two last convolutional blocks). Training more layers leads to overfitting.</p><p>Whitening and dimensionality reduction. The learned representations are of a relatively small size (2048 for ResNet50, ResNet152 and ResNeXt, 512 for VGG16) compared to NetVLAD (size of 32768). The dimensionality of the latent space representation influences the performance of image retrieval and place recognition systems. Techniques for dimensionality reduction have been explored <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref>, usually combined with whitening, which has been demonstrated to have positive effects on image retrieval, especially in the case of CNN-learned representations <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b46">[47]</ref>. It can be learned end-to-end or used as a postprocessing step.</p><p>We evaluated the effect of whitening and dimensionality reduction on our learned descriptors by using PCA as postprocessing. We compute the PCA components on the descriptors of the map images and use them to project both map and query image descriptors. We perform experiments with PCA whitening with different number of dimensions, from 32 to 2048. We applied this strategy to the VGG16-GeM-GCL, ResNet50-GeM-GCL, ResNet152-GeM-GCL and ResNeXt-GeM-GCL models (trained on MSLS) and tested them on the validation and test sets of MSLS and the test sets of Pittsburgh30k and Tokyo 24/7. We observe that PCA whitening further improves the performance of the image representations learned by our models and boosts their generalization capabilities. As shown in <ref type="figure" target="#fig_1">Fig. 13</ref>, the whitened descriptors with higher dimensionality achieve better results on the considered datasets. It is worth noting that the ResNeXt-GeM-GCL maintains the same performance when reduced down to 256 dimensions, and that VGG16-GeM-GCL reaches peak top-5 recall when reduced to 256 features. When using the whitening transform combined with PCA dimensionality reduction down to 128 dimensions of the latent space, our models achieved results comparable to those of models with full-size dimensions (2048 for ResNet and ResNeXt, and 512 for VGG), which outperform their non-whithened counterpart on all the considered datasets. We observed up to a 28.2% of improvement in the case of Tokyo 24/7 and 12.8% on the MSLS validation set.</p><p>Training time. The proposed GCL function and the partial similarity labels contribute to training effective models in a much more efficient way than NetVLAD-based ones, as shown in <ref type="table" target="#tab_7">Table 6</ref>. The training time per epoch of our models is less than that of NetVLAD, as we do not perform hardpair mining. Furthermore, we train our GCL models for only one epoch on the MSLS dataset, while NetVLAD-16 and NetVLAD-64 converged after 22 and 7 epochs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We presented a novel Generalized Contrastive Loss function to effectively train two-ways siamese networks for image retrieval and visual place recognition. For this purpose we defined continuous measures of the degree of image similarity. We implemented three techniques for the estimation of the degree of similarity of pairs of images, based on geometric information such as GPS and compass angle, 6DOF camera pose, and scene 3D reconstruction. We deployed them to re-annotate the MSLS, TB-Places and 7Scenes datasets, providing image pairs with graded similarity, rather than binary labels. The re-annotation process is automatic, thus not requiring any human intervention.</p><p>The CNN architectures that we trained consist of a fully convolutional backbone and a global pooling layer (i.e. global average pooling or GeM). The networks that we optimized using the proposed Generalized Contrastive loss function and the graded similarity label sets consistently achieved higher results (top-5 recall up to 18% higher) than their counterparts trained with a binary Contrastive Loss function on the MSLS, TB-Places and 7Scenes datasets. On the MSLS dataset our ResNeXt-GeM-GCL network achieved a top-5 recall of 76.2%, outperforming NetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD (top-5 recall of 58.8%, 44.3%, 44.5%, 57.6%, respectively), and es-  Nicolai Petkov received the Dr. sc. techn. degree in computer engineering (informationstechnik) from the Dresden University of Technology, Dresden, Germany. Since 1991, he has been a Professor of computer science and the Head of the Intelligent Systems Group, University of Groningen. He has authored two monographs, and has authored or co-authored over 150 scientific papers. He holds four patents. His current research interests include pattern recognition, machine learning, data analytics, and brain-inspired computing, with applications in various areas. He is a member of the editorial boards of several journals.</p><formula xml:id="formula_7">VGG16-GeM-GCL VGG16-GeM-GCL-PCA w ResNet50-GeM-GCL ResNet50-GeM-GCL-PCA w ResNet152-GeM-GCL ResNet152-GeM-GCL-PCA w ResNeXt-GeM-GCL ResNeXt-GeM-GCL-PCA</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ADDITIONAL RESULTS</head><p>In the following, we report extra results and observations on the performance of the proposed methods. In particular, we show how the Generalized Contrastive Loss function contributes to learn representations that better characterize the relevant parts of the input images for a robust computation of their similarity. Furthermore, we presents insights about how the GCL function contributes to a better regularization of the learned latent space.</p><p>We studied the effect of a different pooling layer by training our models on the MSLS dataset using a Global Average Pooling layer and report the results that we obtained. Furthermore, we evaluate our MSLS models on the Pittsburgh250k <ref type="bibr" target="#b4">[5]</ref> and TokyoTM <ref type="bibr" target="#b4">[5]</ref> benchmarks, and provide detailed results for the Extended CMU Seasons <ref type="bibr" target="#b36">[37]</ref> and RobotCar Seasons V2 datasets <ref type="bibr" target="#b36">[37]</ref>, divided by environment and weather condition. We also provide a comparison with the work of Thoma et al <ref type="bibr" target="#b47">[48]</ref> on the CMU Seasons dataset.</p><p>We study the effect that different threshold values for the ground truth similarity associated to the images have on the performance of our models. We consider two ground truth thresholds: one based on the GPS distance in meters between the locations in which images were taken, and another based on the annotated degree of similarity, ?. Finally, we provide extended results on the 7Scenes dataset, including the precision recall curves and a small test to evaluate our descriptors in a visual localization pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Learned latent space</head><p>In <ref type="figure" target="#fig_1">Fig. 14, we</ref> show the 2D projection of the vectors representing the difference of the latent space representation of 2000 image pairs (1000 positive and 1000 negative) randomly selected from the Copenhagen set of the MSLS dataset. For each pair, we compute the difference between the map and the query image latent representation. We use this as input to the t-SNE algorithm <ref type="bibr" target="#b73">[74]</ref>, which projects the representations onto a 2D space. We visualize the vectors produced by two models with a ResNet50-GeM backbone, one trained using the CL function ( <ref type="figure" target="#fig_1">Fig. 14a</ref>) and the other using the GCL <ref type="figure" target="#fig_1">(Fig. 14b)</ref> function. The effect of the proposed GCL function is evident in the better regularized latent space, where the representation of similar image pairs (blue dots) are more consistently distributed towards the center of the space. The representations learned using the CL function, instead, form a more scattered and noisy distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Network activation</head><p>In <ref type="figure" target="#fig_1">Fig. 15</ref>, we show the activation maps of the last convolutional layer of our models with a VGG16-GeM and a ResNet50-GeM backbone, both trained using the Contrastive Loss function (CL) and the proposed Generalized Contrastive Loss (GCL) function. We selected two example image pairs from the MSLS test set <ref type="bibr" target="#b8">[9]</ref> corresponding to the cities of Kampala and Stockholm, one from the Pittsburgh30k test set <ref type="bibr" target="#b3">[4]</ref>, and one from the Tokyo 24/7 dataset <ref type="bibr" target="#b35">[36]</ref>. For all cases, we observed that the model trained with the GCL function produces higher activation for the common visual features of the images, and lower for the irrelevant parts (i.e. the road or the sky), in contrast to the model trained with the binary CL, which focuses less in the concerned areas of the pictures. In the example from Stockholm we can observe that our model does not respond to the cars (which vary from picture to picture), while it does respond strongly to the cranes (which are a more permanent feature). The example from Tokyo 24/7 is also particularly interesting: our model trained with the GCL function has high responses on the common parts of the images even under big changes of illumination. We observed that ResNet architectures tend to produce a peak of activation on the top left corner of the images. This does not seem to occur with the VGG architecture, so our intuition is that this artifact is due to the residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Results with Global Average Pooling</head><p>In addition to using GeM, we also trained the considered backbones (i.e. VGG16, ResNet50, ResNet152 and ResNeXt) using a Global Average Pooling on the MSLS dataset. We show the results in <ref type="table" target="#tab_8">Table 7</ref>. We observed that our method can reach good results with an even simpler pooling layer as is a Global Average Pooling, although a GeM layer usually leads to better results (see main paper). Our methods reaches good results on the validation and test sets of MSLS and generalizes well to unseen datasets such as Pittsburgh30k <ref type="bibr" target="#b3">[4]</ref>, Tokyo24/7 <ref type="bibr" target="#b3">[4]</ref>, RobotCar Seasons V2 <ref type="bibr" target="#b36">[37]</ref> and Extended CMU Seasons <ref type="bibr" target="#b36">[37]</ref>. As we observed also with the GeM models, the Global Average Pooling models can reach better performance when PCA whitening is applied, up to a 72.3% top-5 recall on the test set of MSLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Results on Pittsburgh250k and TokyoTM</head><p>We evaluated the generalization of our models trained on MSLS to the Pittsburgh250k <ref type="bibr" target="#b4">[5]</ref> and TokyoTM <ref type="bibr" target="#b3">[4]</ref> datasets. The test set of the former consists of 83k map and 8k query images taken over the span of several years in Pittsburgh, Pennsylvania, USA. The TokyoTM dataset consists of images collected using the Time Machine tool on Google Street View in Tokyo over several years. Its validation set is divided into a map and a query set, with 49k and 7k images.</p><p>The results that we obtained are in line with those reported in the main paper. Our models trained with a GCL  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Detailed results on RobotCar Seasons v2 and Extended CMU Seasons</head><p>We provide results divided by type of environment for the Extended CMU Dataset and display them in <ref type="table" target="#tab_10">Table 9</ref>. We observed that all models (ours and SoTA ones) tend to reach a higher performance on the urban images. This is logical, as they are all trained on datasets with images depicting mainly urban areas.</p><p>We also provide detailed results for the RobotCar Seasons v2 dataset, organized by weather and illumination conditions, in <ref type="table" target="#tab_0">Table 10</ref>. We observe that all methods obtain a higher successful localization rate on day conditions, but our methods tend to outperform the state-of-the-art on the night-time subsets. The MSLS train dataset includes images taken at night, but those are heavily underrepresented, so it is very interesting that GCL based models can localize images under these conditions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Comparison to Thoma et al [48]</head><p>In <ref type="table" target="#tab_0">Table 11</ref>, we show the results that we obtained on the first version of the CMU dataset, and we compare our performance to the one achieved by the method presented in <ref type="bibr" target="#b47">[48]</ref>. Our method is trained on the MSLS dataset, while the model of <ref type="bibr" target="#b47">[48]</ref> is a NetVLAD model trained on the Oxford RobotCar dataset <ref type="bibr" target="#b37">[38]</ref>. We observe that our method generalizes as well as <ref type="bibr" target="#b47">[48]</ref> to this unseen dataset with the same VGG backbone, and significantly better when using a bigger backbone like ResNeXt. It is worth noting that our method uses a simple pooling method (namely GeM <ref type="bibr" target="#b16">[17]</ref>), while <ref type="bibr" target="#b47">[48]</ref> deploys a VLAD layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Performance for different ground truths</head><p>We performed an evaluation of the performance that the models that we train achieved when varying the ground truth threshold, i.e. when applying different cut values on the criteria that define two images as similar or not. We report results on the MSLS validation dataset. First, we consider the distance, computed using the GPS coordinates associated to image pairs, between the location    from where the images were taken and define a threshold on this value. For instance, we consider as similar all the image pairs that were taken from places with a distance lower than D, and vary D between 5 meters to 50 meters. We report the result in <ref type="figure" target="#fig_1">Fig. 16a</ref>. An image is considered as correctly identified if at least one of the top-5 retrieved images is at a distance smaller than the considered threshold.</p><p>Furthermore, we consider the defined graded similarity ? and threshold it with values between 0 and 0.9. An image is considered as correctly identified if at least one of the top-5 retrieved images is labeled with a similarity greater than the considered threshold. We report the results in <ref type="figure" target="#fig_1">Fig. 16</ref>.</p><p>In both cases, the models trained using the GCL function are more robust than those trained using the CL function to variations of the criteria and values used to establish the ground truth similarity between image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Extra results on 7Scenes</head><p>Precision-recall curve. We report the precision recall curves that we obtained on the 7Scenes dataset in <ref type="figure" target="#fig_1">Fig. 17</ref>. The Localization. We tested the impact that a model trained with the GCL function and graded similarity has on the performance of a camera localization pipeline. We compared it to the case when the same model trained with the original binary Contrastive Loss function is used for the image retrieval task, and report the localization error that we obtained in <ref type="table" target="#tab_0">Table 12</ref>. This experiment is meant only to demonstrate that the general expectation of better retrieval results contribute to better localization' is satisfied by the improved retrieval results achieved using the proposed GCL. For this experiment, we employed the InLoc algorithm <ref type="bibr" target="#b72">[73]</ref>, using our learned representations with the ResNet34 backbone for the retrieval step. The images retrieved using the representation learned with GCL consistently led to an improvement of the localization accuracy w.r.t. the case in which the CL function is used. The enhanced performance of the retrieval task has a positive effect on the precision of localization algorithms. We also report the results of the original InLoc pipeline, based on and optimized to work with a NetVLAD backbone. While the GCL function contributes indirectly to better localization w.r.t. its binary CL counterpart, it requires more work to be effectively embedded in a localization pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B FIELD-OF-VIEW OVERLAP</head><p>We studied the relation among translation distance, rotation difference and the resulting FoV overlap measure. For that, we selected a subset of pairs of the MSLS training set and measured how the value of the FoV overlap varies with respect to the position and orientation difference of the image pairs. In <ref type="figure" target="#fig_1">Fig. 18a</ref>, we plot the relationship between the translation distance and the value of the FoV overlap. We observed a somewhat linear relationship between increasing translation distance and decreasing FoV overlap. Moreover, many of the image pairs with FoV overlap of approximately 50% were taken at a distance of about 25m. The variance that we observed in <ref type="figure" target="#fig_1">Fig. 18a</ref> can be attributed to the changes in orientation. Indeed, the relation between FoV overlap and orientation difference is less clear (see <ref type="figure" target="#fig_1">Fig. 18b</ref>). However, in general, the pairs with smaller orientation distance tend to have a higher FoV overlap.</p><p>We plot the relation of the FoV overlap with both translation and orientation distance in a 3-dimensional plane in <ref type="figure" target="#fig_1">Fig. 18c</ref>, and as a heatmap in <ref type="figure" target="#fig_1">Fig. 18d</ref>. Rotation and translation distance jointly influence the FoV overlap: the smaller the translation and orientation distance, the higher the similarity. Furthermore, as one of the distances has a higher value, the computer FoV overlap decreases, resulting in a lower annotated ground truth similarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C GRADIENT COMPUTATIONS</head><p>Let us consider two input images x i and x j , their latent representationsf (x i ) andf (x j ), and define d(x i , x j ) the Euclidean distance between the representations, such as:</p><formula xml:id="formula_8">d(x i , x j ) = f (x i ) ?f (x j ) 2</formula><p>For simplicity of notation, hereinafter we refer to d(x i , x j ) as d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Contrastive loss</head><p>The Contrastive Loss function is defined as:</p><formula xml:id="formula_9">L CL = 1 2 d 2 , if y = 1 1 2 max(? ? d, 0) 2 , if y = 0</formula><p>where y corresponds to the binary ground truth label and ? corresponds to the margin. In order to compute the gradient for this function, we consider three cases, depending on the ground truth label y and the value of the distance d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 1) y = 1</head><p>The loss function becomes:</p><formula xml:id="formula_10">L CL = 1 2 d 2</formula><p>and its derivative with respect to d is:</p><formula xml:id="formula_11">?L CL = ? ?d (L CL ) = ? ?d 1 2 d 2 = d</formula><p>Case 2) y = 0, d &lt; ? The loss function becomes:</p><formula xml:id="formula_12">L CL = 1 2 (? ? d) 2</formula><p>and its derivative with respect to d is:</p><formula xml:id="formula_13">?L CL = ? ?d (L CL ) = ? ?d 1 2 (? ? d) 2 = = (? ? d)(?1) = d ? ?</formula><p>Case 3) y = 0, d ? ? L CL = 0 and the gradient L CL = 0 as well. Thus, case 2 and case 3, for y = 0, can be grouped as:</p><formula xml:id="formula_14">?L CL = d ? ?, if d &lt; ? 0, if d ? ?</formula><p>and simplified as:</p><formula xml:id="formula_15">?L CL = min(d ? ?, 0)</formula><p>Finally, the gradient of the Contrastive Loss function is:</p><formula xml:id="formula_16">?L CL = d, if y = 1 min(d ? ?, 0), if y = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Generalized Contrastive loss</head><p>We defined the Generalized Contrastive Loss function as:</p><formula xml:id="formula_17">L GCL = ? i,j ? 1 2 d 2 + (1 ? ? i,j ) ? 1 2 max(? ? d, 0) 2</formula><p>where ? i,j is the ground truth degree of similarity between the input images x i and x j , and its values are in the interval [0, 1]. To compute the gradient of the GCL function we consider two cases, namely when 1) the distance d between the representations is lower than the margin ? and 2) the alternative case when d is larger than ? . Case 1) d &lt; ? The Generalized Contrastive loss function becomes:</p><formula xml:id="formula_18">L GCL = ? i,j ? 1 2 d 2 + (1 ? ? i,j ) ? 1 2 (? ? d) 2</formula><p>and its derivative with respect to d is:</p><formula xml:id="formula_19">?L GCL = = ? ?d L GCL = = ? ?d ? i,j ? 1 2 d 2 + (1 ? ? i,j ) ? 1 2 (? ? d) 2 = = ? i,j ? d + (1 ? ? i,j )(? ? d)(?1) = = ? i,j ? d + d ? ? ? ? i,j ? d + ? i,j ? ? = = d + ? (? i,j ? 1) Case 2) d ? ?</formula><p>The Generalized Contrastive loss function becomes:</p><formula xml:id="formula_20">L GCL = ? i,j ? 1 2 d 2 + (1 ? ? i,j ) ? 1 2 (0) 2 = ? i,j ? 1 2 d 2</formula><p>and its derivative with respect to d is:</p><formula xml:id="formula_21">?L GCL = = ? ?d L GCL = = ? ?d ? i,j ? 1 2 d 2 = d ? ? i,j</formula><p>Finally, the gradient of the Generalized Contrastive Loss function is:</p><formula xml:id="formula_22">?L GCL = d + ? (? i,j ? 1), if d &lt; ? d ? ? i,j , if d ? ? (6)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>M. Leyva-Vallina and N. Petkov are with the Intelligent Systems Group of the Bernoulli Institute, at the University of Groningen, the Netherlands. ? N. Strisciuglio is with the Faculty of Electrical Engineering, Mathematics and Computer Science of the University of Twente, the Netherlands. ? E-mail: m.leyva.vallina@rug.nl, n.strisciuglio@utwente.nl</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Examples from MSLS dataset of a query image (a) and three matches with different degrees of similarity. A very close match is (b) with 86% of overlap, while (c) is a borderline case with 52% of commonality, and (d) is a negative match, with no features in common with the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>LFig. 2 .</head><label>2</label><figDesc>GCL (f (xi),f (xj )) Sketch of a siamese architecture where x i and x j are the input images,f represents the convolutional backbone with a pooling layer andf (x i ) andf (x j ) are the representations of the input images. They are used as input for the Generalized Contrastive Loss function L GCL (f (x i ),f (x j )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Example dataset consisting of three images. (A, B) and (B, C) are considered similar (y = 1) because they share part of their content, while (A, C) are dissimilar (y = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>(a) 2D Field-of-View representation with angle ? and radius r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Example image pairs from the MSLS dataset. The first row shows the query images, the second row shows the corresponding matches from the map set, and the third row shows the estimated 2D FoV overlap. The query image is associated with the red camera, while the map image with the blue camera. The first column shows a positive match with 75.5% FoV overlap, and many visual features in common. The second column shows a borderline pair: the two images have 50.31% FoV overlap and some features in common. The third column shows a soft negative match, where the two images have FoV overlap of 16.78%. The fourth column shows a hard negative match, where the two images are taken by cameras looking in opposite directions, and the FoV overlap is 0%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Example image pairs from the TB-Places dataset. The first row shows a positive pair with FoV overlap of 74%. The second row depicts a soft negative pair with FoV overlap equal to 41%. The red camera corresponds to the query image, while the blue one to the map image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>3D Field-of-View overlap examples from the 7Scenes dataset. The first row shows a positive pair with 3D FoV overlap of 75%. The second row depicts a borderline pair with 50% 3D FoV overlap. The last row shows a soft negative image pair with 25% 3D FoV overlap. The red pointcloud corresponds to the query 3D FoV, the blue one is the map, and the magenta represents the overlap between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Configurations of the experiments on the TB-Places dataset. (a) W17 subset is the map set, and W18 is the query. (b) We divide W18 into map and query. For visualization purposes, the trajectories have been downsampled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>MSLS test set VGG-NetVLAD-TL-64-Pitts VGG-NetVLAD-TL-64-Pitts-PCAw VGG-NetVLAD-TL-16-MSLS VGG-NetVLAD-TL-16-MSLS-PCAw VGG-NetVLAD-TL-64-MSLS VGG-NetVLAD-TL-64-MSLS-PCAw VGG-NetVLAD-Comparison of the results achieved by our methods with those of state-of-the-art methods and models trained with a binary contrastive loss for (a) MSLS validation and (b) MSLS test. TL stands for models trained with the Triplet Loss, CL for Contrastive Loss and GCL for Generalized Contrastive Loss. The results obtained by our models are displayed in red, with a solid line for those trained with a GCL function, and a dashed one for those trained with a CL function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison of the results for the TB-Places dataset. In (a) we report the results when using W17 as map and W18 as query set. In (b), we show the top-k recall achieved when dividing W18 into map and query. The results achieved by our method are shown in red, the results of the models trained with the binary Contrastive Loss are in blue, and the results of NetVLAD are displayed in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Recall@K results achieved on the 7 Scenes dataset. The results of the models trained with our Generalized Contrastive Loss are shown in red, while those of the models trained with the binary Contrastive Loss are shown in blue. The Recall@K achieved by NetVLAD off-the-shelf is plotted in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Similarity ground truth distribution for 10000 randomly selected pairs in the MSLS train set when using different batch composition strategies. The vertical axis is in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>(a) ResNet50-GeM-CL (b) ResNet50-GeM-GCL Visualization of the learned embedded space. We selected 1000 random positive pairs and 1000 random negative pairs from the MSLS Copenhagen set, computed the differences between their representations and projected them into a 2D space using T-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>CNN activations for the ResNet50-GeM and the VGG16-GeM models with CL and GCL for several input image pairs. The first two pairs, corresponding to the first four columns, are part of the MSLS test set. The third and fourth belong to the Pittsburgh30k and Tokyo 24/7 test set, respectively. We show the activations for the last layer of the backbone overlapped with the input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>Comparison of the results achieved by methods trained with GCL and CL for (a) different distance thresholds and (b) different ? threshold values. GCL-based methods are plotted in red, while CL-based methods are shown in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>0.0 / 2.5 0.0 / 0.0 / 1.3 0.0 / 0.0 / 1.9 0.0 / 7.9 / 54.9 0.0 / 2.2 / 14.3 4.9 / 23.9 / 85.4 3.3 / 8.8 / 49.8 0.9 / 1.3 / 14.5 0.0 / 12.2 / 66.5 0.0 / 3.3 / 31.8 1.3 / 8.3 / 44.0 1 / 6.4 / 34.4 VGG-avg-GCL 0.0 / 0.5 / 3.9 0.0 / 0.0 / 1.8 0.0 / 0.2 / 2.8 0.6 / 15.2 / 82.9 2.2 / 6.7 / 44.6 7.8 / 31.7 / 95.1 4.2 / 19.5 / 73.0 0.4 / 5.7 / 33.9 2.0 / 21.8 / 86.3 3.8 / 13.7 / 57.8 3.0 / 16.1 / 66.3 2.3 / 12.5 / 51.7 VGG-avg-CL 0.0 / 0.0 / 0.0 0.0 / 0.0 / 0.0 0.0 / 0.0 / 0.0 0.0 / 11.0 / 61.6 0.4 / 4.0 / 20.5 5.9 / 26.3 / 85.4 5.1 / 13.5 / 64.2 3.1 / 8.4 / 40.1 4.1 / 17.3 / 76.6 1.9 / 11.8 / 40.3 3.0 / 13.0 / 54.5 2.3 / 10 / 42 VGG-avg-GCL 0.0 / 0.0 / 1.0 0.0 / 0.0 / 0.4 0.0 / 0.0 / 0.7 0.6 / 17.1 / 79.3 3.1 / 11.2 / 49.1 9.3 / 34.6 / 97.6 6.5 / 17.7 / 80.0 3.5 / 16.3 / 56.4 5.6 / 25.4 / 90.9 3.8 / 18.0 / 69.7 4.7 / 19.9 / 73.9 3.6 / 15.3 / 57.1 VGG-GeM-CL 0.0 / 0.0 / 2.0 0.0 / 0.4 / 1.8 0.0 / 0.2 / 1.9 0.0 / 14.6 / 80.5 2.2 / 6.7 / 45.5 8.3 / 31.2 / 94.1 6.0 / 20.0 / 77.7 3.5 / 11.5 / 47.1 2.0 / 17.8 / 89.8 4.7 / 18.0 / 68.2 4.0 / 17.0 / 70.8 3.1 / 13.2 / 55 VGG-GeM-GCL 0.0 / 0.5 / 7.4 0.0 / 0.0 / 0.9 0.0 / 0.2 / 4.0 0.6 / 16.5 / 82.3 2.2 / 11.6 / 61.2 10.2 / 38.0 / 99.0 7.9 / 23.3 / 83.7 2.6 / 11.5 / 51.1 2.0 / 22.3 / 90.9 7.1 / 20.9 / 70.6 4.8 / 20.4 / 76.2 3.7 / 15.8 / 59.7 VGG-GeM-CL 0.0 / 0.5 / 2.0 0.0 / 0.0 / 0.0 0.0 / 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 17 .</head><label>17</label><figDesc>Precision-recall curves achieved on the 7Scenes dataset by ResNet18 and ResNet34 models trained using the Contrastive Loss function (CL, blue lines) and the proposed Generalized Contrastive loss (GCL, red lines) function. models trained with the GCL function (red lines) consistently outperform their counterpart trained with the CL function (blue lines), when they deploy both the ResNet18 and ResNet34 backbones. The results are consistent for all the sub-sets of the 7Scenes dataset. The case of the chess scene is particularly interesting, for which the ResNet34-GCL model achieved near-perfect performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 18 .</head><label>18</label><figDesc>ti o n d is ta n ce (? ) 0 20 40 60 80T r a n s l a t i o n d i s t a n c e ( m ) Relation of 2D FoV overlap with translation and orientation distance. This data is computed from a subset of the MSLS training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>RobotCar Seasons v2 Extended CMU Seasons Method Loss80.9 90.7 92.6 62.3 76.2 81.1 79</head><label></label><figDesc>60.3 65.5 27.9 40.5 46.5 51.2 71.9 79.7 24.1 39.4 47.0 77.8 81.4 41.7 55.7 60.6 61.6 80.0 86.0 34.0 51.1 61.3 75.1 78.5 36.3 49.0 54.1 64.7 81.5 86.8 36.2 54.0 57.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MSLS-Test</cell><cell>Pitts30k</cell><cell>Tokyo24/7</cell><cell></cell><cell></cell></row><row><cell></cell><cell>CL</cell><cell>N</cell><cell cols="4">512 47.0 3.1</cell><cell>13.2</cell><cell>55.0</cell><cell>2.8</cell><cell>8.6</cell><cell>44.5</cell></row><row><cell>VGG-GeM</cell><cell cols="2">GCL N CL Y</cell><cell cols="4">512 65.9 3.7 512 61.4 4.2</cell><cell>15.8 18.7</cell><cell>59.7 62.5</cell><cell>3.6 4.4</cell><cell>11.2 13.4</cell><cell>55.8 56.5</cell></row><row><cell></cell><cell cols="2">GCL Y</cell><cell cols="3">512 72.0 83.1 85.8 47.0 60.8 65.5 73.3 85.9 89.9 47.6 61.0 69.2</cell><cell>5.4</cell><cell>21.9</cell><cell>69.2</cell><cell>5.7</cell><cell>17.1</cell><cell>66.3</cell></row><row><cell></cell><cell>CL</cell><cell cols="4">N 2048 51.4 66.5 70.8 29.7 44.0 50.7 61.5 80.0 86.9 30.8 46.0 56.5</cell><cell>3.2</cell><cell>15.4</cell><cell>61.5</cell><cell>3.2</cell><cell>9.6</cell><cell>49.5</cell></row><row><cell>ResNet50-GeM</cell><cell cols="5">GCL N 2048 66.2 78.9 81.9 43.3 59.1 65.0 72.3 87.2 91.3 44.1 61.0 66.7 CL Y 1024 63.2 76.6 80.7 37.9 53.0 58.5 66.2 82.2 87.3 36.2 51.8 61.0</cell><cell>2.9 5.0</cell><cell>14.0 21.1</cell><cell>58.8 66.5</cell><cell>3.8 4.7</cell><cell>11.8 13.4</cell><cell>61.6 51.6</cell></row><row><cell></cell><cell cols="5">GCL Y 1024 74.6 84.7 88.1 52.9 65.7 71.9 79.9 90.0 92.8 58.7 71.1 76.8</cell><cell>4.7</cell><cell>20.2</cell><cell>70.0</cell><cell>5.4</cell><cell>16.5</cell><cell>69.9</cell></row><row><cell></cell><cell>CL</cell><cell cols="4">N 2048 58.0 72.7 76.1 34.1 50.8 56.8 66.5 83.8 89.5 34.6 57.1 63.5</cell><cell>3.3</cell><cell>15.2</cell><cell>64.0</cell><cell>3.2</cell><cell>9.7</cell><cell>52.2</cell></row><row><cell>ResNet152-GeM</cell><cell cols="5">GCL N 2048 70.3 82.0 84.9 45.7 62.3 67.9 72.6 87.9 91.6 34.0 51.8 60.6 CL Y 2048 66.9 80.9 83.8 44.8 59.2 64.8 71.2 85.8 89.8 54.3 68.9 75.6</cell><cell>2.9 6.1</cell><cell>13.1 23.5</cell><cell>63.5 68.9</cell><cell>3.6 4.8</cell><cell>11.3 14.2</cell><cell>63.1 55.0</cell></row><row><cell></cell><cell cols="5">GCL Y 2048 79.5 88.1 90.1 57.9 70.7 75.7 80.7 91.5 93.9 69.5 81.0 85.1</cell><cell>6.0</cell><cell>21.6</cell><cell>72.5</cell><cell>5.3</cell><cell>16.1</cell><cell>66.4</cell></row><row><cell></cell><cell>CL</cell><cell cols="4">N 2048 62.6 76.4 79.9 40.8 56.5 62.1 56.0 77.5 85.0 37.8 54.9 62.5</cell><cell>1.9</cell><cell>10.4</cell><cell>54.8</cell><cell>2.9</cell><cell>9.0</cell><cell>52.6</cell></row><row><cell>ResNeXt-GeM</cell><cell cols="5">GCL N 2048 75.5 86.1 88.5 56.0 70.8 75.1 64.0 81.2 86.6 37.8 53.6 62.9 CL Y 1024 74.3 87.0 89.6 49.9 63.8 69.4 70.9 85.7 90.2 50.8 67.6 74.3</cell><cell>2.7 3.8</cell><cell>13.4 17.2</cell><cell>65.2 68.2</cell><cell>3.5 4.9</cell><cell>10.5 14.4</cell><cell>58.8 61.7</cell></row><row><cell></cell><cell cols="5">GCL Y 1024 .2 90.4 93.2 58.1 74.3 78.1</cell><cell>4.7</cell><cell>21.0</cell><cell>74.7</cell><cell>6.1</cell><cell>18.2</cell><cell>74.9</cell></row></table><note>PCAw Dim R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 0.25m/2 ? 0.5m/5 ? 5.0m/10 ? 0.25m/2 ? 0.5m/5 ? 5.0m/10?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 9 90.7 92.6 62.3 76.2 81.1 79</head><label>2</label><figDesc>Comparison of our results with those of state-of-the-art approaches on the considered datasets. For each model, we report if PCA whitening is used and the dimensionality of the learned image latent vector.The best result by our method is underlined, and the overall best in presented in bold font.</figDesc><table><row><cell></cell><cell></cell><cell>MSLS-Val</cell><cell>MSLS-Test</cell><cell>Pitts30k</cell><cell>Tokyo24/7</cell><cell cols="3">RobotCar Seasons v2</cell><cell>Extended CMU Seasons</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7 78.4</cell><cell>5.6</cell><cell>22.0</cell><cell>71.0</cell><cell>10.7</cell><cell>27.8</cell><cell>84.1</cell></row><row><cell>NetVLAD 64 Pitts</cell><cell>Y</cell><cell cols="4">4096 70.7 81.4 84.6 30.6 41.9 47.5 83.7 91.8 94.0 67.0 77.8 80.3</cell><cell>5.8</cell><cell>23.1</cell><cell>73.2</cell><cell>11.6</cell><cell>30.3</cell><cell>87.5</cell></row><row><cell>NetVLAD 64 MSLS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>2.0</cell><cell>9.2</cell><cell>45.5</cell><cell>1.3</cell><cell>4.5</cell><cell>31.9</cell></row><row><cell>NetVLAD 64 MSLS</cell><cell>Y</cell><cell cols="4">4096 70.1 80.8 84.9 45.1 58.8 63.7 68.6 84.7 88.9 34.0 47.6 57.1</cell><cell>4.2</cell><cell>18.0</cell><cell>68.1</cell><cell>3.9</cell><cell>12.1</cell><cell>58.4</cell></row><row><cell>NetVLAD 16 MSLS</cell><cell>N</cell><cell cols="4">8192 49.5 65.0 71.8 29.3 43.5 50.4 48.7 70.6 78.9 13.0 33.0 43.8</cell><cell>1.8</cell><cell>9.2</cell><cell>48.4</cell><cell>1.7</cell><cell>5.5</cell><cell>39.1</cell></row><row><cell>NetVLAD 16 MSLS</cell><cell>Y</cell><cell cols="4">4096 70.5 81.1 84.3 39.4 53.0 57.5 70.3 84.1 89.1 37.8 53.3 61.0</cell><cell>4.8</cell><cell>17.9</cell><cell>65.3</cell><cell>4.4</cell><cell>13.7</cell><cell>61.4</cell></row><row><cell>Patch NetVLAD</cell><cell>Y</cell><cell cols="4">4096 79.5</cell><cell>9.6</cell><cell>35.3</cell><cell>90.9</cell><cell>11.8</cell><cell>36.2</cell><cell>96.2</cell></row><row><cell>ResNet101-AP-GeM</cell><cell>Y</cell><cell cols="4">2048 64.1 75.0 78.2 33.7 44.5 49.4 80.7 91.4 94.0 11.4 22.9 30.5</cell><cell>5.1</cell><cell>20.5</cell><cell>66.1</cell><cell>4.9</cell><cell>14.7</cell><cell>65.2</cell></row><row><cell>NetVLAD-SARE</cell><cell>Y</cell><cell cols="4">4096 68.1 77.3 82.4 34.4 44.3 48.8 87.8 94.3 95.9 79.7 86.7 90.5</cell><cell>7.4</cell><cell>26.5</cell><cell>81.3</cell><cell>6.4</cell><cell>19.4</cell><cell>75.5</cell></row><row><cell>VGG-GeM-GCL</cell><cell>N</cell><cell cols="4">512 65.9 77.8 81.4 41.7 55.7 60.6 61.6 80.0 86.0 34.0 51.1 61.3</cell><cell>3.7</cell><cell>15.8</cell><cell>59.7</cell><cell>3.6</cell><cell>11.2</cell><cell>55.8</cell></row><row><cell>VGG-GeM-GCL</cell><cell>Y</cell><cell cols="4">512 72.0 83.1 85.8 47.0 60.8 65.5 73.3 85.9 89.9 47.6 61.0 69.2</cell><cell>5.4</cell><cell>21.9</cell><cell>69.2</cell><cell>5.7</cell><cell>17.1</cell><cell>66.3</cell></row><row><cell>ResNet50-GeM-GCL</cell><cell>N</cell><cell cols="4">2048 66.2 78.9 81.9 43.3 59.1 65.0 72.3 87.2 91.3 44.1 61.0 66.7</cell><cell>2.9</cell><cell>14.0</cell><cell>58.8</cell><cell>3.8</cell><cell>11.8</cell><cell>61.6</cell></row><row><cell>ResNet50-GeM-GCL</cell><cell>Y</cell><cell cols="4">1024 74.6 84.7 88.1 52.9 65.7 71.9 79.9 90.0 92.8 58.7 71.1 76.8</cell><cell>4.7</cell><cell>20.2</cell><cell>70.0</cell><cell>5.4</cell><cell>16.5</cell><cell>69.9</cell></row><row><cell cols="2">ResNet152-GeM-GCL N</cell><cell cols="4">2048 70.3 82.0 84.9 45.7 62.3 67.9 72.6 87.9 91.6 34.0 51.8 60.6</cell><cell>2.9</cell><cell>13.1</cell><cell>63.5</cell><cell>3.6</cell><cell>11.3</cell><cell>63.1</cell></row><row><cell cols="2">ResNet152-GeM-GCL Y</cell><cell cols="4">2048 79.5 88.1 90.1 57.9 70.7 75.7 80.7 91.5 93.9 69.5 81.0 85.1</cell><cell>6.0</cell><cell>21.6</cell><cell>72.5</cell><cell>5.3</cell><cell>16.1</cell><cell>66.4</cell></row><row><cell>ResNeXt-GeM-GCL</cell><cell>N</cell><cell cols="4">2048 75.5 86.1 88.5 56.0 70.8 75.1 64.0 81.2 86.6 37.8 53.6 62.9</cell><cell>2.7</cell><cell>13.4</cell><cell>65.2</cell><cell>3.5</cell><cell>10.5</cell><cell>58.8</cell></row><row><cell>ResNeXt-GeM-GCL</cell><cell>Y</cell><cell cols="4">1024 80..2 90.4 93.2 58.1 74.3 78.1</cell><cell>4.7</cell><cell>21.0</cell><cell>74.7</cell><cell>6.1</cell><cell>18.2</cell><cell>74.9</cell></row></table><note>Method PCAw Dim R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 0.25m/2 ? 0.5m/5 ? 5.0m/10 ? 0.25m/2 ? 0.5m/5 ? 5.0m/10? NetVLAD 64 Pitts N 32768 47.7 62.8 70.9 30.7 41.9 46.4 82.1 91.4 93.8 62.2 73.N 32768 44.6 61.1 66.4 28.8 44.0 50.7 40.4 64.5 74.2 11.4 24.1 31.5 86.2 87.7 48.1 57.6 60.5 88.7 94.5 95.9 86.0 88.6 90.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 Average</head><label>3</label><figDesc>Precision results obtained by the networks trained with the proposed Generalized Contrastive loss function on the 7Scenes dataset, compared with those achieved by the same network architectures trained using the binary Contrastive loss function and by the NetVLAD off-the-shelf model.</figDesc><table><row><cell></cell><cell>NetVLAD</cell><cell cols="2">ResNet18</cell><cell cols="2">ResNet34</cell></row><row><cell>Scene</cell><cell>off-the-shelf</cell><cell>CL</cell><cell>GCL</cell><cell>CL</cell><cell>GCL</cell></row><row><cell>Heads</cell><cell>0.587</cell><cell>0.739</cell><cell>0.807</cell><cell>0.759</cell><cell>0.853</cell></row><row><cell>Stairs</cell><cell>0.533</cell><cell>0.855</cell><cell>0.883</cell><cell>0.884</cell><cell>0.944</cell></row><row><cell>Pumpkin</cell><cell>0.491</cell><cell>0.768</cell><cell>0.849</cell><cell>0.782</cell><cell>0.914</cell></row><row><cell>Fire</cell><cell>0.539</cell><cell>0.786</cell><cell>0.811</cell><cell>0.796</cell><cell>0.803</cell></row><row><cell>Redkitchen</cell><cell>0.439</cell><cell>0.790</cell><cell>0.876</cell><cell>0.782</cell><cell>0.902</cell></row><row><cell>Chess</cell><cell>0.645</cell><cell>0.943</cell><cell>0.964</cell><cell>0.945</cell><cell>0.974</cell></row><row><cell>Office</cell><cell>0.399</cell><cell>0.794</cell><cell>0.890</cell><cell>0.802</cell><cell>0.896</cell></row><row><cell>Mean</cell><cell>0.519</cell><cell>0.811</cell><cell>0.868</cell><cell>0.821</cell><cell>0.898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Recall@5 in the MSLS validation and test sets for GCL models when using different batch composition strategies. Strategy A is 50% ? ? [0.5, 1], 25% ? ? (0, 0.5) and 25% ? = 0. B is 25% ? ? [0.75, 1], 25% ? ? [0.5, 0.75), 25% ? ? (0, 0.5) and 25% ? = 0. For C, we have 33.3% ? ? [0.5, 1], 33.3% ? ? (0, 0.5) and 33.3% ? = 0. Finally, for strategy D we have 50% ? ? [0.5, 1] and 50% ? ? [0, 0.5).</figDesc><table><row><cell>Model</cell><cell>Strategy</cell><cell>Val</cell><cell>Test</cell></row><row><cell></cell><cell>A</cell><cell cols="2">77.8 55.7</cell></row><row><cell>VGG-GeM-GCL</cell><cell>B C</cell><cell cols="2">78.8 54.9 78.2 56</cell></row><row><cell></cell><cell>D</cell><cell cols="2">73.4 49.3</cell></row><row><cell></cell><cell>A</cell><cell cols="2">78.9 59.1</cell></row><row><cell>ResNet50-GeM-GCL</cell><cell>B C</cell><cell cols="2">76.5 54.5 75.7 52.4</cell></row><row><cell></cell><cell>D</cell><cell cols="2">75.4 54.7</cell></row><row><cell></cell><cell>A</cell><cell>82</cell><cell>62.3</cell></row><row><cell>ResNet152-GeM-GCL</cell><cell>B C</cell><cell cols="2">80.4 59.7 78.6 58.4</cell></row><row><cell></cell><cell>D</cell><cell cols="2">78.5 64.8</cell></row><row><cell></cell><cell>A</cell><cell cols="2">86.1 70.8</cell></row><row><cell>Resnext-GeM-GCL</cell><cell>B C</cell><cell cols="2">86.2 70.7 87.3 72.4</cell></row><row><cell></cell><cell>D</cell><cell cols="2">81.9 64.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Results achieved on the MSLS validation set by our models trained by backpropagating the gradient only partially through the network. The last trained layer column indicates the block of the backbone until which the gradient is backpropagated.</figDesc><table><row><cell></cell><cell cols="3">ResNet50-GeM</cell><cell cols="3">ResNet152-GeM</cell></row><row><cell cols="7">Last trained layer R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>Off-the-shelf</cell><cell>27.2</cell><cell>41.9</cell><cell>47.4</cell><cell>26.5</cell><cell>39.9</cell><cell>44.5</cell></row><row><cell>Layer4</cell><cell>50.1</cell><cell>64.6</cell><cell>70</cell><cell>40.9</cell><cell>56.4</cell><cell>63.8</cell></row><row><cell>Layer3</cell><cell>66.2</cell><cell>78.9</cell><cell>81.9</cell><cell>70.3</cell><cell>82</cell><cell>84.9</cell></row><row><cell>Layer2</cell><cell>60.3</cell><cell>74.2</cell><cell>77.4</cell><cell>65.4</cell><cell>77.4</cell><cell>80</cell></row><row><cell>Full</cell><cell>65</cell><cell>75.8</cell><cell>81.1</cell><cell>66.6</cell><cell>79.2</cell><cell>81.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Fig. 13. Results obtained on the MSLS validation, MSLS test, Pitts-burgh30k and Tokyo 24/7 datasets by our models with and without PCA withening. For all the datasets, reducing the dimensionality of the latent space vectors and applying the whitening transform contribute to an increase of the retrieval performance.</figDesc><table><row><cell></cell><cell></cell><cell>w</cell></row><row><cell></cell><cell>90</cell><cell></cell></row><row><cell>Recall@5 (%)</cell><cell>80</cell><cell>60 70</cell></row><row><cell></cell><cell>70</cell><cell>50</cell></row><row><cell></cell><cell>32 64 128 256 512 1024 2048</cell><cell>32 64 128 256 512 1024 2048</cell></row><row><cell></cell><cell>Dimensions</cell><cell>Dimensions</cell></row><row><cell></cell><cell>(a) MSLS validation set</cell><cell>(b) MSLS test set</cell></row><row><cell></cell><cell></cell><cell>80</cell></row><row><cell></cell><cell>90</cell><cell></cell></row><row><cell>Recall@5 (%)</cell><cell>80</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell>40</cell></row><row><cell></cell><cell>70</cell><cell></cell></row><row><cell></cell><cell>32 64 128 256 512 1024 2048</cell><cell>32 64 128 256 512 1024 2048</cell></row><row><cell></cell><cell>Dimensions</cell><cell>Dimensions</cell></row><row><cell></cell><cell>(c) Pitts30k test set</cell><cell>(d) Tokyo 24/7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Training time on MSLS using one NVIDIA V100 GPU. In the second column, the number in parenthesis is the amount of time a model took to converge. The total training time we report is until convergence.Vallina received her BSc in Software Engineering from the University of Oviedo in 2015 and her MSc in Artificial Intelligence from the Polytechnic University of Catalonia in 2017. She is currently pursuing her PhD with the Intelligent Systems group in the University of Groningen. Her main research interests are in representation learning for computer vision.</figDesc><table><row><cell>Model</cell><cell cols="3">Epochs Time per epoch Total training time</cell></row><row><cell>NetVLAD-16</cell><cell>30 (22)</cell><cell>24h</cell><cell>22d</cell></row><row><cell>NetVLAD-64</cell><cell>10 (7)</cell><cell>36h</cell><cell>10.5d</cell></row><row><cell>VGG16-GeM-GCL</cell><cell>1</cell><cell>5h</cell><cell>5h</cell></row><row><cell>ResNet50-GeM-GCL</cell><cell>1</cell><cell>6h</cell><cell>6h</cell></row><row><cell>ResNet152-GeM-GCL</cell><cell>1</cell><cell>14h</cell><cell>14h</cell></row><row><cell>ResNetXt-GeM-GCL</cell><cell>1 (1/2)</cell><cell>28h</cell><cell>14h</cell></row><row><cell cols="4">tablishing a new state-of-the-art result. Furthermore, as op-</cell></row><row><cell cols="4">posed to the case of NetVLAD-based models, the training</cell></row><row><cell cols="4">procedure that we deploy does not require a complex pair</cell></row><row><cell cols="4">mining process: we only ensure that each batch contains</cell></row><row><cell cols="4">approximately the same number of positive and negative</cell></row><row><cell cols="4">pairs. Our model also reaches competitive performance by</cell></row><row><cell cols="4">generalizing well to the Pittsburgh30k, Tokyo 24/7, Robot-</cell></row><row><cell cols="4">Car Seasons v2 and Extended CMU Seasons datasets.</cell></row></table><note>Mar?a Leyva-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 77.9 90.4 93.5 64.4 77.8 83.</head><label>7</label><figDesc>Ablation study on the considered datasets: all the models are trained on the MSLS train set and deploy a global average pooling layer. CL stands for Contrastive Loss and while GCL for our Generalized Contrastive Loss. For the cases in which PCA whitening is applied we report the dimensionality that achieves the best results on the MSLS validation set.</figDesc><table><row><cell>Method</cell><cell cols="7">MSLS-Val PCAw Dim R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 0.25m/2 ? 0.5m/5 ? 5.0m/10 ? 0.25m/2 ? 0.5m/5 ? 5.0m/10 ? MSLS-Test Pittsburgh30k Tokyo24/7 RobotCar Seasons V2 Extended CMU Seasons</cell></row><row><cell>VGG-avg-CL</cell><cell>No 512 28.8 47.0 53.9 16.9 30.5 36.4 28.9 53.7 64.8 10.5 24.1 35.6</cell><cell>1.0</cell><cell>6.4</cell><cell>34.4</cell><cell>0.9</cell><cell>3.0</cell><cell>22.7</cell></row><row><cell>VGG-avg-GCL</cell><cell>No 512 48.8 67.8 73.1 21.5 31.4 37.9 42.1 66.9 77.3 20.0 42.5 52.1</cell><cell>2.3</cell><cell>12.5</cell><cell>51.7</cell><cell>2.3</cell><cell>7.2</cell><cell>43.3</cell></row><row><cell>VGG-avg-CL</cell><cell>Yes 128 35.0 53.4 60.1 35.2 47.3 54.1 47.1 69.5 78.3 16.2 28.9 40.0</cell><cell>2.3</cell><cell>10.0</cell><cell>42.0</cell><cell>2.1</cell><cell>6.6</cell><cell>34.9</cell></row><row><cell>VGG-avg-GCL</cell><cell>Yes 256 54.5 72.6 78.2 32.9 49.0 56.5 56.2 76.7 83.9 28.6 45.7 54.9</cell><cell>3.6</cell><cell>15.3</cell><cell>57.1</cell><cell>3.7</cell><cell>11.2</cell><cell>52.5</cell></row><row><cell>ResNet50-avg-CL</cell><cell>No 2048 44.3 60.3 65.9 24.9 39.0 44.6 54.0 75.7 83.1 20.6 40.0 50.2</cell><cell>3.1</cell><cell>12.6</cell><cell>51.0</cell><cell>2.6</cell><cell>7.8</cell><cell>43.4</cell></row><row><cell>ResNet50-avg-GCL</cell><cell>No 2048 59.6 72.3 76.2 35.8 52.0 59.0 62.5 82.7 88.4 24.1 44.1 54.6</cell><cell>2.9</cell><cell>12.9</cell><cell>56.3</cell><cell>3.1</cell><cell>9.7</cell><cell>55.1</cell></row><row><cell>ResNet50-avg-CL</cell><cell>Yes 2048 58.8 71.4 75.8 33.1 46.5 53.3 65.8 82.6 88.2 48.6 63.2 70.5</cell><cell>5.9</cell><cell>22.8</cell><cell>62.7</cell><cell>4.7</cell><cell>13.8</cell><cell>50.8</cell></row><row><cell>ResNet50-avg-GCL</cell><cell>Yes 1024 69.5 81.2 85.5 44.2 57.8 63.4 73.3 87.1 91.2 52.1 68.9 72.7</cell><cell>5.9</cell><cell>23.1</cell><cell>69.6</cell><cell>5.4</cell><cell>16.2</cell><cell>66.5</cell></row><row><cell>ResNet152-avg-CL</cell><cell>No 2048 53.1 70.1 75.4 29.7 44.2 51.3 59.7 80.3 87.0 27.0 48.6 58.4</cell><cell>2.5</cell><cell>13.1</cell><cell>56.6</cell><cell>3.0</cell><cell>9.2</cell><cell>49.9</cell></row><row><cell cols="2">ResNet152-avg-GCL No 2048 65.1 80.0 83.8 43.5 59.2 65.2 69.3 87.2 91.3 32.1 52.1 62.2</cell><cell>3.3</cell><cell>13.5</cell><cell>59.9</cell><cell>3.6</cell><cell>11.0</cell><cell>61.2</cell></row><row><cell>ResNet152-avg-CL</cell><cell>Yes 1024 63.0 77.7 81.5 37.7 51.6 56.9 68.8 85.9 90.4 49.8 67.3 74.3</cell><cell>5.4</cell><cell>22.2</cell><cell>64.8</cell><cell>4.8</cell><cell>14.3</cell><cell>59.9</cell></row><row><cell cols="2">ResNet152-avg-GCL Yes 2048 75.8 87.4 89.7 52.7 68.1 74.2 2</cell><cell>6.2</cell><cell>23.9</cell><cell>70.0</cell><cell>5.7</cell><cell>17.0</cell><cell>66.5</cell></row><row><cell>ResNeXt-avg-CL</cell><cell>No 2048 58.9 75.1 79.9 34.5 50.1 57.7 51.3 73.6 81.9 24.8 47.3 56.8</cell><cell>2.7</cell><cell>10.9</cell><cell>61.0</cell><cell>2.0</cell><cell>6.1</cell><cell>40.0</cell></row><row><cell>ResNeXt-avg-GCL</cell><cell>No 2048 72.2 85.1 87.3 51.5 66.9 71.7 62.9 81.0 87.1 39.4 58.1 68.9</cell><cell>2.7</cell><cell>12.4</cell><cell>63.1</cell><cell>3.3</cell><cell>10.2</cell><cell>57.7</cell></row><row><cell>ResNeXt-avg-CL</cell><cell>Yes 1024 71.6 84.7 88.0 46.5 62.9 68.9 69.2 85.3 89.6 44.8 63.5 73.6</cell><cell>4.8</cell><cell>20.4</cell><cell>70.6</cell><cell>4.6</cell><cell>13.4</cell><cell>58.6</cell></row><row><cell>ResNeXt-avg-GCL</cell><cell>Yes 1024 79.3 89.2 90.3 57.8 72.3 77.1 74.8 88.2 91.8 53.0 76.2 80.6</cell><cell>2.7</cell><cell>10.9</cell><cell>61.0</cell><cell>5.6</cell><cell>16.6</cell><cell>70.7</cell></row><row><cell cols="2">function consistently generalize better to unseen datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">than their counterpart trained with a binary Contrastive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Loss function. Their performance is further boosted if PCA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">whitening is applied, up to a top-5 recall of 93.7% on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pittsburgh250k and 96.7% on TokyoTM.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Generalization results of the models trained on the MSLS dataset for the Pittsburgh250k and TokyoTM benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Pittsburgh250k</cell><cell></cell><cell>TokyoTM</cell><cell></cell></row><row><cell>Method</cell><cell>PCAw</cell><cell cols="7">Dim R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>VGG-avg-CL</cell><cell>No</cell><cell>512</cell><cell>20.2</cell><cell>38.0</cell><cell>47.2</cell><cell>38.9</cell><cell>58.5</cell><cell>67.2</cell></row><row><cell>VGG-avg-GCL</cell><cell>No</cell><cell>512</cell><cell>32.1</cell><cell>53.4</cell><cell>62.7</cell><cell>65.6</cell><cell>80.8</cell><cell>85.2</cell></row><row><cell>VGG-avg-CL</cell><cell>Yes</cell><cell>128</cell><cell>39.3</cell><cell>58.9</cell><cell>67.4</cell><cell>66.0</cell><cell>80.3</cell><cell>85.0</cell></row><row><cell>VGG-avg-GCL</cell><cell>Yes</cell><cell>256</cell><cell>51.3</cell><cell>71.0</cell><cell>78.0</cell><cell>78.6</cell><cell>87.7</cell><cell>90.8</cell></row><row><cell>VGG-GeM-CL</cell><cell>No</cell><cell>512</cell><cell>44.5</cell><cell>63.1</cell><cell>70.1</cell><cell>67.7</cell><cell>80.8</cell><cell>85.0</cell></row><row><cell>VGG-GeM-GCL</cell><cell>No</cell><cell>512</cell><cell>53.3</cell><cell>72.4</cell><cell>79.2</cell><cell>75.5</cell><cell>85.4</cell><cell>88.6</cell></row><row><cell>VGG-GeM-CL</cell><cell>Yes</cell><cell>512</cell><cell>65.1</cell><cell>81.2</cell><cell>85.8</cell><cell>83.7</cell><cell>91.1</cell><cell>93.3</cell></row><row><cell>VGG-GeM-GCL</cell><cell>Yes</cell><cell>512</cell><cell>73.4</cell><cell>86.4</cell><cell>89.9</cell><cell>88.2</cell><cell>93.2</cell><cell>94.7</cell></row><row><cell>ResNet50-avg-CL</cell><cell>No</cell><cell>2048</cell><cell>45.1</cell><cell>65.8</cell><cell>73.6</cell><cell>69.7</cell><cell>83.0</cell><cell>87.5</cell></row><row><cell>ResNet50-avg-GCL</cell><cell>No</cell><cell>2048</cell><cell>56.2</cell><cell>77.1</cell><cell>83.8</cell><cell>72.3</cell><cell>84.8</cell><cell>88.4</cell></row><row><cell>ResNet50-avg-CL</cell><cell>Yes</cell><cell>2048</cell><cell>70.6</cell><cell>85.4</cell><cell>89.5</cell><cell>90.5</cell><cell>94.8</cell><cell>96.0</cell></row><row><cell>ResNet50-avg-GCL</cell><cell>Yes</cell><cell>1024</cell><cell>74.6</cell><cell>87.9</cell><cell>91.6</cell><cell>91.7</cell><cell>95.7</cell><cell>96.8</cell></row><row><cell>ResNet50-GeM-CL</cell><cell>No</cell><cell>2048</cell><cell>54.8</cell><cell>74.2</cell><cell>80.7</cell><cell>73.3</cell><cell>85.4</cell><cell>89.2</cell></row><row><cell>ResNet50-GeM-GCL</cell><cell>No</cell><cell>2048</cell><cell>68.2</cell><cell>84.6</cell><cell>89.2</cell><cell>80.1</cell><cell>88.8</cell><cell>91.6</cell></row><row><cell>ResNet50-GeM-CL</cell><cell>Yes</cell><cell>1024</cell><cell>72.4</cell><cell>86.6</cell><cell>90.4</cell><cell>88.7</cell><cell>93.9</cell><cell>95.4</cell></row><row><cell>ResNet50-GeM-GCL</cell><cell>Yes</cell><cell>1024</cell><cell>80.9</cell><cell>91.4</cell><cell>94.3</cell><cell>92.2</cell><cell>95.6</cell><cell>96.8</cell></row><row><cell>ResNet152-avg-CL</cell><cell>No</cell><cell>2048</cell><cell>51.3</cell><cell>73.0</cell><cell>80.1</cell><cell>73.5</cell><cell>86.4</cell><cell>89.9</cell></row><row><cell>ResNet152-avg-GCL</cell><cell>No</cell><cell>2048</cell><cell>64.0</cell><cell>83.6</cell><cell>89.2</cell><cell>78.9</cell><cell>89.0</cell><cell>91.9</cell></row><row><cell>ResNet152-avg-CL</cell><cell>Yes</cell><cell>1024</cell><cell>69.6</cell><cell>85.9</cell><cell>90.6</cell><cell>90.7</cell><cell>95.1</cell><cell>96.3</cell></row><row><cell>ResNet152-avg-GCL</cell><cell>Yes</cell><cell>2048</cell><cell>80.9</cell><cell>92.2</cell><cell>95.3</cell><cell>94.0</cell><cell>96.7</cell><cell>97.3</cell></row><row><cell>ResNet152-GeM-CL</cell><cell>No</cell><cell>2048</cell><cell>60.7</cell><cell>79.0</cell><cell>85.1</cell><cell>77.8</cell><cell>88.3</cell><cell>91.2</cell></row><row><cell>ResNet152-GeM-GCL</cell><cell>No</cell><cell>2048</cell><cell>68.0</cell><cell>84.9</cell><cell>89.8</cell><cell>81.5</cell><cell>90.3</cell><cell>92.8</cell></row><row><cell>ResNet152-GeM-CL</cell><cell>Yes</cell><cell>2048</cell><cell>76.2</cell><cell>89.9</cell><cell>93.4</cell><cell>91.1</cell><cell>95.3</cell><cell>96.4</cell></row><row><cell>ResNet152-GeM-GCL</cell><cell>Yes</cell><cell>2048</cell><cell>83.8</cell><cell>93.7</cell><cell>96.1</cell><cell>93.1</cell><cell>96.1</cell><cell>96.8</cell></row><row><cell>ResNeXt-avg-CL</cell><cell>No</cell><cell>2048</cell><cell>44.2</cell><cell>65.8</cell><cell>73.9</cell><cell>69.0</cell><cell>82.3</cell><cell>86.3</cell></row><row><cell>ResNeXt-avg-GCL</cell><cell>No</cell><cell>2048</cell><cell>57.9</cell><cell>76.1</cell><cell>82.6</cell><cell>77.7</cell><cell>86.9</cell><cell>89.7</cell></row><row><cell>ResNeXt-avg-CL</cell><cell>Yes</cell><cell>1024</cell><cell>69.6</cell><cell>85.4</cell><cell>89.8</cell><cell>88.9</cell><cell>94.1</cell><cell>95.6</cell></row><row><cell>ResNeXt-avg-GCL</cell><cell>Yes</cell><cell>1024</cell><cell>74.7</cell><cell>88.0</cell><cell>91.7</cell><cell>89.6</cell><cell>94.6</cell><cell>95.9</cell></row><row><cell>ResNeXt-GeM-CL</cell><cell>No</cell><cell>2048</cell><cell>50.2</cell><cell>70.8</cell><cell>78.8</cell><cell>66.1</cell><cell>78.5</cell><cell>82.7</cell></row><row><cell>ResNeXt-GeM-GCL</cell><cell>No</cell><cell>2048</cell><cell>58.6</cell><cell>76.2</cell><cell>81.8</cell><cell>78.7</cell><cell>87.1</cell><cell>90.1</cell></row><row><cell>ResNeXt-GeM-CL</cell><cell>Yes</cell><cell>1024</cell><cell>70.3</cell><cell>85.2</cell><cell>89.6</cell><cell>87.2</cell><cell>93.1</cell><cell>94.5</cell></row><row><cell>ResNeXt-GeM-GCL</cell><cell>Yes</cell><cell>1024</cell><cell>78.2</cell><cell>90.1</cell><cell>93.1</cell><cell>91.8</cell><cell>95.4</cell><cell>96.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Detailed results on the Extended CMU dataset. The symbol denotes the models for which PCA whitening has been applied.</figDesc><table><row><cell></cell><cell>Mean</cell><cell>Urban</cell><cell>Suburban</cell><cell>Park</cell></row><row><cell>Method</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc>Detailed results on the RobotCar Seasons v2 dataset, divided by wheather and ilumination conditions. The symbol denotes the models for which PCA whitening has been applied.NetVLAD-64-Pitts30k 1.5 / 1.5 / 10.3 0.4 / 1.8 / 8.0 0.9 / 1.6 / 9.1 1.2 / 23.8 / 97.0 5.4 / 14.3 / 74.1 10.2 / 44.4 / 99.5 9.3 / 31.6 / 94.9 9.7 / 26.4 / 82.8 5.1 / 26.4 / 92.4 6.6 / 30.3 / 88.6 7.0 / 28.1 / 89.4 5.6 / 22 / 71 NetVLAD-64-Pitts30k 2.0 / 3.9 / 13.3 0.0 / 1.3 / 11.5 0.9 / 2.6 / 12.4 1.2 / 22.6 / 100.0 4.9 / 18.8 / 80.8 9.3 / 42.4 / 98.5 10.2 / 32.6 / 94.9 11.5 / 29.5 / 85.5 5.6 / 27.9 / 92.4 7.1 / 30.3 / 90.0 7.3 / 29.2 / 91.3 5.8 / 23.1 / 73.2 NetVLAD-64-MSLS 0.5 / 1.0 / 5.9 0.0 / 0.4 / 4.4 0.2 / 0.7 / 5.1 0.0 / 11.6 / 67.7 0.9 / 4.5 / 28.6 4.4 / 22.0 / 91.2 4.7 / 13.0 / 60.5 3.1 / 8.4 / 32.6 2.5 / 14.2 / 78.7 1.4 / 9.5 / 51.7 2.5 / 11.7 / 57.5 2 / 9.2 / 45.5 NetVLAD-64-MSLS 0.0 / 1.5 / 11.8 0.0 / 1.3 / 12.4 0.0 / 1.4 / 12.1 0.0 / 15.2 / 87.2 3.1 / 12.1 / 66.5 8.8 / 35.6 / 99.0 7.0 / 28.4 / 90.2 9.3 / 21.6 / 77.5 4.1 / 25.4 / 98.0 5.2 / 21.3 / 77.7 5.5 / 22.9 / 84.7 4.2 / 18 / 68.1 NetVLAD-16-MSLS 0.0 / 0.0 / 3.4 0.0 / 0.9 / 5.3 0.0 / 0.5 / 4.4 1.2 / 10.4 / 78.0 0.9 / 6.2 / 33.0 5.9 / 25.4 / 93.7 3.3 / 10.7 / 62.8 1.8 / 6.6 / 29.1 1.5 / 16.2 / 86.8 1.4 / 8.1 / 57.8 2.3 / 11.8 / 61.5 1.8 / 9.2 / 48.4</figDesc><table><row><cell></cell><cell>night rain</cell><cell>night</cell><cell>night all</cell><cell>overcast winter</cell><cell>sun</cell><cell>rain</cell><cell>snow</cell><cell>dawn</cell><cell>dusk</cell><cell>overcast summer</cell><cell>day all</cell><cell>mean</cell></row><row><cell>Method</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell><cell>0.25/0.5/10m 2/5/10 ?</cell></row><row><cell>NetVLAD-16-MSLS</cell><cell cols="12">0.0 / 0.0 / 1.0 0.0 / 0.9 / 7.1 0.0 / 0.5 / 4.2 1.8 / 19.5 / 90.2 4.9 / 11.6 / 67.4 10.7 / 34.1 / 96.1 6.5 / 24.7 / 89.8 7.9 / 24.2 / 74.4 2.5 / 23.4 / 93.9 7.6 / 24.6 / 77.3 6.2 / 23.1 / 83.6 4.8 / 17.9 / 65.3</cell></row><row><cell>AP-GeM</cell><cell cols="12">0.0 / 2.0 / 14.3 0.0 / 0.9 / 10.6 0.0 / 1.4 / 12.4 1.2 / 26.2 / 89.6 4.0 / 17.4 / 62.1 13.7 / 41.0 / 98.0 7.0 / 26.0 / 87.9 8.4 / 24.7 / 71.8 5.1 / 25.4 / 94.4 5.7 / 23.7 / 75.8 6.6 / 26.2 / 82.1 5.1 / 20.5 / 66.1</cell></row><row><cell>NetVLAD-SARE</cell><cell cols="8">3.4 / 8.9 / 38.9 0.9 / 4.4 / 31.4 2.1 / 6.5 / 35.0 2.4 / 29.3 / 97.6 8.9 / 22.3 / 92.9 13.2 / 43.9 / 100.0 12.1 / 36.3 / 98.1 10.6 / 28.2 / 89.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11 TABLE 12</head><label>1112</label><figDesc>Comparison with Thoma et al.<ref type="bibr" target="#b47">[48]</ref>, that outperformed several triplet-based loss functions on the CMU Seasons dataset. Median translation and rotation errors (in cm and degrees, respectively) on the 7Scenes dataset using the descriptors computed using the models trained with the binary Contrastive Loss and the Generalized Contrastive Loss for retrieval and InLoc for localization..44 ? 36.03cm 4.28 ? 7.12cm 1.97 ? 5.87cm 2.39 ? 6.53cm 2.12 ? 5.62cm 1.85 ? 6.01cm 1.99 ? 10.02cm 2.43 ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell>Urban</cell><cell></cell><cell cols="2">Suburban</cell><cell></cell><cell></cell><cell>Park</cell></row><row><cell cols="3">Method 0.25m/2 Thoma et al [48] 8.0</cell><cell>20.5</cell><cell>70.4</cell><cell>12.7</cell><cell>30.7</cell><cell>84.6</cell><cell>5.1</cell><cell>14.9</cell><cell>67.9</cell><cell>4.5</cell><cell>12.6</cell><cell>56.8</cell></row><row><cell>VGG-GeM-GCL</cell><cell></cell><cell>5.9</cell><cell>15.4</cell><cell>58.9</cell><cell>9.6</cell><cell>24.1</cell><cell>76.4</cell><cell>3.1</cell><cell>9.8</cell><cell>56.3</cell><cell>3.3</cell><cell>9.0</cell><cell>42.0</cell></row><row><cell cols="2">VGG-GeM-GCL+PCAw</cell><cell>9.2</cell><cell>22.8</cell><cell>65.8</cell><cell>14.7</cell><cell>34.2</cell><cell>82.6</cell><cell>5.7</cell><cell>16.1</cell><cell>64.6</cell><cell>5.1</cell><cell>14.0</cell><cell>49.1</cell></row><row><cell>Resnet50-GeM-GCL</cell><cell></cell><cell>6.3</cell><cell>16.3</cell><cell>62.8</cell><cell>10.4</cell><cell>25.5</cell><cell>81.2</cell><cell>3.8</cell><cell>11.2</cell><cell>63.9</cell><cell>3.2</cell><cell>9.0</cell><cell>43.3</cell></row><row><cell cols="2">Resnet50-GeM-GCL+PCAw</cell><cell>9.0</cell><cell>22.5</cell><cell>69.3</cell><cell>14.3</cell><cell>33.8</cell><cell>85.1</cell><cell>6.1</cell><cell>17.0</cell><cell>71.1</cell><cell>4.9</cell><cell>13.4</cell><cell>52.2</cell></row><row><cell cols="2">Resnet152-GeM-GCL</cell><cell>6.2</cell><cell>16.2</cell><cell>66.2</cell><cell>10.1</cell><cell>24.8</cell><cell>82.0</cell><cell>4.1</cell><cell>12.2</cell><cell>66.4</cell><cell>3.1</cell><cell>9.2</cell><cell>49.9</cell></row><row><cell cols="2">Resnet152-GeM-GCL+PCAw</cell><cell>8.8</cell><cell>21.6</cell><cell>64.0</cell><cell>14.1</cell><cell>32.4</cell><cell>80.0</cell><cell>5.9</cell><cell>16.8</cell><cell>65.2</cell><cell>4.7</cell><cell>12.7</cell><cell>47.0</cell></row><row><cell>ResNeXt-GeM-GCL</cell><cell></cell><cell>5.8</cell><cell>15.1</cell><cell>63.0</cell><cell>9.5</cell><cell>24.0</cell><cell>81.0</cell><cell>3.5</cell><cell>10.5</cell><cell>62.5</cell><cell>2.9</cell><cell>7.9</cell><cell>44.7</cell></row><row><cell cols="2">ResNeXt-GeM-GCL+PCAw</cell><cell>9.9</cell><cell>24.3</cell><cell>75.5</cell><cell>15.4</cell><cell>36.0</cell><cell>89.6</cell><cell>6.7</cell><cell>18.4</cell><cell>76.8</cell><cell>5.6</cell><cell>14.9</cell><cell>60.3</cell></row><row><cell>Model</cell><cell>heads</cell><cell>stairs</cell><cell></cell><cell cols="2">pumpkin</cell><cell>fire</cell><cell></cell><cell>redkitchen</cell><cell>office</cell><cell></cell><cell>chess</cell><cell></cell><cell>Mean</cell></row><row><cell cols="2">NetVLAD-DensePE 2cm 1.16 ?</cell><cell cols="2">9cm 2.47 ?</cell><cell cols="2">5cm 1.55 ?</cell><cell>3cm 1.07 ?</cell><cell></cell><cell>4cm 1.31 ?</cell><cell cols="2">3cm 1.05 ?</cell><cell>3cm 1.05 ?</cell><cell></cell><cell>4.14cm 1.38 ?</cell></row><row><cell cols="2">ResNet34-CL 2.98cm 2ResNet34-GCL 2.53cm 2.3 ?</cell><cell cols="2">32.13cm 3.9 ?</cell><cell cols="2">6.4cm 1.95 ?</cell><cell cols="3">5.13cm 2.11 ? 5.38cm 2.0 ?</cell><cell cols="2">5.16cm 1.7 ?</cell><cell cols="3">5.66cm 1.93 ? 8.91cm 2.27 ?</cell></row></table><note>? 0.5m/5 ? 5.0m/10 ? 0.25m/2 ? 0.5m/5 ? 5.0m/10 ? 0.25m/2 ? 0.5m/5 ? 5.0m/10 ? 0.25m/2 ? 0.5m/5 ? 5.0m/10?</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This research received partial support from the EU H2020 TrimBot2020 project (grant no. 688007). We would like to thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine HPC cluster. Finally, we would like to thank Dr. Manuel L?pez Antequera for his assistance with the MSLS dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual place recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Visual place recognition with repetitive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seqslam: Visual route-based navigation for sunny summer days and stormy winter nights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Wyeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1643" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we there yet? challenging seqslam on a 3000 km journey across all four seasons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page">2013</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mapillary street-level sequences: A dataset for lifelong place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Warburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual place recognition: A survey from deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107760</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on deep visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Masone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Place and object recognition by cnn-based cosfire filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leyva-Vallina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="66" to="157" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aggregating local deep features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural network-based place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the performance of convnet features for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4297" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Appearance-invariant place recognition by discriminatively training a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomez-Ojeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="89" to="95" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Place recognition in gardens by learning visual representations: data set and benchmark analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leyva-Vallina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAIP</title>
		<imprint>
			<biblScope unit="page" from="324" to="335" />
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic attraction-repulsion embedding for large scale image localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2570" to="2579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial pyramidenhanced netvlad with weighted triplet loss for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="661" to="674" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2575" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7482" to="7491" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint learning of intrinsic images and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Baslamisli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Groenestege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-A</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="286" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three for one and one for three: Flow, segmentation, and surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Baslamisli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical approach to classify food scenes in egocentric photo-streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leyva-Vallina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M K</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="866" to="877" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation via subcategory exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8991" to="9000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning 3d semantic scene graphs from 3d indoor reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3961" to="3970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5974" to="5983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose-guided knowledge transfer for object part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Banik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops, 2020</title>
		<imprint>
			<biblScope unit="page" from="906" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relocnet: Continuous metric learning relocalisation using neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="751" to="767" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Camnet: Coarse-tofine retrieval for camera re-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2871" to="2880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tb-places: A data set for visual place recognition in garden environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leyva-Vallina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tylecek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Benchmarking 6dof outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8601" to="8610" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<ptr target="http://3dvis.ri.cmu.edu/data-sets/localization" />
		<title level="m">The CMU Visual Localization Data Set</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep metric learning beyond binary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2288" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bags of binary words for fast place recognition in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1188" to="1197" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">All about vlad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the performance of convnet features for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4297" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Soft contrastive learning for visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Embedding transfer with label relaxation for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pointnetvlad: Deep point cloud based retrieval for large-scale place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4470" to="4479" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R D</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Benchmarking image retrieval for visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Patchnetvlad: Multi-scale fusion of locally-global descriptors for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ai can be sexist and racist -it&apos;s time to make it fair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schiebinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="page" from="324" to="326" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="737" to="744" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Trimbot2020: an outdoor robot for automatic gardening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tylecek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hemming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Henten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISR. VDE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">InLoc: Indoor Visual Localization with Dense Matching and View Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ResNet50-avg-CL ResNet50-avg-GCL (ours) ResNet152-avg-CL ResNet152-avg-GCL (ours) ResNet50-GeM-CL ResNet50-GeM-GCL (ours) ResNet152-GeM-CL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>ResNet152-GeM-GCL (ours</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet152-Gem-Cl</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
