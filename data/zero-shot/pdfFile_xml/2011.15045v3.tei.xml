<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Deep Video Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashpal</forename><surname>Dev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyas</forename><surname>Mohan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Vincent</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School for Engineering of Matter</orgName>
								<orgName type="institution">Transport, and Energy, ASU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Manzorro</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School for Engineering of Matter</orgName>
								<orgName type="institution">Transport, and Energy, ASU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Crozier</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School for Engineering of Matter</orgName>
								<orgName type="institution">Transport, and Energy, ASU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Robert Bosch Center for Data Science and AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Neural Science</orgName>
								<orgName type="institution" key="instit1">NYU and Flatiron Institute</orgName>
								<orgName type="institution" key="instit2">Simons Foundation</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">NYU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">NYU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Deep Video Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNNs) for video denoising are typically trained with supervision, assuming the availability of clean videos. However, in many applications, such as microscopy, noiseless videos are not available. To address this, we propose an Unsupervised Deep Video Denoiser (UDVD 1 ), a CNN architecture designed to be trained exclusively with noisy data. The performance of UDVD is comparable to the supervised state-of-the-art, even when trained only on a single short noisy video. We demonstrate the promise of our approach in real-world imaging applications by denoising raw video, fluorescencemicroscopy and electron-microscopy data. In contrast to many current approaches to video denoising, UDVD does not require explicit motion compensation. This is advantageous because motion compensation is computationally expensive, and can be unreliable when the input data are noisy. A gradient-based analysis reveals that UDVD automatically adapts to local motion in the input noisy videos. Thus, the network learns to perform implicit motion compensation, even though it is only trained for denoising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video denoising is a fundamental problem in image processing, as well as an important preprocessing step for computer vision tasks. Convolutional neural networks (CNNs) <ref type="bibr" target="#b23">[24]</ref> provide current state-of-the-art solutions for this problem <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref>. These networks are typically trained using a database of clean videos, which are corrupted with simulated noise. However, in applications such as microscopy, noiseless ground truth videos are often not available. To address this issue, we propose a method to train a video denoising CNN without access to super-* equal contribution. <ref type="bibr" target="#b0">1</ref> See https://sreyas-mohan.github.io/udvd/ for code and more results.</p><p>vised data, which we call Unsupervised Deep Video Denoising (UDVD). UDVD is inspired by the "blind-spot" technique, recently introduced for unsupervised still image denoising <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b21">22]</ref>, in which a CNN is trained to estimate each noisy pixel from the surrounding spatial neighborhood without including the pixel itself. Here, we propose a blind-spot architecture that processes the surrounding spatio-temporal neighborhood to denoise videos.</p><p>We show that UDVD is competitive with the current supervised state-of-the-art on standard benchmarks, despite not having access to ground-truth clean videos during training (see <ref type="figure" target="#fig_0">Figure 1</ref>). Moreover, when combined with aggressive data augmentation and early stopping, it can produce high-quality denoising even when trained exclusively on a single brief noisy video sequence (as few as 30 frames), outperforming unsupervised video denoising techniques (e.g. F2F <ref type="bibr" target="#b10">[11]</ref> and MF2F <ref type="bibr" target="#b8">[9]</ref>) which are pre-trained with supervision. Finally, methods based on pre-training are not suitable for imaging applications where clean data is unavailable. In contrast, we demonstrate that UDVD can effectively denoise three different real-world datasets: raw videos from surveillance cameras, fluorescence-microscopy videos of cells, and electron-microscopy videos of catalytic nanoparticles.</p><p>The state-of-the-art performance of UDVD is unexpected. Nearly all existing approaches to video denoising <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b28">28]</ref>, including those based on deep CNNs <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">47]</ref>, use estimates of optical flow to adaptively compensate for the motion of objects in the video. Conventional wisdom suggest that ignoring such motion should lead to denoising results in which moving content is blurred. Contrary to this intuition, UDVD and some recent state-ofthe-art supervised methods for video denoising <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref> yield excellent empirical performance without explicit estimation of optical flow. How can is this achieved? We use a gradient-based analysis to show that both UDVD and supervised CNNs perform spatio-temporal adaptive filter-  <ref type="bibr" target="#b40">[39]</ref>, a supervised method trained on the DAVIS dataset. (d) MF2F <ref type="bibr" target="#b8">[9]</ref>, an unsupervised method which finetunes a pre-trained FastDVDnet on the noisy video (e) Our proposed unsupervised method (UDVD), which uses five frames to denoise each frame, trained on the DAVIS dataset. (f) UDVD trained only on the noisy video itself. Performance is quantified using PSNR / SSIM <ref type="bibr" target="#b43">[42]</ref>, respectively. The corresponding videos, as well as additional examples, are included in Section C of the supplementary material.</p><p>ing, which is aligned with underlying motion. Thus, these CNNs are automatically performing implicit motion compensation. To quantify this, we demonstrate that it is possible to estimate optical flow accurately from the network gradients, even though the network architectures are not designed to account for optical flow, and the models receive no optical-flow information during training.</p><p>Our Contributions:</p><p>? A novel blind-spot architecture/objective for unsupervised video denoising, which achieves performance competitive with state-of-the-art supervised methods. ? A training paradigm using aggressive data augmentation (time and space reversal) and early stopping to achieve state-of-the-art performance from training on a single brief noisy video. ? A demonstration of our method's effectiveness in denoising real-world electron and fluorescence microscopy data, as well as raw videos. Unlike most existing methods for unsupervised video denoising, our proposed method does not require pre-training, which is key in real-world imaging applications. ? An analysis of the denoising mechanism learned by UDVD, demonstrating that it performs implicit motion compensation even though it is only trained for denoising. We apply the analysis to supervised networks, showing that the same conclusion holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Traditional and CNN-based video denoising. Traditional techniques for single image denoising include nonlinear filtering <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b29">29]</ref>, sparse prior methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b6">7]</ref>, and nonlocal means <ref type="bibr" target="#b22">[23]</ref>; many of which have been extended to videos <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr">3]</ref>. In order to exploit the spatio-temporal structure of the video, these methods typically employ motion compensation based on estimates of optical flow. In the last five years, data-driven methods based on deep CNNs <ref type="bibr" target="#b23">[24]</ref> have outperformed all other techniques in image <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5]</ref> and video denoising <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b49">48]</ref>. The CNNs are trained to minimize the mean squared error between the network output and ground truth using large databases of natural images/videos. Many deeplearning techniques also perform explicit motion compensation. DVDnet <ref type="bibr" target="#b39">[38]</ref> applies an image-denoising CNN to each input frame, estimates the optical flow from the denoised frames using DeepFlow <ref type="bibr" target="#b44">[43]</ref> (a CNN pre-trained for this purpose), warps the frames using the flow estimate to align their content, and finally processes the registered frames with a CNN. Ref. <ref type="bibr" target="#b47">[46]</ref> applies a similar pipeline, but jointly trains an optical-flow module with the denoising CNN. Video denoising without motion compensation. Three recent methods perform video denoising without explicit motion estimation. VNLnet <ref type="bibr" target="#b7">[8]</ref> uses a non-local search algorithm to find self-similar patches in the input video, and then uses a CNN to process the patches. ViDeNN <ref type="bibr" target="#b5">[6]</ref> consists of a first stage that denoises each frame using a CNN, and a second stage that exploits temporal structure by using the frames, (t ? 1), t and t + 1 to produce the denoised tth frame. FastDVDnet <ref type="bibr" target="#b40">[39]</ref> uses UNet <ref type="bibr" target="#b36">[36]</ref> blocks, trained end to end, to denoise each frame using five contiguous frames. These methods achieve state-of-the-art performance without any explicit motion compensation, similar to our proposed UDVD. In this work we show that such CNNs actually performs implicit motion estimation, which can be revealed through a gradient-based analysis. Unsupervised denoising. Noise2Noise (N2N) is an unsupervised image-denoising technique where a CNN is trained on pairs of noisy images corresponding to the same clean image <ref type="bibr" target="#b24">[25]</ref>. Frame2Frame (F2F) <ref type="bibr" target="#b10">[11]</ref> exploits this approach to fine-tune a pretrained image-denoising CNN with noisy data. The idea is to register contiguous frames using the optical flow (obtained from TV-L1 <ref type="bibr" target="#b50">[49]</ref>), and treat them as noisy realizations of the same clean image. This scheme is extended to have a trainable flow estimation module in <ref type="bibr" target="#b48">[47]</ref>, additional optical-flow consistency in <ref type="bibr" target="#b13">[14]</ref> and to use multiple noisy frames as input in Multi-Frame2Frame (MF2F) <ref type="bibr" target="#b8">[9]</ref>.</p><p>Using the N2N framework to perform unsupervised video denoising requires warping adjoining frames, which in turn requires explicit motion compensation, and accurate occlusion estimation. In addition, the assumption that contiguous frames can be registered may not hold, particularly if the motion speeds in the video are large relative to the frame rate or local intensity changes are not due to translation. In order to bypass these issues, we develop a blindspot network that trains denoising CNNs by fitting the noisy data directly. The CNN is trained to estimate each noisy pixel value using the surrounding spatio-temporal neighborhood, but without taking into account the noisy pixel itself in order to avoid the trivial identity solution. This "blind spot" can be enforced through architecture design <ref type="bibr" target="#b21">[22]</ref>, or by masking <ref type="bibr">[2,</ref><ref type="bibr" target="#b19">20]</ref>. For still images, several variations of this approach have been shown to provide effective denoising for natural images and noisy images from fluorescence microscopy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b16">17</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unsupervised Deep Video Denoising</head><p>In this section we describe our proposed architecture (see <ref type="figure">Figure 2</ref> for a detailed diagram). Multi-frame blind-spot architecture. Our CNN maps five contiguous noisy frames to a denoised estimate of the middle frame. Building on the "blind spot" idea proposed in <ref type="bibr" target="#b21">[22]</ref> for single-image denoising, we design the architecture so that each output pixel is estimated from a spatiotemporal neighbourhood that does not include the pixel itself. We rotate the input frames by multiples of 90 ? and process them through four separate branches containing asymmetric convolutional filters that are vertically causal. As a result, the branches produce outputs that only depend on the pixels above (0 ? rotation), to the left (90 ? ), below (180 ? ) or to the right (270 ? ) of the output pixel. These partial outputs are then derotated and combined using a three-layered cascade of 1 ? 1 convolutions and nonlinearities to produce the final output. The resulting field of view does not include the pixel being denoised, as depicted at the bottom of <ref type="figure">Figure 2</ref>.</p><p>UDVD processes the video in two stages as shown in <ref type="figure">Figure 2</ref>, similar to previously proposed networks for supervised video denoising <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">39]</ref>. A first stage, consisting of three UNets <ref type="bibr" target="#b36">[36]</ref> (D1 in the diagram) with shared parameters, maps each group of three contiguous frames (i.e. (t ? 2, t ? 1, t), (t ? 1, t, t + 1) and (t, t + 1, t + 2)) to a separate feature map. These features are then mapped to a single output using another UNet (D2). See Suppl. A for a detailed description of the architecture. Bias-free architecture. Inspired by <ref type="bibr" target="#b31">[31]</ref>, we remove all additive terms from the convolutional layers in UDVD. This provides automatic generalization to varying noise levels not encountered during training, and facilitates our proposed analysis to interpret the denoising mechanisms learned by the network (see Section 5 and 6).</p><p>Using the missing pixel. The denoised value generated by the proposed architecture at each pixel is computed without using the noisy observation at that location. This avoids overfitting -i.e. learning the trivial identity map that minimizes the mean-squared error cost function -but ignores important information provided by the noisy pixel. In the special case of Gaussian additive noise, we can use this information via a precision-weighted average between the network output and the noisy pixel value. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, the weights in the average are derived by assuming a Gaussian distribution for the error in the blindspot estimates of the color pixel values. Specifically, we model the distribution of the three color channels of a pixel x ? R 3 given the noisy neighbourhood ? y as p(x|? y ) = N (? x , ? x ), where ? x ? R 3 and ? x ? R 3 represent the mean vector and covariance matrix. Let y = x + ?, ? ? N (0, ? 2 I 3 ) be the observed noisy pixel. We integrate the information in the noisy pixel with the UDVD output by computing the mean of the posterior p(x|y, ? y ), given by</p><formula xml:id="formula_0">E[x|y] = (? ?1 x + ? ?2 I) ?1 (? ?1 x ? x + ? ?2 y). (1)</formula><p>See Suppl. A for more details. The CNN architecture is trained to estimate the mean and covariance of this distribution at each pixel by maximizing the log likelihood of the noisy data:</p><formula xml:id="formula_1">L(? x , ? x ) = 1 2 [(y ? ? x ) T (? x + ? 2 I) ?1 (y ? ? x )] + 1 2 log |? x + ? 2 I|.<label>(2)</label></formula><p>When the noise process is unknown, we simply minimize <ref type="figure">Figure 2</ref>. Unsupervised Deep Video Denoising (UDVD) Network Architecture. The network takes 5 consecutive noisy frames as input and produces a denoised central frame as output. We rotate the input frames by multiples of 90 ? and process them in four separate branches with shared parameters, each containing asymmetric convolutional filters that are vertically causal. As a result, the branches produce outputs that only depend on the pixels above (0 ? rotation, blue region), to the left (90 ? , pink region), below (180 ? , yellow region) or to the right (270 ? , green region) of the output pixel. Each branch consists of a cascade of 2 Unet-style blocks (D1 and D2) to combine information over frames. These outputs are then derotated and linearly combined (using a 1 ? 1 convolutions) followed by a ReLU nonlinearity to produce the final output. The resulting "field of view" is depicted at the bottom with each color representing the contribution of the corresponding branch.</p><p>the MSE between the denoised output and noisy video, and ignore the center pixel (see Suppl. A for more details). Data augmentation and early stopping. In supervised denoising with simulated noise, training can rely on the generation of a virtually unlimited set of fresh noise realizations, which prevents overfitting. In the unsupervised setting, this is not possible, which makes it more challenging to train models that can denoise short video sequences. To address this, we (a) leverage data augmentation strategies: spatial flipping and time reversal, and (b) perform early stopping by monitoring the mean squared error between the network output and noisy frames on a held-out set of frames. These strategies make it possible to train UDVD with short video sequences (as few as 30 frames), while achieving denoising performance that is on par with or superior to both unsupervised and supervised networks trained on much larger datasets (see <ref type="figure" target="#fig_0">Figure 1</ref>, <ref type="table">Table 2</ref> and Suppl. D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>We demonstrate the broad applicability of our approach by validating it on domains with different signal and noise structure: natural videos, raw videos, fluorescence microscopy, and electron microscopy. Natural videos. We perform controlled experiments on natural videos by adding iid Gaussian noise to the DAVIS dataset <ref type="bibr" target="#b33">[33]</ref>. The training/validation/test split is 60/30/30 videos, respectively. We use three additional datasets for testing -Set8 <ref type="bibr" target="#b40">[39]</ref> composed of 4 videos from the Derfs Test Media collection and 4 videos captured with a GoPro camera, Derfs <ref type="bibr" target="#b8">[9]</ref> with 7 videos, and the first 10 videos from Vid3oC <ref type="bibr" target="#b17">[18]</ref> dataset (See Suppl. D for details). Raw videos. We evaluate UDVD on a dataset of raw videos i.e with frame color channels interleaved according to the sensor mosaic containing real noise introduced in <ref type="bibr" target="#b49">[48]</ref>. The dataset contains 11 unique videos, each containing 7 frames, captured at five different ISO levels using a surveillance camera. Each video has 10 different noise realizations per frame, which are averaged to obtain an estimated clean version of the video. Fluorescence microscopy. We apply our approach to fluorescence-microscopy recordings of live cells in <ref type="bibr" target="#b42">[41]</ref>. We use two videos: Fluo-C2DL-MSC (CTC-MSC) depicting mesenchymal stem cells, and Fluo-N2DH-GOWT1  <ref type="table">Table 2</ref>. Results for UDVD trained on individual noisy videos. The top row shows PSNR/VMAF <ref type="bibr" target="#b25">[26]</ref> values (averaged over the entire dataset) for UDVD trained on each individual video sequence with early stopping (labelled UDVD-S) using the last 5 frames of a video as a held-out set. We augmented the dataset with spatial flipping and time reversal (see Suppl. D for an ablation study). With the augmentations and early stopping, UDVD-S is comparable to (and often outperforms) UDVD or FastDVDnet trained on the full DAVIS dataset (indicated by * ) and MF2F, which fine-tunes a pre-trained CNN on each individual video. See Suppl. D for results on individual video sequences.</p><p>(CTC-N2DH) depicting GOWT1 cells. This dataset illustrates the challenges of applying supervised approaches to real data: there is no ground-truth clean data. Electron microscopy. We also apply our methodology to a transmission electron microscopy dataset from <ref type="bibr" target="#b32">[32]</ref>. The data consist of a 40-frame video depicting a platinum nanoparticle supported on a cerium oxide base. The average image intensity is 0.45 electrons/pixel, which results in an extremely low signal-to-noise ratio. As with the fluorescence-microscopy data, no ground-truth clean images are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>Comparison with other approaches on natural videos. We train UDVD on the DAVIS training set (see Suppl. A for the training procedure). Following <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">2]</ref>, we add iid Gaussian noise with standard deviation ? = 30 on the clean videos during training. UDVD is evaluated on the DAVIS test set and on Set8 by comparing to the clean ground-truth videos via PSNR. We compare UDVD with several popular methods: Bayesian processing of spatio-temporal patches (VNLB <ref type="bibr" target="#b22">[23]</ref>), an extension of the popular image-denoising algorithm BM3D (VBM4D <ref type="bibr" target="#b28">[28]</ref>) and supervised CNNs (VNLnet <ref type="bibr" target="#b7">[8]</ref>, DVD-net <ref type="bibr" target="#b39">[38]</ref>, FastDVDnet <ref type="bibr" target="#b40">[39]</ref>). As shown in <ref type="table" target="#tab_0">Table 1</ref>, UDVD achieves comparable performance to the supervised stateof-the-art on the DAVIS test set and slightly outperforms these methods on an independent test set (Set8) at multiple noise levels. It also outperforms traditional unsupervised techniques such as VNLB and VBM4D (see <ref type="figure" target="#fig_0">Figure 1</ref> and Suppl. C for visual examples).</p><p>Unsupervised denoising from limited data. In order to validate our approach on a more challenging setting that is closer to the practical applications of unsupervised denoising, we trained and tested UDVD on individual videos from our test sets. As shown in <ref type="table" target="#tab_3">Table 3</ref> and 4 in Suppl. D, when combined with data augmentation and early stopping (using the last 5 frames of each video as a held-out validation set), this version of UDVD (called UDVD-S) achieves comparable results, or often outperforms supervised FastDVDnet and unsupervised UDVD trained on a large dataset (DAVIS) (see <ref type="table">Table 2</ref> for results on 4 different datasets).</p><p>To the best of our knowledge, all the existing unsupervised video denoising techniques are based on the F2F <ref type="bibr" target="#b10">[11]</ref> framework, where a backbone CNN pre-trained with supervision is fine-tuned on the video to be denoised. We compared UDVD-S against the most recent such method -MF2F <ref type="bibr" target="#b8">[9]</ref> which fine-tunes a FastDVDnet <ref type="bibr" target="#b40">[39]</ref>   <ref type="figure">Figure 3</ref>. Denoising real-world data. Results from applying UDVD to the raw video, fluorescence-microscopy and electron-microscopy datasets described in Section 4. Qualitatively, UDVD succeeds in removing noise while preserving the underlying signal structure, even for the highly noisy electron-microscopy data. Raw videos are converted to RGB for visualization. See Suppl. D and F for denoised videos.</p><p>with supervision on natural videos using an objective involving optical flow computed on consecutive noisy frames (see Section 2). Without any pre-training, UDVD-S outperforms MF2F in almost all videos in <ref type="table" target="#tab_3">Table 3</ref> and 4 in Suppl. D, and datasets in <ref type="table">Table 2</ref> (See <ref type="table" target="#tab_5">Table 5</ref> in Suppl. D.3 for measure of confidence). Note that (a) we trained MF2F using all the 5 training schemes provided in the paper and reported the best results in <ref type="table">Table 2</ref>, and (b) the metric we used to measure performance in <ref type="table">Table 2</ref> is the average PSNR of all denoised frames, unlike in Ref. <ref type="bibr" target="#b8">[9]</ref> where the first 10 frames of each video were excluded (see Suppl. D.3 for more details and results). Use of temporal information. UDVD estimates each frame from k surrounding contiguous frames. To validate the effect of using more temporal information, we tested k ? {1, 3, 5}. As shown in <ref type="table" target="#tab_0">Table 1</ref>, performance improves substantially and monotonically with k (see Suppl. B for more noise levels ). This is in agreement with the literature on supervised learning <ref type="bibr" target="#b40">[39]</ref>. The performance gains arising from a longer temporal context are more substantial at higher noise levels (see <ref type="table" target="#tab_0">Table 1</ref>). This is consistent with our analysis in Section 6 which shows that, at low noise levels, UDVD(k = 5) tends to ignore the distant frames, but relies on them more at higher noise levels (see <ref type="figure" target="#fig_2">Figure 4</ref> &amp; Suppl. G).</p><p>Generalization across noise levels. UDVD generalizes strongly across noise levels not encountered during training.</p><p>The results in <ref type="table" target="#tab_0">Table 1</ref>   and then fine-tuned with supervision on 6 training videos from the raw video dataset. UDVD outperforms RViDeNet at all noise levels (see <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure">Fig 3)</ref>. Note that UDVD was directly trained on the mosaiced raw videos. Existing unsupervised video denoising methods, like MF2F, cannot be applied directly on this dataset as their pre-trained backbone expects an input in the RGB domain (more details in Suppl. E).</p><p>Real-world microscopy data. We train UDVD on the fluorescence-microscopy data described in Section 4 following the same procedure as for the natural videos, including data augmentation. For the electron-microscopy data, we trained on the first 35 frames of the video, and used the remaining 5 as a validation set to perform early stopping based on mean-squared error. UDVD is able to effectively denoise the fluorescence-microscopy and the electron-microscopy datasets described in Section 4. This can be appreciated qualitatively in <ref type="figure">Figure 3</ref> and Suppl. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Automatic Motion Compensation</head><p>Most previous approaches for video denoising rely on explicit motion compensation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b28">28]</ref>. This requires estimating the optical flow, which is the local translational motion of features in the image arising from the motion of objects and surfaces in a visual scene relative to the cam-  era. Several CNN-based denoisers build motion estimation into the architecture <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b47">46]</ref>. In particular, motion compensation is critical to the F2F and MF2F frameworks for unsupervised denoising, which use motion compensation to register contiguous images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9]</ref>. In contrast, recent supervised video denoising networks like FastDVDnet <ref type="bibr" target="#b40">[39]</ref> and ViDeNN <ref type="bibr" target="#b5">[6]</ref>, as well as our unsupervised UDVD, do not perform any explicit motion compensation. Despite this, they achieve state-of-the-art results. The empirical performance of these approaches suggests that the networks must somehow be exploiting temporal information successfully.</p><formula xml:id="formula_2">d t (i) = 2 k=?2 y t?k</formula><formula xml:id="formula_3">y t d t a(t ? 1, i) a(t, i) a(t + 1, i)</formula><p>Here, we study this phenomenon through an analysis of the denoising mapping, which reveals that these networks perform an implicit form of motion compensation. Gradient-based analysis. We use the approach of <ref type="bibr" target="#b31">[31]</ref> to analyze CNNs trained for image denoising. Let y ? R nT be a flattened video sequence containing T noisy frames with n pixels each, processed by a CNN. We define the denoising function f i : R nT ? R as the map between the noisy video and the denoised value d i := f i (y) of the CNN output at the ith pixel. A first-order Taylor decomposition of the denoising function may be written as:</p><formula xml:id="formula_4">d i := f i (y) = ?f i (y), y + b,<label>(3)</label></formula><p>where ?f i (y) ? R nT denotes the gradient of f i at y. The constant b := f i (y) ? ?f i (y), y is the net bias of the network, a combined function of all additive constants in the convolutional and batch-normalization layers of the CNN. Our proposed architecture is bias-free (i.e., all additive constants are removed from the architecture, as proposed in <ref type="bibr" target="#b31">[31]</ref>), and thus b = 0. As a result, the denoised value at the ith pixel may be written as:</p><formula xml:id="formula_5">d(i) = ?f i (y), y = T k=1 a(k, i), y k ,<label>(4)</label></formula><p>where y k denotes each of the T flattened frames that compose the noisy video, and the weights a(k, i) correspond to the gradient of f i with respect to y. Each vector a(k, i) can be interpreted as an equivalent filter that produces an estimate of the denoised video at pixel i via a weighted average of the noisy observations over space and time. Interpreting equivalent filters. Visualizing these equivalent filters reveals that UDVD learns to denoise by performing averaging over an adaptive spatiotemporal neighborhood of each pixel. As illustrated in <ref type="figure" target="#fig_2">Figure 4</ref> (and Suppl. G), when the noise level increases, the averaging is carried out over larger regions. This intuitive behavior is also seen in classical linear Wiener filters <ref type="bibr" target="#b45">[44]</ref>, where the filters are larger for higher levels of noise. The crucial difference is that in the case of CNNs, the equivalent filters are adapted to the local video content: they respect object boundaries in space and time, taking into account their motion. This is apparent in <ref type="figure" target="#fig_2">Figure 4</ref>: equivalent filters in adjoining frames are automatically shifted spatially to compensate for the movement of the skier (additional examples in Suppl. G). We find that this implicit motion compensation is not unique to UDVD: CNNs trained in a supervised fashion have the same property (see also Suppl. G).</p><p>Optical-flow estimation. In order to validate our observation that CNNs exclusively trained for denoising implicitly detect and exploit video motion, we use the equivalent filters of the networks to estimate the optical flow. To estimate the optical flow from the t th frame to the (t + 1) th frame at the ith pixel, we compute the difference between the position of the centroid of the equivalent filter corresponding to the pixel at times t, a(t, i), and time t + 1, a(t + 1, i). To increase the stability of the estimated flow, we compute the filter centroid through a robust weighted average that only includes entries with relatively large values (within 20% of maximum value in the filter).</p><p>The optical-flow estimates obtained from the gradients of the trained UDVD network are surprisingly precise, even at very high noise levels. <ref type="figure">Figure 5</ref>, and additional figures in Suppl. G, show that the results are similar to those obtained by applying an algorithm for optical-flow estimation (Deep-Flow <ref type="bibr" target="#b44">[43]</ref>) on the corresponding clean video. This demonstrates that the CNNs are able to implicitly estimate motion from data, despite the fact that they were not trained on that problem, and even in the presence of substantial noise corruption, a setting that is quite challenging for optical-flow estimation techniques. We also observe that the optical-flow estimates obtained from UDVD gradients tend to be less accurate for pixels near strongly oriented features where local motion is only partially constrained (known as the aperture problem) or in homogeneous regions, where the local motion is unconstrained (the blank wall problem).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work we propose a method for unsupervised deep video denoising that achieves comparable performance to state-of-the-art supervised approaches. Combined with data-augmentation techniques and early stopping, the method achieves effective denoising even when trained exclusively on short individual noisy sequences, which enables its application to real-world noisy data. In addition, we perform a gradient-based analysis of denoising CNNs, which reveals that they learn to perform implicit adaptive motion compensation. This suggests several interesting research directions. For example, denoising may be a useful pretraining task for optical-flow estimation or other computer-vision tasks requiring motion estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details of Unsupervised Deep Video Denoising</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Restricting field of view</head><p>In UDVD, we rotate the input frames by multiples of 90 ? and process them through four separate branches (with shared parameters) containing asymmetric convolutional filters that are vertically causal. As a result, the branches produce outputs that only depend on the pixels above (0 ? rotation), to the left (90 ? ), below (180 ? ) or to the right (270 ? ) of the output pixel. We use a UNet <ref type="bibr" target="#b36">[36]</ref> style architecture for each branch of UDVD. The field of view of the UNet is constrained by restricting the field of view of the convolutional, downsampling and upsampling layers that are used to build the UNet.</p><p>Convolutional Layers: We restrict the receptive field of each convolutional layer to extend only upwards following the strategy proposed in <ref type="bibr" target="#b21">[22]</ref>. Let the filter size be h ? w. We zero-pad the top region of the input tensor with k = h/2 zero rows before convolution and remove the bottom k rows after convolution. This is equivalent to convolving with a filter, where all weights below the center row are zero, so that the field of view only extends upwards.</p><p>Downsampling and Upsampling Layers: Following <ref type="bibr" target="#b21">[22]</ref> we restrict the receptive field of the downsampling layer by creating an offset of one pixel (zero-pad with a row of zeros on the top and remove a row of pixels from below) before performing max-pooling using a 2 ? 2 kernel. This operation restricts the field of view of the downsampling and upsampling operation pair. Note that we do not use BatchNorm <ref type="bibr" target="#b15">[16]</ref> layers in UDVD as computing the spatial mean and variance would modify the field of view to include the center pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Adding the Noisy Pixel Back</head><p>The denoised generated by the proposed architecture at each pixel is computed without using the noisy observation at that location. This avoids overfitting -i.e. learning the trivial identity map that minimizes the mean-squared error cost function -but ignores important information provided by the noisy pixel. In the case of Gaussian additive noise, we can use this information via a precision-weighted average between the network output and the noisy pixel value. Following <ref type="bibr" target="#b21">[22]</ref>, the weights in the average are derived by assuming a Gaussian distribution for the error in the blind-spot estimates of the (color) pixel values. The CNN architecture is trained to estimate the mean and covariance of this distribution at each pixel by maximizing the log likelihood of the noisy data. We explain this in detail in the following paragraphs.</p><p>UDVD estimates the value of a pixel based on the noisy pixels in its neighbourhood. We model the distribution of the three color channels of a pixel x ? R 3 given the noisy neighbourhood ? y as p(x|? y ) = N (? x , ? x ), where ? x ? R 3 and ? x ? R 3 represent the mean vector and covariance matrix. Let y = x + ?, ? ? N (0, ? 2 I 3 ) be the observed noisy pixel. We integrate the information in the noisy pixel with the UDVD output by computing the mean of the posterior p(x|y, ? y ), given by p(x|y, ? y ) ? p(y|x) p(x|? y )</p><p>where p(x|? y ) is the prior and p(y|x) is the noise model. Since both the prior and the noise model are Gaussian, we can write the optimal posterior estimate as,</p><formula xml:id="formula_7">E[x|y] = (? ?1 x + ? ?2 I) ?1 (? ?1 x ? x + ? ?2 y).<label>(6)</label></formula><p>Note that the posterior mean has a very intuitive interpretation. When the signal variance is high compared to noise variance (i.e. the uncertainty in our estimation is high) the posterior mean favours noisy pixel value. We estimate ? x and ? x as a function of the neighbourhood ? y using the network architecture discussed earlier. If x is a grayscale image, then the output of the network consists of two channels -one for ? x and one for ? x . When the input image has k channels, the output consists of k channels for ? x and k(k ? 1)/2 for the upper-triangular entries of ? x One can estimate ? x and ? x directly from the noisy data by maximizing the likelihood. Using our distributional assumptions, the noisy pixels y follows a Gaussian distribution, y ? N (? y , ? y ), where ? y = ? x and ? y = ? x + ? 2 I. Therefore, the loss function or the negative log likelihood is:</p><formula xml:id="formula_8">L(? x , ? x ) = 1 2 [(y ? ? x ) T (? x + ? 2 I) ?1 (y ? ? x )] + 1 2 log |? x + ? 2 I|.<label>(7)</label></formula><p>If ? is unknown during training and has to be estimated, we use a separate neural network with the same architecture to do so. In such cases, we add a small regularization term equal to ?0.1? for numerical stability, following <ref type="bibr" target="#b21">[22]</ref>.</p><p>For the experiments with real data, the noise distribution is unknown, so we simply ignore the central pixel.  <ref type="table">Table 4</ref>. Network architecture used for UDVD. The convolution and pooling layers are the blind-spot variants described in Section A.1. k1 and k2 represent the number of input and output channels of the base network respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Architecture and Training</head><p>Architecture: The overall architecture is explained in Section 3 of the paper. The network architecture for the D1 and D2 blocks is described in <ref type="table">Table 4</ref>. D1 has k 1 = 9 input channels and k 2 = 32 output channels. D2 has k 1 = 96 input channels and k 2 = 96 output channels. The architecture of D1 and D2 are analogous to the blocks in FastDVDnet <ref type="bibr" target="#b40">[39]</ref> to facilitate fair comparison with the supervised models. As described in <ref type="figure">Fig. 2</ref> of the paper, D2 is followed by a derotation and the output is passed to a series of three cascaded 1 ? 1 convolutions and non-linearity for reconstruction with 4 and 96 intermediate output channels, as in <ref type="bibr" target="#b21">[22]</ref>. The final convolutional layer is linear and has 9 output channels, 3 representing the RGB value of the denoised image and 6 representing its covariance matrix. We use the same architecture for fluorescence microscopy and electron microscopy with the number of input channels to UDVD modified to 5 and number of output channels modified to 1.</p><p>Training Details: Following the convention in image and video denoising, we train UDVD on 128 ? 128 patches extracted from our dataset <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b39">38]</ref> (this is also consistent with the training methodology of the supervised baselines). For the natural video and fluorescence microscopy datasets, no data augmentation was applied. For electron microscopy dataset, we applied spatial flipping, time reversal and time subsampling (i.e. skipping frames).</p><p>Optimization Details: All networks were trained using Adam <ref type="bibr" target="#b18">[19]</ref> optimizer with a starting learning of 10 ?4 . The learning rate was decreased by a factor of 2 at checkpoints <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30]</ref> during a total training of 40 epochs. We did not experiment with other learning rate schedules such as cosine scheduling, which is a popular choice in unsupervised image denoising <ref type="bibr" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study on Number of Input Frames</head><p>We perform an ablation study on the number of frames k UDVD uses as input, k ? {1, 3, 5}. UDVD with k = 1 is equivalent to the blind-spot network proposed for image denoising in <ref type="bibr" target="#b21">[22]</ref>. In this section we describe the architectural and training details for UDVD with k ? {1, 3, 5} and present some additional results.</p><p>Architectural Details: UDVD with k = 1 contains only one UNet style network in each branch with architecture described in <ref type="table">Table 4</ref> and Section A.3. There are 3 input channels and 9 output channels (3 for the RGB channels in each denoised pixel and 6 for the corresponding covariance matrix). UDVD with k = 3 has a similar architecture as for k = 1 but has 9 input channels instead (3 channels for each frame). The architecture for k = 5 is described in Section A.3.</p><p>Training Details: UDVD with k ? {1, 3, 5} was trained on the DAVIS dataset with ? = 30. The training details were as described in Section A.3. <ref type="table" target="#tab_0">Table 1</ref> of the paper and <ref type="table" target="#tab_5">Table 5</ref> performance improves substantially and monotonically with k (the number of surrounding frames used to denoise each frame) across a wide range of noise levels. This difference in performance can also be observed visually. <ref type="figure" target="#fig_4">Fig 6 shows</ref> an example where the texture details of the brick wall and the fence are not well recovered when using only a single noisy frame. The texture is estimated better when using 5 noisy frames to predict the denoised output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Denoising Results on Natural Video Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Comparison to Supervised Video Denoising</head><p>In this section we provide additional comparisons between UDVD and supervised CNN-based methods. 1. <ref type="table" target="#tab_5">Table 5</ref> shows the performance of UDVD trained at ? = 30, and FastDVDnet trained for ? ? <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">55]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Comparison to Burst Denoising</head><p>When several photographs are captured in quick succession to each other, the resulting set of images are often blurry or noisy (particularly when the object is in motion). Burst denoising aims to recover estimate the original scene from the set of burst photographs. Recent methods have solved burst denoising by applying deep neural network to map a stack of burst images to a single clean frame <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b46">45]</ref>. A popular burst denoising method, KPN <ref type="bibr" target="#b30">[30]</ref> achieved a PSNR of 27.83 on the DAVIS dataset at ? = 30 3 , while UDVD achieves a PSNR of 33.92. UDVD is expected to outperform burst denoising methods as these methods (1) are trained for jittered motions, and cannot exploit systematic motion in natural videos like video denoising methods, and (2) often do not expect a motion change of more than 2 pixels from one frame to another <ref type="bibr" target="#b30">[30]</ref>, while the motion in natural videos is usually much larger (see Section 6 in main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. UDVD-S: Denoising Using Only a Single Video</head><p>UDVD, combined with aggressive data augmentation and early stopping, achieves state-of-the-art performance even when trained on only a single short video. In this section, we analyze the contribution of each of the data augmentation and early stopping scheme to the performance of UDVD-S through an ablation study. We also provide more details about our comparison to MF2F <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Details of test sets.</head><p>We evaluate UDVD-S and baselines on the following four datasets: 1. DAVIS <ref type="bibr" target="#b33">[33]</ref>: We take all the 30 sequences from the test set of the DAVIS Challenge 2017.  <ref type="table">Table 6</ref>. Results for UDVD and MF2F trained on individual noisy videos for ? = 30. The top block show PSNR values for UDVD trained on each individual video sequence with and without data augmentation (spatial flipping(F) and time-reversal(TR)) and early stopping (ES). Early stopping was performed using the last 5 frames of each video as the held-out set. The last block shows the result of running MF2F <ref type="bibr" target="#b8">[9]</ref> with all the 5 different fine-tuning scheme proposed in Ref. <ref type="bibr" target="#b8">[9]</ref>. With the augmentations and early stopping, UDVD-S, on average outperforms UDVD and FastDVDnet trained on the full DAVIS dataset (indicated by * ) and MF2F which fine-tunes a pre-trained FastDVDNet on each individual video. The best performing method for each video is highlighted in bold and the best performing method in each block is highlighted in italics. The tennis-vest video is from DAVIS and the rest of the 8 videos are from Set8.  <ref type="table">Table 7</ref>. Results for UDVD and MF2F trained on individual noisy videos for ? = 90. The top block show PSNR values for UDVD trained on each individual video sequence with and without data augmentation (spatial flipping(F) and time-reversal(TR)) and early stopping (ES). Early stopping was performed using the last 5 frames of each video as the held-out set. The last block shows the result of running MF2F <ref type="bibr" target="#b8">[9]</ref> with all the 5 different fine-tuning scheme proposed in Ref. <ref type="bibr" target="#b8">[9]</ref>. With the augmentations and early stopping, UDVD-S, on average outperforms, UDVD or FastDVDnet trained on the full DAVIS dataset (indicated by * ) and MF2F which fine-tunes on a pretrained FastDVDNet on each individual video. The best performing method for each video is highlighted in bold and the best performing method in each block is highlighted in italics. The tennis-vest video is from DAVIS and the rest of the 8 videos are from Set8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Ablation study</head><p>We train UDVD-S on 128 ? 128 patches extracted from the noisy video. (see Section A.3) for more details). For each patch, we apply each of the following data augmentations at random:</p><p>1. Spatial flipping: We flip all the 5 input patches vertically or horizontally. This operation only rearranges the pixel location and does not combine the pixel together in anyway, making sure that the noise statistics is still preserved after the augmentation. 2. Time reversal: We reverse the order of frames in the input to generate a new video. Like spatial flipping, this operation also preserves the noise statistics. In addition to data augmentation, we employ early stopping by choosing the model parameters which produced the best error on a a held-out set of frames. We pick the last 5 frames of each video as our held out set. <ref type="table">Tables 6 and 7</ref> show an ablation study over data augmentations and early stopping for 9 different videos at two different noise levels, ? = 30 and ? = 90. Across videos and noise levels, data augmentation and early stopping significantly increase the performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Comparison with MF2F</head><p>We compare the performance of UDVD-S to an unsupervised denoising method MF2F <ref type="bibr" target="#b8">[9]</ref>. MF2F fine-tunes a pretrained CNN on the noisy video using an objective function involving optical flow. The pre-trained CNN used in MF2F is FastDVDNet <ref type="bibr" target="#b40">[39]</ref>, which is trained with supervised on a large dataset of natural videos(DAVIS <ref type="bibr" target="#b33">[33]</ref>). The authors of MF2F provide five different schemes for fine-tuning: 8 sigmas, online no teacher, online with teacher, offline no teacher and offline with teacher. <ref type="table">Tables 6 and 7</ref> show the denoising results using each of these five training schemes. The best result (on 4 different dataets) is reported in <ref type="table">Table 2</ref> of the main paper. In addition to this, we also apply MF2F on real electron microscopy data (see <ref type="figure">Figure 7)</ref>, where it fails. We used the official implementation 4 for all the training schemes.  <ref type="figure">Figure 7</ref>. UDVD-S outperforms MF2F on electron microscopy data. UDVD-S is able to effectively denoise real-world data acquired from an electron-microscopy, but MF2F fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Measure of Confidence on Improvements</head><p>We compute the mean and standard deviation of the improvement of UDVD-S with respect to MF2F in <ref type="table">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS Set8</head><p>Derfs Vid3oC ? = 30 -0.33 ? 0.18 0.99 ? 0.21 1.09 ? 0.50 -0.54 ? 0.52 ? = 90 0.15 ? 0.09 0.65 ? 0.23 1.13 ? 0.39 0.26 ? 0.28 <ref type="table">Table 8</ref>. Measure of confidence on improvement of UDVD-S with respect to MF2F. We compute the mean and standard deviation of the difference between performance on UDVD-S and MF2F (in PSNR) on four different datasets and two different noise levels (? = 30, 90). UDVD-S outperforms MF2F with high certainty on two datasets at low noise level (? = 30) and all the four datasets at high noise level (? = 90). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Denoising Results on Real-world Datasets</head><p>Raw videos: The estimated ground truth, noisy raw data <ref type="bibr" target="#b49">[48]</ref>, and the denoised videos obtained with UDVD can be found on the official github repository (raw video.mp4). The videos were converted to RGB for illustration.</p><p>As discussed in the main paper, UDVD was directly trained on the mosaiced raw videos. Existing unsupervised video denoising methods, like MF2F <ref type="bibr" target="#b8">[9]</ref>, cannot be applied directly on this dataset as their pre-trained backbone expects an input in the RGB domain. In Ref. <ref type="bibr" target="#b8">[9]</ref>, the authors convert mosaiced videos into the RGB domain, apply MF2F <ref type="bibr" target="#b8">[9]</ref> and transform the denoised RGB videos back.</p><p>Fluorescence and electron microscopy data: The noisy fluorescence microscopy and electron microscopy data, and the denoised videos obtained with UDVD can be found on the official github repository (fluoro 1.mp4, fluoro 2.mp4 and electron.mp4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Generalization Across Noise and Frame Rate</head><p>Ideally, a denoiser should be able to denoise videos corrupted at a wide range of noise levels. This is usually achieved by training the CNN on examples corrupted with a range of noise strength <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b39">38]</ref>. The range of noise levels on which the network is trained is called the training range of the network.</p><p>Generalization outside the training range: The authors of <ref type="bibr" target="#b31">[31]</ref> showed that CNNs trained for image denoising generalize well on test images corrupted with noise in the training range, but fails catastrophically when corrupted with noise strength outside the training range. The authors provided evidence that the overfitting is due to additive terms in the convolutional layers (and BatchNorm <ref type="bibr" target="#b15">[16]</ref> ) and showed that a CNN with no additive terms, called a bias-free CNN generalizes well outside the training range. UDVD uses a bias-free architecture and generalizes well to noise levels outside its training range <ref type="figure" target="#fig_6">(Fig 8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization across frame rates:</head><p>To test generalization across frame rates, we simulated faster videos by skipping frames of videos in Set8. <ref type="figure" target="#fig_6">Fig 8 shows</ref> that UDVD generalizes robustly to faster videos and maintains a significant gain in performance over single-image denoising even when tested on videos where a large number of frames have been skipped (i.e. at a very low frame rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis of CNN-based Video Denoising</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Natural Videos</head><p>In Section 7 and <ref type="figure" target="#fig_2">Fig 4 of</ref> the paper we examined the equivalent filters and illustrated that UDVD learns to denoise by performing an average over a spatiotemporal neighbourhood of each pixel. Here we examine equivalent filters for more videos and a supervised CNN (FastDVDnet) and show that similar observations hold.</p><p>Adaptive filtering: <ref type="figure" target="#fig_0">Fig 10, 11, 12 and 13</ref> shows filters computed at a pixel for 4 different videos at 4 different noise levels. The filters adapt to the underlying signal content. They span larger areas as the noise level increases. These observations also holds for FastDVDnet, which is trained with supervision ( <ref type="figure" target="#fig_0">Fig 14)</ref> Contribution of neighbouring frames for denoising: UDVD tends to ignore temporally distant frames at lower noise levels as shown in <ref type="figure" target="#fig_0">Fig 10, 11, 12 and 13</ref>. This phenomenon is quantified in <ref type="figure" target="#fig_8">Fig 9 by</ref> plotting the contribution of each frame to the denoised pixel by averaging over 5000 pixels from 250 random patches of size 128 ? 128. At higher noise levels, UDVD seems to use distant frames more. This is consistent with the ablation study, which shows that for higher noise levels using more surrounding frames improves the denoising performance. Similar results hold for supervised CNN FastDVDnet, as shown in <ref type="figure" target="#fig_0">Fig 14.</ref> Local Averaging: The weighting functions or equivalent filters perform an approximate averaging operation. They are mostly non-negative (although they do have some negative entries as depicted in blue in <ref type="figure" target="#fig_0">Fig 10, 11, 12 and 13)</ref> and they approximately sum up to one (see <ref type="figure" target="#fig_8">Fig 9)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Real-world Data</head><p>Equivalent filters for the raw video, the fluorescence-microscopy and the electron-microscopy data are shown in <ref type="figure" target="#fig_0">Fig 15.</ref> The fluorescence -microscopy data have a low noise level. As expected from the results on natural videos (see Section C), the weighting functions are mostly confined to the middle frame (as quantified in <ref type="figure" target="#fig_8">Fig 9)</ref>. In the electron-microscopy dataset the weighting functions shows that the network relies on adjacent frames to estimate the denoised (as quantified in <ref type="figure" target="#fig_8">Fig 9)</ref>. <ref type="figure" target="#fig_0">Figures 10, 11, 12 and 13</ref> show that the equivalent filters in adjoining frames are automatically shifted spatially to account for the movement of objects in the videos. We extracted motion information using the shift as explained in Section 6. <ref type="figure" target="#fig_0">Figures 16, 17, 18 and 19</ref> show additional examples for UDVD and FastDVDnet. The estimated optical flow is mostly consistent with the estimated obtained by DeepFlow <ref type="bibr" target="#b44">[43]</ref> applied on the clean videos. The motion estimates obtained from the equivalent filters tends to be less accurate for pixels near strongly correlated features or highly homogeneous regions where the local motion is ambiguous. Sum of values of equivalent filter       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Motion Estimation</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Clean frame, PSNR / SSIM (b) Noisy Input, 19.06 / 0.279 (c) Supervised (FastDVDnet), 31.73 / 0.873 (d) Unsupervised (MF2F) , 30.35 / 0.825 (e) UDVD, 31.62 / 0.869 (f) UDVD-S, 31.39 / 0.865 Unsupervised denoising matches the performance of supervised denoising. Frame from a video in the Set8 dataset denoised using different approaches. (a) Clean frame. (b) Frame corrupted with Gaussian noise of standard deviation 30 (relative to intensity range [0-255]). (c) FastDVDnet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>,</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Video denoising as spatiotemporal adaptive filtering. Visualization of the equivalent linear weights (a(k, i), Eq. 4) used to compute two example denoised pixels using UDVD. The left two columns show noisy frames yt at two noise levels, and the corresponding denoised frames, dt. Three successive clean frames {xt?1, xt, xt+1} are shown in top row, for reference. Corresponding weights a(k, i) for pixel i (intersection of the dashed white lines) in these three frames, are shown in the last three columns. The weights are seen to adapt to underlying video content, with their mode shifting to track the motion of the skier. As the noise level ? increases (bottom row), their spatial extent grows, averaging out more of the noise while respecting object boundaries. For each denoised pixel, the sum of weights (over all pixel locations and frames) is approximately one, and thus can be interpreted as computing a local average (but note that some weights are negative, depicted in blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )Figure 5 .</head><label>a5</label><figDesc>Noisy frame (? = 30) (b) Motion estimate from clean video (c) Motion estimate from UDVD gradients CNNs trained for denoising automatically learn to perform motion estimation. (a) Noisy frame from a video in the DAVIS dataset. (b) Optical flow direction at multiple locations of the image obtained using a state-of-the-art algorithm applied to the clean video. (c) Optical flow direction estimated from the shift of the adaptive filter obtained by differentiating the network, which is trained exclusively with noisy videos and no optical flow information. Optical flow estimates are well-matched to those in (b), but deviate according to the aperture problem at oriented features (see black vertical edge of bus door), and in homogeneous regions (see bus roof, top right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of blind image and video denoising. Example from the DAVIS dataset. (a) Ground truth frame. (b) Noisy frame. (c) Reconstruction using a single frame. The texture details of the brick wall and the fence are not recovered well. Reconstruction using (d) 3 and (e) 5 surrounding frames produces an improved texture estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Generalization across noise levels and frame rates. (left) UDVD trained at only ? = 30 generalizes well to noise levels not seen during training. The plotted points represent mean PSNR values evaluated on Set8. (right) UDVD generalizes well to faster videos (created by skipping frames) and consistently outperforms a baseline image denoiser (UDVD with a single input frame, shown as a green dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Quantitative analysis of equivalent filters. Left column: The graphs show the sum of the entries of the equivalent filters in each frame, averaged over 5000 pixels from 250 random patches of size 128 ? 128. For all datasets, the central frame dominates. For the DAVIS dataset (top), the contribution from the other frames increases with the noise level. For the fluorescence-microscopy data (mid) the contribution of the other frames is rather low, due to the high signal-to-noise ratio. For the electron-microscopy dataset the contribution of the other frames is larger (bottom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>? 2, i) a(t ? 1, i) a(t, i) a(t + 1, i) a(t + 2, i) Video denoising as spatiotemporal adaptive filtering; giant-slalom video from the DAVIS dataset. Visualization of the linear weighting functions (a(k, i), Section 6 of paper) of UDVD. The left two columns show the noisy frame yt at four levels of noise, and the corresponding denoised frame, dt. Weighting functions a(k, i) corresponding to the pixel i (at the intersection of the dashed white lines), for five successive frames, are shown in the last five columns. The weighting functions adapt to underlying image content, and are shifted to track the motion of the skier. As the noise level ? increases, their spatial extent grows, averaging out more of the noise while respecting object boundaries. The weighting functions corresponding to the five frames approximately sum to one, and thus compute a local average (although some weights are negative, depicted in blue) as explained in Section G.1. Video denoising as spatiotemporal adaptive filtering; rafting video from the GoPro dataset. Visualization of the equivalent filters, as described inFig 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Video denoising as spatiotemporal adaptive filtering; tractor video from Set8. Visualization of the equivalent filters, as described inFig 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>? 2, i) a(t ? 1, i) a(t, i) a(t + 1, i) a(t + 2, i) Video denoising as spatiotemporal adaptive filtering; bus video from the DAVIS dataset. Visualization of the equivalent filters, as described inFig 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>? 2, i) a(t ? 1, i) a(t, i) a(t + 1, i) a(t + 2, i) Video denoising using FastDVDnet as spatiotemporal adaptive filtering; bus video from the DAVIS dataset. Visualization of the linear weighting functions (a(k, i), Section 6 of paper) of FastDVDnet which is trained with supervision. The left two columns show the noisy frame yt at four levels of noise, and the corresponding denoised frame, dt. Weighting functions a(k, i) corresponding to the pixel i (at the intersection of the dashed white lines), for five successive frames, are shown in the last five columns. The weighting functions adapt to underlying image content, and are shifted to track the motion of the stop sign. As the noise level ? increases, their spatial extent grows, averaging out more of the noise while respecting object boundaries. The behavior is very similar to the corresponding filters of UDVD as shown inFig 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .Figure 16 .Figure 17 .Figure 18 .Figure 19 .</head><label>1516171819</label><figDesc>Equivalent filters of UDVD when applied to real-world data. Visualization of the linear weighting functions (a(k, i), Section 6 of paper) of UDVD trained to denoise raw video, fluorescence and electron microscopy data. The left two columns show the noisy frame yt and the corresponding denoised frame, dt. Weighting functions a(k, i) corresponding to the pixel i (at the intersection of the dashed white lines), for five successive frames, are shown in the last five columns. In raw video data and fluorescence-microscopy data, the contributions from neighbouring frames are smaller. For electron-microscopy data they are larger (see alsoFig 9). CNNs trained for denoising automatically learn to perform motion estimation. (a) Noisy frame from giant-slalom video in the DAVIS dataset. (b) Optical flow direction at multiple locations of the image obtained using a state-of-the-art algorithm applied to the clean video. Optical flow direction estimated from the shift of the adaptive filter obtained from the gradients of (c) FastDVDnet and (d) UDVD, both of which are trained with no optical flow information. FastDVDnet is trained with supervision. Optical flow estimates are well-matched to those in (b), but are not as accurated at oriented features, and in homogeneous regions where local motion is not well defined (e.g. in the background). Each row corresponds to a different noise levels. At higher noise levels, the networks perform averages over more frames, improving the motion estimation results. CNNs trained for denoising automatically learn to perform motion estimation; rafting video from Set8. Motion estimated from the gradients of UDVD and FastDVDnet. See description ofFigure 16. CNNs trained for denoising automatically learn to perform motion estimation; tractor video from Set8. Motion estimated from the gradients of UDVD and FastDVDnet. See description ofFigure 16. CNNs trained for denoising automatically learn to perform motion estimation; bus video from the DAVIS dataset. Motion estimated from the gradients of UDVD and FastDVDnet. See description ofFigure 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Denoising results on natural video datasets. All networks are trained on the DAVIS train set. Performance values are PSNR of each trained network averaged over held-out test data. UDVD, operating on 5 frames, outperforms the supervised methods on Set8 and is competitive on the DAVIS test set. Unsupervised denoisers with more temporal frames show a consistent improvement in denoising performance. DVDnet and FastDVDnet are trained using varying noise levels (? ? [0, 55]) and VNLnet is trained and evaluated on each specified noise level. All UDVD networks are trained only at ? = 30, showing that they generalize well on unseen noise levels. See Sections C and F in the supplementary material for additional results. The PSNR values for all methods except UDVD are taken from<ref type="bibr" target="#b40">[39]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Traditional</cell><cell></cell><cell>Supervised CNN</cell><cell></cell><cell cols="3">Unsupervised CNN (UDVD)</cell></row><row><cell></cell><cell>test set</cell><cell>?</cell><cell cols="7">VNLB VBM4D VNLnet DVDnet FastDVDnet 1 frame 3 frames</cell><cell>5 frames</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell>33.73</cell><cell>31.65</cell><cell>-</cell><cell>34.08</cell><cell>34.06</cell><cell>32.80</cell><cell>33.48</cell><cell>33.92</cell></row><row><cell></cell><cell>DAVIS</cell><cell>40</cell><cell>32.32</cell><cell>30.05</cell><cell>32.32</cell><cell>32.86</cell><cell>32.80</cell><cell>31.48</cell><cell>32.20</cell><cell>32.68</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>31.13</cell><cell>28.80</cell><cell>31.43</cell><cell>31.85</cell><cell>31.83</cell><cell>30.47</cell><cell>31.20</cell><cell>31.70</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell>31.74</cell><cell>30.00</cell><cell>-</cell><cell>31.79</cell><cell>31.60</cell><cell>30.91</cell><cell>31.62</cell><cell>32.01</cell></row><row><cell></cell><cell>Set8</cell><cell>40</cell><cell>30.39</cell><cell>28.48</cell><cell>30.55</cell><cell>30.55</cell><cell>30.37</cell><cell>29.63</cell><cell>30.42</cell><cell>30.82</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>29.24</cell><cell>27.33</cell><cell>29.47</cell><cell>29.56</cell><cell>29.42</cell><cell>28.65</cell><cell>29.47</cell><cell>29.89</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 90</cell></row><row><cell></cell><cell>DAVIS</cell><cell></cell><cell>Set8</cell><cell></cell><cell>Derfs</cell><cell>Vid3oC</cell><cell>DAVIS</cell><cell>Set8</cell><cell></cell><cell>Derfs</cell><cell>Vid3oC</cell></row><row><cell>UDVD-S</cell><cell cols="2">33.68 / 78.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>32.90 / 81.85 33.95 / 81.91 34.65 / 84.60 29.05 / 53.53 28.07 / 55.35 29.42 / 59.25 29.94 / 63.79 UDVD* 33.78 / 79.88 31.90 / 82.53 32.58 / 81.44 34.24 / 83.96 28.87 / 51.22 27.25 / 51.84 28.26 / 52.44 29.23 / 60.08 FastDVDnet* 33.91 / 76.99 31.81 / 80.21 32.45 / 81.64 35.05 / 84.44 28.01 / 47.53 26.54 / 50.16 27.36 / 52.87 28.42 / 55.99 MF2F 33.91 / 80.01 31.84 / 80.55 32.87 / 82.22 35.18 / 85.71 28.81 / 51.24 27.25 / 52.78 28.29 / 55.06 29.67 / 61.28</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are obtained with a network trained only at a fixed noise level of ? = 30. This generalization ability is consistent with bias-free networks for image denoising<ref type="bibr" target="#b31">[31]</ref>. See Suppl. F for more discussion and results. Raw videos with real noise. We train UDVD on the first 9 realizations of the 5 videos from the test set of the raw video dataset (see Section 4), holding out the last realization for early stopping. We compare our performance with RViDeNet<ref type="bibr" target="#b49">[48]</ref> which is pre-trained on a simulated dataset</figDesc><table><row><cell>CNN</cell><cell>ISO</cell><cell>1600</cell><cell>3200</cell><cell>6400</cell><cell cols="2">12800 25600 mean</cell></row><row><cell cols="2">UDVD</cell><cell cols="3">48.04 46.24 44.70</cell><cell>42.19</cell><cell>42.11</cell><cell>44.69</cell></row><row><cell cols="5">RViDeNet [48] 47.74 45.91 43.85</cell><cell>41.20</cell><cell>41.17</cell><cell>43.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Raw video denoising. PSNR values evaluated on the test set of the raw video dataset (Section 4) when denoised with (a) UDVD trained only the noisy test videos and (b) RViDeNet trained with supervision on a large dataset. The columns correspond to different ISO levels, with larger levels resulting in noisier data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance of UDVD. Table shows the mean PSNR values of a state-of-the-art supervised video denoiser (FastDVDnet<ref type="bibr" target="#b40">[39]</ref> ) and UDVD with the denoised frame being predicted from k ? {1, 3, 5} surrounding frames. The performance of UDVD monotonically increases with k and is comparable for supervised denoising across all noise levels. All the three UDVD networks reported here are trained for only ? = 30. FastDVDnet is trained for ? ?<ref type="bibr" target="#b4">[5,</ref> 55].</figDesc><table><row><cell>Set8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>when evaluated on the DAVIS test set and Set8 corrupted with ? ? {20, 40, . . . , 80}. UDVD achieves comparable performance to FastDVDnet on DAVIS test set and slightly outperforms it on Set8 at all noise levels. 2. Examples of noisy videos, and denoised counterparts obtained using UDVD are included in the official github repository 2 (hypermooth.mp4, rafting.mp4, motorbike.mp4 and snowboard.mp4).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>). Right column: Histogram of the sum of all entries in the equivalent filters (over all 5 frames) for 5000 pixels from 250 random patches of size 128 ? 128 from the DAVIS test set (top), the fluorescence-microscopy dataset (mid) and the electron-microscopy dataset (bottom). For the DAVIS and fluorescence-microscopy datasets, the filters sum to 1 in most cases. The peak of electron microscopy deviates significantly from 1. This could be due to the noise model, which has non-Gaussian characteristics (it is Poisson with low counts).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/sreyas-mohan/udvd</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Evaluated using the pre-trained model provided here: https://github.com/z-bingo/kernel-prediction-networks-PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/cmla/mf2f</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>This work was supported by HHMI, NSF NRT HDR Award 1922658, CBET 1604971, OAC-1940263 and OAC-1940097. We thank the HPC staff at NYU, ASU and RBCDSAI, IIT Madras for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video denoising via empirical Bayesian estimation of space-time patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="93" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Noise2self: Blind denoising by self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Royer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Patch-based video denoising with optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose-Luis</forename><surname>Lisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Miladinovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2573" to="2586" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>S Grace Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Videnn: Deep blind video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1843" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A non-local CNN for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2409" to="2413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised training for blind multi-frame video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valery</forename><surname>Dewil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Anger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising by soft-thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Info Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="613" to="627" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-blind video denoising via frame-to-frame training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11361" to="11370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep burst denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03241</idno>
		<title level="m">Learning model-blind temporal denoisers without ground truths</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised Poisson-Gaussian denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Minnerath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2131" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The vid3oc and intvid datasets for video super resolution and quality mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanju</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Fuoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise2void -learning denoising from single noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim-Oliver</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jug</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00651</idno>
		<title level="m">Probabilistic noise2void: Unsupervised content-aware denoising</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-quality self-supervised deep image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A nonlocal Bayesian image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Noise2noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ioannis Katsavounidis, Anush Moorthy, and Megha Manohara. Toward a practical perceptual video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Aaron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Netflix Technology Blog</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A high-quality video denoising algorithm based on reliable motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William T Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="706" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A tour of modern image filtering: New insights and methods, both practical and theoretical. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="106" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust and interpretable blind image denoising via bias-free convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyas</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep denoising for scientific discovery: A case study in electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyas</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Manzorro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashpal</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Crozier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez-Granda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12970</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image denoising using scale mixtures of Gaussians in the wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Strela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fully unsupervised probabilistic noise2void</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mangal</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manan</forename><surname>Lalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jug</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12291</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Noise removal via Bayesian wavelet coring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 3rd IEEE Int&apos;l Conf on Image Proc</title>
		<meeting>3rd IEEE Int&apos;l Conf on Image<address><addrLine>Lausanne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="379" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dvdnet: A fast network for deep video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fastdvdnet: Towards real-time deep video denoising without flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An objective comparison of celltracking algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladim?r Ulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1141" to="1152" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
		</imprint>
	</monogr>
	<note type="report_type">Technology Press</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Basis prediction networks for effective burst denoising with large kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint learning of blind video denoising and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumjun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supervised raw video denoising with a benchmark dataset on dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">we use 4 sequences from the GoPro set (hypersmooth, motorbike, rafting, snowboard) and 4 sequences from the Derfs Test Media Collection</title>
		<idno>Set8 [39]: Following FastDVDNet [39</idno>
		<imprint/>
	</monogr>
	<note>park joy, sunflower, touchdown, tractor</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">we use 7 sequences from the Derfs Test Media Collection, which are park joy, sunflower, touchdown, tractor (shared with Set8), and blue sky, old town cross, pedestrian area. We use the first 85 frames from each sequences with a spatial-resolution of 960 ? 540</title>
		<idno>Derfs: Following [9</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
