<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Salesforce Research ? Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
							<email>k.hashimoto@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Salesforce Research ? Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Salesforce Research ? Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<email>rsocher@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Salesforce Research ? Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Salesforce Research ? Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graphbased recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Open-domain Question Answering (QA) is the task of answering a question given a large collection of text documents (e.g., Wikipedia). Most state-of-the-art approaches for open-domain QA <ref type="bibr" target="#b1">(Chen et al., 2017;</ref><ref type="bibr" target="#b36">Wang et al., 2018a;</ref><ref type="bibr" target="#b14">Lee et al., 2018;</ref><ref type="bibr" target="#b5">Yang et al., 2019)</ref> leverage non-parameterized models (e.g., TF-IDF or BM25) to retrieve a fixed set of documents, where an answer span is extracted by a neural reading comprehension model. Despite the success of these pipeline methods in singlehop QA, whose questions can be answered based on a single paragraph, they often fail to retrieve the required evidence for answering multi-hop questions, e.g., the question in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="bibr">Multi-hop QA (Yang et al., 2018)</ref> usually requires finding more than one evidence document, one of which often consists of little lexical overlap or semantic relationship to the original question. However, retrieving a fixed list of documents independently does not capture relationships between evidence documents through bridge entities that are required for multi-hop reasoning.</p><p>Recent open-domain QA methods learn end-to-end models to jointly retrieve and read documents <ref type="bibr" target="#b33">(Seo et al., 2019;</ref>. These methods, however, face challenges for entity-centric questions since compressing the necessary information into an embedding space does not capture lexical information in entities. Cognitive Graph <ref type="bibr" target="#b5">(Ding et al., 2019)</ref> incorporates entity links between documents for multi-hop QA to extend the list of retrieved documents. This method, however, compiles a fixed list of documents independently and expects the reader to find the reasoning paths.</p><p>In this paper, we introduce a new recurrent graph-based retrieval method that learns to retrieve evidence documents as reasoning paths for answering complex questions. Our method sequentially retrieves each evidence document, given the history of previously retrieved documents to form several reasoning paths in a graph of entities. Our method then leverages an existing reading comprehension model to answer questions by ranking the retrieved reasoning paths. The strong interplay between the retriever model and reader model enables our entire method to answer complex questions by exploring more accurate reasoning paths compared to other methods. To be more specific, our method (sketched in <ref type="figure" target="#fig_1">Figure 2</ref>) constructs the Wikipedia paragraph graph using Wikipedia hyperlinks and document structures to model the relationships between paragraphs. Our retriever trains a recurrent neural network to score reasoning paths in this graph by maximizing the likelihood of selecting a correct evidence paragraph at each step and fine-tuning paragraph BERT encodings. Our reader model is a multi-task learner to score each reasoning path according to its likelihood of containing and extracting the correct answer phrase. We leverage data augmentation and negative example mining for robust training of both models.</p><p>Our experimental results show that our method achieves the state-of-the-art results on HotpotQA full wiki and HotpotQA distractor settings <ref type="bibr">(Yang et al., 2018)</ref>, outperforming the previous stateof-the-art methods by more than 14 points absolute gain on the full wiki setting. We also evaluate our approach on SQuAD Open <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> and Natural Questions Open  without changing any architectural designs, achieving better or comparable to the state of the art, which suggests that our method is robust across different datasets. Additionally, our framework provides interpretable insights into the underlying entity relationships used for multi-hop reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Neural open-domain question answering Most current open-domain QA methods use a pipeline approach that includes a retriever and reader. <ref type="bibr" target="#b1">Chen et al. (2017)</ref> incorporate a TF-IDF-based retriever with a state-of-the-art neural reading comprehension model. The subsequent work improves the heuristic retriever by re-ranking retrieved documents <ref type="bibr" target="#b36">(Wang et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b14">Lee et al., 2018;</ref><ref type="bibr" target="#b16">Lin et al., 2018)</ref>. The performance of these methods is still bounded by the performance of the initial retrieval process. In multi-hop QA, non-parameterized retrievers face the challenge of retrieving all the relevant documents, one or some of which are lexically distant from the question. Recently,  and <ref type="bibr" target="#b33">Seo et al. (2019)</ref> introduce fully trainable models that retrieve a few candidates directly from large-scale Wikipedia collections. All these methods find evidence documents independently without the knowledge of previously selected documents or relationships between documents. This would result in failing to conduct multi-hop retrieval.</p><p>Retrievers guided by entity links Most relevant to our work are recent studies that attempt to use entity links for multi-hop open-domain QA. Cognitive Graph <ref type="bibr" target="#b5">(Ding et al., 2019)</ref> retrieves evidence documents offline, and trains a reading comprehension model to jointly predict possible answer spans and next-hop spans to extend the reasoning chain. Instead, we train our retriever to find reasoning paths directly. Concurrent with our work, Entity-centric IR <ref type="bibr" target="#b8">(Godbole et al., 2019)</ref> uses entity linking for multi-hop retrieval. Unlike our method, this method does not learn to retrieve reasoning paths sequentially, nor study the interplay between retriever and reader. Moreover, while the previous approaches require a system to encode all possible nodes, our beam search decoding process only encodes the nodes on the reasoning paths, which significantly reduces the computational costs. PullNet <ref type="bibr" target="#b34">(Sun et al., 2019)</ref> learns to retrieve question-aware sub-graphs from text corpora and knowledge bases (e.g., Freebase), while we focus on open-domain QA solely based on text.</p><p>Multi-step (iterative) retrievers Similar to our recurrent retriever, multi-step retrievers explore multiple evidence documents iteratively. Multi-step reasoner  repeats the retrieval process for a fixed number of steps, interacting with a reading comprehension model by reformulating the query in a latent space to enhance retrieval performance. <ref type="bibr" target="#b6">Feldman &amp; El-Yaniv (2019)</ref> also propose a query reformulation mechanism with a focus on multi-hop open-domain QA. Most recently, <ref type="bibr" target="#b27">Qi et al. (2019)</ref> introduce GoldEn Retriever, which reads and generates search queries for two steps to search documents for HotpotQA full wiki. These methods do not use the graph  structure of the documents during the iterative retrieval process. In addition, all of these multi-step retrieval methods do not accommodate arbitrary steps of reasoning and the termination condition is hard-coded. In contrast, our method leverages the Wikipedia graph to retrieve documents that are lexically or semantically distant to questions, and is adaptive to any reasoning path lengths, which leads to significant improvement over the previous work in HotpotQA and SQuAD Open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPEN-DOMAIN QUESTION ANSWERING OVER WIKIPEDIA GRAPH</head><p>Overview This paper introduces a new graph-based recurrent retrieval method (Section 3.1) that learns to find evidence documents as reasoning paths for answering complex questions. We then extend an existing reading comprehension model (Section 3.2) to answer questions given a collection of reasoning paths. Our method uses a strong interplay between retrieving and reading steps such that the retrieval method learns to retrieve a set of reasoning paths to narrow down the search space for our reader model, for robust pipeline process. <ref type="figure" target="#fig_1">Figure 2</ref> sketches the overview of our QA model. We use Wikipedia for open-domain QA, where each article is divided into paragraphs, resulting in millions of paragraphs in total. Each paragraph p is considered as our retrieval target. Given a question q, our framework aims at deriving its answer a by retrieving and reading reasoning paths, each of which is represented with a sequence of paragraphs: E = [p i , . . . , p k ]. We formulate the task by decomposing the objective into the retriever objective S retr (q, E) that selects reasoning paths E relevant to the question, and the reader objective S read (q, E, a) that finds the answer a in E:</p><formula xml:id="formula_0">arg max E,a S(q, E, a) s.t. S(q, E, a) = S retr (q, E) + S read (q, E, a).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LEARNING TO RETRIEVE REASONING PATHS</head><p>Our method learns to retrieve reasoning paths across a graph structure. Evidence paragraphs for a complex question do not necessarily have lexical overlaps with the question, but one of them is likely to be retrieved, and its entity mentions and the question often entail another paragraph (e.g., <ref type="figure" target="#fig_0">Figure 1</ref>). To perform such multi-hop reasoning, we first construct a graph of paragraphs, covering all the Wikipedia paragraphs. Each node of the Wikipedia graph G represents a single paragraph p i .</p><p>Constructing the Wikipedia graph Hyperlinks are commonly used to construct relationships between articles on the web, usually maintained by article writers, and are thus useful knowledge resources. Wikipedia consists of its internal hyperlinks to connect articles. We use the hyperlinks to construct the direct edges in G. We also consider symmetric within-document links, allowing a paragraph to hop to other paragraphs in the same article. The Wikipedia graph G is densely connected and covers a wide range of topics that provide useful evidence for open-domain questions. This graph is constructed offline and is reused throughout training and inference for any question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">THE GRAPH-BASED RECURRENT RETRIEVER</head><p>General formulation with a recurrent retriever We use a Recurrent Neural Network (RNN) to model the reasoning paths for the question q. At the t-th time step (t ? 1) our model selects a paragraph p i among candidate paragraphs C t given the current hidden state h t of the RNN. The initial hidden state h 1 is independent of any questions or paragraphs, and based on a parameterized vector. We use BERT's [CLS] token representation  to independently encode each candidate paragraph p i along with q. <ref type="bibr">2</ref> We then compute the probability P (p i |h t ) that p i is selected. The RNN selection procedure captures relationships between paragraphs in the reasoning path by conditioning on the selection history. The process is terminated when <ref type="bibr">[EOE]</ref>, the end-ofevidence symbol, is selected, to allow it to capture reasoning paths with arbitrary length given each question. More specifically, the process of selecting p i at the t-th step is formulated as follows:</p><formula xml:id="formula_1">w i = BERT [CLS] (q, p i ) ? R d , (2) P (p i |h t ) = ?(w i ? h t + b),<label>(3)</label></formula><formula xml:id="formula_2">h t+1 = RNN(h t , w i ) ? R d ,<label>(4)</label></formula><p>where b ? R 1 is a bias term. Motivated by <ref type="bibr" target="#b31">Salimans &amp; Kingma (2016)</ref>, we normalize the RNN states to control the scale of logits in Equation <ref type="formula" target="#formula_1">(3)</ref> and allow the model to learn multiple reasoning paths. The details of Equation (4) are described in Appendix A.1. The next candidate set C t+1 is constructed to include paragraphs that are linked from the selected paragraph p i in the graph. To allow our model to flexibly retrieve multiple paragraphs within C t , we also add K-best paragraphs other than p i (from C t ) to C t+1 , based on the probabilities. We typically set K = 1 in this paper.</p><p>Beam search for candidate paragraphs It is computationally expensive to compute Equation (2) over millions of the possible paragraphs. Moreover, a fully trainable retriever often performs poorly for entity-centric questions such as SQuAD, since it does not explicitly maintain lexical information . To navigate our retriever in the large-scale graph effectively, we initialize candidate paragraphs with a TF-IDF-based retrieval and guide the search over the Wikipedia graph. In particular, the initial candidate set C 1 includes F paragraphs with the highest TF-IDF scores with respect to the question. We expand C t (t ? 2) by appending the [EOE] symbol. We additionally use a beam search to explore paths in the directed graph. We define the score of a reasoning path E = [p i , . . . , p k ] by multiplying the probabilities of selecting the paragraphs: P (p i |h 1 ) . . . P (p k |h |E| ). The beam search outputs the top B reasoning paths E = {E 1 , . . . , E B } with the highest scores to pass to the reader model i.e., S(q, E, a) = S read (q, E, a) for E ? E.</p><p>In terms of the computational cost, the number of the paragraphs processed by Equation <ref type="formula">(2)</ref> is bounded by O(|C 1 | + B t?2 |C t |), where B is the beam size and |C t | is the average size of C t over the B hypothesises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">TRAINING OF THE GRAPH-BASED RECURRENT RETRIEVER</head><p>Data augmentation We train our retriever in a supervised fashion using evidence paragraphs annotated for each question. For multi-hop QA, we have multiple paragraphs for each question, and single paragraph for single-hop QA. We first derive a ground-truth reasoning path g = [p 1 , . . . , p |g| ] using the available annotated data in each dataset. p |g| is set to <ref type="bibr">[EOE]</ref> for the termination condition.</p><p>To relax and stabilize the training process, we augment the training data with additional reasoning paths -not necessarily the shortest paths -that can derive the answer. In particular, we add a new training path g r = [p r , p 1 , . . . , p |g| ] by adding a paragraph p r ? C 1 that has a high TF-IDF score and is linked to the first paragraph p 1 in the ground-truth path g. Adding these new training paths helps at the test time when the first paragraph in the reasoning path does not necessarily appear among the paragraphs that initialize the Wikipedia search using the heuristic TF-IDF retrieval.</p><p>Negative examples for robustness Our graph-based recurrent retriever needs to be trained to discriminate between relevant and irrelevant paragraphs at each step. We therefore use negative examples along with the ground-truth paragraphs; to be more specific, we use two types of negative examples: (1) TF-IDF-based and (2) hyperlink-based ones. For single-hop QA, we only use the type (1). For multi-hop QA, we use both types, and the type (2) is especially important to prevent our retriever from being distracted by reasoning paths without correct answer spans. We typically set the number of the negative examples to 50.</p><p>Loss function For the sequential prediction task, we estimate P (p i |h t ) independently in Equation (3) and use the binary cross-entropy loss to maximize probability values of all the possible paths. Note that using the widely-used cross-entropy loss with the softmax normalization over C t is not desirable here; maximizing the probabilities of g and g r contradict with each other. More specifically, the loss function of g at the t-th step is defined as follows:</p><formula xml:id="formula_3">L retr (p t , h t ) = ? log P (p t |h t ) ? p?Ct log (1 ? P (p|h t )),<label>(5)</label></formula><p>whereC t is a set of the negative examples described above, and includes [EOE] for t &lt; |g|. We exclude p r fromC 1 for the sake of our multi-path learning. The loss is also defined with respect to g r in the same way. All the model parameters, including those in BERT, are jointly optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">READING AND ANSWERING GIVEN REASONING PATHS</head><p>Our reader model first verifies each reasoning path in E, and finally outputs an answer span a from the most plausible reasoning path. This interplay is effective in making our framework robust; this is further discussed in Appendix A.3. We model the reader as a multi-task learning of (1) reading comprehension, that extracts an answer span from a reasoning path E using a standard approach <ref type="bibr" target="#b32">(Seo et al., 2017;</ref><ref type="bibr" target="#b40">Xiong et al., 2017;</ref>, and (2) reasoning path re-ranking, that re-ranks the retrieved reasoning paths by computing the probability that the path includes the answer.</p><p>For the reading comprehension task, we use BERT , where the input is the concatenation of the question text and the text of all the paragraphs in E. This lets our reader to fully leverage the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths; this paragraph interaction is crucial for multi-hop reasoning <ref type="bibr" target="#b35">(Wang et al., 2019a)</ref>.</p><p>We share the same model for re-ranking, and use the BERT's [CLS] representation to estimate the probability of selecting E to answer the question:</p><formula xml:id="formula_4">P (E|q) = ?(w n ? u E ) s.t. u E = BERT [CLS] (q, E) ? R D ,<label>(6)</label></formula><p>where w n ? R D is a weight vector. At the inference time, we select the best evidence E best ? E by P (E|q), and output the answer span by S read :</p><formula xml:id="formula_5">E best = arg max E?E P (E|q), S read = arg max i,j, i?j P start i P end j ,<label>(7)</label></formula><p>where P start i , P end j denote the probability that the i-th and j-th tokens in E best are the start and end positions, respectively, of the answer span, and are calculated by following .</p><p>Training examples To train the multi-task reader model, we use the ground-truth evidence paragraphs used for training our retriever. It is known to be effective in open-domain QA to use distantly supervised examples, which are not originally associated with the questions but include expected answer strings <ref type="bibr" target="#b1">(Chen et al., 2017;</ref><ref type="bibr" target="#b36">Wang et al., 2018a;</ref><ref type="bibr" target="#b9">Hu et al., 2019)</ref>. These distantly supervised examples are also effective to simulate the inference time process. Therefore, we combine distantly supervised examples from a TF-IDF retriever with the original supervised examples. Following the procedures in <ref type="bibr" target="#b1">Chen et al. (2017)</ref>, we add up to one distantly supervised example for each supervised example. We set the answer span as the string that matches a and appears first.</p><p>To train our reader model to discriminate between relevant and irrelevant reasoning paths, we augment the original training data with additional negative examples to simulate incomplete evidence. In particular, we add paragraphs that appear to be relevant to the given question but actually do not contain the answer. For multi-hop QA, we select one ground-truth paragraph including the answer span, and swap it with one of the TF-IDF top ranked paragraphs. For single-hop QA, we simply replace the single ground-truth paragraph with TF-IDF-based negative examples which do not include the expected answer string. For the distorted evidence?, we aim at minimizing P (?|q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task loss function</head><p>The objective is the sum of cross entropy losses for the span prediction and re-ranking tasks. The loss for the question q and its evidence candidate E is as follows:</p><formula xml:id="formula_6">L read = L span + L no answer = (? log P start y start ? log P end y end ) ? log P r ,<label>(8)</label></formula><p>where y start and y end are the ground-truth start and end indices, respectively. L no answer corresponds to the loss of the re-ranking model, to discriminate the distorted reasoning paths with no answers. P r is P (E|q) if E is the ground-truth evidence; otherwise P r = 1 ? P (E|q). We mask the span losses for negative examples, in order to avoid unexpected effects to the span predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>We evaluate our method in three open-domain Wikipedia-sourced datasets: HotpotQA, SQuAD Open and Natural Questions Open. We target all the English Wikipedia paragraphs for SQuAD Open and Natural Questions Open, and the first paragraph (introductory paragraph) of each article for HotpotQA following previous studies. More details can be found in Appendix B.</p><p>HotpotQA HotpotQA (Yang et al., 2018) is a human-annotated large-scale multi-hop QA dataset.</p><p>Each answer can be extracted from a collection of 10 paragraphs in the distractor setting, and from the entire Wikipedia in the full wiki setting. Two evidence paragraphs are associated with each question for training. Our primary target is the full wiki setting due to its open-domain scenario, and we use the distractor setting to evaluate how well our method works in a closed scenario where the two evidence paragraphs are always included. The dataset also provides annotations to evaluate the prediction of supporting sentences, and we adapt our retriever to the supporting fact prediction. Note that this subtask is specific to HotpotQA. More details are described in Appendix A.5.</p><p>SQuAD Open SQuAD Open <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> is composed of questions from the original SQuAD dataset <ref type="bibr" target="#b29">(Rajpurkar et al., 2016)</ref>. This is a single-hop QA task, and a single paragraph is associated with each question in the training data.</p><p>Natural Questions Open Natural Questions Open  is composed of questions from the Natural Questions dataset , 3 which is based on Google Search queries independently from the existing articles. A single paragraph is associated with each question, but our preliminary analysis showed that some questions benefit from multi-hop reasoning.</p><p>Metrics We report standard F1 and EM scores for HotpotQA and SQuAD Open, and EM score for Natural Questions Open to evaluate the overall QA accuracy to find the correct answers. For Hot-potQA, we also report Supporting Fact F1 (SP F1) and Supporting Fact EM (SP EM) to evaluate the sentence-level supporting fact retrieval accuracy. To evaluate the paragraph-level retrieval accuracy for the multi-hop reasoning, we use the following metrics: Answer Recall (AR), which evaluates the recall of the answer string among top paragraphs <ref type="bibr" target="#b36">(Wang et al., 2018a;</ref>, Paragraph Recall (PR), which evaluates if at least one of the ground-truth paragraphs is included among the retrieved paragraphs, and Paragraph Exact Match (P EM), which evaluates if both of the ground-truth paragraphs for multi-hop reasoning are included among the retrieved paragraphs.</p><p>Evidence Corpus and the Wikipedia graph We use English Wikipedia as the evidence corpus and do not use other data such as Google search snippets or external structured knowledge bases. We use the several versions of Wikipedia dumps for the three datasets (See Appendix B.5). To construct the Wikipedia graph, the hyperlinks are automatically extracted from the raw HTML source files. Directed edges are added between a paragraph p i and all of the paragraphs included in the target article. The constructed graph consists of 32.7M nodes and 205.4M edges. For HotpotQA we only use the introductory paragraphs in the graph that includes about 5.2M nodes and 23.4M edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We use the pre-trained BERT models  using the uncased base configuration (d = 768) for our retriever and the whole word masking uncased large (wwm) configuration (d = 1024) for our readers. We follow <ref type="bibr" target="#b1">Chen et al. (2017)</ref> for the TF-IDF-based retrieval model and use the same hyper-parameters. We tuned the most important hyper-parameters, F , the number of the initial TF-IDF-based paragraphs, and B, the beam size, by mainly using the HotpotQA development set (the effects of increasing F are shown in <ref type="figure" target="#fig_3">Figure 5</ref> in Appendix C.3 along with the results with B = 1). If not specified, we set B = 8 for all the datasets, F = 500 for HotpotQA full wiki and SQuAD Open, and F = 100 for Natural Questions Open.</p><formula xml:id="formula_7">full wiki distractor QA SP QA SP Models F1 EM F1 EM F1 EM F1 EM</formula><p>Semantic Retrieval <ref type="bibr" target="#b22">(Nie et al., 2019)</ref> 58.8 46.5 71.5 39.9 ----GoldEn Retriever <ref type="bibr" target="#b27">(Qi et al., 2019)</ref> 49.8 -64.6 -----Cognitive Graph <ref type="bibr" target="#b5">(Ding et al., 2019)</ref> 49.4 37.6 58.5 23.1 ----DecompRC <ref type="bibr" target="#b21">(Min et al., 2019c)</ref> 43 We can see that our method, even with the BERT base configuration for our reader, significantly outperforms all the previous QA scores. Moreover, our method shows significant improvement in predicting supporting facts in the full wiki setting. We compare the performance of our approach to other models on the HotpotQA full wiki official hidden test set in <ref type="table" target="#tab_3">Table 2</ref>. We outperform all the published and unpublished models including up-to-date work (marked with ? ) by large margins in terms of QA performance.</p><formula xml:id="formula_8">.3 - - - 70.6 - - - MUPPET (Feldman &amp; El-Yaniv,</formula><p>On SQuAD Open, our model outperforms the concurrent state-of-the-art model <ref type="bibr" target="#b38">(Wang et al., 2019b)</ref> by 2.9 F1 and 3.5 EM scores as shown in <ref type="table" target="#tab_5">Table 3</ref>. Due to the fewer lexical overlap between questions and paragraphs on Natural Questions, pipelined approaches using term-based retrievers often face difficulties finding associated articles. Nevertheless, our approach matches the performance of the best end-to-end retriever (ORQA), as shown in <ref type="table" target="#tab_6">Table 4</ref>. In addition to its competitive performance, our retriever can be handled on a single GPU machine, while a fully end-to-end retriever in general requires industry-scale computational resources for training <ref type="bibr" target="#b33">(Seo et al., 2019)</ref>. More results on these two datasets are discussed in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PERFORMANCE OF REASONING PATH RETRIEVAL</head><p>We compare our retriever with competitive retrieval methods for HotpotQA full wiki, with F = 20.</p><p>TF-IDF <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>, the widely used retrieval method that scores paragraphs according to the TF-IDF scores of the question-paragraph pairs. We simply select the top-2 paragraphs. Re-rank <ref type="bibr" target="#b24">(Nogueira &amp; Cho, 2019</ref>) that learns to retrieve paragraphs by fine-tuning BERT to re-rank the top F TF-IDF paragraphs. We select the top-2 paragraphs after re-ranking.</p><p>Re-rank 2hop which extends Re-rank to accommodate two-hop reasoning. It first adds paragraphs linked from the top TF-IDF paragraphs. It then uses the same BERT model to select the paragraphs. Entity-centric IR is our re-implementation of <ref type="bibr" target="#b8">Godbole et al. (2019)</ref> that is related to Re-rank 2hop, but instead of simply selecting the top two paragraphs, they re-rank the possible combinations of the paragraphs that are linked to each other. Retrieval results <ref type="table" target="#tab_7">Table 5</ref> shows that our recurrent retriever yields 8.8 P EM and 9.   <ref type="bibr" target="#b9">(Hu et al., 2019)</ref> 50.2 41.9 MUPPET <ref type="bibr" target="#b6">(Feldman &amp; El-Yaniv, 2019</ref><ref type="bibr">) 46.2 39.3 BERTserini (Yang et al., 2019</ref> 46.1 38.6 DENSPI-hybrid <ref type="bibr" target="#b33">(Seo et al., 2019)</ref> 44.4 36.2 MINIMAL <ref type="bibr" target="#b18">(Min et al., 2018)</ref> 42.5 34.7 Multi-step Reasoner      Re-rank2hop to Entity-centric IR demonstrates that exploring entity links from the initially retrieved documents helps to retrieve the paragraphs with fewer lexical overlaps. On the other hand, comparing our retriever with Entity-centric IR and Semantic Retrieval shows the importance of learning to sequentially retrieve reasoning paths in the Wikipedia graph. It should be noted that our method with F = 20 outperforms all the QA EM scores in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ANALYSIS</head><p>We conduct detailed analysis of our framework on the HotpotQA full wiki development set.</p><p>Ablation study of our framework To study the effectiveness of our modeling choices, we compare the performance of variants of our framework. We ablate the retriever with 1) No recurrent module, which removes the recurrence from our retriever, and computes the probability of each paragraph to be included in reasoning paths independently and selects the path with the highest joint probability path on the graph; 2) No beam search, which uses a greedy search (B = 1) in our recurrent retriever;     Ablation results <ref type="table" target="#tab_9">Table 6</ref> shows that removing any of the listed components gives notable performance drop. The most critical component in our retriever model is the recurrent module, dropping the EM by 17.4 points. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, multi-step retrieval often relies on information mentioned in another paragraph. Therefore, without conditioning on the previous time steps, the model fails to retrieve the complete evidence. Training without hyperlink-based negative examples results in the second largest performance drop, indicating that the model can be easily distracted by reasoning paths without a correct answer and the importance of negative sampling for training. Replacing the beam search with the greedy search gives a performance drop of about 4 points on EM, which demonstrates that being aware of the graph structure is helpful in finding the best reasoning paths.</p><p>Performance drop by removing the reasoning path re-ranking indicates the importance of verifying the reasoning paths in our reader. Not using negative examples to train the reader degrades EM more than 16 points, due to the over-confident predictions as discussed in <ref type="bibr" target="#b2">Clark &amp; Gardner (2018)</ref>.</p><p>The performance with an off-the-shelf entity linking system Although the existence of the hyperlinks is not special on the web, one question is how well our method works without the Wikipedia hyperlinks. We evaluate our method on the development set of HotpotQA full wiki with an off-theshelf entity linking system <ref type="bibr" target="#b7">(Ferragina &amp; Scaiella, 2011)</ref> to construct the document graph in our method. More details about this experimental setup can be found in Appendix B.7. <ref type="table" target="#tab_10">Table 7</ref> shows that our approach with the entity linking system shows only 2.3 F1 and 2.2 EM lower scores than those with the hyperlinks, still achieving the state of the art. This suggests that our approach is not restricted to the existence of the hyperlink information, and using hyperlinks is promising.</p><p>The effectiveness of arbitrary-step retrieval The existing iterative retrieval methods fix the number of reasoning steps <ref type="bibr" target="#b27">(Qi et al., 2019;</ref><ref type="bibr" target="#b8">Godbole et al., 2019;</ref><ref type="bibr" target="#b6">Feldman &amp; El-Yaniv, 2019)</ref>, while our approach accommodates arbitrary steps of reasoning. We also evaluate our method by fixing the length of the reasoning path (L = {1, 2, 3, 4}). <ref type="table" target="#tab_11">Table 8</ref> shows that out adaptive retrieval performs the best, although the length of all the annotated reasoning paths in HotpotQA is two. As discussed in <ref type="bibr" target="#b20">Min et al. (2019b)</ref>, we also observe that some questions are answerable based on a single paragraph, where our model flexibly selects a single paragraph and then terminates retrieval.</p><p>The effectiveness of the interplay between retriever and reader <ref type="table" target="#tab_9">Table 6</ref> shows that the interplay between our retriever and reader models is effective. To understand this, we investigate the length of reasoning paths selected by our retriever and reader, and their final QA performance. <ref type="table" target="#tab_12">Table 9</ref> shows that the average length selected by our reader is notably longer than that by our retriever. The 2017 League 1 Cup known as the 2017 iPro Sport cup for sponsorship reasons is the third running of the competition, first played in 2015.</p><p>The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as what for sponsorship reasons?</p><p>The EFL Cup, currently known as the FA Cup EFL Cup Carabao Cup for sponsorship reasons...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FA Cup</head><p>The 2017-18 season is Wigan Athletic's ... their first back in League One ..., the club will also participate in the FA Cup, EFL Cup and EFL Trophy.</p><p>2017-18 Wigan Athletic F.C. <ref type="figure">Figure 4</ref>: Reasoning examples by our retriever (the bottom paragraph) and our reader (two paragraphs connected by a dotted line). Highlighted text denotes a bridge entity, and blue-underlined text represents hyperlinks.</p><p>(L = {1, 2, 3}). We observe that our framework performs the best when it selects the reasoning paths with L = 3, showing 63.0 EM score. Based on these observations, we expect the retriever favors a shorter path, while the reader tends to select a longer and more convincing multi-hop reasoning path to derive an answer string.</p><p>Qualitative examples of retrieved reasoning paths Finally, we show two examples from Hot-potQA full wiki, and Appendix C.5 presents more qualitative examples. In <ref type="figure">Figure 3</ref>, our approach successfully retrieves the correct reasoning path and answers correctly, while Re-rank fails. The top two paragraphs next to the graph are the introductory paragraphs of the two entities on the reasoning path, and the paragraph at the bottom shows the wrong paragraph selected by Re-rank. The "Millwall F.C." has fewer lexical overlaps and the bridge entity "Millwall" is not stated in the given question. Thus, Re-rank chooses a wrong paragraph with high lexical overlaps to the given question.</p><p>In <ref type="figure">Figure 4</ref>, we compare the reasoning paths ranked highest by our retriever and reader. Although the gold path is included among the top 8 paths selected by the beam search, our retriever model selects a wrong paragraph as the best reasoning path. By re-ranking the reasoning paths, the reader eventually selects the correct reasoning path ("2017-18 Wigan Athletic F.C. season" ? "EFL Cup"). This example shows the effectiveness of the strong interplay of our retriever and reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper introduces a new graph-based recurrent retrieval approach, which retrieves reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model learns to sequentially retrieve evidence paragraphs to form the reasoning path. Subsequently, our reader model re-ranks the reasoning paths, and it determines the final answer as the one extracted from the best reasoning path. Our experimental results significantly advance the state of the art on HotpotQA by more than 14 points absolute gain on the full wiki setting. Our approach also achieves the state-of-the-art performance on SQuAD Open and Natural Questions Open without any architectural changes, demonstrating the robustness of our method. Our method provides insights into the underlying entity relationships, and the discrete reasoning paths are helpful in interpreting our framework's reasoning process. Future work involves end-to-end training of our graph-based recurrent retriever and reader for improving upon our current two-stage training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We acknowledge grants from ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS1616112, IIS1252835), and Samsung GRO. We thank Sewon Min, David Wadden, Yizhong Wang, Akhilesh Gotmare, Tong Niu, and UW NLP group and Salesforce research members for their insightful discussions. We would also like to show our gratitude to Melvin Gruesbeck for providing us with the artistic figures presented in this paper. We thank the anonymous reviewers for their helpful and thoughtful comments. Akari Asai is supported by The Nakajima Foundation Fellowship. </p><formula xml:id="formula_9">a t+1 = W r [h t ; w i ] + b r , h t+1 = ? a t+1 a t+1 ,<label>(9)</label></formula><p>where W r ? R d?2d is a weight matrix, b r ? R d is a bias vector, and ? ? R 1 is a scalar parameter (initialized with 1.0). We set the global initial state a 1 to a parameterized vector s ? R d , and we also parameterize an [EOE] vector w [EOE] ? R d for the [EOE] symbol. The use of w i for both the input and output layers is inspired by Inan et al. <ref type="formula" target="#formula_5">(2017)</ref>; <ref type="bibr" target="#b26">Press &amp; Wolf (2017)</ref>. In addition, we align the norm of w <ref type="bibr">[EOE]</ref> with those of w i , by applying layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> of the last layer in BERT because w <ref type="bibr">[EOE]</ref> is used along with the BERT outputs. Without the layer normalization, the L2-norms of w i and w <ref type="bibr">[EOE]</ref> can be quite different, and the model can easily discriminate between them by the difference of the norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 QUESTION-PARAGRAPH ENCODING IN OUR RETRIEVER COMPONENT</head><p>Equation <ref type="formula">(2)</ref> shows that we compute each paragraph representation w i conditioned on the question q. An alternative approach is separately encoding the paragraphs and the question, to directly retrieve paragraphs <ref type="bibr" target="#b33">Seo et al., 2019;</ref>. However, due to the lack of explicit interactions between the paragraphs and the question, such a neural retriever using questionindependent paragraph encodings suffers from compressing the necessary information into fixeddimensional vectors, resulting in low performance on entity-centric questions . It has been shown that attention-based paragraph-question interactions improve the retrieval accuracy if the retrieval scale is tractable <ref type="bibr" target="#b36">(Wang et al., 2018a;</ref><ref type="bibr" target="#b14">Lee et al., 2018)</ref>. There is a trade-off between the scalability and the accuracy, and this work aims at striking the balance by jointly using the lexical matching retrieval and the graphs, followed by the rich question-paragraph encodings.</p><p>A question-independent variant We can also formulate our retriever model by using a questionindependent approach. There are only two simple modifications. First, we reformulate Equation <ref type="formula">(2)</ref> as follows:</p><formula xml:id="formula_10">w i = BERT [CLS] (p i ),<label>(10)</label></formula><p>where we no longer input the question q together with the paragraphs. Next, we condition the initial RNN state h 1 on the question information. More specifically, we compute h 1 by using Equation (4) as follows:</p><formula xml:id="formula_11">w q = BERT [CLS] (q),<label>(11)</label></formula><formula xml:id="formula_12">h 1 = RNN(h 1 , w q ),<label>(12)</label></formula><p>where w q is computed by using the same BERT encoder as in Equation <ref type="formula" target="#formula_10">(10)</ref>, and h 1 is the original h 1 used in our question-dependent approach as described in Appendix A.1. The remaining parts are exactly the same, and we can perform the reasoning path retrieval in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 WHY IS THE INTERPLAY IMPORTANT?</head><p>Our retriever model learns to predict plausibility of the reasoning paths by capturing the paragraph interactions through the BERT's [CLS] representations, after independently encoding the paragraphs along with the question; this makes our retriever scalable to the open-domain scenario. By contrast, our reader jointly learns to predict the plausibility and answer the question, and moreover, fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths; this paragraph interaction is crucial for multi-hop reasoning <ref type="bibr" target="#b35">(Wang et al., 2019a)</ref>. In summary, our retriever is scalable, but the top-1 prediction is not always enough to fully capture multi-hop reasoning to answer the question. Therefore, the additional re-ranking process mitigates the uncertainty and makes our framework more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HANDLING YES-NO QUESTIONS IN OUR READER COMPONENT</head><p>In the HotpotQA dataset, we need to handle yes-no questions as well as extracting answer spans from the paragraphs. We treat the two special types of the answers, yes and no, by extending the re-ranking model in Equation <ref type="formula" target="#formula_4">(6)</ref>. In particular, we extend the binary classification to a multi-class classification task, where the positive "answerable" class is decomposed into the following three classes: span, yes, and no. If the probability of "yes" or "no" is the largest among the three classes, our reader directly outputs the label as the answer, without any span extraction. Otherwise, our reader uses the span extraction model to output the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 SUPPORTING FACT PREDICTION IN HOTPOTQA</head><p>We adapt our recurrent retriever to the subtask of the supporting fact prediction in HotpotQA <ref type="bibr">(Yang et al., 2018)</ref>. The task is outputting sentences which support to answer the question. Such supporting sentences are annotated for the two ground-truth paragraphs in the training data. Since our framework outputs the most plausible reasoning path E along with the answer, we can add an additional step to select supporting facts (sentences) from the paragraphs in E. We train our recurrent retriever by using the training examples for the supporting fact prediction task, where the model parameters are not shared with those of our paragraph retriever. We replace the question-paragraph encoding in Equation <ref type="formula">(2)</ref> with question-answer-sentence encoding for the task, where a question string is concatenated with its answer string. The answer string is the ground-truth one during the training time. We then maximize the probability of selecting the ground-truth sequence of the supporting fact sentences, while setting the other sentences as negative examples. At test time, we use the best reasoning path and its predicted answer string from our retriever and reader models to finally output the supporting facts for each question. The supporting fact prediction task is performed after finalizing the reasoning path and the answer for each question, and hence this additional task does not affect the QA accuracy. Section 3.1.2 describes our training strategy for our recurrent retriever. We apply the data augmentation technique to HotpotQA and Natural Questions to consider multi-hop reasoning. To derive the ground-truth reasoning path g, we use the ground-truth evidence paragraphs associated with the questions in the training data for each dataset. For SQuAD and Natural Questions Open, each training example has only single paragraph p, and thus it is trivial to derive g as [p, <ref type="bibr">[EOE]</ref>]. For the multi-hop case, HotpotQA, we have two ground-truth paragraphs p 1 , p 2 for each question. Assuming that p 2 includes the answer string, we set g = [p 1 , p 2 , [EOE]].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 DETAILS ABOUT NEGATIVE EXAMPLES FOR OUR READER MODEL IN SQUAD OPEN AND NATURAL QUESTIONS OPEN</head><p>To train our reader model for SQuAD Open, in addition to the TF-IDF top-ranked paragraphs, we add two types of additional negative examples: (i) paragraphs, which do not include the answer string, from the originally annotated articles, and (ii) "unanswerable" questions from SQuAD 2.0 <ref type="bibr" target="#b30">(Rajpurkar et al., 2018)</ref>. For Natural Questions Open, we add negative examples of the type (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 TRAINING SETTINGS</head><p>To use the pre-trained BERT models, we used the public code base, pytorch-transformers, 4 written in PyTorch. 5 For optimization, we used the code base's implementation of the Adam optimizer <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref>, with a weight-decay coefficient of 0.01 for non-bias parameters. A warm-up strategy in the code base was also used, with a warm-up rate of 0.1. Most of the settings follow the default settings. To train our recurrent retriever, we set the learning rate to 3 ? 10 ?5 , and the maximum number of the training epochs to three. The mini-batch size is four; a mini-batch example consists of a question with its corresponding paragraphs. To train our reader model, we set the learning rate to 3 ? 10 ?5 , and the maximum number of training epochs to two. Empirically we observe better performance with a larger batch size as discussed in previous work <ref type="bibr" target="#b17">(Liu et al., 2019;</ref><ref type="bibr" target="#b25">Ott et al., 2018)</ref>, and thus we set the mini-batch size to 120. A mini-batch example consists of a question with its evidence paragraphs. We will release our code to follow our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 THE WIKIPEDIA DUMPS FOR EACH DATASET</head><p>For HotpotQA full wiki, we use the pre-processed English Wikipedia dump from October, 2017, provided by the HotpotQA authors. 6 For Natural Questions Open, we use the English Wikipedia dump from December 20, 2018, following  and <ref type="bibr" target="#b19">Min et al. (2019a)</ref>. For SQuAD Open, we use the Wikipedia dump provided by <ref type="bibr" target="#b1">Chen et al. (2017)</ref>.</p><p>Although using a single dump for different open-domain QA datasets is a common practice <ref type="bibr" target="#b1">(Chen et al., 2017;</ref><ref type="bibr" target="#b36">Wang et al., 2018a;</ref><ref type="bibr" target="#b14">Lee et al., 2018)</ref>, this potentially causes inconsistent or even unfair evaluation across different experimental settings, due to the temporal inconsistency of the Wikipedia articles. More concretely, every Wikipedia article is editable and and as a result, a fact can be rephrased or could be removed. For instance, a question from the SQuAD development set, "Where does Kenya rank on the CPI scale?" is originally paired with a paragraph from the article of Kenya. Based on a single sentence "Kenya ranks low on Transparency International's Corruption Perception Index (CPI)" from the paragraph, an annotated answer span is "low." However, this sentence has been rewritten as "Kenya has a high degree of corruption according to Transparency International's Corruption Perception Index (CPI)" in a later version of the same article. 7 This is problematic considering the major evaluation metrics based on string matching.</p><p>Another problem exists especially in Natural Questions Open. The dataset contains real Google search queries, and some of them reflect temporal trends at the time when the queries were executed. If a query is related to a TV show broadcasted in 2018, we can hardly expect to extract the answer from a dump in 2017.</p><p>Like this, although Wikipedia is a useful knowledge source for open-domain QA research, its rapidly evolving nature should be considered more carefully for the reproducibility. We will make all of the data including pre-processed Wikipedia articles for each experiment available for future research.</p><p>B.6 DETAILS ABOUT INITIAL CANDIDATES C 1 SELECTION To retrieve the initial candidates C 1 for each question, we use a TF-IDF based retriever with the bi-gram hashing <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>. For HotpotQA full wiki, we retrieve top F introductory paragraphs, for each question, from a corpus including all the introductory paragraphs. For SQuAD Open and Natural Questions Open, we first retrieve 50 Wikipedia articles through the same TF-IDF retriever, and further run another TF-IDF-based paragraph retriever <ref type="bibr" target="#b2">(Clark &amp; Gardner, 2018;</ref><ref type="bibr" target="#b19">Min et al., 2019a)</ref> to retrieve F paragraphs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 DETAILS ABOUT ENTITY LINKING EXPERIMENT</head><p>We experiment with a variant of our approach, where we incorporate an entity linking system with our framework, in place of the Wikipedia hyperlinks. In this experiment, we first retrieve seed paragraphs using TF-IDF (F = 100), and run an off-the-shelf entity linker (TagMe by <ref type="bibr" target="#b7">Ferragina &amp; Scaiella (2011)</ref>) over the paragraphs. If the entity linker detects some entities, we retrieve their corresponding Wikipedia articles, and add edges from the seed paragraphs to the entity-linked paragraphs. Once we build the graph, then we re-run all of the experiments while the other components are exactly the same. We use the TagMe official Python wrapper. 8 C ADDITIONAL RESULTS ON HOTPOTQA C.1 UPPER-BOUND OF OUR RETRIEVAL MODULE For scalability and computational efficiency, we bootstrap our retrieval module with TF-IDF retrieval; we first retrieval F paragraphs using TF-IDF with the method described in Section B.6 and initialize C 1 with these TF-IDF paragraphs. Although we expand our candidate paragraphs at each time step using the Wikipedia graph, if our method failed to retrieve paragraphs a few-hops away from the answer paragraphs, it is likely to fail to reach the answer paragraphs. To estimate the paragraph EM upper-bound, we have checked if two gold paragraphs are included in the top 20 TF-IDF paragraphs and their hyperlinked paragraphs in the HotpotQA full wiki setting. We found that for 75.4% of the questions, all of the gold paragraphs are included in the collections of the TF-IDF paragraphs and the hyperlinked paragraphs. Also, it should be noted when we only consider the TF-IDF retrieval results, the upper-bound drops to 35.1%, which suggests that the TF-IDF-based retrieval cannot effectively discover the paragraphs multi-hop away due to the few lexical overlap. When we increase the number of F to 100 and 500, the upper-bound reaches 84.1% and 89.2%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 PER-CATEGORY QUESTION ANSWERING AND RETRIEVAL PERFORMANCE ON HOTPOTQA FULL WIKI</head><p>In HotpotQA, there are two types of questions, bridge and comparison. While comparison-type questions explicitly mention the two entities related to the given questions, in bridge-type questions, the bridge entities are rarely explicitly stated. This makes it hard for a retrieval system to discover the paragraphs entailed by the bridge entities only.   gain and 15.1 EM gain over Semantic Retrieval for the challenging bridge-type questions. For the comparison-type questions, our method achieves almost 10 point higher QA EM than Semantic Retrieval. We observed that some of the comparison-type questions can be answered based on single paragraph, and thus our model selects only one paragraph for some of these comparisontype questions, resulting in lower P EM scores on the comparison-type questions. We show several examples of the questions where we can answer based on single paragraph in Section C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 ON THE ROBUSTNESS TO THE INCREASE OF THE PARAGRAPHS</head><p>As we discussed in 3.1.1, we aim at significantly reducing the search space and thus scaling the number of initial TF-IDF candidates. Increasing the number of the initial retrieved paragraphs often improves the recall of the evidence paragraphs of the datasets. On the other hand, increasing the candidate paragraphs introduces additional noises, may distract models, and eventually hurt the performance <ref type="bibr" target="#b12">(Kratzwald &amp; Feuerriegel, 2018)</ref>. We compare the performance of three different approaches: (i) ours, (ii) ours (greedy, without reasoning path re-ranking), and (iii) Re-rank.</p><p>We increase the number of the TF-IDF-based retrieved paragraphs from 10 to 500 (For Re-rank, we compare the performance up to 200 paragraphs). <ref type="figure" target="#fig_3">Figure 5</ref> clearly shows that our approach is robust towards the increase of the initial candidate paragraphs, and thus can constantly yield performance gains with more candidate paragraphs. Our approach with the greedy search also shows performance improvements; however, after a certain number, the greedy approach stops improving the performance. Re-rank starts suffering from the noises caused by many distracting paragraphs included in the initial candidate paragraphs at F = 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 RESULTS OF QUESTION-INDEPENDENT PARAGRAPH ENCODING FOR OUR RETRIEVER</head><p>To show the importance of the question-paragraph encoding in our retriever model, we conduct an experiment on the development set of HotpotQA, by replacing it with the question-independent encoding described in Appendix A.2. For a fair comparison, we use the same initial TF-IDF-based retrieval (only for the full wiki setting), hyperlink-based Wikipedia graph, beam search, and reader model (BERT wwm). We train the alternative model without using the data augmentation technique (described in Section 3.  <ref type="table" target="#tab_1">Table 11</ref>: Effects of the question-dependent paragraph encoding: Comparing our retriever model with and without the query-dependent encoding. For our question-dependent approach, the full wiki results correspond to "retriever, no link-based negatives" in <ref type="table" target="#tab_9">Table 6</ref>, and the distractor results correspond to "Ours (Reader: BERT wwm)" <ref type="table" target="#tab_1">Table 1</ref>, to make the results comparable. We can also see that the performance drop on the distractor setting is much smaller than that on the full wiki setting. This is due to its closed nature; for each question, we are given only ten paragraphs and the two gold paragraphs are always included, which significantly narrows the searching space down and makes the retrieval task much easier than that in the full wiki setting. Therefore, our recurrent retriever model is likely to discover the gold reasoning paths by the beam search, and our reader model can select the gold paths by the robust re-ranking approach. To verify this hypothesis, we checked the P EM score as a retrieval accuracy in the distractor setting. If we only consider the top-1 path from the beam search, the P EM score of the question-independent model is 12% lower than that of our question-dependent model. However, if we consider all the reasoning paths produced by the beam search, the coverage of the gold paths is almost the same. As a result, our reader model can perform similarly with both the question-dependent/independent approaches. This additionally shows the robustness of our re-ranking approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 MORE QUALITATIVE ANALYSIS ON THE REASONING PATH ON HOTPOTQA FULL WIKI</head><p>In this section, we conduct more qualitative analysis on the reasoning paths predicted by our model. Explicitly retrieving plausible reasoning paths and re-ranking the paths provide us interpretable insights into the underlying entity relationships used for multi-hop reasoning.</p><p>As shown in <ref type="table" target="#tab_12">Table 9</ref>, our model flexibly selects one or more paragraphs for each question. To understand these behaviors, we conduct qualitative analysis on these examples whose reasoning paths are shorter or longer than the original gold reasoning paths.</p><p>Reasoning path only with single paragraph First, we show two examples (one is a bridge-type question and the other is a comparison-type question), where our retriever selects single paragraph and terminates without selecting any additional paragraphs.</p><p>The bridge-type question in <ref type="table" target="#tab_1">Table 12</ref> shows that, while originally this question requires a system to read two paragraphs, Before I Go to Sleep (film) and Nicole Kidman, our retriever and reader eventually choose Nicole Kidman only. The second paragraph has a lot of lexical overlaps to the given question, and thus, a system may not need to read both of the paragraphs to answer.</p><p>The comparison-type question in <ref type="table" target="#tab_1">Table 12</ref> also shows that even comparison-type questions do not always require two paragraphs to answer the questions, and our model only selects one paragraph necessary to answer the given example question. In this example, the question has large lexical overlap with one of the ground-truth paragraph (The Bears and I), resulting in allowing our model to answer the question based on the single paragraph. <ref type="bibr" target="#b20">Min et al. (2019b)</ref> also observed that some of the questions do not necessarily require multi-hop reasoning, while HotpotQA is designed to require multi-hop reasoning <ref type="bibr">(Yang et al., 2018)</ref>. In that sense, we can say that our method automatical detects potentially single-hop questions.</p><p>Reasoning path with three paragraphs All of the HotpotQA questions are authored by annotators who are shown two relevant paragraphs, and thus, originally the length of ground-truth reason- <ref type="table" target="#tab_1">Table 12</ref>: Two examples of the questions that our model retrieves a reasoning path with only one paragraph. We partly remove sentences irrelevant to the questions. Words in red correspond to the answer strings.</p><p>ing paths is always two. On the other hand, as our model accommodates arbitrary steps of reasoning, it often selects reasoning paths longer than the original annotations as shown in <ref type="table" target="#tab_12">Table 9</ref>. When our model selects a longer reasoning path for a HotpotQA question, does it contain paragraphs that provide additional evidence? We show an example in <ref type="table" target="#tab_1">Table 13</ref>, so as to answer this question. Our model selects an additional paragraph, Blue Jeans (Lana Del Rey song) at the first step, and then selects the two annotated gold paragraphs. This first paragraph is strongly relevant to the given question, but does not contain the answer. This additional evidence might help the reader to find the correct bridge entity ("Back to December").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 QUALITATIVE ANALYSIS ON THE REASONING PATH ON HOTPOTQA DISTRACTOR</head><p>Although the main focus in this paper is on open-domain QA, we show the state-of-the-art performance on the HotpotQA distractor setting as well with the exactly same architecture. We conduct qualitative analysis to understand our model's behavior in the closed setting. In this setting, the two ground-truth paragraphs are always given for each question. <ref type="table" target="#tab_1">Table 14</ref> shows two examples from the HotpotQA distractor setting. In the first example, P1 and P2 are its corresponding ground-truth paragraphs. At the first time step, our retriever does not expect that P2 is related to the evidence to answer the question, as the retriever is not aware of the bridge entity, "Pasek &amp; Paul". If we simply adopt the Re-rank strategy, P3 with the second highest probability is selected, resulting in a wrong paragraph selection. In our framework, our retriever is conditioned on the previous retrieval history and thus, at the second time step, it chooses the correct paragraph, P2, lowering the probability of P3. This clearly shows the effectiveness of our multi-step retrieval method in the closed setting as well. At the third step, our model stops the prediction by   <ref type="table" target="#tab_1">Table 15</ref>: Statistics of the reasoning paths for SQuAD Open and Natural Questions Open: the average length and the distribution of length of the reasoning paths selected by our retriever and reader for SQuAD Open and Natural Questions Open.</p><p>outputting <ref type="bibr">[EOS]</ref>. In 588 examples (7.9%) of the entire distractor development dataset, the paragraph selection by our graph-based recurrent retriever differs from the top-2 strategy.</p><p>We present another example, where only the graph-based recurrent retrieval model succeeds in finding the correct paragraph pair, (P1, P2). The second question in <ref type="table" target="#tab_1">Table 14</ref> shows that at the first time step our retriever successfully selects P1, but does not pay attention to P2 at all, as the retriever is not aware of the bridge entity, "the Russian Civil War". Again, once it is conditioned on P1, which includes the bridge entity, it can select P2 at the second time step. Like this, we can see how our model successfully learns to model relationships between paragraphs for multi-hop reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS ON SQUAD OPEN AND NATURAL QUESTIONS OPEN</head><p>Although the main focus of this work is on multi-hop open-domain QA, our framework shows competitive performance on the two open-domain QA datasets, SQuAD Open and Natural Questions Open. Both of the two dataets are originally created by assigning a single ground-truth paragraph for each question, and in that sense, our framework is not specific to multi-hop reasoning tasks. In this section, we further analyze our experimental results on the two datasets. <ref type="table" target="#tab_1">Table 15</ref> shows statistics of the lengths of the selected reasoning paths on our SQuAD Open experiment. This table is analogous to <ref type="table" target="#tab_12">Table 9</ref> on our HotpotQA experiments. We can clearly see that our recurrent retriever always outputs a single paragraph for each question, if we only use the top-1 predictions. This is because our retriever model for this dataset is trained with the single-paragraph annotations. Our beam search can find longer reasoning paths, and as a result, the re-ranking process in our reader model somtimes selects the reasoning paths including two paragraphs. The trend is consistent with that in <ref type="table" target="#tab_12">Table 9</ref>. However, the effects of selecting more than one paragraph do not have a big impact; we observed only 0.1% F1/EM improvement over our method with restricting the path length to one (based on the same experiment with L = 1 in <ref type="table" target="#tab_11">Table 8</ref>). Considering that SQuAD is a single-hop QA dataset, the result matches our intuition. <ref type="table" target="#tab_1">Table 15</ref> also shows the results on Natural Questions Open, where we see the same trend again. Thanks to the ground-truth path augmentation technique, our recurrent retriever model prefers longer reasoning paths than those on SQuAD Open. We observed 1% EM improvement over the L = 1 baseline on Natural Questions Open, and next we show an example to discuss why our reasoning path approach can be effective on this dataset. <ref type="table" target="#tab_1">Table 16</ref> shows one example where our model finds a multi-hop reasoning path effectively in Natural Questions Open (development set). The question "who sang the original version of killing me so" has relatively fewer lexical overlap with the originally annotated paragraph (Killing Me Softly with His Song (V) in <ref type="table" target="#tab_1">Table 16</ref>). Moreover, there are several entities named as "killing me softly" in Wikipedia, because many artists cover the song. To answer this question correctly, our retriever first selects Roberta Flack (I), and then hops to the originally annotated paragraph, Killing Me Softly with His Song (V). Our reader further verifies this reasoning path and extracts the correct answer from Killing Me Softly with His Song (V). This example shows that even without gold <ref type="table" target="#tab_1">Table 16</ref>: An example from Natural Questions Open. The bold text represents titles and paragraph indices (e.g., (I) denotes that the paragraph is an introductory paragraph). The highlighted phrase represents a bridge entity and the text in red represents an answer span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD Open</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Questions Open</head><p>reasoning paths annotations, our model trained on the augmented examples learns to retrieve multihop reasoning paths from the entire Wikipedia.</p><p>These detailed experimental results on the two other open-domain QA datasets demonstrate that our framework learns to retrieve reasoning paths flexibly with evidence sufficient to answer a given question, according to each dataset's nature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of open-domain multi-hop question from HotpotQA. Paragraph 2 is unlikely to be retrieved using TF-IDF retrievers due to little lexical overlap to the given question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Cognitive Graph (Ding et al., 2019) that uses the provided prediction results of the Cognitive Graph model on the HotpotQA development dataset. Semantic Retrieval (Nie et al., 2019) that uses the provided prediction results of the state-of-the-art Semantic Retrieval model on the HotpotQA development dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Robustness to the increase of F . We compare the F1 scores of our model, our model without a beam search and Re-rank with different number of F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>P1:</head><label></label><figDesc>A Christmas Story: The Musical is a musical version of the film "A Christmas Story ... The musical has music and lyrics written by Pasek &amp; Paul and the book by Joseph Robinette. 0.98 0.00 0.00 P2: Benj Pasek and Justin Paul, known together as Pasek and Paul, are an American songwriting duo and composing team for musical theater, films, and television. ... they won both the Golden Globe and Academy Award for Best Original Song for the song "City of Stars". 0.08 0.89 0.00 P3: La La Land" is a song recorded by American singer Demi Lovato. It was written by Lovato, Joe Jonas, Nick Jonas and Kevin Jonas and produced by the Jonas Brothers alongside John Fields, for Lovato's debut studio album, "Dont Forget" (2008). 0.12 0.00 0.00 Q: Alexander Kerensky was defeated and destroyed by the Bolsheviks in the course of a civil war that ended when ? P1: The Socialist Revolutionary Party, or Party of Socialists-Revolutionaries sery") was a major political party in early 20th century Russia and a key player in the Russian Revolution. ... The anti-Bolshevik faction of this party, known as the Right SRs, which remained loyal to the Provisional Government leader Alexander Kerensky was defeated and destroyed by the Bolsheviks in the course of the Russian Civil War and subsequent persecution. 0.95 0.00 0.00 P2: The Russian Civil War (November 1917 October 1922) was a multi-party war in the former Russian Empire immediately after the Russian Revolutions of 1917, as many factions vied to determine Russia? political future. 0.00 0.87 0.00 P3: Alexander Fyodorovich Kerensky was a Russian lawyer and key political figure in the Russian Revolution of 1917. 0.08 0.09 0.00</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>HotpotQA development set results: QA and SP (supporting fact prediction) results on HotpotQA's full wiki and distractor settings. "-" denotes no results are available.4.2 OVERALL RESULTSTable 1compares our method with previous published methods on the HotpotQA development set.Our method significantly outperforms all the previous results across the evaluation metrics under both the full wiki and distractor settings. Notably, our method achieves 14.5 F1 and 14.0 EM gains compared to state-of-the-art Semantic Retrieval<ref type="bibr" target="#b22">(Nie et al., 2019)</ref> and 10.9 F1 gains over the concurrent Transformer-XH model (Zhao et al., 2020) on full wiki.</figDesc><table><row><cell>2019)</cell><cell cols="4">40.4 31.1 47.7 17.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DFGN (Xiao et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">69.2 55.4</cell><cell>-</cell><cell>-</cell></row><row><cell>QFE (Nishida et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">68.7 53.7 84.7 58.8</cell></row><row><cell>Baseline (Yang et al., 2018)</cell><cell cols="3">34.4 24.7 41.0</cell><cell>5.3</cell><cell cols="4">58.3 44.4 66.7 22.0</cell></row><row><cell>Transformer-XH (Zhao et al., 2020)</cell><cell cols="4">62.4 50.2 71.6 42.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Reader: BERT wwm)</cell><cell cols="8">73.3 60.5 76.1 49.3 81.2 68.0 85.2 58.6</cell></row><row><cell>Ours (Reader: BERT base)</cell><cell cols="8">65.8 52.7 75.0 47.9 73.3 59.4 84.6 57.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 AR, leading to the improvement of 10.3 QA EM over Semantic Retrieval. The significant improvement from</figDesc><table><row><cell>Models</cell><cell>QA</cell><cell></cell><cell>SP</cell><cell></cell></row><row><cell>(*: anonymous)</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell></row><row><cell>Semantic Retrieval</cell><cell cols="4">57.3 45.3 70.8 38.7</cell></row><row><cell>GoldEn Retriever</cell><cell cols="4">48.6 37.9 64.2 30.7</cell></row><row><cell>Cognitive Graph</cell><cell cols="4">48.9 37.1 57.7 22.8</cell></row><row><cell>Entity-centric IR</cell><cell cols="4">46.3 35.4 43.2 0.06</cell></row><row><cell>MUPPET</cell><cell cols="4">40.3 30.6 47.3 16.7</cell></row><row><cell>DecompRC</cell><cell cols="2">40.7 30.0</cell><cell>-</cell><cell>-</cell></row><row><cell>QFE</cell><cell cols="4">38.1 28.7 44.4 14.2</cell></row><row><cell>Baseline</cell><cell cols="3">32.9 24.0 37.7</cell><cell>3.9</cell></row><row><cell>HGN* ?</cell><cell cols="4">69.2 56.7 76.4 50.0</cell></row><row><cell>MIR+EPS+BERT* ?</cell><cell cols="4">64.8 52.9 72.0 42.8</cell></row><row><cell>Transformer-XH*</cell><cell cols="4">60.8 49.0 70.0 41.7</cell></row><row><cell>Ours</cell><cell cols="4">73.0 60.0 76.4 49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>HotpotQA full wiki test set results: official leaderboard results (on November 6, 2019) on the hidden test set of the HotpotQA full wiki setting. Work marked with ? appeared after September 25.</figDesc><table><row><cell>Models</cell><cell>F1</cell><cell>EM</cell></row><row><cell>multi-passage (Wang et al., 2019b)</cell><cell cols="2">60.9 53.0</cell></row><row><cell>ORQA (Lee et al., 2019)</cell><cell>-</cell><cell>20.2</cell></row><row><cell>BM25+BERT (Lee et al., 2019)</cell><cell>-</cell><cell>33.2</cell></row><row><cell>Weaver (Raison et al., 2018)</cell><cell>-</cell><cell>42.3</cell></row><row><cell>RE 3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>SQuAD Open results: we report F1 and EM scores on the test set of SQuAD Open, following previous work.</figDesc><table><row><cell></cell><cell>EM</cell></row><row><cell>Models</cell><cell>Dev Test</cell></row><row><cell>ORQA (Lee et al., 2019)</cell><cell>31.3 33.3</cell></row><row><cell>Hard EM (Min et al., 2019a)</cell><cell>28.8 28.1</cell></row><row><cell cols="2">BERT + BM 25 (Lee et al., 2019) 24.8 26.5</cell></row><row><cell>Ours</cell><cell>31.7 32.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Natural Questions Open results: we report EM scores on the test and development sets of Natural Questions Open, following previous work.</figDesc><table><row><cell>Models</cell><cell>AR</cell><cell>PR</cell><cell cols="2">P EM EM</cell></row><row><cell>Ours (F = 20)</cell><cell cols="2">87.0 93.3</cell><cell>72.7</cell><cell>56.8</cell></row><row><cell>TF-IDF</cell><cell cols="2">39.7 66.9</cell><cell>10.0</cell><cell>18.2</cell></row><row><cell>Re-rank</cell><cell cols="2">55.1 85.9</cell><cell>29.6</cell><cell>35.7</cell></row><row><cell>Re-rank 2hop</cell><cell cols="2">56.0 70.1</cell><cell>26.1</cell><cell>38.8</cell></row><row><cell>Entity-centric IR</cell><cell cols="2">63.4 87.3</cell><cell>34.9</cell><cell>42.0</cell></row><row><cell>Cognitive Graph</cell><cell cols="2">76.0 87.6</cell><cell>57.8</cell><cell>37.6</cell></row><row><cell cols="3">Semantic Retrieval 77.9 93.2</cell><cell>63.9</cell><cell>46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Retrieval evaluation: Comparing our retrieval method with other methods across Answer Recall, Paragraph Recall, Paragraph EM, and QA EM metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study: evaluating different variants of our model on HotpotQA full wiki.</figDesc><table><row><cell>Settings (F = 100)</cell><cell>F1</cell><cell>EM</cell></row><row><cell>with hyperlinks</cell><cell cols="2">72.4 59.5</cell></row><row><cell cols="3">with entity linking system 70.1 57.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance with different link structures: comparing our results on the Hotpot QA full wiki development set when we use an off-the-shelf entity linking system instead of the Wikipedia hyperlinks.</figDesc><table><row><cell cols="2">Settings (F = 100)</cell><cell>F1</cell><cell>EM</cell></row><row><cell cols="2">Adaptive retrieval</cell><cell cols="2">72.4 59.5</cell></row><row><cell></cell><cell cols="3">L = 1 45.8 35.5</cell></row><row><cell>L-step retrieval</cell><cell cols="3">L = 2 71.4 58.5 L = 3 70.1 57.7</cell></row><row><cell></cell><cell cols="3">L = 4 66.3 53.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell>F = 100)</cell><cell cols="2">Retriever Reader</cell><cell>EM</cell></row><row><cell>Avg. # of L</cell><cell>1.96</cell><cell>2.21</cell><cell>with L</cell></row><row><cell>1</cell><cell>539</cell><cell>403</cell><cell>31.2</cell></row><row><cell>2</cell><cell>6,639</cell><cell>5,655</cell><cell>60.0</cell></row><row><cell>3</cell><cell>227</cell><cell>1,347</cell><cell>63.0</cell></row></table><note>Performance with different reason- ing path length: comparing the performance with different path length on HotpotQA full wiki. L-step retrieval sets the number of the reasoning steps to a fixed number.(</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Statistics of the reasoning paths: the average length and the distribution of length of the reasoning paths selected by our retriever and reader for HotpotQA full wiki. Avg. EM repre- sents QA EM performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc>When was the football club founded in which Walter Otto Davis played at centre forward?Millwall Football Club is a professional football club in South East London, ? Founded as Millwall Rovers in 1885.</figDesc><table><row><cell>foootballer</cell><cell></cell><cell></cell></row><row><cell>Millwall F.C.</cell><cell></cell><cell>EFL Cup</cell></row><row><cell></cell><cell></cell><cell>FA Cup</cell></row><row><cell>Welsh</cell><cell>Tranmere Rovers Football Club is an</cell><cell></cell></row><row><cell>Football club</cell><cell>English professional association football club founded in 1884, and based in</cell><cell></cell></row><row><cell></cell><cell>Welsh Birkenhead, Wirral.</cell><cell></cell></row><row><cell>Walter Davis (footballer)</cell><cell>Walter Otto Davis was a Welsh professional footballer who played at</cell><cell>EFL League One</cell></row><row><cell></cell><cell>centre forward for Millwall for ten years</cell><cell></cell></row><row><cell>Centre forward</cell><cell>in the 1910s.</cell><cell></cell></row><row><cell></cell><cell>Top Two Paragraphs Selected by Re-rank</cell><cell>Selected as Best Path by Retriever</cell></row><row><cell cols="2">Figure 3: Reasoning examples by our model (two</cell><cell></cell></row><row><cell cols="2">paragraphs connected by a dotted line) and Re-rank</cell><cell></cell></row><row><cell cols="2">(the bottom two paragraphs). Highlighted text de-</cell><cell></cell></row><row><cell cols="2">notes a bridge entity, and blue-underlined text rep-</cell><cell></cell></row><row><cell>resents hyperlinks.</cell><cell></cell><cell></cell></row></table><note>also presents the EM scores averaged over the questions with certain length of reasoning paths</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with BERTserini. In NAACL (Demonstrations), 2019.</figDesc><table><row><cell>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,</cell></row><row><cell>and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question</cell></row><row><cell>answering. In EMNLP, 2018.</cell></row><row><cell>Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary.</cell></row><row><cell>Transformer-XH: Multi-hop question answering with extra hop attention. In ICLR, 2020.</cell></row><row><cell>APPENDIX</cell></row><row><cell>A DETAILS ABOUT MODELING</cell></row><row><cell>A.1 A NORMALIZED RNN</cell></row><row><cell>We decompose Equation (4) as follows:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>DATASET DETAILS OF HOTPOTQA, SQUAD OPEN AND NATURAL QUESTIONS OPEN HotpotQA The HotpotQA training, development, and test datasets contain 90,564, 7,405 and 7,405 questions, respectively. To train our retriever model for the distractor setting, we use the distractor training data, where only the original ten paragraphs are associated with each question. The retriever model trained with this setting is also used in our ablation study as "retriever, no linkbased negatives" inTable 6. For the full wiki setting, we train our retriever model with the data augmentation technique and the additional negative examples described in Section 3.1.2. We use the same reader model, for both the settings, trained with the augmented additional references andthe negative examples described in Section 3.2. For both the SQuAD Open and Natural Questions Open, we train our reader on the original examples with the augmented additional negative examples and the distantly supervised examples described in Section 3.2. B.2 DERIVING GROUND-TRUTH REASONING PATHS</figDesc><table><row><cell>B DETAILS ABOUT EXPERIMENTS</cell></row><row><cell>B.1</cell></row></table><note>SQuAD Open and Natural Questions Open For SQuAD Open, we use the original training set (containing 78,713 questions) as our training data, and the original development set (containing 10,570 questions) as our test data. For Natural Questions Open, we follow the dataset splits provided by Min et al. (2019a), and the training, development and test datasets contain 79,168, 8,757 and 3,610, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Retrieval evaluation: Comparing our retrieval method with other methods across Answer Recall, Paragraph Recall, Paragraph EM, and QA EM metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11</head><label>11</label><figDesc>shows the results in both the full wiki and distractor settings. As seen in this table, the QA F1 and EM performance significantly deteriorates on the full wiki setting, which demonstrates the importance of the question-dependent encoding for complex and entity-centric open-domain question answering.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Two examples from the HotpotQA distractor development set. Highlighted text shows the bridge entities for multi-hop reasoning, and also the words in red denote the predicted answer.</figDesc><table><row><cell></cell><cell cols="2">SQuAD Open</cell><cell cols="2">Natural Questions Open</cell></row><row><cell></cell><cell cols="3">Retriever Reader Retriever</cell><cell>Reader</cell></row><row><cell>Avg. # of L</cell><cell>1.00</cell><cell>1.08</cell><cell>1.23</cell><cell>1.54</cell></row><row><cell>1</cell><cell>10,570</cell><cell>9,759</cell><cell>6,719</cell><cell>4,047</cell></row><row><cell>2</cell><cell>0</cell><cell>811</cell><cell>2,038</cell><cell>4,702</cell></row><row><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Appendix A.2 discusses the motivation, and Appendix C.4 shows results with an alternative approach.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use train/dev/test splits provided by<ref type="bibr" target="#b19">Min et al. (2019a)</ref>, which can be downloaded from https: //drive.google.com/file/d/1qsN5Oyi_OtT2LyaFZFH26vT8Sqjb89-s/view.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/huggingface/pytorch-transformers. 5 https://pytorch.org/. 6 https://hotpotqa.github.io/wiki-readme.html. 7 https://en.wikipedia.org/wiki/Kenya on October 25, 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/marcocor/tagme-python</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We evaluate the question answering and paragraph retrieval performance for each of the two question types. We compare the PR, P EM and QA EM for each of the two categories with two state-of-theart models, Cognitive Graph <ref type="bibr" target="#b5">(Ding et al., 2019)</ref> and Semantic Retrieval <ref type="bibr" target="#b22">(Nie et al., 2019)</ref>. Here, we set our initial TF-IDF number F to 500. <ref type="table">Table 10</ref> shows that our retriever yields 16.5 P EM</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-step retrieverreader interaction for scalable open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-hop paragraph retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate annotation of short texts with wikipedia pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE software</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="75" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-step entitycentric information retrieval for multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Kavarthapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrieve, read, rerank: Towards endto-end multi-document reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive document retrieval for deep question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kratzwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Feuerriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. TACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ranking paragraphs for improving answer recall in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miyoung</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising distantly supervised opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and robust question answering from minimal context over documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discrete hard em approach for weakly supervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional questions do not necessitate multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-hop reading comprehension through question decomposition and rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagata</forename><surname>Masaaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Answering complex open-domain questions through iterative query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Weaver: Deep coencoding of questions and documents for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10490</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time open-domain question answering with dense-sparse phrase index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PullNet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Do multi-hop readers dream of reasoning chains?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">R 3 : Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-passage BERT: A globally normalized bert model for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Before I Go to Sleep stars an Australian actress, producer and occasional what? Before I Go to Sleep (film): Before I Go to Sleep is a 2014 mystery psychological thriller film written and directed by Rowan Joff and based on the 2011 novel of the same name by</title>
		<editor>S. J. Watson</editor>
		<imprint>
			<pubPlace>Mark Strong, Colin Firth, and Anne-Marie Duff</pubPlace>
		</imprint>
	</monogr>
	<note>An international co-production between the United Kingdom, the United States, France, and Sweden, the film stars Nicole Kidman</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Buena Vista Distribution? The Bears and I: The Bears and I is a 1974 American drama film directed by Bernard McEveety and written by John Whedon. The film stars Patrick Wayne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Kidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">between The Bears and I and Oceans which was released on July 31</title>
		<editor>Buena Vista Distribution</editor>
		<meeting><address><addrLine>Chief Dan George, Andrew Duggan, Michael Ansara and Robert Pine</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1974-07-31" />
		</imprint>
		<respStmt>
			<orgName>Nicole Mary Kidman</orgName>
		</respStmt>
	</monogr>
	<note>The film was released on</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Annotated reasoning path: The Bears and I, Oceans (film) Predicted reasoning path: The Bears and I Q: Yoann Lemoine, a French video director, has created music videos for Lana Del Rey, Katy Perry, and an orchestral country pop ballad by which top pop artist? Yoann Lemoine: Yoann Lemoine (born 16 March 1983) is a French music video director, graphic designer and singer-songwriter. His most notable works include his music video direction for Katy Perry&apos;s &quot;Teenage Dream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oceans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oceans is a 2009 French nature documentary film directed</title>
		<editor>Jacques Perrin</editor>
		<imprint/>
	</monogr>
	<note>Taylor Swift&apos;s single &quot;Back to December. Lana Del Rey&apos;s &quot;Born to Die&quot; and Mystery Jets&apos; &quot;Dreaming of Another World</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Back to December&quot; is considered an orchestral country pop ballad and its lyrics are a remorseful plea for forgiveness for breaking up with a former lover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">December</forename><surname>Back To</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Back to December&quot; is a song written and recorded by American singer/songwriter Taylor Swift for her third studio album &quot;Speak Now</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">song) ? Yoann Lemoin ? Back to December Table 13: An example question where our model predicts reasoning paths of the length of three. Our model expects that the question is answerable based on the last paragraph of the annotated path. Q: Which songwriting duo composed music for</title>
	</analytic>
	<monogr>
		<title level="m">Annotated reasoning path: Yoann Lemoin ? Back to December Predicted reasoning path: Blue Jeans</title>
		<editor>Blue Jeans (Lana Del Rey song</editor>
		<meeting><address><addrLine>Lana Del Rey; La La Land</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Blue Jeans&quot; reached the top 10 in Belgium, Poland, and Israel. The second was shot and directed by Yoann Lemoine, featuring film noir elements and crocodiles. and created lyrics for &quot;A Christmas Story: The Musica&quot;? Q: who sang the original version of killing me softly</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">) is an American singer. She is known for her No. 1 singles &quot;The First Time Ever I Saw Your Face</title>
		<editor>Roberta Flack (I): Roberta Cleopatra Flack</editor>
		<imprint>
			<date type="published" when="1937-02-10" />
		</imprint>
	</monogr>
	<note>Killing Me Softly with His Song</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The song was written in collaboration with Lori Lieberman, who recorded the song in late 1971</title>
	</analytic>
	<monogr>
		<title level="m">Killing Me Softly with His Song (V)</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
	<note>it became a number -one hit in the US and Canada for Roberta Flack. Many artists have covered the song...</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Annotated reasoning path: Killing Me Softly with His Song (V) Predicted reasoning Path: Roberta Flack (I) ? Killing Me Softly with His Song (V)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
