<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geography-Aware Self-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Ayush</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Tanmay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iit</forename><surname>Kharagpur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geography-Aware Self-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning methods have significantly narrowed the gap between supervised and unsupervised learning on computer vision tasks. In this paper, we explore their application to geo-located datasets, e.g. remote sensing, where unlabeled data is often abundant but labeled data is scarce. We first show that due to their different characteristics, a non-trivial gap persists between contrastive and supervised learning on standard benchmarks. To close the gap, we propose novel training methods that exploit the spatio-temporal structure of remote sensing data. We leverage spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location to design pre-text tasks. Our experiments show that our proposed method closes the gap between contrastive and supervised learning on image classification, object detection and semantic segmentation for remote sensing. Moreover, we demonstrate that the proposed method can also be applied to geo-tagged ImageNet images, improving downstream performance on various tasks. Project Webpage can be found at this link geography-aware-ssl.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by the success of self-supervised learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>, we explore their application to large-scale remote sensing datasets (satellite images) and geo-tagged natural image datasets. It has been recently shown that selfsupervised learning methods perform comparably well or even better than their supervised learning counterpart on image classification, object detection, and semantic segmentation on traditional computer vision datasets <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>. However, their application to remote sensing images is largely unexplored, despite the fact that collecting and la-* Equal Contribution.</p><p>Contact: {kayush, buzkent, chen-lin}@cs.stanford.edu beling remote sensing images is particularly costly as annotations often require domain expertise <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In this direction, we first experimentally evaluate the performance of an existing self-supervised contrastive learning method, MoCo-v2 <ref type="bibr" target="#b12">[13]</ref>, on remote sensing datasets, finding a performance gap with supervised learning using labels. For instance, on the Functional Map of the World (fMoW) image classification benchmark <ref type="bibr" target="#b4">[5]</ref>, we observe an 8% gap in top 1 accuracy between supervised and self-supervised methods.</p><p>To bridge this gap, we propose geography-aware contrastive learning to leverage the spatio-temporal structure of remote sensing data. In contrast to typical computer vision images, remote sensing data are often geo-located and might provide multiple images of the same location over time. Contrastive methods encourage closeness of representations of images that are likely to be semantically similar (positive pairs). Unlike contrastive learning for traditional computer vision images where different views (augmentations) of the same image serve as a positive pair, we propose to use temporal positive pairs from spatially aligned images over time. Utilizing such information allows the representations to be invariant to subtle variations over time (e.g., due to seasonality). This can in turn result in more discriminative features for tasks focusing on spatial variation, such as object detection or semantic segmentation (but not necessarily for tasks involving temporal variation such as change detection). In addition, we design a novel unsupervised learning method that leverages geo-location information, i.e., knowledge about where the images were taken. More specifically, we consider the pretext task of predicting where in the world an image comes from, similar to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. This can complement the information-theoretic objectives typically used by self-supervised learning methods by encouraging representations that reflect geographical information, which is often useful in remote sensing tasks <ref type="bibr" target="#b32">[31]</ref>. Finally, we integrate the two proposed methods <ref type="figure">Figure 1</ref>: Left shows the original MoCo-v2 <ref type="bibr" target="#b2">[3]</ref> framework. Right shows the schematic overview of our approach.</p><p>into a single geography-aware contrastive learning objective.</p><p>Our experiments on the functional Map of the World [5] dataset consisting of high spatial resolution satellite images show that we improve MoCo-v2 baseline significantly. In particular, we can improve the accuracy on target applications utilizing image recognition <ref type="bibr" target="#b4">[5]</ref>, object detection <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b0">1]</ref>, and semantic segmentation <ref type="bibr" target="#b48">[46]</ref>. In particular, we improve it by ? 8% classification accuracy when testing the learned representations on image classification, ? 2% AP on object detection, ? 1% mIoU on semantic segmentation, and ? 3% top-1 accuracy on land cover classifica-tion?nterestingly, our geography-aware learning can even outperform the supervised learning counterpart on temporal data classification by ? 2%. To further demonstrate the effectiveness of our geography-aware learning approach, we extract the geo-location information of ImageNet images using FLICKR API similar to <ref type="bibr" target="#b6">[7]</ref>, which provides us with a subset of 543,435 geo-tagged ImageNet images. We extend the proposed approaches to geo-located ImageNet, and show that geography-aware learning can improve the performance of MoCo-v2 by ? 2% on image classification, showing the potential application of our approach to any geo-tagged dataset. <ref type="figure">Figure 1</ref> shows our contributions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised methods use unlabeled data to learn representations that are transferable to downstream tasks (e.g. image classification). Two commonly seen self-supervised methods are pre-text task and contrastive learning.</p><p>Pre-text tasks Pre-text task based learning <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b29">28]</ref> can be used to learn feature representations when data labels are not available. <ref type="bibr" target="#b8">[9]</ref> rotates an image and then trains a model to predict the rotation angle. <ref type="bibr" target="#b50">[48]</ref> trains a network to perform colorization of a grayscale image. <ref type="bibr" target="#b26">[26]</ref> represents an image as a grid, permuting the grid and then predicting the permutation index. In this study, we use geo-location classification as a pre-text task, in which a deep network is trained to predict a coarse geo-location of where in the world the image might come from.</p><p>Contrastive Learning Recent self-supervised contrastive learning approaches such as MoCo <ref type="bibr" target="#b12">[13]</ref>, MoCo-v2 <ref type="bibr" target="#b2">[3]</ref>, Sim-CLR <ref type="bibr" target="#b1">[2]</ref>, PIRL <ref type="bibr" target="#b22">[22]</ref>, and FixMatch <ref type="bibr" target="#b33">[32]</ref> have demonstrated superior performance and have emerged as the fore-runner on various downstream tasks. The intuition behind these methods are to learn representations by pulling positive image pairs from the same instance closer in latent space while pushing negative pairs from difference instances further away. These methods, on the other hand, differ in the type of contrastive loss, generation of positive and negative pairs, and sampling method.</p><p>Although growing rapidly in self-supervised learning area, contrastive learning methods have not been explored on large-scale remote sensing dataset. In this work, we provide a principled and effective approach for improving representation learning using MoCo-v2 <ref type="bibr" target="#b12">[13]</ref> for remote sensing data as well geo-located conventional datasets.</p><p>Unsupervised Learning in Remote Sensing Images Unlike in traditional computer vision areas, unsupervised learning on remote sensing domain has not been studied comprehensively. Most of the studies utilize small-scale datasets specific to a small geographical region <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">19]</ref>, a few classes <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b25">25]</ref> or a highly-specific modality, i.e. hyperspectral images <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b49">47]</ref>. Most of these studies focus on the UCM-21 dataset <ref type="bibr" target="#b47">[45]</ref> consisting of less than 1,000 images from 21 classes. A more recent study <ref type="bibr" target="#b37">[36]</ref> proposes large-scale weakly supervised learning using a multi-modal dataset consisting of satellite images and paired geo-located wikipedia articles. While being effective, this method requires each satellite image to be paired to its corresponding article, limiting the number of images that can be used.</p><p>Geography-aware Computer Vision Geo-location data has been studied extensively in prior works. Most of these "gsd": "img_width": "img_height": "country_code": "cloud_cover": "timestamp":    <ref type="figure">Figure 3</ref>: Some examples from GeoImageNet dataset. Below each image, we list their latitudes, longitudes, city, country name. In our study, we use the latitude and longitude information for unsupervised learning. We recommend readers to zoom-in to visualize the details of the pictures. studies utilizes geo-location of an image as a prior to improve image recognition accuracy <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">6]</ref>. Other studies <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">42]</ref> use geo-tagged training datasets to learn how to predict the geo-location of previously unseen images at test time. In our study, we leverage geo-tag information to improve unsupervised and self-supervised learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition</head><p>We consider a geo-tagged visual dataset</p><formula xml:id="formula_0">{((x 1 i , ? ? ? , x Ti i ), lat i , lon i )} N i=1 ,</formula><p>where the ith data-point consists of a sequence of images X i = (x 1 i , ? ? ? , x Ti i ) at a shared location, with latitude and longitude equal to lat i , lon i respectively, over time t i = 1, ..., T i . When T i &gt; 1, we refer to the dataset to have temporal information or structure. Although temporal information is often not available in natural image datasets (e.g. ImageNet), it is common in remote sensing. While the temporal structure is similar to that of conventional videos, there are some key differences that we exploit in this work. First, we consider relatively short temporal sequences, where the time difference between two consecutive "frames" could range from months to years. Additionally unlike conventional videos we consider datasets where there is no viewpoint change across the image sequence.</p><p>Given our setup, we want to obtain visual representations z ti i of images x ti i such that the learned representation can be transferred to various downstream tasks. We do not assume access to any labels or human supervision beyond the lat i , lon i geo-tags. The quality of the representations is measured by their performance on various downstream tasks. Our primary goal is to improve the performance of self-supervised learning by utilizing the geo-coordinates and the unique temporal structure of remote sensing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Functional Map of the World</head><p>Functional Map of the World (fMoW) is a large-scale publicly available remote sensing dataset <ref type="bibr" target="#b4">[5]</ref> consisting of approximately 363,571 training images and 53,041 test images across 62 highly granular class categories. It provides images (temporal views) from the same location over time (x 1 i , ? ? ? , x Ti i ) as well as geo-location metadata (lat i , lon i ) for each image. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the histogram of the number of temporal views in fMoW dataset. We can see that most of the areas have multiple temporal views where T i can range from 1 to 21, and on average there is about 2.5-3 years of difference between the images from an area. Also, we show examples of spatially aligned images in <ref type="figure" target="#fig_0">Fig. 2</ref>. As seen in <ref type="figure" target="#fig_3">Fig. 5</ref>, fMoW is a global dataset consisting of images from seven continents which can be ideal for learning global remote sensing representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GeoImageNet</head><p>Following <ref type="bibr" target="#b6">[7]</ref>, we extract geo-coordinates for a subset of images in ImageNet <ref type="bibr" target="#b7">[8]</ref> using the FLICKR API. More specifically, we searched for geo-tagged images in ImageNet using the FLICKR API, and were able to find 543,435 images with their associated coordinates (lat i , lon i ) across 5150 class categories. This dataset is more challenging than ImageNet-1k as it is highly imbalanced and contains about 5? more classes. In the rest of the paper, we refer to this geo-tagged subset of ImageNet as GeoImageNet.</p><p>We show some examples from GeoImageNet in <ref type="figure">Fig. 3</ref>. As shown in the figure, for some images we have geocoordinates that can be predicted from visual cues. For example, we see that a picture of a person with a Sombrero hat was captured in Mexico. Similarly, an Indian Elephant picture was captured in India, where there is a large population of Indian Elephants. Next to it, we show the picture of an African Elephant (which is larger in size). If a model is trained to predict where in the world the image was taken, it should be able to identify visual cues that are transferable to other tasks (e.g., visual cues to differentiate Indian Elephants from the African counterparts). <ref type="figure" target="#fig_3">Figure 5</ref> shows the distribution of images in the GeoImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we briefly review contrastive loss functions for unsupervised learning and detail our proposed approach to improve Moco-v2 <ref type="bibr" target="#b2">[3]</ref>, a recent contrastive learning framework, on geo-located data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Contrastive Learning Framework</head><p>Contrastive <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b28">27]</ref> methods attempt to learn a mapping f q : x t i ? z t i ? R d from raw pixels x t i to semantically meaningful representations z t i in an unsuper- vised way. The training objective encourages representations corresponding to pairs of images that are known a priori to be semantically similar (positive pairs) to be closer to each other than typical unrelated pairs (negative pairs). With similarity measured by dot product, recent approaches in contrastive learning differ in the type of contrastive loss and generation of positive and negative pairs. In this work, we focus on the state-of-the-art contrastive learning framework MoCo-v2 <ref type="bibr" target="#b2">[3]</ref>, an improved version of MoCo <ref type="bibr" target="#b12">[13]</ref>, and study improved methods for the construction of positive and negative pairs tailored to remote sensing applications. The contrastive loss function used in the MoCo-v2 framework is InfoNCE <ref type="bibr" target="#b28">[27]</ref>, which is defined as follows for a given data sample:</p><formula xml:id="formula_1">L z = ? log exp(z ??/?) exp(z ??/?) + N j=1 exp(z ? k j /?) ,<label>(1)</label></formula><p>where z and? are query and key representations obtained by passing the two augmented views of x t i (denoted v and v in <ref type="figure">Fig. 1</ref>) through query and key encoders, f q and f k parameterized by ? q and ? k respectively. Here z and? form a positive pair. The N negative samples, {k j } N j=1 , come from a dictionary of representations built as a queue. We refer readers to <ref type="bibr" target="#b12">[13]</ref> for details on this. ? ? R + is the temperature hyperparameter.</p><p>The key idea here is to encourage representations of positive (semantically similar) pairs to be closer, and negative (semantically unrelated) pairs to be far apart as measured by dot product. The construction of positive and negative pairs plays a crucial role in this contrastive learning framework. MoCo and MoCo-v2 both use perturbations (also called "data augmentation") from the same image to create a positive example and perturbations from different images to create a negative example. Commonly used perturbations include random color jittering, random horizontal flip, and random grayscale conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2016-04-17T15:49:27Z</head><p>2012-11-21T15:17:29Z 2016-11-10T16:00:51Z 2016-11-10T16:00:51Z 2011-06-06T15:56:51Z <ref type="figure">Figure 6</ref>: Demonstration of temporal positives in eq. 2. An image from an area is paired to the other images including itself from the same area captured at different time. We show the time stamps for each image underneath the images. We can see the color changes in the stadium seatings and surrounding areas.</p><p>Temporal Positive Pairs Different from many commonly seen natural image datasets, remote sensing datasets often have extra temporal information, meaning that for a given location (lat i , lon i ), there exists a sequence of spatially aligned images X i = (x 1 i , ? ? ? , x Ti i ) over time. Unlike in traditional videos where nearby frames could experience large changes in content (e.g. from a cat to a tree), in remote sensing the content is often more stable across time due to the fixed viewpoint. For instance, a place on ocean is likely to remain as ocean for months or years, in which case satellite images taken across time at the same location should share high semantic similarities. Even for locations where non-trivial changes do occur over time, certain semantic similarities could still remain. For instance, key features of a construction site are likely to remain the same even as the appearance changes due to seasonality.</p><p>Given these observations, it is natural to leverage temporal information for remote sensing while constructing positive or negative pairs since it can provide us with extra semantically meaningful information of a place over time. More specifically, given an image x t1 i collected at time t 1 , we can randomly select another image x t2 i that is spatially aligned with x t1 i (i.e. x t2 i ? X i ). We then apply perturbations (e.g. random color jittering) as used in MoCo-v2 to the spatially aligned image pair x t1 i and x t2 i , providing us with a temporal positive pair (denoted v and v in <ref type="figure">Figure 1</ref>) that can be used for training the contrastive learning framework by passing them through query and key encoders, f q and f k respectively (see <ref type="figure">Fig. 1</ref>). Note that when t 1 = t 2 , the temporal positive pair is the same as the positive pair used in MoCo-v2.</p><p>Given a data sample x t1 i , our TemporalInfoNCE objective function can be formulated as follows:</p><formula xml:id="formula_2">L z t 1 i = ? log exp(z t 1 i ? z t 2 i /?) exp(z t 1 i ? z t 2 i /?) + N j=1 exp(z t 1 i ? kj/?) ,<label>(2)</label></formula><p>where z t1 i and z t2 i are the encoded representations of the randomly perturbed temporal positive pair x t1 i , x t2 i . Similar as equation 1, N is number of negative samples, {k j } N j=1 are the encoded negative pairs and ? ? R + is the temperature hyperparameter. Again, we refer readers to <ref type="bibr" target="#b12">[13]</ref> for details on construction of these negative pairs.</p><p>Note that compared to equation 1, we use two real images from the same area over time to create positive pairs. We believe that relying on real images for positive pairs encourages the network to learn better representations for real data than the one relying on synthetic images. On the other hand, our objective in equation 2 enforces the representations to be invariant to changes over time. Depending on the target task, such inductive bias can be desirable or undesirable. For example, for a change detection task, learning representations that are highly sensitive to temporal changes can be more preferable. However, for image classification or object detection task, learning temporally invariant features should not degrade the downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Geo-location Classification as a Pre-text Task</head><p>In this section, we explore another aspect of remote sensing images, geolocation metadata, to further improve the quality of the learned representations. In this direction, we design a pre-text task for unsupervised learning. In our pre-text task, we cluster the images in the dataset using their coordinates (lat i , lon i ). We use a clustering method to construct K clusters and assign an area with coordinates (lat i , lon i ) a categorical geo-label c i ? C = {1, ? ? ? , K}. Using the cross entropy loss function, we then optimize a geo-location predictor network f c as</p><formula xml:id="formula_3">L g = K k=1 ?p(c i = k) log(p(c i = k|f c (x t i )),<label>(3)</label></formula><p>where p represent the probability of the cluster k representing the true cluster andp represents the predicted probabilities for K clusters. In our experiments, we represent f c with a CNN parameterized by ? c . With this objective, our goal is to learn location-aware representations that can potentially transfer well to different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combining Geo-location and Contrastive Learning Losses</head><p>In the previous section, we designed a pre-text task leveraging the geo-location meta-data of the images to learn location-aware representations in a standalone setting. In this section, we combine geo-location prediction and contrastive learning tasks in a single objective to improve the contrastive learning-only and geo-location learning-only tasks. In this direction, we first integrate the geo-location learning task into the contrastive learning framework using the cross-entropy loss function where the geo-location prediction network uses features z t i from the query encoder as</p><formula xml:id="formula_4">L g = ? K i=1 p(c i = k) log(p(c i = k|f c (z t i )).<label>(4)</label></formula><p>In the combined framework (see <ref type="figure">Fig. 1</ref>), f c is represented by a linear layer parameterized by ? c . Finally, we propose the objective for joint learning as the linear combination of TemporalInfoNCE and geo-classification loss with coefficients ? and ? representing the importance of contrastive learning and geo-location learning losses as arg min</p><formula xml:id="formula_5">?q,? k ,?c L f = ?L z t 1 + ?L g .<label>(5)</label></formula><p>By combining two tasks, we learn representations to jointly maximize agreement between spatio-temporal positive pairs, minimize agreement between negative pairs and predict the geo-label of the images from the positive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this study, we perform unsupervised representation learning on fMoW and GeoImageNet datasets. We then evaluate the learned representations/pre-trained models on a variety of downstream tasks including image recognition, object detection and semantic segmentation benchmarks on remote sensing and conventional images. Implementation Details for Unsupervised Learning For contrastive learning, similar to MoCo-v2 <ref type="bibr" target="#b2">[3]</ref>, we use ResNet-50 to paramaterize the query and key encoders, f q and f k , in all experiments. We use following hyperparameters in the MoCo-v2 pre-training step: learning rate of 1e-3, batch size of 256, dictionary queue of size 65536, temperature scaling of 0.2 and SGD optimizer. We use similar setups for both fMoW and GeoImageNet datasets. Finally, for each downstream experiment, we report results for the representations learned after 200 epochs.</p><p>For geo-location classification task, we run k-Means clustering algorithm to cluster fMoW and GeoImageNet into K = 100 geo-clusters given their latitude and longitude pairs. We show the clusters in <ref type="figure" target="#fig_5">Fig. 8</ref>. As seen in the figure, while both datasets have similar clusters there are some differences, particularly in North America and Europe. In <ref type="figure" target="#fig_4">Fig. 7</ref> we analyze the clusters in GeoImageNet and fMoW. The figure shows that the number of clusters per class on GeoImageNet tend to be skewed towards smaller numbers than fMoW whereas the number of unique classes per cluster on GeoImageNet has more of a uniform distribution. For fMoW, we can conclude that each cluster contain samples from most of the classes. Finally, when adding the geolocation classification task into the contrastive learning we tune ? and ? to be 1.</p><p>Methods We compare our unsupervised learning approach to supervised learning for image recognition task. For object detection, and semantic segmentation we compare them to pre-trained weights obtained using (a) supervised learning, and (b) random initilization while fine-tuning on the target task dataset. Finally, for ablation analysis we provide results using different combinations of our methods. When appending only geo-location classification task or temporal positives into MoCo-v2 we use MoCo-v2+Geo and MoCo-v2+TP. When adding both of our approaches into MoCo-v2 we use MoCo-v2+Geo+TP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on fMoW</head><p>We first perform experiments on fMoW image recognition task. Similar to the common protocol of evaluating unsupervised pre-training methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>, we freeze the features and train a supervised linear classifier. However, in practice, it is more common to finetune the features end-toend in a downstream task. For completeness and a better comparison, we report end-to-end finetuning results for the 62-class fMoW classification as well. We report both top-1 accuracy and F1-scores for this task. Frozen corresponds to linear classification on frozen features. Finetune corresponds to end-to-end finetuning results for the fmow classification.</p><p>Classifying Single Images In <ref type="table">Table 1</ref>, we report the results on single image classification on fMoW. We would like to highlight that in this case we classify each image individually. In other words, we do not use the prior information that multiple images over the same area (x 1 i , x 2 i , . . . , x Ti i ) have the same labels (y i , y i , . . . , y i ). For evaluation, we use 53,041 images. Our results on this task (linear classification on frozen features) show that MoCo-v2 performs reasonably well on a large-scale dataset with 60.69% accuracy, 8% less than the supervised learning methods. Sup. Learning (IN wts. init.) and Sup. Learning (Scratch) correspond to supervised learning method starting from imagenet pre-trained weights and random weights respectively. This result aligns with MoCo-v2's performance on the Ima-geNet dataset <ref type="bibr" target="#b2">[3]</ref>. Next, by incorporating geo-location classification task into MoCo-v2, we improve by 3.38% in top-1 classification accuracy. We further improve the results to 68.32% using temporal positives, bridging the gap between the MoCo-v2 baseline and supervised learning to less than 1%. However, when we perform end-to-end finetuning for the classification task, we observe that our method surpasses the supervised learning methods by more than 2%. For completeness, we also include results for MoCo-v2 pre-trained on Imagenet dataset (4th row in <ref type="table">Table 1</ref>) and find that the distribution shift between Imagenet and downstream dataset leads to suboptimal performance.</p><p>Classifying Temporal Data In the next step, we change how we perform testing across multiple images over an area at different times. In this case, we predict labels from images over an area i.e. make a prediction for each t ? {1, . . . , T i }, and average the predictions from that area. We then use the most confident class prediction to get areaspecific class predictions. In this case, we evaluate the performance on 11,231 unique areas that are represented by multiple images at different times. Our results in <ref type="table" target="#tab_3">Table 2</ref> show that doing area-specific inference improves the classification accuracies by 4-8% over image-specific inference. Even incorporating temporal positives, we can improve the accuracy by 6.1% by switching from image classification to temporal data classification. Overall, our methods outperform the baseline Moco-v2 by 4-6% and supervised learning by 1-2%. Here we only report temporal classification on top of frozen features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Transfer Learning Experiments</head><p>Previously, we performed pre-training experiments on fMoW dataset and quantified the quality of the representations by supervised training a linear layer for image recognition on fMoW. In this section, we perform transfer learning experiments on different low level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Object Detection</head><p>For object detection, we use the xView dataset <ref type="bibr" target="#b15">[16]</ref> consisting of high resolution satellite images captured with similar sensors to the ones in the fMoW dataset. The xView dataset  Implementation Details We first divide the set of large images into 700 training and 146 test images. Then, we process the large images to create 416?416 pixels images by randomly sampling the bounding box coordinates of the small image and we repeat this process 100 times for each large image. In this process, we ensure that there is less than 25% overlap between any two bounding boxes from the same image. We then use RetinaNet <ref type="bibr" target="#b18">[18]</ref> with pre-trained ResNet-50 backbone and fine-tune the full network on the xView training set. To train RetinaNet, we use learning rate of 1e-5 and a batch size of 4 and Adam optimizer. <ref type="table" target="#tab_5">Table 3</ref> shows the object detection performance on the xView test set. We achieve the best results with the addition of temporal positive pair, and geolocation classification pre-text task into MoCo-v2. With our final model, we can outperform the randomly initialized weights by 7% AP and the supervised learning on the fMoW by 3.3% AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Image Segmentation</head><p>In this section, we perform downstream experiments on the task of Semantic Segmentation on SpaceNet dataset <ref type="bibr" target="#b41">[40]</ref>. The SpaceNet datasets consists of 5000 high resolution satellite images with segmentation masks for buildings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We divide our SpaceNet dataset into training and test sets of 4000 and 1000 images respectively. We use PSAnet <ref type="bibr" target="#b52">[50]</ref> network with ResNet-50 backbone to perform semantic segmentation. We train PSAnet network with a batch size of 16 and a learning rate of 0.01 for 100 epochs and use SGD optimizer.</p><p>Qualitative Analysis <ref type="table" target="#tab_7">Table 4</ref> shows the segmentation performance of differently initialized backbone weights on the SpaceNet test set. Similar to object detection, we achieve the best IoU scores with the addition of temporal positives and geo-location classification task. Our final model outperforms the randomly initialized weights and supervised learning by 3.58% and 2.94% IoU scores. We observe that the gap between the best and worst models shrinks going from the image recognition to object detection, and semantic segmentation task. This aligns with the performance of the MoCo-v2 pre-trained on ImageNet and fine-tuned on the Pascal-VOC object detection and semantic segmentation experiments <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref>.   <ref type="table">Table 5</ref>: Land Cover Classification on NAIP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Land Cover Classification</head><p>Finally, we perform transfer learning experiments on land cover classification across 66 land cover classes using high resolution remote sensing images obtained by the USDA's National Agricultural Imagery Program (NAIP). We use the images from the California's Central Valley for the year of 2016. Our final dataset consists of 100,000 training and 50,000 test images. <ref type="table">Table 5</ref> shows that our method outperforms the randomly initialized weights by 6.34% and supervised learning by 3.77%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments on GeoImageNet</head><p>After fMoW, we adopt our methods for unsupervised learning on fMoW for improving representation learning on the GeoImageNet. Unfortunately, since ImageNet does not contain images from the same area over time we are not able to integrate the temporal positive pairs into the MoCo-v2 objective. However, in our GeoImageNet experiments we show that we can improve MoCo-v2 by introducing geolocation classification pre-text task. <ref type="table" target="#tab_9">Table 6</ref> shows the top-1 and top-5 classification accuracy scores on the test set of GeoIma-geNet. Surprisingly, with only geo-location classification task we can achieve 22.26% top-1 accuracy. With MoCo-v2 baseline, we get 38.51 accuracy, about 3.47% more than supervised learning method. With the addition of geo-location classification, we can further improve the top-1 accuracy by 1.45%. These results are interesting in a way that MoCo-v2 (200 epochs) on ImageNet-1k performs 8% worse than supervised learning whereas it outperforms supervised learning on our highly imbalanced GeoImageNet with 5150 class categories which is about 5? more than ImageNet-1k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Top-1 (Accuracy) ?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we provide a self-supervised learning framework for remote sensing data, where unlabeled data is often plentiful but labeled data is scarce. By leveraging spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location in the design of pre-text tasks, we are able to close the gap between self-supervised and supervised learning on image classification, object detection and semantic segmentation on remote sensing and other geo-tagged image datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Images over time concept in the fMoW dataset. The metadata associated with each image is shown underneath. We can see changes in contrast, brightness, cloud cover etc. in the images. These changes render spatially aligned images over time useful for constructing additional positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Left The histogram of number of views. Right the histogram of standard deviation in years per area in fMoW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Top shows the distribution of the fMoW and Bottom shows the distribution of GeoImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Left shows the number of clusters per label and Right shows the number of unique labels per cluster in fMoW and GeoImageNet. Labels represent the original classes in fMoW and GeoImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Top and Bottom show the distributions of the fMoW and GeoImageNet clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experiments on fMoW on classifying temporal data. In the table, we compare the results to the ones on single image classification. Here we present results corresponding to linear classification on frozen features only.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Object detection results on the xView dataset.</figDesc><table><row><cell>consists of 846 very large (?2000?2000 pixels) satellite</cell></row><row><cell>images with bounding box annotations for 60 different class</cell></row><row><cell>categories including airplane, passenger vehicle, maritime</cell></row><row><cell>vessel, helicopter etc.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Semantic segmentation results on Space-Net.</figDesc><table><row><cell>pre-train</cell><cell>Top-1 Accuracy ?</cell></row><row><cell>Random Init.</cell><cell>51.89</cell></row><row><cell>Imagenet Init.</cell><cell>53.46</cell></row><row><cell>Sup. Learning (IN wts. init.)</cell><cell>54.67</cell></row><row><cell>Sup. Learning (Scratch)</cell><cell>54.46</cell></row><row><cell>MoCo-V2</cell><cell>55.18 (+3.29)</cell></row><row><cell>MoCo-V2-Geo</cell><cell>58.23 (+6.34)</cell></row><row><cell>MoCo-V2-TP</cell><cell>57.10 (+5.21)</cell></row><row><cell>MoCo-V2-Geo+TP</cell><cell>57.63 (+5.74)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Experiments on GeoImageNet. We divide the dataset into 443,435 training and 100,000 test images across 5150 classes. We train MoCo-V2 and MoCo-V2+Geo for 200 epochs whereas Sup. and Geoloc. Learning are trained until they converge.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Geography-Aware Self-Supervised Learning Kumar Ayush * Stanford University Burak Uzkent * Stanford University Chenlin Meng * Stanford University Kumar Tanmay IIT Kharagpur Marshall Burke Stanford University</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via 2021-2011000004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p><p>This research was also supported by Stanford Data for Development Initiative, HAI, IARPA SMART, ONR (N00014-19-1-2145), and NSF grants #1651565 and #1733686.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generating interpretable poverty maps using object detection in satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Kumar Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01612</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheriyadat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="439" to="451" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Functional map of the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Fendley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6172" to="6180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ieee conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal location estimation of videos and images</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Density estimation for geolocation via convolutional mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayate</forename><surname>Iso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoko</forename><surname>Wakamiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Aramaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02750</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tile2vec: Unsupervised representation learning for spatially distributed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherrie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Samar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Azzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3967" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kuzma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Klaric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Bulatov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mccord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xview</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07856</idno>
		<title level="m">Objects in context in overhead imagery</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised multilayer feature learning for satellite image scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="161" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification by unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5148" to="5157" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Presenceonly geographical priors for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9596" to="9606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised spectral-spatial feature learning via deep residual conv-deconv network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedram</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geolocation estimation of photos using a hierarchical model and scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Muller-Budack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kader</forename><surname>Pustu-Iren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Ewerth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wesam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised deep feature extraction for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1349" to="1362" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting economic development using geolocated wikipedia articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2698" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving image classification with location context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1008" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking in aerial hyperspectral videos using deep kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneesh</forename><surname>Rangnekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="449" to="461" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02506</idno>
		<title level="m">Learning to interpret satellite images in global scale using wikipedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to interpret satellite images using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to interpret satellite images using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3620" to="3626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient object detection in large images using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1824" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Van Etten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Bacastow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01232</idno>
		<title level="m">Spacenet: A remote sensing dataset and challenge series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting im2gps in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Planetphoto geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="37" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems</title>
		<meeting>the 18th SIGSPATIAL international conference on advances in geographic information systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic annotation of high-resolution satellite images via weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3660" to="3671" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hyperspectral image unsupervised classification by robust manifold matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">485</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="169" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
